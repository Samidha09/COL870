
>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/default', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.07 | rmse_val: 3.3741
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.06 | rmse_val: 2.9943
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.07 | rmse_val: 1.7410
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.07 | rmse_val: 1.1327
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.06 | rmse_val: 0.9800
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.06 | rmse_val: 1.0872
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.06 | rmse_val: 1.1652
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.07 | rmse_val: 1.1518
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.08 | rmse_val: 1.2344
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.07 | rmse_val: 1.3389
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.07 | rmse_val: 1.3391
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.06 | rmse_val: 0.9978
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.06 | rmse_val: 1.2773
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.08 | rmse_val: 1.1648
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.08 | rmse_val: 1.2310
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.06 | rmse_val: 1.4026
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.10 | rmse_val: 1.1990
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.06 | rmse_val: 1.2226
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.06 | rmse_val: 1.1331
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.06 | rmse_val: 1.1435
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.06 | rmse_val: 1.1230
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.06 | rmse_val: 1.7135
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.06 | rmse_val: 1.7576
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.07 | rmse_val: 1.1399
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.07 | rmse_val: 1.2766
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.07 | rmse_val: 1.1888
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.07 | rmse_val: 1.6370
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.07 | rmse_val: 1.7470
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.06 | rmse_val: 1.6875
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.06 | rmse_val: 1.5689
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.07 | rmse_val: 1.0698
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.08 | rmse_val: 1.0978
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.06 | rmse_val: 1.2972
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.07 | rmse_val: 1.1596
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.07 | rmse_val: 1.8801
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.08 | rmse_val: 1.0040
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.09 | rmse_val: 1.1605
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.08 | rmse_val: 1.1127
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.07 | rmse_val: 1.3111
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.08 | rmse_val: 1.0541
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.07 | rmse_val: 0.9032
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.06 | rmse_val: 1.1433
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.05 | rmse_val: 1.2399
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.07 | rmse_val: 0.9962
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.08 | rmse_val: 1.1889
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.08 | rmse_val: 1.0389
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.06 | rmse_val: 1.1820
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.06 | rmse_val: 1.0117
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.07 | rmse_val: 1.2784
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.05 | rmse_val: 1.0770
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.07 | rmse_val: 1.0232
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.07 | rmse_val: 1.1284
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.07 | rmse_val: 1.1607
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.05 | rmse_val: 1.0688
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.06 | rmse_val: 1.1700
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.06 | rmse_val: 1.1267
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.07 | rmse_val: 1.1829
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.05 | rmse_val: 0.9879
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.06 | rmse_val: 0.9795
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.06 | rmse_val: 1.1594
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.06 | rmse_val: 0.8617
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.07 | rmse_val: 1.0771
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.06 | rmse_val: 0.9418
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.06 | rmse_val: 1.1806
Optimization Finished!
Train cost: 679.8575s
Loading 17th epoch
f1_test: 0.06 | rmse_test: 1.1806

>>> run.py: Namespace(dataset='LINUX', device=1, experiment='default', log_path='log/nagphormer/LINUX/default', path='data/nagphormer', plot_path='plots/nagphormer/LINUX/default', pyg_path='data/pyg/LINUX')

>>> Training split = 1.0

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/default', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.05 | rmse_val: 7.4451
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 7.4450
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.08 | rmse_val: 6.8450
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.03 | rmse_val: 6.0641
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.05 | rmse_val: 4.4422
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.06 | rmse_val: 1.9609
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.06 | rmse_val: 1.4814
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.06 | rmse_val: 2.2505
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.07 | rmse_val: 1.4667
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.07 | rmse_val: 1.3041
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.6533
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.06 | rmse_val: 1.0961
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.06 | rmse_val: 1.5658
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.05 | rmse_val: 1.5932
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.05 | rmse_val: 1.1988
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.04 | rmse_val: 1.2835
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.05 | rmse_val: 1.7141
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.06 | rmse_val: 1.2771
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.05 | rmse_val: 0.9793
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.08 | rmse_val: 1.8279
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.07 | rmse_val: 1.3268
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.04 | rmse_val: 1.8338
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.06 | rmse_val: 1.1240
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.03 | rmse_val: 1.8121
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.07 | rmse_val: 1.6883
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.09 | rmse_val: 0.7529
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.07 | rmse_val: 1.7207
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.07 | rmse_val: 1.3494
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.04 | rmse_val: 0.8378
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.07 | rmse_val: 1.1355
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.06 | rmse_val: 0.8232
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.06 | rmse_val: 1.9919
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.07 | rmse_val: 1.4889
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.05 | rmse_val: 0.7824
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.05 | rmse_val: 2.1047
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.06 | rmse_val: 1.1257
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.04 | rmse_val: 1.1966
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.07 | rmse_val: 1.5883
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.06 | rmse_val: 2.1858
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.04 | rmse_val: 1.4801
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.07 | rmse_val: 0.8435
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.06 | rmse_val: 1.0060
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.06 | rmse_val: 1.1348
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.07 | rmse_val: 2.5679
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.07 | rmse_val: 2.5789
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.04 | rmse_val: 1.4546
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.08 | rmse_val: 1.9236
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.06 | rmse_val: 1.3427
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.08 | rmse_val: 1.5619
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.06 | rmse_val: 1.4774
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.05 | rmse_val: 1.6202
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.05 | rmse_val: 1.0127
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.05 | rmse_val: 1.4628
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.05 | rmse_val: 1.4804
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.08 | rmse_val: 1.2230
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.06 | rmse_val: 1.5972
Optimization Finished!
Train cost: 306.7713s
Loading 26th epoch
f1_test: 0.06 | rmse_test: 1.5972

>>> run.py: Namespace(dataset='AIDS700nef', device=1, experiment='default', log_path='log/nagphormer/AIDS700nef/default', path='data/nagphormer', plot_path='plots/nagphormer/AIDS700nef/default', pyg_path='data/pyg/AIDS700nef')

>>> Training split = 1.0

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/one_hot', n_heads=8, n_layers=1, name=None, one_hot=True, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=11, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 151106
Epoch: 0001 | loss_train: 20.5374 loss_val: 19.6294 | f1_val: 0.06 | rmse_val: 3.4429
Epoch: 0002 | loss_train: 18.7703 loss_val: 14.4914 | f1_val: 0.05 | rmse_val: 2.9110
Epoch: 0003 | loss_train: 12.7951 loss_val: 10.2197 | f1_val: 0.06 | rmse_val: 1.4466
Epoch: 0004 | loss_train: 5.4022 loss_val: 4.2923 | f1_val: 0.06 | rmse_val: 1.3588
Epoch: 0005 | loss_train: 4.3597 loss_val: 4.0324 | f1_val: 0.06 | rmse_val: 1.1771
Epoch: 0006 | loss_train: 4.3655 loss_val: 4.0574 | f1_val: 0.06 | rmse_val: 0.9961
Epoch: 0007 | loss_train: 4.3612 loss_val: 4.0445 | f1_val: 0.07 | rmse_val: 1.1110
Epoch: 0008 | loss_train: 3.7878 loss_val: 4.8962 | f1_val: 0.09 | rmse_val: 0.9924
Epoch: 0009 | loss_train: 4.2643 loss_val: 5.8357 | f1_val: 0.07 | rmse_val: 1.1923
Epoch: 0010 | loss_train: 4.1227 loss_val: 5.2732 | f1_val: 0.06 | rmse_val: 1.2662
Epoch: 0011 | loss_train: 4.5179 loss_val: 4.4285 | f1_val: 0.07 | rmse_val: 1.3295
Epoch: 0012 | loss_train: 4.2861 loss_val: 3.8738 | f1_val: 0.05 | rmse_val: 0.9471
Epoch: 0013 | loss_train: 4.3897 loss_val: 4.2638 | f1_val: 0.07 | rmse_val: 1.1875
Epoch: 0014 | loss_train: 4.4361 loss_val: 4.5081 | f1_val: 0.07 | rmse_val: 1.2108
Epoch: 0015 | loss_train: 4.3117 loss_val: 3.9884 | f1_val: 0.07 | rmse_val: 1.1715
Epoch: 0016 | loss_train: 4.3236 loss_val: 5.3236 | f1_val: 0.07 | rmse_val: 1.4005
Epoch: 0017 | loss_train: 4.5705 loss_val: 4.0040 | f1_val: 0.07 | rmse_val: 1.2227
Epoch: 0018 | loss_train: 4.9052 loss_val: 5.3511 | f1_val: 0.09 | rmse_val: 1.1623
Epoch: 0019 | loss_train: 4.4132 loss_val: 4.7322 | f1_val: 0.07 | rmse_val: 0.9822
Epoch: 0020 | loss_train: 4.3202 loss_val: 4.2643 | f1_val: 0.07 | rmse_val: 1.1196
Epoch: 0021 | loss_train: 4.0959 loss_val: 5.2066 | f1_val: 0.06 | rmse_val: 0.7452
Epoch: 0022 | loss_train: 4.2872 loss_val: 6.3435 | f1_val: 0.06 | rmse_val: 1.4476
Epoch: 0023 | loss_train: 3.5361 loss_val: 4.6833 | f1_val: 0.06 | rmse_val: 1.1632
Epoch: 0024 | loss_train: 4.3930 loss_val: 3.9477 | f1_val: 0.06 | rmse_val: 0.8693
Epoch: 0025 | loss_train: 3.5849 loss_val: 5.4617 | f1_val: 0.05 | rmse_val: 0.9953
Epoch: 0026 | loss_train: 3.4656 loss_val: 4.0707 | f1_val: 0.07 | rmse_val: 0.8745
Epoch: 0027 | loss_train: 4.2395 loss_val: 4.2913 | f1_val: 0.06 | rmse_val: 1.4264
Epoch: 0028 | loss_train: 4.0718 loss_val: 5.4942 | f1_val: 0.06 | rmse_val: 1.3362
Epoch: 0029 | loss_train: 3.8981 loss_val: 3.4584 | f1_val: 0.05 | rmse_val: 1.3986
Epoch: 0030 | loss_train: 4.3673 loss_val: 3.5853 | f1_val: 0.07 | rmse_val: 1.7343
Epoch: 0031 | loss_train: 4.4374 loss_val: 4.6953 | f1_val: 0.07 | rmse_val: 1.1513
Epoch: 0032 | loss_train: 3.7712 loss_val: 4.2581 | f1_val: 0.05 | rmse_val: 0.9622
Epoch: 0033 | loss_train: 4.0482 loss_val: 3.8465 | f1_val: 0.05 | rmse_val: 1.2015
Epoch: 0034 | loss_train: 3.8955 loss_val: 3.6366 | f1_val: 0.05 | rmse_val: 1.1579
Epoch: 0035 | loss_train: 4.0700 loss_val: 5.2920 | f1_val: 0.06 | rmse_val: 1.9846
Epoch: 0036 | loss_train: 4.6618 loss_val: 3.5729 | f1_val: 0.06 | rmse_val: 1.1630
Epoch: 0037 | loss_train: 4.2922 loss_val: 3.8901 | f1_val: 0.07 | rmse_val: 1.0945
Epoch: 0038 | loss_train: 4.2386 loss_val: 4.0306 | f1_val: 0.08 | rmse_val: 0.7742
Epoch: 0039 | loss_train: 3.7183 loss_val: 3.9069 | f1_val: 0.06 | rmse_val: 1.1043
Epoch: 0040 | loss_train: 4.0949 loss_val: 3.6160 | f1_val: 0.05 | rmse_val: 0.7746
Epoch: 0041 | loss_train: 4.0672 loss_val: 4.5915 | f1_val: 0.05 | rmse_val: 0.7777
Epoch: 0042 | loss_train: 3.8102 loss_val: 3.6631 | f1_val: 0.06 | rmse_val: 0.7337
Epoch: 0043 | loss_train: 3.6961 loss_val: 4.5228 | f1_val: 0.05 | rmse_val: 1.0377
Epoch: 0044 | loss_train: 3.8969 loss_val: 3.2133 | f1_val: 0.04 | rmse_val: 0.9354
Epoch: 0045 | loss_train: 4.0664 loss_val: 4.0245 | f1_val: 0.07 | rmse_val: 0.9875
Epoch: 0046 | loss_train: 4.3798 loss_val: 3.6776 | f1_val: 0.06 | rmse_val: 0.7214
Epoch: 0047 | loss_train: 3.6438 loss_val: 5.8708 | f1_val: 0.06 | rmse_val: 0.9380
Epoch: 0048 | loss_train: 3.6976 loss_val: 4.7198 | f1_val: 0.07 | rmse_val: 0.7775
Epoch: 0049 | loss_train: 3.7060 loss_val: 3.6673 | f1_val: 0.05 | rmse_val: 1.1015
Epoch: 0050 | loss_train: 3.9183 loss_val: 4.7001 | f1_val: 0.05 | rmse_val: 0.8732
Epoch: 0051 | loss_train: 3.6899 loss_val: 3.8530 | f1_val: 0.05 | rmse_val: 0.7408
Epoch: 0052 | loss_train: 3.5090 loss_val: 3.0619 | f1_val: 0.06 | rmse_val: 0.7589
Epoch: 0053 | loss_train: 3.5423 loss_val: 4.6605 | f1_val: 0.06 | rmse_val: 0.9551
Epoch: 0054 | loss_train: 4.2327 loss_val: 4.2280 | f1_val: 0.06 | rmse_val: 1.0007
Epoch: 0055 | loss_train: 4.1543 loss_val: 4.3537 | f1_val: 0.05 | rmse_val: 0.7590
Epoch: 0056 | loss_train: 3.4666 loss_val: 3.7721 | f1_val: 0.06 | rmse_val: 0.6503
Epoch: 0057 | loss_train: 3.9755 loss_val: 3.5119 | f1_val: 0.07 | rmse_val: 0.9315
Epoch: 0058 | loss_train: 3.2957 loss_val: 3.1298 | f1_val: 0.06 | rmse_val: 0.9064
Epoch: 0059 | loss_train: 3.6468 loss_val: 3.4415 | f1_val: 0.05 | rmse_val: 0.7476
Epoch: 0060 | loss_train: 3.8624 loss_val: 4.2445 | f1_val: 0.06 | rmse_val: 0.8722
Epoch: 0061 | loss_train: 4.0217 loss_val: 4.7710 | f1_val: 0.06 | rmse_val: 0.6826
Epoch: 0062 | loss_train: 3.7486 loss_val: 3.4502 | f1_val: 0.06 | rmse_val: 0.9070
Epoch: 0063 | loss_train: 3.7859 loss_val: 4.3714 | f1_val: 0.06 | rmse_val: 0.7602
Epoch: 0064 | loss_train: 3.7342 loss_val: 4.3327 | f1_val: 0.05 | rmse_val: 0.9018
Epoch: 0065 | loss_train: 3.7166 loss_val: 3.3831 | f1_val: 0.06 | rmse_val: 0.8272
Epoch: 0066 | loss_train: 3.7123 loss_val: 4.7209 | f1_val: 0.04 | rmse_val: 0.7867
Epoch: 0067 | loss_train: 3.3572 loss_val: 4.8789 | f1_val: 0.07 | rmse_val: 0.9762
Epoch: 0068 | loss_train: 3.9212 loss_val: 3.4047 | f1_val: 0.06 | rmse_val: 0.8014
Epoch: 0069 | loss_train: 3.9442 loss_val: 4.9955 | f1_val: 0.05 | rmse_val: 0.7527
Epoch: 0070 | loss_train: 3.4800 loss_val: 4.6846 | f1_val: 0.05 | rmse_val: 0.9064
Epoch: 0071 | loss_train: 3.6105 loss_val: 3.5747 | f1_val: 0.04 | rmse_val: 0.7759
Epoch: 0072 | loss_train: 3.3203 loss_val: 3.3922 | f1_val: 0.07 | rmse_val: 0.7262
Epoch: 0073 | loss_train: 3.8470 loss_val: 4.0881 | f1_val: 0.05 | rmse_val: 0.8532
Epoch: 0074 | loss_train: 3.4697 loss_val: 3.8824 | f1_val: 0.05 | rmse_val: 0.8005
Epoch: 0075 | loss_train: 3.6889 loss_val: 4.4764 | f1_val: 0.06 | rmse_val: 0.9624
Epoch: 0076 | loss_train: 3.5434 loss_val: 3.8075 | f1_val: 0.06 | rmse_val: 0.7634
Epoch: 0077 | loss_train: 3.4710 loss_val: 3.0455 | f1_val: 0.06 | rmse_val: 0.8946
Epoch: 0078 | loss_train: 3.6211 loss_val: 5.4039 | f1_val: 0.06 | rmse_val: 0.8781
Epoch: 0079 | loss_train: 3.6080 loss_val: 4.0693 | f1_val: 0.06 | rmse_val: 0.9754
Epoch: 0080 | loss_train: 3.4072 loss_val: 2.7538 | f1_val: 0.05 | rmse_val: 0.8455
Epoch: 0081 | loss_train: 3.4745 loss_val: 2.4323 | f1_val: 0.07 | rmse_val: 0.7925
Epoch: 0082 | loss_train: 3.4494 loss_val: 5.0567 | f1_val: 0.05 | rmse_val: 0.8957
Epoch: 0083 | loss_train: 3.2042 loss_val: 4.3218 | f1_val: 0.08 | rmse_val: 0.8795
Epoch: 0084 | loss_train: 3.8012 loss_val: 3.6092 | f1_val: 0.05 | rmse_val: 0.6678
Epoch: 0085 | loss_train: 3.4957 loss_val: 5.4541 | f1_val: 0.06 | rmse_val: 0.9018
Epoch: 0086 | loss_train: 3.7317 loss_val: 5.4198 | f1_val: 0.08 | rmse_val: 0.6931
Epoch: 0087 | loss_train: 3.7168 loss_val: 3.5828 | f1_val: 0.04 | rmse_val: 0.9725
Epoch: 0088 | loss_train: 3.2692 loss_val: 3.3553 | f1_val: 0.07 | rmse_val: 0.7833
Epoch: 0089 | loss_train: 3.8261 loss_val: 3.2019 | f1_val: 0.06 | rmse_val: 0.8042
Epoch: 0090 | loss_train: 3.6060 loss_val: 6.4814 | f1_val: 0.05 | rmse_val: 1.0340
Epoch: 0091 | loss_train: 3.8446 loss_val: 3.6376 | f1_val: 0.06 | rmse_val: 0.7788
Epoch: 0092 | loss_train: 3.4020 loss_val: 5.2753 | f1_val: 0.05 | rmse_val: 0.7355
Epoch: 0093 | loss_train: 3.6367 loss_val: 3.2021 | f1_val: 0.07 | rmse_val: 1.0056
Epoch: 0094 | loss_train: 3.6722 loss_val: 4.1417 | f1_val: 0.06 | rmse_val: 0.8229
Epoch: 0095 | loss_train: 3.7577 loss_val: 4.2114 | f1_val: 0.07 | rmse_val: 0.8131
Epoch: 0096 | loss_train: 3.7942 loss_val: 3.7071 | f1_val: 0.05 | rmse_val: 0.7654
Epoch: 0097 | loss_train: 4.1662 loss_val: 4.0837 | f1_val: 0.05 | rmse_val: 0.9372
Epoch: 0098 | loss_train: 3.3856 loss_val: 2.7414 | f1_val: 0.07 | rmse_val: 0.8761
Epoch: 0099 | loss_train: 3.8631 loss_val: 4.3537 | f1_val: 0.06 | rmse_val: 0.7157
Epoch: 0100 | loss_train: 3.7427 loss_val: 4.4825 | f1_val: 0.06 | rmse_val: 0.8381
Epoch: 0101 | loss_train: 3.6337 loss_val: 3.7511 | f1_val: 0.06 | rmse_val: 0.7042
Epoch: 0102 | loss_train: 3.5215 loss_val: 3.3741 | f1_val: 0.05 | rmse_val: 1.0358
Epoch: 0103 | loss_train: 3.6719 loss_val: 4.4217 | f1_val: 0.05 | rmse_val: 0.8611
Epoch: 0104 | loss_train: 3.4822 loss_val: 3.9166 | f1_val: 0.06 | rmse_val: 0.9981
Epoch: 0105 | loss_train: 3.6457 loss_val: 3.1387 | f1_val: 0.05 | rmse_val: 0.9143
Epoch: 0106 | loss_train: 3.6226 loss_val: 2.9298 | f1_val: 0.06 | rmse_val: 0.8368
Epoch: 0107 | loss_train: 3.6533 loss_val: 3.7866 | f1_val: 0.06 | rmse_val: 0.9579
Epoch: 0108 | loss_train: 3.4581 loss_val: 4.4005 | f1_val: 0.05 | rmse_val: 0.9140
Epoch: 0109 | loss_train: 4.4167 loss_val: 4.0912 | f1_val: 0.05 | rmse_val: 0.6532
Epoch: 0110 | loss_train: 3.4325 loss_val: 3.8595 | f1_val: 0.06 | rmse_val: 0.8973
Epoch: 0111 | loss_train: 3.6331 loss_val: 2.4414 | f1_val: 0.06 | rmse_val: 0.9481
Optimization Finished!
Train cost: 816.5767s
Loading 18th epoch
f1_test: 0.06 | rmse_test: 0.9481

>>> run.py: Namespace(dataset='LINUX', device=1, experiment='one_hot', log_path='log/nagphormer/LINUX/one_hot', path='data/nagphormer', plot_path='plots/nagphormer/LINUX/one_hot', pyg_path='data/pyg/LINUX')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/one_hot', n_heads=8, n_layers=1, name=None, one_hot=True, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=39, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 154690
Epoch: 0001 | loss_train: 81.5196 loss_val: 76.6284 | f1_val: 0.07 | rmse_val: 7.2059
Epoch: 0002 | loss_train: 77.1371 loss_val: 87.3683 | f1_val: 0.03 | rmse_val: 7.0852
Epoch: 0003 | loss_train: 71.8469 loss_val: 63.0372 | f1_val: 0.06 | rmse_val: 6.4555
Epoch: 0004 | loss_train: 57.2929 loss_val: 49.1815 | f1_val: 0.04 | rmse_val: 5.3222
Epoch: 0005 | loss_train: 37.1547 loss_val: 29.0726 | f1_val: 0.05 | rmse_val: 2.8936
Epoch: 0006 | loss_train: 17.4151 loss_val: 6.9588 | f1_val: 0.06 | rmse_val: 1.0262
Epoch: 0007 | loss_train: 7.7120 loss_val: 6.9935 | f1_val: 0.04 | rmse_val: 2.3925
Epoch: 0008 | loss_train: 7.5780 loss_val: 3.5483 | f1_val: 0.05 | rmse_val: 1.4392
Epoch: 0009 | loss_train: 5.8580 loss_val: 6.7706 | f1_val: 0.07 | rmse_val: 1.1159
Epoch: 0010 | loss_train: 6.5929 loss_val: 9.1353 | f1_val: 0.05 | rmse_val: 1.2948
Epoch: 0011 | loss_train: 7.6943 loss_val: 7.1661 | f1_val: 0.07 | rmse_val: 1.4712
Epoch: 0012 | loss_train: 6.4026 loss_val: 7.7580 | f1_val: 0.06 | rmse_val: 0.9119
Epoch: 0013 | loss_train: 6.5375 loss_val: 7.9749 | f1_val: 0.04 | rmse_val: 1.5433
Epoch: 0014 | loss_train: 7.7653 loss_val: 10.1728 | f1_val: 0.04 | rmse_val: 1.6073
Epoch: 0015 | loss_train: 7.5678 loss_val: 7.9857 | f1_val: 0.07 | rmse_val: 1.3926
Epoch: 0016 | loss_train: 6.5915 loss_val: 4.2609 | f1_val: 0.08 | rmse_val: 1.0481
Epoch: 0017 | loss_train: 7.5538 loss_val: 7.2239 | f1_val: 0.07 | rmse_val: 1.5118
Epoch: 0018 | loss_train: 6.5985 loss_val: 6.6166 | f1_val: 0.06 | rmse_val: 1.2337
Epoch: 0019 | loss_train: 5.5507 loss_val: 9.4772 | f1_val: 0.06 | rmse_val: 0.8914
Epoch: 0020 | loss_train: 6.3782 loss_val: 8.7819 | f1_val: 0.06 | rmse_val: 1.6557
Epoch: 0021 | loss_train: 6.9328 loss_val: 5.3650 | f1_val: 0.06 | rmse_val: 1.4619
Epoch: 0022 | loss_train: 5.8608 loss_val: 7.6232 | f1_val: 0.06 | rmse_val: 1.8600
Epoch: 0023 | loss_train: 6.5207 loss_val: 3.4667 | f1_val: 0.06 | rmse_val: 1.0561
Epoch: 0024 | loss_train: 5.6639 loss_val: 7.9595 | f1_val: 0.06 | rmse_val: 2.3448
Epoch: 0025 | loss_train: 6.6455 loss_val: 6.8720 | f1_val: 0.07 | rmse_val: 1.6704
Epoch: 0026 | loss_train: 7.4171 loss_val: 8.4336 | f1_val: 0.08 | rmse_val: 0.8024
Epoch: 0027 | loss_train: 6.5109 loss_val: 7.7611 | f1_val: 0.07 | rmse_val: 1.6240
Epoch: 0028 | loss_train: 5.6386 loss_val: 8.8924 | f1_val: 0.06 | rmse_val: 1.3830
Epoch: 0029 | loss_train: 5.4829 loss_val: 7.5041 | f1_val: 0.05 | rmse_val: 1.1976
Epoch: 0030 | loss_train: 5.5652 loss_val: 7.3030 | f1_val: 0.04 | rmse_val: 1.0243
Epoch: 0031 | loss_train: 6.1292 loss_val: 7.3008 | f1_val: 0.07 | rmse_val: 0.8393
Epoch: 0032 | loss_train: 8.2334 loss_val: 8.6305 | f1_val: 0.07 | rmse_val: 1.7112
Epoch: 0033 | loss_train: 7.4054 loss_val: 8.7300 | f1_val: 0.07 | rmse_val: 1.5558
Epoch: 0034 | loss_train: 6.2897 loss_val: 4.4880 | f1_val: 0.06 | rmse_val: 0.8462
Epoch: 0035 | loss_train: 5.9846 loss_val: 5.1406 | f1_val: 0.05 | rmse_val: 1.2854
Epoch: 0036 | loss_train: 5.5006 loss_val: 6.1573 | f1_val: 0.06 | rmse_val: 1.1589
Epoch: 0037 | loss_train: 5.7804 loss_val: 7.0999 | f1_val: 0.04 | rmse_val: 1.1299
Epoch: 0038 | loss_train: 6.4902 loss_val: 4.5840 | f1_val: 0.07 | rmse_val: 1.3091
Epoch: 0039 | loss_train: 5.6082 loss_val: 7.7275 | f1_val: 0.07 | rmse_val: 2.2377
Epoch: 0040 | loss_train: 5.2165 loss_val: 6.5745 | f1_val: 0.07 | rmse_val: 1.4608
Epoch: 0041 | loss_train: 5.7614 loss_val: 7.6082 | f1_val: 0.05 | rmse_val: 0.9526
Epoch: 0042 | loss_train: 5.3023 loss_val: 5.8089 | f1_val: 0.06 | rmse_val: 0.9444
Epoch: 0043 | loss_train: 6.0913 loss_val: 5.3619 | f1_val: 0.05 | rmse_val: 0.9754
Epoch: 0044 | loss_train: 6.5860 loss_val: 9.2926 | f1_val: 0.06 | rmse_val: 2.6046
Epoch: 0045 | loss_train: 6.7420 loss_val: 9.4968 | f1_val: 0.05 | rmse_val: 3.1208
Epoch: 0046 | loss_train: 26.7680 loss_val: 16.6881 | f1_val: 0.05 | rmse_val: 1.7839
Epoch: 0047 | loss_train: 12.3063 loss_val: 7.2022 | f1_val: 0.05 | rmse_val: 2.1770
Epoch: 0048 | loss_train: 7.1414 loss_val: 11.1536 | f1_val: 0.07 | rmse_val: 0.9038
Epoch: 0049 | loss_train: 7.0245 loss_val: 7.0145 | f1_val: 0.05 | rmse_val: 1.8625
Epoch: 0050 | loss_train: 6.1965 loss_val: 5.9552 | f1_val: 0.06 | rmse_val: 0.9986
Epoch: 0051 | loss_train: 5.8343 loss_val: 5.1153 | f1_val: 0.06 | rmse_val: 1.9647
Epoch: 0052 | loss_train: 6.2786 loss_val: 8.4333 | f1_val: 0.07 | rmse_val: 0.7636
Epoch: 0053 | loss_train: 7.8634 loss_val: 8.7038 | f1_val: 0.05 | rmse_val: 1.9780
Epoch: 0054 | loss_train: 5.3280 loss_val: 7.8720 | f1_val: 0.05 | rmse_val: 1.2279
Epoch: 0055 | loss_train: 6.5068 loss_val: 9.3499 | f1_val: 0.05 | rmse_val: 1.2297
Epoch: 0056 | loss_train: 6.6480 loss_val: 5.2767 | f1_val: 0.06 | rmse_val: 1.6293
Optimization Finished!
Train cost: 236.6042s
Loading 26th epoch
f1_test: 0.06 | rmse_test: 1.6293

>>> run.py: Namespace(dataset='AIDS700nef', device=1, experiment='one_hot', log_path='log/nagphormer/AIDS700nef/one_hot', path='data/nagphormer', plot_path='plots/nagphormer/AIDS700nef/one_hot', pyg_path='data/pyg/AIDS700nef')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=0.2, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 18.5153 loss_val: 24.8269 | f1_val: 0.05 | rmse_val: 3.5290
Epoch: 0002 | loss_train: 15.9254 loss_val: 26.8438 | f1_val: 0.06 | rmse_val: 3.3528
Epoch: 0003 | loss_train: 22.0709 loss_val: 23.5692 | f1_val: 0.05 | rmse_val: 3.4719
Epoch: 0004 | loss_train: 19.4384 loss_val: 20.8519 | f1_val: 0.05 | rmse_val: 3.2009
Epoch: 0005 | loss_train: 20.5031 loss_val: 22.3897 | f1_val: 0.06 | rmse_val: 3.2993
Epoch: 0006 | loss_train: 23.1638 loss_val: 19.4936 | f1_val: 0.05 | rmse_val: 3.2053
Epoch: 0007 | loss_train: 19.6065 loss_val: 18.6634 | f1_val: 0.07 | rmse_val: 3.0343
Epoch: 0008 | loss_train: 17.9116 loss_val: 20.5660 | f1_val: 0.05 | rmse_val: 2.9368
Epoch: 0009 | loss_train: 13.1133 loss_val: 15.8492 | f1_val: 0.06 | rmse_val: 2.8010
Epoch: 0010 | loss_train: 14.5144 loss_val: 17.1482 | f1_val: 0.06 | rmse_val: 2.7153
Epoch: 0011 | loss_train: 18.5769 loss_val: 17.0785 | f1_val: 0.05 | rmse_val: 2.5651
Epoch: 0012 | loss_train: 13.8220 loss_val: 13.0194 | f1_val: 0.04 | rmse_val: 2.2199
Epoch: 0013 | loss_train: 10.8724 loss_val: 12.7463 | f1_val: 0.05 | rmse_val: 1.8626
Epoch: 0014 | loss_train: 9.1276 loss_val: 8.1662 | f1_val: 0.06 | rmse_val: 1.8450
Epoch: 0015 | loss_train: 8.7772 loss_val: 9.2572 | f1_val: 0.06 | rmse_val: 1.3814
Epoch: 0016 | loss_train: 5.8162 loss_val: 6.7379 | f1_val: 0.07 | rmse_val: 1.0552
Epoch: 0017 | loss_train: 5.5222 loss_val: 4.3330 | f1_val: 0.07 | rmse_val: 0.8623
Epoch: 0018 | loss_train: 5.4371 loss_val: 4.0879 | f1_val: 0.07 | rmse_val: 1.0081
Epoch: 0019 | loss_train: 4.5451 loss_val: 4.0351 | f1_val: 0.05 | rmse_val: 1.3972
Epoch: 0020 | loss_train: 4.8554 loss_val: 4.4783 | f1_val: 0.06 | rmse_val: 1.4186
Epoch: 0021 | loss_train: 4.3258 loss_val: 5.5774 | f1_val: 0.07 | rmse_val: 1.3091
Epoch: 0022 | loss_train: 4.1680 loss_val: 4.0229 | f1_val: 0.06 | rmse_val: 1.1031
Epoch: 0023 | loss_train: 4.1686 loss_val: 6.3180 | f1_val: 0.05 | rmse_val: 0.9301
Epoch: 0024 | loss_train: 4.4526 loss_val: 3.5455 | f1_val: 0.05 | rmse_val: 0.9445
Epoch: 0025 | loss_train: 4.1979 loss_val: 5.7164 | f1_val: 0.08 | rmse_val: 0.9939
Epoch: 0026 | loss_train: 4.3856 loss_val: 3.3141 | f1_val: 0.07 | rmse_val: 1.0106
Epoch: 0027 | loss_train: 3.4516 loss_val: 4.0854 | f1_val: 0.07 | rmse_val: 1.0904
Epoch: 0028 | loss_train: 4.0708 loss_val: 3.1583 | f1_val: 0.06 | rmse_val: 1.1768
Epoch: 0029 | loss_train: 4.6984 loss_val: 6.2707 | f1_val: 0.07 | rmse_val: 1.1623
Epoch: 0030 | loss_train: 3.6327 loss_val: 4.5429 | f1_val: 0.06 | rmse_val: 0.8980
Epoch: 0031 | loss_train: 4.2430 loss_val: 3.7034 | f1_val: 0.06 | rmse_val: 0.9213
Epoch: 0032 | loss_train: 4.4916 loss_val: 5.1715 | f1_val: 0.08 | rmse_val: 0.9306
Epoch: 0033 | loss_train: 4.3317 loss_val: 4.2703 | f1_val: 0.07 | rmse_val: 1.1100
Epoch: 0034 | loss_train: 4.0754 loss_val: 5.4291 | f1_val: 0.07 | rmse_val: 1.0140
Epoch: 0035 | loss_train: 4.8325 loss_val: 4.2833 | f1_val: 0.06 | rmse_val: 1.0999
Epoch: 0036 | loss_train: 3.6697 loss_val: 3.9749 | f1_val: 0.07 | rmse_val: 1.0472
Epoch: 0037 | loss_train: 3.7186 loss_val: 4.5173 | f1_val: 0.07 | rmse_val: 0.9072
Epoch: 0038 | loss_train: 3.8851 loss_val: 4.3622 | f1_val: 0.06 | rmse_val: 0.9362
Epoch: 0039 | loss_train: 4.9552 loss_val: 4.4118 | f1_val: 0.06 | rmse_val: 1.0661
Epoch: 0040 | loss_train: 3.4441 loss_val: 3.1349 | f1_val: 0.08 | rmse_val: 1.2418
Epoch: 0041 | loss_train: 4.8891 loss_val: 3.9764 | f1_val: 0.05 | rmse_val: 1.2054
Epoch: 0042 | loss_train: 4.2159 loss_val: 5.6948 | f1_val: 0.06 | rmse_val: 1.2139
Epoch: 0043 | loss_train: 5.0939 loss_val: 3.7056 | f1_val: 0.06 | rmse_val: 1.0778
Epoch: 0044 | loss_train: 4.7505 loss_val: 4.4743 | f1_val: 0.06 | rmse_val: 1.0778
Epoch: 0045 | loss_train: 4.1200 loss_val: 4.4193 | f1_val: 0.07 | rmse_val: 1.1580
Epoch: 0046 | loss_train: 3.1595 loss_val: 3.4332 | f1_val: 0.06 | rmse_val: 1.1466
Epoch: 0047 | loss_train: 4.9160 loss_val: 4.6431 | f1_val: 0.05 | rmse_val: 1.1217
Epoch: 0048 | loss_train: 4.1204 loss_val: 3.8783 | f1_val: 0.06 | rmse_val: 1.1598
Epoch: 0049 | loss_train: 3.9188 loss_val: 5.1055 | f1_val: 0.08 | rmse_val: 0.8941
Epoch: 0050 | loss_train: 4.9018 loss_val: 4.0268 | f1_val: 0.06 | rmse_val: 1.0074
Epoch: 0051 | loss_train: 3.7666 loss_val: 4.5579 | f1_val: 0.07 | rmse_val: 1.3364
Epoch: 0052 | loss_train: 4.7186 loss_val: 3.2195 | f1_val: 0.06 | rmse_val: 1.2355
Epoch: 0053 | loss_train: 4.0585 loss_val: 5.3971 | f1_val: 0.06 | rmse_val: 0.8865
Epoch: 0054 | loss_train: 4.9870 loss_val: 5.5021 | f1_val: 0.08 | rmse_val: 0.8974
Epoch: 0055 | loss_train: 3.8861 loss_val: 3.1641 | f1_val: 0.07 | rmse_val: 1.1468
Epoch: 0056 | loss_train: 5.0918 loss_val: 4.5729 | f1_val: 0.07 | rmse_val: 1.1203
Epoch: 0057 | loss_train: 4.1675 loss_val: 4.1926 | f1_val: 0.06 | rmse_val: 0.9578
Epoch: 0058 | loss_train: 4.4378 loss_val: 4.8827 | f1_val: 0.06 | rmse_val: 0.9066
Epoch: 0059 | loss_train: 4.1963 loss_val: 4.2608 | f1_val: 0.07 | rmse_val: 0.9850
Epoch: 0060 | loss_train: 4.6536 loss_val: 5.0154 | f1_val: 0.07 | rmse_val: 1.1943
Epoch: 0061 | loss_train: 3.9830 loss_val: 4.4864 | f1_val: 0.06 | rmse_val: 1.2071
Epoch: 0062 | loss_train: 4.8214 loss_val: 3.1301 | f1_val: 0.06 | rmse_val: 0.9677
Epoch: 0063 | loss_train: 4.5366 loss_val: 3.7445 | f1_val: 0.09 | rmse_val: 1.0817
Epoch: 0064 | loss_train: 3.4545 loss_val: 3.8126 | f1_val: 0.05 | rmse_val: 1.2197
Epoch: 0065 | loss_train: 5.8687 loss_val: 4.9876 | f1_val: 0.08 | rmse_val: 1.1110
Epoch: 0066 | loss_train: 3.4549 loss_val: 6.3772 | f1_val: 0.08 | rmse_val: 0.8832
Epoch: 0067 | loss_train: 5.2000 loss_val: 5.6985 | f1_val: 0.05 | rmse_val: 0.8780
Epoch: 0068 | loss_train: 3.5092 loss_val: 5.0549 | f1_val: 0.08 | rmse_val: 1.0909
Epoch: 0069 | loss_train: 4.9910 loss_val: 3.9270 | f1_val: 0.06 | rmse_val: 1.3381
Epoch: 0070 | loss_train: 4.6740 loss_val: 3.7013 | f1_val: 0.06 | rmse_val: 1.0273
Epoch: 0071 | loss_train: 4.3429 loss_val: 5.0838 | f1_val: 0.06 | rmse_val: 1.1000
Epoch: 0072 | loss_train: 3.8987 loss_val: 4.1160 | f1_val: 0.07 | rmse_val: 0.8830
Epoch: 0073 | loss_train: 4.1585 loss_val: 4.0125 | f1_val: 0.07 | rmse_val: 1.0299
Epoch: 0074 | loss_train: 4.3898 loss_val: 5.0677 | f1_val: 0.06 | rmse_val: 1.1520
Epoch: 0075 | loss_train: 4.4437 loss_val: 4.6680 | f1_val: 0.08 | rmse_val: 1.1573
Epoch: 0076 | loss_train: 3.5290 loss_val: 4.6047 | f1_val: 0.06 | rmse_val: 1.0750
Epoch: 0077 | loss_train: 5.0231 loss_val: 3.2397 | f1_val: 0.05 | rmse_val: 0.9928
Epoch: 0078 | loss_train: 4.0376 loss_val: 3.4708 | f1_val: 0.07 | rmse_val: 0.8890
Epoch: 0079 | loss_train: 3.8155 loss_val: 4.6409 | f1_val: 0.08 | rmse_val: 0.7732
Epoch: 0080 | loss_train: 4.2886 loss_val: 3.9118 | f1_val: 0.07 | rmse_val: 1.1116
Epoch: 0081 | loss_train: 4.7222 loss_val: 3.9761 | f1_val: 0.06 | rmse_val: 1.1029
Epoch: 0082 | loss_train: 3.6025 loss_val: 4.1487 | f1_val: 0.07 | rmse_val: 1.1076
Epoch: 0083 | loss_train: 3.7826 loss_val: 3.8992 | f1_val: 0.07 | rmse_val: 1.1061
Epoch: 0084 | loss_train: 3.5883 loss_val: 4.7133 | f1_val: 0.08 | rmse_val: 0.8483
Epoch: 0085 | loss_train: 4.9378 loss_val: 3.6692 | f1_val: 0.06 | rmse_val: 1.0158
Epoch: 0086 | loss_train: 3.8291 loss_val: 3.7254 | f1_val: 0.07 | rmse_val: 1.2715
Epoch: 0087 | loss_train: 5.0821 loss_val: 3.8542 | f1_val: 0.06 | rmse_val: 1.0143
Epoch: 0088 | loss_train: 4.0070 loss_val: 5.4147 | f1_val: 0.08 | rmse_val: 1.0245
Epoch: 0089 | loss_train: 4.0192 loss_val: 3.8948 | f1_val: 0.05 | rmse_val: 0.8026
Epoch: 0090 | loss_train: 5.3796 loss_val: 3.7256 | f1_val: 0.05 | rmse_val: 0.9729
Epoch: 0091 | loss_train: 4.5163 loss_val: 3.6023 | f1_val: 0.06 | rmse_val: 1.0132
Epoch: 0092 | loss_train: 4.3717 loss_val: 4.3116 | f1_val: 0.08 | rmse_val: 1.0798
Epoch: 0093 | loss_train: 4.8523 loss_val: 4.6230 | f1_val: 0.08 | rmse_val: 1.4431
Optimization Finished!
Train cost: 463.5793s
Loading 63th epoch
f1_test: 0.08 | rmse_test: 1.4431

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=0.4, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 20.2674 loss_val: 23.2391 | f1_val: 0.05 | rmse_val: 3.4679
Epoch: 0002 | loss_train: 17.5968 loss_val: 23.5495 | f1_val: 0.05 | rmse_val: 3.2929
Epoch: 0003 | loss_train: 17.2973 loss_val: 23.1382 | f1_val: 0.05 | rmse_val: 3.2306
Epoch: 0004 | loss_train: 15.6660 loss_val: 15.4964 | f1_val: 0.07 | rmse_val: 3.0042
Epoch: 0005 | loss_train: 15.5650 loss_val: 19.7710 | f1_val: 0.04 | rmse_val: 2.8023
Epoch: 0006 | loss_train: 13.0415 loss_val: 11.9533 | f1_val: 0.05 | rmse_val: 2.1643
Epoch: 0007 | loss_train: 9.0364 loss_val: 11.5139 | f1_val: 0.05 | rmse_val: 1.6521
Epoch: 0008 | loss_train: 7.4996 loss_val: 6.5213 | f1_val: 0.07 | rmse_val: 1.0089
Epoch: 0009 | loss_train: 4.9903 loss_val: 4.4150 | f1_val: 0.07 | rmse_val: 1.1183
Epoch: 0010 | loss_train: 3.8328 loss_val: 4.6597 | f1_val: 0.05 | rmse_val: 1.4290
Epoch: 0011 | loss_train: 3.9197 loss_val: 4.4099 | f1_val: 0.09 | rmse_val: 1.0038
Epoch: 0012 | loss_train: 4.1196 loss_val: 4.4733 | f1_val: 0.07 | rmse_val: 0.8489
Epoch: 0013 | loss_train: 4.0636 loss_val: 5.4180 | f1_val: 0.06 | rmse_val: 0.9485
Epoch: 0014 | loss_train: 4.1557 loss_val: 4.8093 | f1_val: 0.04 | rmse_val: 1.0910
Epoch: 0015 | loss_train: 3.9400 loss_val: 4.7256 | f1_val: 0.05 | rmse_val: 1.1936
Epoch: 0016 | loss_train: 3.8512 loss_val: 4.1343 | f1_val: 0.06 | rmse_val: 1.0843
Epoch: 0017 | loss_train: 3.7188 loss_val: 4.7331 | f1_val: 0.06 | rmse_val: 1.1231
Epoch: 0018 | loss_train: 4.0676 loss_val: 3.1862 | f1_val: 0.06 | rmse_val: 1.0062
Epoch: 0019 | loss_train: 4.3707 loss_val: 5.8907 | f1_val: 0.05 | rmse_val: 0.8586
Epoch: 0020 | loss_train: 4.2842 loss_val: 4.2608 | f1_val: 0.07 | rmse_val: 1.1222
Epoch: 0021 | loss_train: 3.9277 loss_val: 5.0928 | f1_val: 0.06 | rmse_val: 1.0849
Epoch: 0022 | loss_train: 3.6831 loss_val: 4.8874 | f1_val: 0.05 | rmse_val: 1.2382
Epoch: 0023 | loss_train: 4.3695 loss_val: 3.5884 | f1_val: 0.07 | rmse_val: 1.0960
Epoch: 0024 | loss_train: 4.7054 loss_val: 5.2291 | f1_val: 0.08 | rmse_val: 1.1595
Epoch: 0025 | loss_train: 3.8141 loss_val: 4.3569 | f1_val: 0.07 | rmse_val: 1.0369
Epoch: 0026 | loss_train: 3.5960 loss_val: 3.4128 | f1_val: 0.08 | rmse_val: 1.2723
Epoch: 0027 | loss_train: 4.9332 loss_val: 5.4297 | f1_val: 0.04 | rmse_val: 0.9631
Epoch: 0028 | loss_train: 4.2121 loss_val: 4.1524 | f1_val: 0.08 | rmse_val: 1.0401
Epoch: 0029 | loss_train: 3.6146 loss_val: 4.1553 | f1_val: 0.06 | rmse_val: 1.3000
Epoch: 0030 | loss_train: 4.0890 loss_val: 4.6367 | f1_val: 0.07 | rmse_val: 1.1132
Epoch: 0031 | loss_train: 3.5843 loss_val: 2.7382 | f1_val: 0.08 | rmse_val: 0.9102
Epoch: 0032 | loss_train: 3.8208 loss_val: 4.3090 | f1_val: 0.07 | rmse_val: 1.3246
Epoch: 0033 | loss_train: 3.7647 loss_val: 6.1045 | f1_val: 0.05 | rmse_val: 0.8521
Epoch: 0034 | loss_train: 3.7199 loss_val: 4.8459 | f1_val: 0.08 | rmse_val: 1.4217
Epoch: 0035 | loss_train: 3.7483 loss_val: 5.5160 | f1_val: 0.06 | rmse_val: 0.8733
Epoch: 0036 | loss_train: 4.7311 loss_val: 5.2151 | f1_val: 0.07 | rmse_val: 1.2926
Epoch: 0037 | loss_train: 3.4477 loss_val: 5.1991 | f1_val: 0.06 | rmse_val: 0.9421
Epoch: 0038 | loss_train: 4.0418 loss_val: 4.0409 | f1_val: 0.06 | rmse_val: 1.2546
Epoch: 0039 | loss_train: 4.4660 loss_val: 5.4357 | f1_val: 0.06 | rmse_val: 0.7687
Epoch: 0040 | loss_train: 4.3882 loss_val: 4.0194 | f1_val: 0.06 | rmse_val: 1.2715
Epoch: 0041 | loss_train: 4.0075 loss_val: 3.6094 | f1_val: 0.07 | rmse_val: 1.1379
Epoch: 0042 | loss_train: 3.6586 loss_val: 5.5972 | f1_val: 0.04 | rmse_val: 0.9271
Epoch: 0043 | loss_train: 4.1571 loss_val: 4.7079 | f1_val: 0.06 | rmse_val: 1.1634
Epoch: 0044 | loss_train: 4.0792 loss_val: 4.8709 | f1_val: 0.06 | rmse_val: 1.0992
Epoch: 0045 | loss_train: 4.1236 loss_val: 4.4988 | f1_val: 0.07 | rmse_val: 0.8901
Epoch: 0046 | loss_train: 4.1252 loss_val: 4.1901 | f1_val: 0.06 | rmse_val: 1.3035
Epoch: 0047 | loss_train: 4.2244 loss_val: 3.8312 | f1_val: 0.06 | rmse_val: 0.9666
Epoch: 0048 | loss_train: 4.6870 loss_val: 2.4194 | f1_val: 0.07 | rmse_val: 1.1995
Epoch: 0049 | loss_train: 4.0058 loss_val: 4.4023 | f1_val: 0.07 | rmse_val: 0.8955
Epoch: 0050 | loss_train: 3.7656 loss_val: 4.4997 | f1_val: 0.05 | rmse_val: 1.3123
Epoch: 0051 | loss_train: 5.1795 loss_val: 4.6110 | f1_val: 0.06 | rmse_val: 0.8266
Epoch: 0052 | loss_train: 3.8423 loss_val: 4.7940 | f1_val: 0.07 | rmse_val: 1.4333
Epoch: 0053 | loss_train: 4.1430 loss_val: 7.3627 | f1_val: 0.06 | rmse_val: 0.8705
Epoch: 0054 | loss_train: 3.9902 loss_val: 4.0284 | f1_val: 0.06 | rmse_val: 1.2063
Epoch: 0055 | loss_train: 4.0961 loss_val: 3.7773 | f1_val: 0.06 | rmse_val: 0.8295
Epoch: 0056 | loss_train: 3.8995 loss_val: 3.7511 | f1_val: 0.06 | rmse_val: 1.2019
Epoch: 0057 | loss_train: 3.7135 loss_val: 3.4229 | f1_val: 0.06 | rmse_val: 0.8399
Epoch: 0058 | loss_train: 4.1708 loss_val: 4.7186 | f1_val: 0.07 | rmse_val: 1.1155
Epoch: 0059 | loss_train: 4.1637 loss_val: 5.1434 | f1_val: 0.07 | rmse_val: 1.2221
Epoch: 0060 | loss_train: 4.0128 loss_val: 5.3643 | f1_val: 0.08 | rmse_val: 0.8500
Epoch: 0061 | loss_train: 4.4043 loss_val: 5.0165 | f1_val: 0.06 | rmse_val: 1.6821
Epoch: 0062 | loss_train: 4.4380 loss_val: 3.4373 | f1_val: 0.04 | rmse_val: 1.0237
Epoch: 0063 | loss_train: 4.1141 loss_val: 4.3975 | f1_val: 0.05 | rmse_val: 1.1494
Epoch: 0064 | loss_train: 3.7839 loss_val: 4.2026 | f1_val: 0.06 | rmse_val: 1.0728
Epoch: 0065 | loss_train: 3.6046 loss_val: 4.8564 | f1_val: 0.07 | rmse_val: 0.9943
Epoch: 0066 | loss_train: 4.2450 loss_val: 4.2112 | f1_val: 0.06 | rmse_val: 1.2311
Epoch: 0067 | loss_train: 4.1329 loss_val: 4.4345 | f1_val: 0.06 | rmse_val: 1.2125
Epoch: 0068 | loss_train: 4.2294 loss_val: 3.5952 | f1_val: 0.06 | rmse_val: 1.0176
Epoch: 0069 | loss_train: 3.7323 loss_val: 5.6670 | f1_val: 0.08 | rmse_val: 0.8152
Epoch: 0070 | loss_train: 4.2823 loss_val: 5.7383 | f1_val: 0.05 | rmse_val: 1.2883
Epoch: 0071 | loss_train: 4.0474 loss_val: 3.6558 | f1_val: 0.08 | rmse_val: 1.0936
Epoch: 0072 | loss_train: 4.0550 loss_val: 4.6943 | f1_val: 0.07 | rmse_val: 1.1168
Epoch: 0073 | loss_train: 3.6858 loss_val: 4.7192 | f1_val: 0.08 | rmse_val: 0.9829
Epoch: 0074 | loss_train: 4.3221 loss_val: 4.9941 | f1_val: 0.05 | rmse_val: 0.9231
Epoch: 0075 | loss_train: 3.6947 loss_val: 4.4978 | f1_val: 0.04 | rmse_val: 1.2288
Epoch: 0076 | loss_train: 4.6349 loss_val: 4.7465 | f1_val: 0.09 | rmse_val: 0.8910
Epoch: 0077 | loss_train: 4.0750 loss_val: 5.7320 | f1_val: 0.07 | rmse_val: 0.8914
Epoch: 0078 | loss_train: 3.9361 loss_val: 5.5557 | f1_val: 0.06 | rmse_val: 1.3700
Epoch: 0079 | loss_train: 3.2193 loss_val: 3.0009 | f1_val: 0.07 | rmse_val: 0.9769
Epoch: 0080 | loss_train: 4.7122 loss_val: 5.5709 | f1_val: 0.08 | rmse_val: 0.7852
Epoch: 0081 | loss_train: 3.6910 loss_val: 3.9161 | f1_val: 0.09 | rmse_val: 1.2955
Epoch: 0082 | loss_train: 3.9330 loss_val: 4.2396 | f1_val: 0.07 | rmse_val: 0.9812
Epoch: 0083 | loss_train: 3.9235 loss_val: 4.8287 | f1_val: 0.08 | rmse_val: 1.1763
Epoch: 0084 | loss_train: 3.9339 loss_val: 4.1193 | f1_val: 0.08 | rmse_val: 0.9030
Epoch: 0085 | loss_train: 3.7206 loss_val: 5.6969 | f1_val: 0.05 | rmse_val: 0.7232
Epoch: 0086 | loss_train: 4.2636 loss_val: 4.3981 | f1_val: 0.07 | rmse_val: 1.1301
Epoch: 0087 | loss_train: 4.1484 loss_val: 4.7074 | f1_val: 0.06 | rmse_val: 0.9099
Epoch: 0088 | loss_train: 4.0918 loss_val: 4.7041 | f1_val: 0.05 | rmse_val: 0.9494
Epoch: 0089 | loss_train: 3.9911 loss_val: 3.6803 | f1_val: 0.05 | rmse_val: 1.1108
Epoch: 0090 | loss_train: 3.7833 loss_val: 3.8938 | f1_val: 0.07 | rmse_val: 1.0015
Epoch: 0091 | loss_train: 4.2414 loss_val: 4.4497 | f1_val: 0.06 | rmse_val: 1.1177
Epoch: 0092 | loss_train: 3.7282 loss_val: 4.5692 | f1_val: 0.08 | rmse_val: 1.0005
Epoch: 0093 | loss_train: 3.8030 loss_val: 3.7526 | f1_val: 0.06 | rmse_val: 1.0758
Epoch: 0094 | loss_train: 4.0261 loss_val: 5.2399 | f1_val: 0.05 | rmse_val: 0.9899
Epoch: 0095 | loss_train: 4.1261 loss_val: 4.2683 | f1_val: 0.07 | rmse_val: 1.0689
Epoch: 0096 | loss_train: 4.0615 loss_val: 3.4801 | f1_val: 0.06 | rmse_val: 1.1857
Epoch: 0097 | loss_train: 4.3992 loss_val: 4.6214 | f1_val: 0.05 | rmse_val: 1.0961
Epoch: 0098 | loss_train: 3.8976 loss_val: 5.0632 | f1_val: 0.08 | rmse_val: 0.8512
Epoch: 0099 | loss_train: 3.5460 loss_val: 4.1299 | f1_val: 0.08 | rmse_val: 1.0741
Epoch: 0100 | loss_train: 4.4303 loss_val: 5.3484 | f1_val: 0.05 | rmse_val: 1.1463
Epoch: 0101 | loss_train: 4.2991 loss_val: 4.0030 | f1_val: 0.05 | rmse_val: 1.0402
Epoch: 0102 | loss_train: 4.1720 loss_val: 3.7577 | f1_val: 0.06 | rmse_val: 1.0104
Epoch: 0103 | loss_train: 5.1404 loss_val: 3.9750 | f1_val: 0.05 | rmse_val: 1.0874
Epoch: 0104 | loss_train: 4.1245 loss_val: 6.2480 | f1_val: 0.05 | rmse_val: 0.8432
Epoch: 0105 | loss_train: 3.9022 loss_val: 3.7240 | f1_val: 0.06 | rmse_val: 1.1985
Epoch: 0106 | loss_train: 4.0814 loss_val: 4.4288 | f1_val: 0.05 | rmse_val: 1.1459
Optimization Finished!
Train cost: 588.9717s
Loading 76th epoch
f1_test: 0.05 | rmse_test: 1.1459

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=0.6, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.8437 loss_val: 22.1772 | f1_val: 0.06 | rmse_val: 3.3920
Epoch: 0002 | loss_train: 18.3562 loss_val: 22.6468 | f1_val: 0.05 | rmse_val: 3.1999
Epoch: 0003 | loss_train: 15.7619 loss_val: 22.4038 | f1_val: 0.07 | rmse_val: 2.9734
Epoch: 0004 | loss_train: 15.7485 loss_val: 15.9551 | f1_val: 0.05 | rmse_val: 2.6511
Epoch: 0005 | loss_train: 11.6172 loss_val: 10.8271 | f1_val: 0.04 | rmse_val: 1.8948
Epoch: 0006 | loss_train: 7.7740 loss_val: 6.2283 | f1_val: 0.05 | rmse_val: 0.9249
Epoch: 0007 | loss_train: 4.2247 loss_val: 4.5587 | f1_val: 0.06 | rmse_val: 1.6181
Epoch: 0008 | loss_train: 4.2553 loss_val: 5.1460 | f1_val: 0.07 | rmse_val: 0.9146
Epoch: 0009 | loss_train: 4.6826 loss_val: 4.3173 | f1_val: 0.07 | rmse_val: 1.0205
Epoch: 0010 | loss_train: 4.2469 loss_val: 5.6683 | f1_val: 0.06 | rmse_val: 0.9422
Epoch: 0011 | loss_train: 4.4442 loss_val: 3.9843 | f1_val: 0.07 | rmse_val: 0.9967
Epoch: 0012 | loss_train: 3.8909 loss_val: 4.5334 | f1_val: 0.07 | rmse_val: 0.9605
Epoch: 0013 | loss_train: 3.7572 loss_val: 4.9333 | f1_val: 0.04 | rmse_val: 0.9982
Epoch: 0014 | loss_train: 4.0454 loss_val: 3.6029 | f1_val: 0.05 | rmse_val: 1.1890
Epoch: 0015 | loss_train: 4.1352 loss_val: 4.4516 | f1_val: 0.06 | rmse_val: 0.9453
Epoch: 0016 | loss_train: 4.8164 loss_val: 3.2638 | f1_val: 0.06 | rmse_val: 1.3562
Epoch: 0017 | loss_train: 4.3366 loss_val: 4.3759 | f1_val: 0.06 | rmse_val: 0.8972
Epoch: 0018 | loss_train: 4.2712 loss_val: 4.6952 | f1_val: 0.06 | rmse_val: 1.4699
Epoch: 0019 | loss_train: 3.3581 loss_val: 2.5562 | f1_val: 0.06 | rmse_val: 1.0693
Epoch: 0020 | loss_train: 4.4351 loss_val: 3.6212 | f1_val: 0.06 | rmse_val: 0.9753
Epoch: 0021 | loss_train: 4.3182 loss_val: 4.9627 | f1_val: 0.05 | rmse_val: 1.2378
Epoch: 0022 | loss_train: 4.1026 loss_val: 5.6939 | f1_val: 0.05 | rmse_val: 1.0262
Epoch: 0023 | loss_train: 5.2609 loss_val: 3.4607 | f1_val: 0.05 | rmse_val: 1.2016
Epoch: 0024 | loss_train: 4.2564 loss_val: 4.2045 | f1_val: 0.05 | rmse_val: 1.2011
Epoch: 0025 | loss_train: 3.9992 loss_val: 4.1735 | f1_val: 0.05 | rmse_val: 1.1493
Epoch: 0026 | loss_train: 4.6845 loss_val: 4.8687 | f1_val: 0.07 | rmse_val: 0.8873
Epoch: 0027 | loss_train: 4.5060 loss_val: 6.1755 | f1_val: 0.07 | rmse_val: 1.7671
Epoch: 0028 | loss_train: 4.4178 loss_val: 5.7664 | f1_val: 0.06 | rmse_val: 0.7897
Epoch: 0029 | loss_train: 4.2585 loss_val: 3.4240 | f1_val: 0.06 | rmse_val: 1.1446
Epoch: 0030 | loss_train: 4.4099 loss_val: 3.7780 | f1_val: 0.05 | rmse_val: 1.0534
Epoch: 0031 | loss_train: 4.2734 loss_val: 3.8917 | f1_val: 0.06 | rmse_val: 1.2561
Epoch: 0032 | loss_train: 4.0701 loss_val: 3.5762 | f1_val: 0.05 | rmse_val: 1.3400
Epoch: 0033 | loss_train: 4.1357 loss_val: 3.6944 | f1_val: 0.05 | rmse_val: 1.0208
Epoch: 0034 | loss_train: 4.1847 loss_val: 4.5070 | f1_val: 0.06 | rmse_val: 1.1091
Epoch: 0035 | loss_train: 4.0553 loss_val: 5.2443 | f1_val: 0.06 | rmse_val: 1.0582
Epoch: 0036 | loss_train: 3.8034 loss_val: 3.5627 | f1_val: 0.06 | rmse_val: 1.4575
Epoch: 0037 | loss_train: 4.5382 loss_val: 3.1288 | f1_val: 0.06 | rmse_val: 0.9826
Epoch: 0038 | loss_train: 4.0678 loss_val: 4.5240 | f1_val: 0.05 | rmse_val: 1.1982
Epoch: 0039 | loss_train: 4.1966 loss_val: 4.2663 | f1_val: 0.06 | rmse_val: 0.8403
Epoch: 0040 | loss_train: 4.1166 loss_val: 3.5260 | f1_val: 0.06 | rmse_val: 0.9003
Epoch: 0041 | loss_train: 4.3975 loss_val: 3.2270 | f1_val: 0.06 | rmse_val: 1.1056
Epoch: 0042 | loss_train: 4.2650 loss_val: 4.6226 | f1_val: 0.06 | rmse_val: 1.4031
Epoch: 0043 | loss_train: 4.0303 loss_val: 5.5881 | f1_val: 0.05 | rmse_val: 0.8360
Epoch: 0044 | loss_train: 4.5734 loss_val: 5.5471 | f1_val: 0.06 | rmse_val: 0.8907
Epoch: 0045 | loss_train: 3.9443 loss_val: 4.5632 | f1_val: 0.05 | rmse_val: 1.1428
Epoch: 0046 | loss_train: 4.6066 loss_val: 5.6767 | f1_val: 0.05 | rmse_val: 1.2336
Epoch: 0047 | loss_train: 4.7815 loss_val: 5.9643 | f1_val: 0.07 | rmse_val: 0.9181
Epoch: 0048 | loss_train: 5.3769 loss_val: 4.6892 | f1_val: 0.05 | rmse_val: 1.3362
Epoch: 0049 | loss_train: 4.4818 loss_val: 4.6668 | f1_val: 0.04 | rmse_val: 0.9729
Optimization Finished!
Train cost: 435.2663s
Loading 12th epoch
f1_test: 0.04 | rmse_test: 0.9729

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=0.8, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 20.5287 loss_val: 22.7380 | f1_val: 0.05 | rmse_val: 3.3476
Epoch: 0002 | loss_train: 17.5720 loss_val: 20.7508 | f1_val: 0.06 | rmse_val: 3.1746
Epoch: 0003 | loss_train: 15.6983 loss_val: 16.5402 | f1_val: 0.05 | rmse_val: 2.6263
Epoch: 0004 | loss_train: 12.9069 loss_val: 10.5509 | f1_val: 0.06 | rmse_val: 1.6245
Epoch: 0005 | loss_train: 5.8495 loss_val: 5.3949 | f1_val: 0.05 | rmse_val: 1.1663
Epoch: 0006 | loss_train: 4.1219 loss_val: 4.7122 | f1_val: 0.05 | rmse_val: 1.2900
Epoch: 0007 | loss_train: 4.6495 loss_val: 5.6676 | f1_val: 0.05 | rmse_val: 1.0173
Epoch: 0008 | loss_train: 4.1077 loss_val: 5.3036 | f1_val: 0.06 | rmse_val: 1.1423
Epoch: 0009 | loss_train: 4.6791 loss_val: 3.7266 | f1_val: 0.06 | rmse_val: 0.9796
Epoch: 0010 | loss_train: 4.1092 loss_val: 5.0388 | f1_val: 0.05 | rmse_val: 1.1763
Epoch: 0011 | loss_train: 4.0888 loss_val: 5.9119 | f1_val: 0.06 | rmse_val: 1.1245
Epoch: 0012 | loss_train: 4.1018 loss_val: 4.2738 | f1_val: 0.05 | rmse_val: 1.1433
Epoch: 0013 | loss_train: 3.9028 loss_val: 4.3744 | f1_val: 0.06 | rmse_val: 0.8355
Epoch: 0014 | loss_train: 4.5238 loss_val: 4.3005 | f1_val: 0.08 | rmse_val: 1.1602
Epoch: 0015 | loss_train: 4.2418 loss_val: 4.8018 | f1_val: 0.06 | rmse_val: 1.0992
Epoch: 0016 | loss_train: 4.0682 loss_val: 4.1702 | f1_val: 0.05 | rmse_val: 1.0505
Epoch: 0017 | loss_train: 4.3361 loss_val: 3.4327 | f1_val: 0.07 | rmse_val: 0.7238
Epoch: 0018 | loss_train: 3.9538 loss_val: 7.0120 | f1_val: 0.05 | rmse_val: 1.2727
Epoch: 0019 | loss_train: 3.9666 loss_val: 4.7755 | f1_val: 0.04 | rmse_val: 1.0783
Epoch: 0020 | loss_train: 4.4115 loss_val: 4.6312 | f1_val: 0.05 | rmse_val: 1.0327
Epoch: 0021 | loss_train: 4.3146 loss_val: 5.3110 | f1_val: 0.07 | rmse_val: 1.5429
Epoch: 0022 | loss_train: 4.1245 loss_val: 5.1707 | f1_val: 0.07 | rmse_val: 1.0236
Epoch: 0023 | loss_train: 4.2620 loss_val: 4.9696 | f1_val: 0.06 | rmse_val: 0.9808
Epoch: 0024 | loss_train: 3.8067 loss_val: 4.2735 | f1_val: 0.05 | rmse_val: 1.0004
Epoch: 0025 | loss_train: 3.8257 loss_val: 4.2756 | f1_val: 0.05 | rmse_val: 1.0782
Epoch: 0026 | loss_train: 4.3810 loss_val: 5.6346 | f1_val: 0.05 | rmse_val: 1.6318
Epoch: 0027 | loss_train: 4.7325 loss_val: 6.6576 | f1_val: 0.05 | rmse_val: 1.3313
Epoch: 0028 | loss_train: 4.3997 loss_val: 4.3086 | f1_val: 0.05 | rmse_val: 1.0162
Epoch: 0029 | loss_train: 3.7977 loss_val: 4.5111 | f1_val: 0.08 | rmse_val: 1.0976
Epoch: 0030 | loss_train: 4.1677 loss_val: 4.1205 | f1_val: 0.07 | rmse_val: 0.9132
Epoch: 0031 | loss_train: 4.3711 loss_val: 2.9652 | f1_val: 0.06 | rmse_val: 1.5747
Epoch: 0032 | loss_train: 3.7729 loss_val: 4.1711 | f1_val: 0.05 | rmse_val: 1.3241
Epoch: 0033 | loss_train: 3.9637 loss_val: 4.5541 | f1_val: 0.06 | rmse_val: 1.2687
Epoch: 0034 | loss_train: 4.3390 loss_val: 5.1293 | f1_val: 0.08 | rmse_val: 1.4135
Epoch: 0035 | loss_train: 4.0883 loss_val: 2.6549 | f1_val: 0.05 | rmse_val: 0.8816
Epoch: 0036 | loss_train: 3.9799 loss_val: 4.8765 | f1_val: 0.06 | rmse_val: 1.3865
Epoch: 0037 | loss_train: 3.8898 loss_val: 4.8653 | f1_val: 0.05 | rmse_val: 0.8651
Epoch: 0038 | loss_train: 3.9624 loss_val: 4.5337 | f1_val: 0.05 | rmse_val: 1.0978
Epoch: 0039 | loss_train: 3.8902 loss_val: 4.0306 | f1_val: 0.06 | rmse_val: 1.1617
Epoch: 0040 | loss_train: 3.5004 loss_val: 3.3523 | f1_val: 0.05 | rmse_val: 0.9308
Epoch: 0041 | loss_train: 4.4583 loss_val: 3.7243 | f1_val: 0.06 | rmse_val: 0.9133
Epoch: 0042 | loss_train: 4.3557 loss_val: 4.3354 | f1_val: 0.06 | rmse_val: 0.9493
Epoch: 0043 | loss_train: 3.8533 loss_val: 3.6652 | f1_val: 0.06 | rmse_val: 1.1982
Epoch: 0044 | loss_train: 4.0913 loss_val: 4.3950 | f1_val: 0.06 | rmse_val: 0.8262
Epoch: 0045 | loss_train: 3.9460 loss_val: 4.4768 | f1_val: 0.06 | rmse_val: 1.3244
Epoch: 0046 | loss_train: 4.6653 loss_val: 5.0002 | f1_val: 0.07 | rmse_val: 0.8056
Epoch: 0047 | loss_train: 3.7538 loss_val: 3.0838 | f1_val: 0.06 | rmse_val: 0.9713
Epoch: 0048 | loss_train: 4.1921 loss_val: 5.0626 | f1_val: 0.05 | rmse_val: 0.9365
Epoch: 0049 | loss_train: 4.1949 loss_val: 4.5003 | f1_val: 0.05 | rmse_val: 1.0904
Epoch: 0050 | loss_train: 4.2082 loss_val: 3.9320 | f1_val: 0.05 | rmse_val: 0.9959
Epoch: 0051 | loss_train: 3.8193 loss_val: 4.5787 | f1_val: 0.06 | rmse_val: 1.1821
Epoch: 0052 | loss_train: 4.0778 loss_val: 3.9744 | f1_val: 0.07 | rmse_val: 0.8705
Epoch: 0053 | loss_train: 4.0199 loss_val: 3.7392 | f1_val: 0.06 | rmse_val: 1.0563
Epoch: 0054 | loss_train: 4.2362 loss_val: 3.3837 | f1_val: 0.06 | rmse_val: 0.9763
Epoch: 0055 | loss_train: 3.8206 loss_val: 3.8002 | f1_val: 0.06 | rmse_val: 1.1193
Epoch: 0056 | loss_train: 4.3948 loss_val: 3.4329 | f1_val: 0.06 | rmse_val: 0.8084
Epoch: 0057 | loss_train: 3.6747 loss_val: 3.7342 | f1_val: 0.05 | rmse_val: 0.9778
Epoch: 0058 | loss_train: 3.7791 loss_val: 2.8908 | f1_val: 0.05 | rmse_val: 0.8838
Epoch: 0059 | loss_train: 3.8188 loss_val: 4.8629 | f1_val: 0.07 | rmse_val: 1.1661
Epoch: 0060 | loss_train: 4.0049 loss_val: 2.9809 | f1_val: 0.06 | rmse_val: 1.0977
Epoch: 0061 | loss_train: 3.6898 loss_val: 4.1487 | f1_val: 0.06 | rmse_val: 1.1740
Epoch: 0062 | loss_train: 4.0641 loss_val: 4.8803 | f1_val: 0.05 | rmse_val: 0.9319
Epoch: 0063 | loss_train: 3.8224 loss_val: 2.8291 | f1_val: 0.07 | rmse_val: 1.2708
Epoch: 0064 | loss_train: 4.2175 loss_val: 4.7383 | f1_val: 0.05 | rmse_val: 1.0179
Epoch: 0065 | loss_train: 4.0327 loss_val: 3.1292 | f1_val: 0.04 | rmse_val: 0.9096
Optimization Finished!
Train cost: 811.4835s
Loading 29th epoch
f1_test: 0.04 | rmse_test: 0.9096

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.07 | rmse_val: 3.3741
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.06 | rmse_val: 2.9943
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.07 | rmse_val: 1.7410
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.07 | rmse_val: 1.1327
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.06 | rmse_val: 0.9800
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.06 | rmse_val: 1.0872
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.06 | rmse_val: 1.1652
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.07 | rmse_val: 1.1518
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.08 | rmse_val: 1.2344
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.07 | rmse_val: 1.3389
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.07 | rmse_val: 1.3391
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.06 | rmse_val: 0.9978
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.06 | rmse_val: 1.2773
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.08 | rmse_val: 1.1648
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.08 | rmse_val: 1.2310
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.06 | rmse_val: 1.4026
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.10 | rmse_val: 1.1990
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.06 | rmse_val: 1.2226
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.06 | rmse_val: 1.1331
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.06 | rmse_val: 1.1435
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.06 | rmse_val: 1.1230
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.06 | rmse_val: 1.7135
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.06 | rmse_val: 1.7576
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.07 | rmse_val: 1.1399
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.07 | rmse_val: 1.2766
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.07 | rmse_val: 1.1888
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.07 | rmse_val: 1.6370
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.07 | rmse_val: 1.7470
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.06 | rmse_val: 1.6875
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.06 | rmse_val: 1.5689
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.07 | rmse_val: 1.0698
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.08 | rmse_val: 1.0978
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.06 | rmse_val: 1.2972
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.07 | rmse_val: 1.1596
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.07 | rmse_val: 1.8801
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.08 | rmse_val: 1.0040
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.09 | rmse_val: 1.1605
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.08 | rmse_val: 1.1127
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.07 | rmse_val: 1.3111
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.08 | rmse_val: 1.0541
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.07 | rmse_val: 0.9032
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.06 | rmse_val: 1.1433
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.05 | rmse_val: 1.2399
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.07 | rmse_val: 0.9962
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.08 | rmse_val: 1.1889
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.08 | rmse_val: 1.0389
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.06 | rmse_val: 1.1820
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.06 | rmse_val: 1.0117
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.07 | rmse_val: 1.2784
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.05 | rmse_val: 1.0770
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.07 | rmse_val: 1.0232
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.07 | rmse_val: 1.1284
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.07 | rmse_val: 1.1607
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.05 | rmse_val: 1.0688
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.06 | rmse_val: 1.1700
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.06 | rmse_val: 1.1267
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.07 | rmse_val: 1.1829
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.05 | rmse_val: 0.9879
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.06 | rmse_val: 0.9795
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.06 | rmse_val: 1.1594
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.06 | rmse_val: 0.8617
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.07 | rmse_val: 1.0771
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.06 | rmse_val: 0.9418
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.06 | rmse_val: 1.1806
Optimization Finished!
Train cost: 931.9387s
Loading 17th epoch
f1_test: 0.06 | rmse_test: 1.1806

>>> run.py: Namespace(dataset='LINUX', device=1, experiment='training-data', log_path='log/nagphormer/LINUX/training-data', path='data/nagphormer', plot_path='plots/nagphormer/LINUX/training-data', pyg_path='data/pyg/LINUX')

>>> Training split = 0.2

>>> Training split = 0.4

>>> Training split = 0.6

>>> Training split = 0.8

>>> Training split = 1.0

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=0.2, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 86.4368 loss_val: 86.2494 | f1_val: 0.05 | rmse_val: 7.6888
Epoch: 0002 | loss_train: 91.1244 loss_val: 83.8849 | f1_val: 0.04 | rmse_val: 7.8835
Epoch: 0003 | loss_train: 92.3820 loss_val: 78.8466 | f1_val: 0.05 | rmse_val: 7.5724
Epoch: 0004 | loss_train: 85.8962 loss_val: 88.1948 | f1_val: 0.05 | rmse_val: 7.9235
Epoch: 0005 | loss_train: 91.5793 loss_val: 77.2828 | f1_val: 0.04 | rmse_val: 7.8901
Epoch: 0006 | loss_train: 84.4352 loss_val: 82.8196 | f1_val: 0.05 | rmse_val: 7.6068
Epoch: 0007 | loss_train: 76.3778 loss_val: 77.1009 | f1_val: 0.04 | rmse_val: 7.7477
Epoch: 0008 | loss_train: 75.2049 loss_val: 90.4026 | f1_val: 0.07 | rmse_val: 7.6320
Epoch: 0009 | loss_train: 74.4367 loss_val: 82.4746 | f1_val: 0.06 | rmse_val: 7.5066
Epoch: 0010 | loss_train: 80.4014 loss_val: 83.8435 | f1_val: 0.04 | rmse_val: 7.6509
Epoch: 0011 | loss_train: 68.5293 loss_val: 89.8261 | f1_val: 0.06 | rmse_val: 7.5516
Epoch: 0012 | loss_train: 78.0232 loss_val: 63.1389 | f1_val: 0.04 | rmse_val: 7.1568
Epoch: 0013 | loss_train: 63.3493 loss_val: 91.4466 | f1_val: 0.04 | rmse_val: 7.1401
Epoch: 0014 | loss_train: 74.9713 loss_val: 62.7576 | f1_val: 0.04 | rmse_val: 7.0681
Epoch: 0015 | loss_train: 74.5089 loss_val: 56.8854 | f1_val: 0.05 | rmse_val: 6.9644
Epoch: 0016 | loss_train: 74.2713 loss_val: 61.8862 | f1_val: 0.06 | rmse_val: 6.8572
Epoch: 0017 | loss_train: 76.4468 loss_val: 63.6135 | f1_val: 0.05 | rmse_val: 6.6277
Epoch: 0018 | loss_train: 66.8893 loss_val: 62.3898 | f1_val: 0.08 | rmse_val: 6.3324
Epoch: 0019 | loss_train: 66.2299 loss_val: 59.4026 | f1_val: 0.04 | rmse_val: 6.1219
Epoch: 0020 | loss_train: 53.9348 loss_val: 37.0433 | f1_val: 0.08 | rmse_val: 5.8045
Epoch: 0021 | loss_train: 60.0597 loss_val: 53.8799 | f1_val: 0.07 | rmse_val: 5.3381
Epoch: 0022 | loss_train: 42.9918 loss_val: 49.1467 | f1_val: 0.06 | rmse_val: 5.3140
Epoch: 0023 | loss_train: 44.7857 loss_val: 44.3658 | f1_val: 0.06 | rmse_val: 4.7181
Epoch: 0024 | loss_train: 34.3228 loss_val: 39.6436 | f1_val: 0.04 | rmse_val: 4.2706
Epoch: 0025 | loss_train: 31.5259 loss_val: 25.7709 | f1_val: 0.05 | rmse_val: 3.7170
Epoch: 0026 | loss_train: 32.7590 loss_val: 17.1037 | f1_val: 0.06 | rmse_val: 2.9013
Epoch: 0027 | loss_train: 21.7777 loss_val: 17.1816 | f1_val: 0.03 | rmse_val: 1.7267
Epoch: 0028 | loss_train: 20.2126 loss_val: 8.8106 | f1_val: 0.05 | rmse_val: 1.2677
Epoch: 0029 | loss_train: 10.6042 loss_val: 10.2070 | f1_val: 0.05 | rmse_val: 1.1577
Epoch: 0030 | loss_train: 11.5585 loss_val: 6.1171 | f1_val: 0.05 | rmse_val: 1.0212
Epoch: 0031 | loss_train: 11.5643 loss_val: 6.3729 | f1_val: 0.03 | rmse_val: 1.3266
Epoch: 0032 | loss_train: 7.2250 loss_val: 8.7532 | f1_val: 0.04 | rmse_val: 1.7000
Epoch: 0033 | loss_train: 11.5187 loss_val: 8.9072 | f1_val: 0.07 | rmse_val: 2.1644
Epoch: 0034 | loss_train: 10.4303 loss_val: 8.2490 | f1_val: 0.06 | rmse_val: 2.3886
Epoch: 0035 | loss_train: 5.8330 loss_val: 6.1640 | f1_val: 0.04 | rmse_val: 2.4388
Epoch: 0036 | loss_train: 8.6590 loss_val: 9.4884 | f1_val: 0.05 | rmse_val: 2.4824
Epoch: 0037 | loss_train: 11.2489 loss_val: 12.3403 | f1_val: 0.05 | rmse_val: 2.4296
Epoch: 0038 | loss_train: 7.8056 loss_val: 7.8182 | f1_val: 0.05 | rmse_val: 2.1497
Epoch: 0039 | loss_train: 11.1275 loss_val: 10.6381 | f1_val: 0.06 | rmse_val: 1.7019
Epoch: 0040 | loss_train: 9.1249 loss_val: 4.9831 | f1_val: 0.05 | rmse_val: 1.6007
Epoch: 0041 | loss_train: 7.2856 loss_val: 6.7599 | f1_val: 0.05 | rmse_val: 1.2973
Epoch: 0042 | loss_train: 8.4805 loss_val: 4.7560 | f1_val: 0.05 | rmse_val: 0.9474
Epoch: 0043 | loss_train: 8.5952 loss_val: 11.0049 | f1_val: 0.07 | rmse_val: 1.0472
Epoch: 0044 | loss_train: 8.6077 loss_val: 5.6568 | f1_val: 0.06 | rmse_val: 1.3282
Epoch: 0045 | loss_train: 8.1884 loss_val: 7.4470 | f1_val: 0.05 | rmse_val: 1.4346
Epoch: 0046 | loss_train: 10.8640 loss_val: 5.3994 | f1_val: 0.06 | rmse_val: 1.6299
Epoch: 0047 | loss_train: 8.6152 loss_val: 8.2202 | f1_val: 0.05 | rmse_val: 1.7703
Epoch: 0048 | loss_train: 8.7448 loss_val: 7.4358 | f1_val: 0.06 | rmse_val: 1.9266
Epoch: 0049 | loss_train: 10.0670 loss_val: 8.1317 | f1_val: 0.07 | rmse_val: 2.0097
Epoch: 0050 | loss_train: 8.7302 loss_val: 6.5169 | f1_val: 0.06 | rmse_val: 1.7306
Epoch: 0051 | loss_train: 12.4325 loss_val: 8.1075 | f1_val: 0.03 | rmse_val: 1.7077
Epoch: 0052 | loss_train: 7.2983 loss_val: 8.0181 | f1_val: 0.05 | rmse_val: 1.8170
Epoch: 0053 | loss_train: 8.1148 loss_val: 9.3565 | f1_val: 0.06 | rmse_val: 1.4500
Epoch: 0054 | loss_train: 10.3596 loss_val: 6.4890 | f1_val: 0.07 | rmse_val: 1.2930
Epoch: 0055 | loss_train: 8.7258 loss_val: 9.1216 | f1_val: 0.06 | rmse_val: 1.1875
Epoch: 0056 | loss_train: 13.5421 loss_val: 7.2972 | f1_val: 0.05 | rmse_val: 1.0653
Epoch: 0057 | loss_train: 9.9708 loss_val: 6.7568 | f1_val: 0.04 | rmse_val: 1.2530
Epoch: 0058 | loss_train: 9.5877 loss_val: 5.2672 | f1_val: 0.08 | rmse_val: 1.4673
Epoch: 0059 | loss_train: 12.0155 loss_val: 6.6156 | f1_val: 0.07 | rmse_val: 1.7495
Epoch: 0060 | loss_train: 11.2033 loss_val: 8.2075 | f1_val: 0.07 | rmse_val: 1.5997
Epoch: 0061 | loss_train: 10.2273 loss_val: 6.5101 | f1_val: 0.06 | rmse_val: 1.7480
Epoch: 0062 | loss_train: 10.0185 loss_val: 9.8967 | f1_val: 0.08 | rmse_val: 1.5817
Epoch: 0063 | loss_train: 9.0241 loss_val: 9.1363 | f1_val: 0.06 | rmse_val: 1.4132
Epoch: 0064 | loss_train: 6.2305 loss_val: 8.6384 | f1_val: 0.06 | rmse_val: 1.6562
Epoch: 0065 | loss_train: 8.0912 loss_val: 6.1360 | f1_val: 0.07 | rmse_val: 1.3075
Epoch: 0066 | loss_train: 13.7131 loss_val: 6.7235 | f1_val: 0.06 | rmse_val: 1.3607
Epoch: 0067 | loss_train: 10.2626 loss_val: 9.4842 | f1_val: 0.08 | rmse_val: 1.1043
Epoch: 0068 | loss_train: 10.9179 loss_val: 5.0082 | f1_val: 0.06 | rmse_val: 1.2506
Epoch: 0069 | loss_train: 10.3046 loss_val: 8.0060 | f1_val: 0.08 | rmse_val: 1.4753
Epoch: 0070 | loss_train: 7.7149 loss_val: 9.1870 | f1_val: 0.06 | rmse_val: 1.5256
Epoch: 0071 | loss_train: 9.1417 loss_val: 7.9373 | f1_val: 0.05 | rmse_val: 1.9194
Epoch: 0072 | loss_train: 9.7567 loss_val: 6.0844 | f1_val: 0.05 | rmse_val: 1.8881
Optimization Finished!
Train cost: 354.3861s
Loading 20th epoch
f1_test: 0.05 | rmse_test: 1.8881

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=0.4, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 80.8962 loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0002 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0003 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0004 | loss_train: nan loss_val: nan | f1_val: 0.08 | rmse_val: nan
Epoch: 0005 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0006 | loss_train: nan loss_val: nan | f1_val: 0.08 | rmse_val: nan
Epoch: 0007 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0008 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0009 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0010 | loss_train: nan loss_val: nan | f1_val: 0.08 | rmse_val: nan
Epoch: 0011 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0012 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0013 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0014 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0015 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0016 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0017 | loss_train: nan loss_val: nan | f1_val: 0.08 | rmse_val: nan
Epoch: 0018 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0019 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0020 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0021 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0022 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0023 | loss_train: nan loss_val: nan | f1_val: 0.09 | rmse_val: nan
Epoch: 0024 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0025 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0026 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0027 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0028 | loss_train: nan loss_val: nan | f1_val: 0.09 | rmse_val: nan
Epoch: 0029 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0030 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0031 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0032 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0033 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0034 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0035 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0036 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0037 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0038 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0039 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0040 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0041 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0042 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0043 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0044 | loss_train: nan loss_val: nan | f1_val: 0.06 | rmse_val: nan
Epoch: 0045 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0046 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0047 | loss_train: nan loss_val: nan | f1_val: 0.04 | rmse_val: nan
Epoch: 0048 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0049 | loss_train: nan loss_val: nan | f1_val: 0.05 | rmse_val: nan
Epoch: 0050 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0051 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0052 | loss_train: nan loss_val: nan | f1_val: 0.07 | rmse_val: nan
Epoch: 0053 | loss_train: nan loss_val: nan | f1_val: 0.08 | rmse_val: nan
Optimization Finished!
Train cost: 323.6481s
Loading 23th epoch
f1_test: 0.08 | rmse_test: nan

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=0.6, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 80.8465 loss_val: 86.8311 | f1_val: 0.05 | rmse_val: 7.4896
Epoch: 0002 | loss_train: 78.9404 loss_val: 79.2801 | f1_val: 0.04 | rmse_val: 7.3916
Epoch: 0003 | loss_train: 74.9677 loss_val: 77.8413 | f1_val: 0.04 | rmse_val: 7.4463
Epoch: 0004 | loss_train: 79.8537 loss_val: 70.1343 | f1_val: 0.05 | rmse_val: 6.9655
Epoch: 0005 | loss_train: 73.4142 loss_val: 60.4874 | f1_val: 0.03 | rmse_val: 6.8028
Epoch: 0006 | loss_train: 68.9045 loss_val: 68.8779 | f1_val: 0.03 | rmse_val: 6.1205
Epoch: 0007 | loss_train: 56.1308 loss_val: 50.6454 | f1_val: 0.06 | rmse_val: 5.2984
Epoch: 0008 | loss_train: 45.7964 loss_val: 35.6969 | f1_val: 0.06 | rmse_val: 3.9710
Epoch: 0009 | loss_train: 29.2563 loss_val: 22.1941 | f1_val: 0.02 | rmse_val: 2.1321
Epoch: 0010 | loss_train: 14.0677 loss_val: 9.6184 | f1_val: 0.07 | rmse_val: 0.7900
Epoch: 0011 | loss_train: 8.9307 loss_val: 9.9141 | f1_val: 0.06 | rmse_val: 1.8790
Epoch: 0012 | loss_train: 8.8695 loss_val: 7.3075 | f1_val: 0.07 | rmse_val: 2.2260
Epoch: 0013 | loss_train: 6.3666 loss_val: 6.1833 | f1_val: 0.04 | rmse_val: 1.5511
Epoch: 0014 | loss_train: 8.5051 loss_val: 5.9712 | f1_val: 0.05 | rmse_val: 1.0314
Epoch: 0015 | loss_train: 7.0803 loss_val: 10.0169 | f1_val: 0.05 | rmse_val: 1.4573
Epoch: 0016 | loss_train: 6.8022 loss_val: 7.3342 | f1_val: 0.05 | rmse_val: 1.4251
Epoch: 0017 | loss_train: 6.5778 loss_val: 7.9230 | f1_val: 0.04 | rmse_val: 1.4400
Epoch: 0018 | loss_train: 6.6182 loss_val: 4.0214 | f1_val: 0.06 | rmse_val: 1.5524
Epoch: 0019 | loss_train: 8.8396 loss_val: 8.6548 | f1_val: 0.06 | rmse_val: 1.3804
Epoch: 0020 | loss_train: 7.1838 loss_val: 8.6041 | f1_val: 0.07 | rmse_val: 1.2569
Epoch: 0021 | loss_train: 8.5548 loss_val: 9.2149 | f1_val: 0.06 | rmse_val: 1.1603
Epoch: 0022 | loss_train: 6.2871 loss_val: 9.6172 | f1_val: 0.05 | rmse_val: 1.2881
Epoch: 0023 | loss_train: 6.4944 loss_val: 7.4503 | f1_val: 0.05 | rmse_val: 1.5860
Epoch: 0024 | loss_train: 6.9808 loss_val: 10.8312 | f1_val: 0.06 | rmse_val: 1.1398
Epoch: 0025 | loss_train: 6.1878 loss_val: 7.6991 | f1_val: 0.06 | rmse_val: 0.9391
Epoch: 0026 | loss_train: 6.4587 loss_val: 8.7997 | f1_val: 0.07 | rmse_val: 0.9614
Epoch: 0027 | loss_train: 7.3202 loss_val: 10.4916 | f1_val: 0.08 | rmse_val: 1.3066
Epoch: 0028 | loss_train: 7.3113 loss_val: 6.7839 | f1_val: 0.07 | rmse_val: 2.1281
Epoch: 0029 | loss_train: 7.0161 loss_val: 7.0462 | f1_val: 0.07 | rmse_val: 1.5584
Epoch: 0030 | loss_train: 6.0297 loss_val: 6.4773 | f1_val: 0.04 | rmse_val: 1.4131
Epoch: 0031 | loss_train: 6.3903 loss_val: 8.0109 | f1_val: 0.05 | rmse_val: 1.3920
Epoch: 0032 | loss_train: 7.5539 loss_val: 9.0838 | f1_val: 0.06 | rmse_val: 1.4442
Epoch: 0033 | loss_train: 7.3526 loss_val: 7.2876 | f1_val: 0.07 | rmse_val: 1.4172
Epoch: 0034 | loss_train: 7.1298 loss_val: 9.9407 | f1_val: 0.08 | rmse_val: 1.9535
Epoch: 0035 | loss_train: 7.1483 loss_val: 8.0721 | f1_val: 0.06 | rmse_val: 1.4747
Epoch: 0036 | loss_train: 7.0595 loss_val: 9.8598 | f1_val: 0.05 | rmse_val: 1.0590
Epoch: 0037 | loss_train: 7.1863 loss_val: 6.9839 | f1_val: 0.07 | rmse_val: 1.5717
Epoch: 0038 | loss_train: 6.7937 loss_val: 7.2570 | f1_val: 0.07 | rmse_val: 1.1644
Epoch: 0039 | loss_train: 7.1470 loss_val: 5.8028 | f1_val: 0.06 | rmse_val: 0.9252
Epoch: 0040 | loss_train: 6.5696 loss_val: 11.1214 | f1_val: 0.06 | rmse_val: 0.9427
Epoch: 0041 | loss_train: 8.0920 loss_val: 4.5616 | f1_val: 0.06 | rmse_val: 1.1233
Epoch: 0042 | loss_train: 6.1808 loss_val: 8.8804 | f1_val: 0.07 | rmse_val: 1.7800
Epoch: 0043 | loss_train: 6.6615 loss_val: 7.3750 | f1_val: 0.07 | rmse_val: 1.3465
Epoch: 0044 | loss_train: 7.7447 loss_val: 8.3673 | f1_val: 0.07 | rmse_val: 1.4006
Epoch: 0045 | loss_train: 6.6720 loss_val: 6.9614 | f1_val: 0.10 | rmse_val: 0.9004
Epoch: 0046 | loss_train: 7.4076 loss_val: 8.7809 | f1_val: 0.05 | rmse_val: 1.6853
Epoch: 0047 | loss_train: 7.6566 loss_val: 6.0564 | f1_val: 0.07 | rmse_val: 0.8507
Epoch: 0048 | loss_train: 9.8216 loss_val: 6.2517 | f1_val: 0.07 | rmse_val: 2.0742
Epoch: 0049 | loss_train: 9.2858 loss_val: 5.7606 | f1_val: 0.06 | rmse_val: 2.3153
Epoch: 0050 | loss_train: 6.7886 loss_val: 9.8527 | f1_val: 0.05 | rmse_val: 1.0036
Epoch: 0051 | loss_train: 7.0767 loss_val: 6.8549 | f1_val: 0.07 | rmse_val: 1.3554
Epoch: 0052 | loss_train: 7.1041 loss_val: 6.4286 | f1_val: 0.08 | rmse_val: 1.3726
Epoch: 0053 | loss_train: 6.7595 loss_val: 8.8401 | f1_val: 0.07 | rmse_val: 0.7820
Epoch: 0054 | loss_train: 8.5543 loss_val: 6.3895 | f1_val: 0.06 | rmse_val: 1.8603
Epoch: 0055 | loss_train: 7.3392 loss_val: 10.4408 | f1_val: 0.07 | rmse_val: 0.8674
Epoch: 0056 | loss_train: 7.9041 loss_val: 10.9843 | f1_val: 0.08 | rmse_val: 1.2160
Epoch: 0057 | loss_train: 8.2474 loss_val: 3.4307 | f1_val: 0.06 | rmse_val: 1.3288
Epoch: 0058 | loss_train: 6.5885 loss_val: 4.4541 | f1_val: 0.07 | rmse_val: 0.8997
Epoch: 0059 | loss_train: 5.9454 loss_val: 8.8598 | f1_val: 0.07 | rmse_val: 1.8622
Epoch: 0060 | loss_train: 6.4439 loss_val: 3.4868 | f1_val: 0.07 | rmse_val: 1.1233
Epoch: 0061 | loss_train: 6.4326 loss_val: 3.3059 | f1_val: 0.06 | rmse_val: 1.2386
Epoch: 0062 | loss_train: 5.4089 loss_val: 6.8646 | f1_val: 0.06 | rmse_val: 1.5323
Epoch: 0063 | loss_train: 5.3806 loss_val: 5.9037 | f1_val: 0.07 | rmse_val: 1.1413
Epoch: 0064 | loss_train: 6.6480 loss_val: 12.0067 | f1_val: 0.06 | rmse_val: 1.3254
Epoch: 0065 | loss_train: 43.2917 loss_val: 55.2491 | f1_val: 0.05 | rmse_val: 5.6307
Epoch: 0066 | loss_train: 33.6830 loss_val: 11.0754 | f1_val: 0.04 | rmse_val: 2.0714
Epoch: 0067 | loss_train: 16.8053 loss_val: 11.3808 | f1_val: 0.06 | rmse_val: 1.0553
Epoch: 0068 | loss_train: 11.7072 loss_val: 6.9998 | f1_val: 0.05 | rmse_val: 1.5996
Epoch: 0069 | loss_train: 8.0161 loss_val: 9.0865 | f1_val: 0.06 | rmse_val: 1.5321
Epoch: 0070 | loss_train: 9.0738 loss_val: 8.0784 | f1_val: 0.07 | rmse_val: 0.8489
Epoch: 0071 | loss_train: 7.1803 loss_val: 7.6500 | f1_val: 0.06 | rmse_val: 1.5354
Epoch: 0072 | loss_train: 6.6365 loss_val: 7.1520 | f1_val: 0.06 | rmse_val: 1.2818
Epoch: 0073 | loss_train: 6.0150 loss_val: 6.8575 | f1_val: 0.04 | rmse_val: 1.3788
Epoch: 0074 | loss_train: 6.5330 loss_val: 4.3844 | f1_val: 0.05 | rmse_val: 1.6699
Epoch: 0075 | loss_train: 5.4518 loss_val: 4.4412 | f1_val: 0.03 | rmse_val: 1.0717
Epoch: 0076 | loss_train: 5.6941 loss_val: 4.6598 | f1_val: 0.07 | rmse_val: 1.4798
Epoch: 0077 | loss_train: 5.4562 loss_val: 6.8593 | f1_val: 0.06 | rmse_val: 1.6455
Epoch: 0078 | loss_train: 5.7233 loss_val: 6.8390 | f1_val: 0.06 | rmse_val: 0.9884
Epoch: 0079 | loss_train: 7.1697 loss_val: 4.6437 | f1_val: 0.05 | rmse_val: 1.7063
Epoch: 0080 | loss_train: 5.5478 loss_val: 7.1355 | f1_val: 0.06 | rmse_val: 1.6164
Epoch: 0081 | loss_train: 6.1471 loss_val: 8.0139 | f1_val: 0.05 | rmse_val: 0.9287
Epoch: 0082 | loss_train: 6.7110 loss_val: 8.9701 | f1_val: 0.07 | rmse_val: 0.7575
Epoch: 0083 | loss_train: 4.5111 loss_val: 4.0890 | f1_val: 0.05 | rmse_val: 2.0028
Epoch: 0084 | loss_train: 5.4292 loss_val: 5.2401 | f1_val: 0.07 | rmse_val: 1.0114
Epoch: 0085 | loss_train: 6.3101 loss_val: 8.7388 | f1_val: 0.09 | rmse_val: 0.7798
Epoch: 0086 | loss_train: 8.0116 loss_val: 8.6450 | f1_val: 0.06 | rmse_val: 1.2574
Epoch: 0087 | loss_train: 7.4971 loss_val: 9.4538 | f1_val: 0.06 | rmse_val: 1.8171
Epoch: 0088 | loss_train: 6.1243 loss_val: 5.8027 | f1_val: 0.07 | rmse_val: 1.1631
Epoch: 0089 | loss_train: 5.1570 loss_val: 5.9923 | f1_val: 0.05 | rmse_val: 1.4594
Epoch: 0090 | loss_train: 4.7331 loss_val: 4.3387 | f1_val: 0.06 | rmse_val: 1.7853
Epoch: 0091 | loss_train: 6.4102 loss_val: 4.9759 | f1_val: 0.06 | rmse_val: 1.4612
Optimization Finished!
Train cost: 608.6359s
Loading 45th epoch
f1_test: 0.06 | rmse_test: 1.4612

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=0.8, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.8169 loss_val: 78.7172 | f1_val: 0.03 | rmse_val: 7.5776
Epoch: 0002 | loss_train: 76.7888 loss_val: 78.6957 | f1_val: 0.05 | rmse_val: 7.2614
Epoch: 0003 | loss_train: 78.6547 loss_val: 86.3034 | f1_val: 0.03 | rmse_val: 7.2742
Epoch: 0004 | loss_train: 72.5483 loss_val: 69.8821 | f1_val: 0.05 | rmse_val: 6.7417
Epoch: 0005 | loss_train: 62.8729 loss_val: 67.1625 | f1_val: 0.03 | rmse_val: 6.0531
Epoch: 0006 | loss_train: 52.6783 loss_val: 50.5685 | f1_val: 0.05 | rmse_val: 4.7901
Epoch: 0007 | loss_train: 34.2719 loss_val: 17.8084 | f1_val: 0.06 | rmse_val: 2.8777
Epoch: 0008 | loss_train: 14.5342 loss_val: 7.2715 | f1_val: 0.07 | rmse_val: 0.8930
Epoch: 0009 | loss_train: 7.0007 loss_val: 6.7536 | f1_val: 0.05 | rmse_val: 2.0298
Epoch: 0010 | loss_train: 7.3933 loss_val: 8.1243 | f1_val: 0.06 | rmse_val: 1.8349
Epoch: 0011 | loss_train: 6.4771 loss_val: 7.3860 | f1_val: 0.05 | rmse_val: 1.2072
Epoch: 0012 | loss_train: 6.5101 loss_val: 3.5277 | f1_val: 0.05 | rmse_val: 1.4878
Epoch: 0013 | loss_train: 7.0875 loss_val: 8.0730 | f1_val: 0.06 | rmse_val: 1.4706
Epoch: 0014 | loss_train: 5.7266 loss_val: 9.8656 | f1_val: 0.06 | rmse_val: 1.6495
Epoch: 0015 | loss_train: 6.2887 loss_val: 8.3579 | f1_val: 0.06 | rmse_val: 1.3605
Epoch: 0016 | loss_train: 6.0469 loss_val: 10.0744 | f1_val: 0.07 | rmse_val: 1.2636
Epoch: 0017 | loss_train: 7.2348 loss_val: 9.0912 | f1_val: 0.07 | rmse_val: 1.5020
Epoch: 0018 | loss_train: 7.4186 loss_val: 10.5598 | f1_val: 0.06 | rmse_val: 1.6304
Epoch: 0019 | loss_train: 7.0973 loss_val: 3.2484 | f1_val: 0.05 | rmse_val: 1.0161
Epoch: 0020 | loss_train: 6.6131 loss_val: 5.6636 | f1_val: 0.08 | rmse_val: 1.7364
Epoch: 0021 | loss_train: 6.6877 loss_val: 7.6846 | f1_val: 0.07 | rmse_val: 1.4842
Epoch: 0022 | loss_train: 6.0520 loss_val: 7.2713 | f1_val: 0.08 | rmse_val: 1.1557
Epoch: 0023 | loss_train: 7.0523 loss_val: 8.3980 | f1_val: 0.06 | rmse_val: 1.4282
Epoch: 0024 | loss_train: 7.0147 loss_val: 6.5270 | f1_val: 0.04 | rmse_val: 2.2802
Epoch: 0025 | loss_train: 10.1931 loss_val: 10.4067 | f1_val: 0.05 | rmse_val: 1.1198
Epoch: 0026 | loss_train: 8.3535 loss_val: 8.2059 | f1_val: 0.07 | rmse_val: 1.3413
Epoch: 0027 | loss_train: 6.7042 loss_val: 7.2570 | f1_val: 0.08 | rmse_val: 1.3536
Epoch: 0028 | loss_train: 6.3102 loss_val: 7.2015 | f1_val: 0.04 | rmse_val: 1.2865
Epoch: 0029 | loss_train: 6.3043 loss_val: 8.4700 | f1_val: 0.07 | rmse_val: 1.3365
Epoch: 0030 | loss_train: 6.7082 loss_val: 7.5483 | f1_val: 0.04 | rmse_val: 1.6406
Epoch: 0031 | loss_train: 6.7847 loss_val: 6.2049 | f1_val: 0.05 | rmse_val: 1.1961
Epoch: 0032 | loss_train: 6.1717 loss_val: 2.9190 | f1_val: 0.07 | rmse_val: 1.5732
Epoch: 0033 | loss_train: 7.6919 loss_val: 6.5446 | f1_val: 0.06 | rmse_val: 1.1579
Epoch: 0034 | loss_train: 6.2847 loss_val: 9.0450 | f1_val: 0.08 | rmse_val: 1.1110
Epoch: 0035 | loss_train: 7.1231 loss_val: 10.0184 | f1_val: 0.06 | rmse_val: 1.5315
Epoch: 0036 | loss_train: 5.9989 loss_val: 4.8482 | f1_val: 0.07 | rmse_val: 1.2103
Epoch: 0037 | loss_train: 5.4659 loss_val: 5.9558 | f1_val: 0.06 | rmse_val: 1.4081
Epoch: 0038 | loss_train: 6.0254 loss_val: 10.0223 | f1_val: 0.06 | rmse_val: 0.9636
Epoch: 0039 | loss_train: 6.7671 loss_val: 6.6830 | f1_val: 0.05 | rmse_val: 1.2535
Epoch: 0040 | loss_train: 5.9666 loss_val: 7.4667 | f1_val: 0.07 | rmse_val: 1.2267
Epoch: 0041 | loss_train: 5.6853 loss_val: 8.1956 | f1_val: 0.05 | rmse_val: 1.0873
Epoch: 0042 | loss_train: 6.1842 loss_val: 5.9478 | f1_val: 0.07 | rmse_val: 0.8852
Epoch: 0043 | loss_train: 6.9411 loss_val: 6.3273 | f1_val: 0.07 | rmse_val: 1.8265
Epoch: 0044 | loss_train: 6.4014 loss_val: 6.1508 | f1_val: 0.08 | rmse_val: 0.9869
Epoch: 0045 | loss_train: 5.7982 loss_val: 7.7193 | f1_val: 0.06 | rmse_val: 1.0243
Epoch: 0046 | loss_train: 6.3919 loss_val: 6.0810 | f1_val: 0.04 | rmse_val: 1.7237
Epoch: 0047 | loss_train: 5.4055 loss_val: 8.0179 | f1_val: 0.04 | rmse_val: 1.1300
Epoch: 0048 | loss_train: 5.8837 loss_val: 8.0500 | f1_val: 0.07 | rmse_val: 1.1008
Epoch: 0049 | loss_train: 5.6126 loss_val: 8.8510 | f1_val: 0.07 | rmse_val: 1.4546
Epoch: 0050 | loss_train: 4.5622 loss_val: 8.7357 | f1_val: 0.06 | rmse_val: 0.9491
Epoch: 0051 | loss_train: 6.0816 loss_val: 10.2083 | f1_val: 0.07 | rmse_val: 1.5538
Epoch: 0052 | loss_train: 6.6691 loss_val: 8.0740 | f1_val: 0.07 | rmse_val: 1.1708
Epoch: 0053 | loss_train: 6.4545 loss_val: 7.7994 | f1_val: 0.06 | rmse_val: 0.9865
Epoch: 0054 | loss_train: 6.4251 loss_val: 7.0216 | f1_val: 0.08 | rmse_val: 2.0002
Epoch: 0055 | loss_train: 6.4186 loss_val: 6.8194 | f1_val: 0.06 | rmse_val: 1.4676
Epoch: 0056 | loss_train: 7.0670 loss_val: 7.9461 | f1_val: 0.06 | rmse_val: 0.7764
Epoch: 0057 | loss_train: 5.8061 loss_val: 4.5346 | f1_val: 0.05 | rmse_val: 1.2892
Epoch: 0058 | loss_train: 6.0701 loss_val: 4.8079 | f1_val: 0.08 | rmse_val: 1.4206
Epoch: 0059 | loss_train: 6.7291 loss_val: 5.5175 | f1_val: 0.07 | rmse_val: 1.3899
Epoch: 0060 | loss_train: 5.1949 loss_val: 4.9799 | f1_val: 0.06 | rmse_val: 0.9948
Epoch: 0061 | loss_train: 5.5859 loss_val: 6.1839 | f1_val: 0.07 | rmse_val: 1.6125
Epoch: 0062 | loss_train: 6.2333 loss_val: 8.0401 | f1_val: 0.08 | rmse_val: 0.7466
Epoch: 0063 | loss_train: 6.6738 loss_val: 4.9178 | f1_val: 0.06 | rmse_val: 1.0114
Epoch: 0064 | loss_train: 6.2757 loss_val: 4.5632 | f1_val: 0.07 | rmse_val: 1.4372
Epoch: 0065 | loss_train: 5.9399 loss_val: 4.9371 | f1_val: 0.07 | rmse_val: 0.9646
Epoch: 0066 | loss_train: 6.6606 loss_val: 7.3294 | f1_val: 0.06 | rmse_val: 1.4772
Epoch: 0067 | loss_train: 4.6647 loss_val: 2.8669 | f1_val: 0.06 | rmse_val: 1.1496
Epoch: 0068 | loss_train: 5.0799 loss_val: 5.7302 | f1_val: 0.05 | rmse_val: 1.4585
Epoch: 0069 | loss_train: 5.1264 loss_val: 6.4869 | f1_val: 0.06 | rmse_val: 1.3534
Epoch: 0070 | loss_train: 5.6346 loss_val: 3.7630 | f1_val: 0.07 | rmse_val: 1.3469
Epoch: 0071 | loss_train: 6.9901 loss_val: 10.6709 | f1_val: 0.06 | rmse_val: 0.7783
Epoch: 0072 | loss_train: 7.2426 loss_val: 7.1304 | f1_val: 0.08 | rmse_val: 1.5569
Epoch: 0073 | loss_train: 6.1577 loss_val: 3.6674 | f1_val: 0.06 | rmse_val: 1.1118
Epoch: 0074 | loss_train: 5.7119 loss_val: 7.0024 | f1_val: 0.05 | rmse_val: 1.4668
Epoch: 0075 | loss_train: 7.2459 loss_val: 6.3836 | f1_val: 0.08 | rmse_val: 1.2145
Epoch: 0076 | loss_train: 5.5193 loss_val: 9.2441 | f1_val: 0.06 | rmse_val: 0.9536
Epoch: 0077 | loss_train: 6.0401 loss_val: 4.9372 | f1_val: 0.06 | rmse_val: 1.4114
Epoch: 0078 | loss_train: 5.7578 loss_val: 4.2087 | f1_val: 0.07 | rmse_val: 0.9262
Epoch: 0079 | loss_train: 5.1316 loss_val: 4.6719 | f1_val: 0.08 | rmse_val: 1.3104
Epoch: 0080 | loss_train: 4.8021 loss_val: 8.3284 | f1_val: 0.05 | rmse_val: 1.0622
Epoch: 0081 | loss_train: 6.9259 loss_val: 5.4309 | f1_val: 0.05 | rmse_val: 1.1259
Epoch: 0082 | loss_train: 6.0207 loss_val: 6.2222 | f1_val: 0.07 | rmse_val: 1.2011
Epoch: 0083 | loss_train: 6.6664 loss_val: 3.8045 | f1_val: 0.06 | rmse_val: 0.7726
Epoch: 0084 | loss_train: 5.1329 loss_val: 7.2394 | f1_val: 0.06 | rmse_val: 1.5479
Epoch: 0085 | loss_train: 5.3360 loss_val: 6.5355 | f1_val: 0.06 | rmse_val: 1.1876
Epoch: 0086 | loss_train: 5.2115 loss_val: 8.3038 | f1_val: 0.07 | rmse_val: 1.1685
Epoch: 0087 | loss_train: 5.3163 loss_val: 8.8153 | f1_val: 0.07 | rmse_val: 1.5080
Epoch: 0088 | loss_train: 5.0002 loss_val: 6.5690 | f1_val: 0.06 | rmse_val: 1.4111
Epoch: 0089 | loss_train: 5.5892 loss_val: 7.6424 | f1_val: 0.05 | rmse_val: 1.0605
Epoch: 0090 | loss_train: 5.2329 loss_val: 5.6137 | f1_val: 0.05 | rmse_val: 1.1934
Epoch: 0091 | loss_train: 4.8368 loss_val: 5.6683 | f1_val: 0.07 | rmse_val: 1.5074
Epoch: 0092 | loss_train: 5.7736 loss_val: 4.2977 | f1_val: 0.07 | rmse_val: 1.1325
Epoch: 0093 | loss_train: 5.2061 loss_val: 5.6551 | f1_val: 0.07 | rmse_val: 1.0091
Epoch: 0094 | loss_train: 4.7974 loss_val: 8.4717 | f1_val: 0.06 | rmse_val: 1.0144
Epoch: 0095 | loss_train: 6.0114 loss_val: 13.1214 | f1_val: 0.05 | rmse_val: 1.0479
Epoch: 0096 | loss_train: 5.6111 loss_val: 3.5442 | f1_val: 0.05 | rmse_val: 0.9776
Epoch: 0097 | loss_train: 5.7261 loss_val: 8.7477 | f1_val: 0.07 | rmse_val: 1.3717
Optimization Finished!
Train cost: 596.4756s
Loading 58th epoch
f1_test: 0.07 | rmse_test: 1.3717

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/training-data', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.05 | rmse_val: 7.4451
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 7.4450
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.08 | rmse_val: 6.8450
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.03 | rmse_val: 6.0641
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.05 | rmse_val: 4.4422
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.06 | rmse_val: 1.9609
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.06 | rmse_val: 1.4814
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.06 | rmse_val: 2.2505
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.07 | rmse_val: 1.4667
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.07 | rmse_val: 1.3041
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.6533
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.06 | rmse_val: 1.0961
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.06 | rmse_val: 1.5658
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.05 | rmse_val: 1.5932
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.05 | rmse_val: 1.1988
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.04 | rmse_val: 1.2835
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.05 | rmse_val: 1.7141
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.06 | rmse_val: 1.2771
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.05 | rmse_val: 0.9793
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.08 | rmse_val: 1.8279
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.07 | rmse_val: 1.3268
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.04 | rmse_val: 1.8338
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.06 | rmse_val: 1.1240
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.03 | rmse_val: 1.8121
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.07 | rmse_val: 1.6883
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.09 | rmse_val: 0.7529
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.07 | rmse_val: 1.7207
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.07 | rmse_val: 1.3494
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.04 | rmse_val: 0.8378
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.07 | rmse_val: 1.1355
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.06 | rmse_val: 0.8232
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.06 | rmse_val: 1.9919
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.07 | rmse_val: 1.4889
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.05 | rmse_val: 0.7824
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.05 | rmse_val: 2.1047
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.06 | rmse_val: 1.1257
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.04 | rmse_val: 1.1966
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.07 | rmse_val: 1.5883
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.06 | rmse_val: 2.1858
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.04 | rmse_val: 1.4801
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.07 | rmse_val: 0.8435
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.06 | rmse_val: 1.0060
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.06 | rmse_val: 1.1348
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.07 | rmse_val: 2.5679
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.07 | rmse_val: 2.5789
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.04 | rmse_val: 1.4546
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.08 | rmse_val: 1.9236
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.06 | rmse_val: 1.3427
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.08 | rmse_val: 1.5619
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.06 | rmse_val: 1.4774
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.05 | rmse_val: 1.6202
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.05 | rmse_val: 1.0127
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.05 | rmse_val: 1.4628
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.05 | rmse_val: 1.4804
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.08 | rmse_val: 1.2230
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.06 | rmse_val: 1.5972
Optimization Finished!
Train cost: 372.0367s
Loading 26th epoch
f1_test: 0.06 | rmse_test: 1.5972

>>> run.py: Namespace(dataset='AIDS700nef', device=1, experiment='training-data', log_path='log/nagphormer/AIDS700nef/training-data', path='data/nagphormer', plot_path='plots/nagphormer/AIDS700nef/training-data', pyg_path='data/pyg/AIDS700nef')

>>> Training split = 0.2

>>> Training split = 0.4

>>> Training split = 0.6

>>> Training split = 0.8

>>> Training split = 1.0

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=1, log_path='log/nagphormer/LINUX/hops', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.7183 loss_val: 18.6308 | f1_val: 0.05 | rmse_val: 3.3773
Epoch: 0002 | loss_train: 18.3984 loss_val: 14.7327 | f1_val: 0.04 | rmse_val: 3.0053
Epoch: 0003 | loss_train: 13.5621 loss_val: 11.2706 | f1_val: 0.07 | rmse_val: 1.6216
Epoch: 0004 | loss_train: 5.5490 loss_val: 4.8485 | f1_val: 0.06 | rmse_val: 1.4193
Epoch: 0005 | loss_train: 4.3287 loss_val: 4.3171 | f1_val: 0.05 | rmse_val: 0.9536
Epoch: 0006 | loss_train: 4.2676 loss_val: 4.0913 | f1_val: 0.06 | rmse_val: 1.0835
Epoch: 0007 | loss_train: 4.3574 loss_val: 4.2114 | f1_val: 0.06 | rmse_val: 1.1652
Epoch: 0008 | loss_train: 3.7907 loss_val: 5.0153 | f1_val: 0.06 | rmse_val: 1.1515
Epoch: 0009 | loss_train: 4.2835 loss_val: 5.9351 | f1_val: 0.07 | rmse_val: 1.2368
Epoch: 0010 | loss_train: 4.1340 loss_val: 5.5015 | f1_val: 0.07 | rmse_val: 1.3375
Epoch: 0011 | loss_train: 4.5579 loss_val: 4.4264 | f1_val: 0.05 | rmse_val: 1.3362
Epoch: 0012 | loss_train: 4.3063 loss_val: 3.8434 | f1_val: 0.06 | rmse_val: 1.0593
Epoch: 0013 | loss_train: 4.4305 loss_val: 4.3752 | f1_val: 0.06 | rmse_val: 1.2827
Epoch: 0014 | loss_train: 4.4900 loss_val: 4.5184 | f1_val: 0.08 | rmse_val: 1.1263
Epoch: 0015 | loss_train: 4.3118 loss_val: 4.0233 | f1_val: 0.05 | rmse_val: 1.2109
Epoch: 0016 | loss_train: 4.3188 loss_val: 5.4415 | f1_val: 0.07 | rmse_val: 1.4146
Epoch: 0017 | loss_train: 4.5651 loss_val: 4.0683 | f1_val: 0.07 | rmse_val: 1.2040
Epoch: 0018 | loss_train: 4.9300 loss_val: 5.3878 | f1_val: 0.06 | rmse_val: 1.2255
Epoch: 0019 | loss_train: 4.4515 loss_val: 4.8370 | f1_val: 0.06 | rmse_val: 1.1325
Epoch: 0020 | loss_train: 4.3415 loss_val: 4.0106 | f1_val: 0.08 | rmse_val: 1.1406
Epoch: 0021 | loss_train: 4.1153 loss_val: 4.7960 | f1_val: 0.05 | rmse_val: 1.1053
Epoch: 0022 | loss_train: 4.4274 loss_val: 6.1922 | f1_val: 0.06 | rmse_val: 1.7001
Epoch: 0023 | loss_train: 3.9832 loss_val: 5.4429 | f1_val: 0.06 | rmse_val: 1.7440
Epoch: 0024 | loss_train: 4.7310 loss_val: 3.9543 | f1_val: 0.07 | rmse_val: 1.0545
Epoch: 0025 | loss_train: 4.1106 loss_val: 5.9680 | f1_val: 0.06 | rmse_val: 1.4700
Epoch: 0026 | loss_train: 3.9596 loss_val: 4.0284 | f1_val: 0.06 | rmse_val: 1.4162
Epoch: 0027 | loss_train: 4.6700 loss_val: 4.6524 | f1_val: 0.07 | rmse_val: 1.6974
Epoch: 0028 | loss_train: 4.7600 loss_val: 6.0877 | f1_val: 0.06 | rmse_val: 1.6183
Epoch: 0029 | loss_train: 4.4352 loss_val: 3.8732 | f1_val: 0.08 | rmse_val: 1.4389
Epoch: 0030 | loss_train: 4.9476 loss_val: 3.6464 | f1_val: 0.06 | rmse_val: 1.3749
Epoch: 0031 | loss_train: 4.2956 loss_val: 4.0584 | f1_val: 0.06 | rmse_val: 0.9957
Epoch: 0032 | loss_train: 4.0922 loss_val: 4.4374 | f1_val: 0.07 | rmse_val: 1.1314
Epoch: 0033 | loss_train: 4.2245 loss_val: 3.6447 | f1_val: 0.06 | rmse_val: 1.2930
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3743 | f1_val: 0.08 | rmse_val: 1.1391
Epoch: 0035 | loss_train: 4.2619 loss_val: 5.2550 | f1_val: 0.07 | rmse_val: 1.8602
Epoch: 0036 | loss_train: 4.9957 loss_val: 3.4311 | f1_val: 0.07 | rmse_val: 1.0065
Epoch: 0037 | loss_train: 4.4894 loss_val: 3.4718 | f1_val: 0.06 | rmse_val: 1.1810
Epoch: 0038 | loss_train: 4.3922 loss_val: 4.4969 | f1_val: 0.07 | rmse_val: 1.1273
Epoch: 0039 | loss_train: 4.3707 loss_val: 4.4645 | f1_val: 0.06 | rmse_val: 1.3178
Epoch: 0040 | loss_train: 4.5613 loss_val: 4.5393 | f1_val: 0.08 | rmse_val: 1.0640
Epoch: 0041 | loss_train: 4.4296 loss_val: 5.6538 | f1_val: 0.07 | rmse_val: 0.9171
Epoch: 0042 | loss_train: 4.1962 loss_val: 3.7916 | f1_val: 0.05 | rmse_val: 1.1541
Epoch: 0043 | loss_train: 4.2461 loss_val: 4.6681 | f1_val: 0.06 | rmse_val: 1.2499
Epoch: 0044 | loss_train: 4.3159 loss_val: 3.5087 | f1_val: 0.05 | rmse_val: 1.0028
Epoch: 0045 | loss_train: 4.1880 loss_val: 4.4349 | f1_val: 0.08 | rmse_val: 1.1915
Epoch: 0046 | loss_train: 4.8551 loss_val: 3.6980 | f1_val: 0.05 | rmse_val: 1.0546
Epoch: 0047 | loss_train: 4.0501 loss_val: 5.3157 | f1_val: 0.06 | rmse_val: 1.1862
Epoch: 0048 | loss_train: 4.3127 loss_val: 5.8924 | f1_val: 0.05 | rmse_val: 1.0245
Epoch: 0049 | loss_train: 4.1737 loss_val: 3.8466 | f1_val: 0.06 | rmse_val: 1.2773
Epoch: 0050 | loss_train: 4.2834 loss_val: 4.8555 | f1_val: 0.05 | rmse_val: 1.0912
Epoch: 0051 | loss_train: 4.0496 loss_val: 3.9169 | f1_val: 0.06 | rmse_val: 1.0300
Epoch: 0052 | loss_train: 3.9175 loss_val: 3.5916 | f1_val: 0.06 | rmse_val: 1.1439
Epoch: 0053 | loss_train: 4.0366 loss_val: 4.8168 | f1_val: 0.05 | rmse_val: 1.1740
Epoch: 0054 | loss_train: 4.6486 loss_val: 5.2425 | f1_val: 0.06 | rmse_val: 1.0834
Epoch: 0055 | loss_train: 4.7385 loss_val: 4.7423 | f1_val: 0.07 | rmse_val: 1.1851
Epoch: 0056 | loss_train: 3.8693 loss_val: 3.3971 | f1_val: 0.06 | rmse_val: 1.1436
Epoch: 0057 | loss_train: 4.5220 loss_val: 3.7888 | f1_val: 0.06 | rmse_val: 1.1248
Epoch: 0058 | loss_train: 3.8462 loss_val: 3.4747 | f1_val: 0.05 | rmse_val: 1.0380
Epoch: 0059 | loss_train: 4.0969 loss_val: 3.8934 | f1_val: 0.07 | rmse_val: 0.9952
Epoch: 0060 | loss_train: 4.4159 loss_val: 3.9003 | f1_val: 0.06 | rmse_val: 1.1300
Epoch: 0061 | loss_train: 4.2099 loss_val: 5.4855 | f1_val: 0.05 | rmse_val: 0.9470
Epoch: 0062 | loss_train: 4.3737 loss_val: 3.7285 | f1_val: 0.06 | rmse_val: 1.0475
Epoch: 0063 | loss_train: 4.2035 loss_val: 4.7101 | f1_val: 0.07 | rmse_val: 1.0209
Epoch: 0064 | loss_train: 4.2125 loss_val: 4.2582 | f1_val: 0.05 | rmse_val: 1.1574
Optimization Finished!
Train cost: 743.1220s
Loading 34th epoch
f1_test: 0.05 | rmse_test: 1.1574

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=2, log_path='log/nagphormer/LINUX/hops', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6924 loss_val: 18.6095 | f1_val: 0.06 | rmse_val: 3.3740
Epoch: 0002 | loss_train: 18.3276 loss_val: 14.6855 | f1_val: 0.05 | rmse_val: 2.9962
Epoch: 0003 | loss_train: 13.6359 loss_val: 11.7944 | f1_val: 0.05 | rmse_val: 1.7074
Epoch: 0004 | loss_train: 6.1112 loss_val: 4.4722 | f1_val: 0.07 | rmse_val: 1.2527
Epoch: 0005 | loss_train: 4.2870 loss_val: 4.2929 | f1_val: 0.06 | rmse_val: 0.9702
Epoch: 0006 | loss_train: 4.2800 loss_val: 4.0901 | f1_val: 0.07 | rmse_val: 1.0745
Epoch: 0007 | loss_train: 4.3563 loss_val: 4.2114 | f1_val: 0.06 | rmse_val: 1.1661
Epoch: 0008 | loss_train: 3.7909 loss_val: 5.0126 | f1_val: 0.07 | rmse_val: 1.1501
Epoch: 0009 | loss_train: 4.2844 loss_val: 5.9356 | f1_val: 0.09 | rmse_val: 1.2333
Epoch: 0010 | loss_train: 4.1348 loss_val: 5.4874 | f1_val: 0.06 | rmse_val: 1.3288
Epoch: 0011 | loss_train: 4.5537 loss_val: 4.4246 | f1_val: 0.06 | rmse_val: 1.3220
Epoch: 0012 | loss_train: 4.3088 loss_val: 3.8825 | f1_val: 0.05 | rmse_val: 0.9912
Epoch: 0013 | loss_train: 4.4043 loss_val: 4.3711 | f1_val: 0.07 | rmse_val: 1.2755
Epoch: 0014 | loss_train: 4.4679 loss_val: 4.5523 | f1_val: 0.06 | rmse_val: 1.1623
Epoch: 0015 | loss_train: 4.3187 loss_val: 4.0394 | f1_val: 0.07 | rmse_val: 1.2317
Epoch: 0016 | loss_train: 4.3339 loss_val: 5.4210 | f1_val: 0.07 | rmse_val: 1.3915
Epoch: 0017 | loss_train: 4.5601 loss_val: 4.0553 | f1_val: 0.08 | rmse_val: 1.1734
Epoch: 0018 | loss_train: 4.9268 loss_val: 5.3973 | f1_val: 0.08 | rmse_val: 1.2008
Epoch: 0019 | loss_train: 4.4700 loss_val: 4.8491 | f1_val: 0.06 | rmse_val: 1.1154
Epoch: 0020 | loss_train: 4.3447 loss_val: 4.0354 | f1_val: 0.06 | rmse_val: 1.1307
Epoch: 0021 | loss_train: 4.1177 loss_val: 4.7970 | f1_val: 0.06 | rmse_val: 1.1298
Epoch: 0022 | loss_train: 4.4207 loss_val: 6.2205 | f1_val: 0.06 | rmse_val: 1.7131
Epoch: 0023 | loss_train: 4.0131 loss_val: 5.4389 | f1_val: 0.05 | rmse_val: 1.7419
Epoch: 0024 | loss_train: 4.7311 loss_val: 3.9526 | f1_val: 0.08 | rmse_val: 1.0532
Epoch: 0025 | loss_train: 4.0973 loss_val: 6.1207 | f1_val: 0.06 | rmse_val: 1.5488
Epoch: 0026 | loss_train: 3.9713 loss_val: 4.0802 | f1_val: 0.08 | rmse_val: 1.4846
Epoch: 0027 | loss_train: 4.7411 loss_val: 4.6432 | f1_val: 0.06 | rmse_val: 1.6963
Epoch: 0028 | loss_train: 4.7875 loss_val: 6.0825 | f1_val: 0.07 | rmse_val: 1.6204
Epoch: 0029 | loss_train: 4.4250 loss_val: 3.8692 | f1_val: 0.07 | rmse_val: 1.4421
Epoch: 0030 | loss_train: 4.9398 loss_val: 3.6502 | f1_val: 0.06 | rmse_val: 1.3640
Epoch: 0031 | loss_train: 4.2800 loss_val: 4.0560 | f1_val: 0.06 | rmse_val: 0.9899
Epoch: 0032 | loss_train: 4.0940 loss_val: 4.4481 | f1_val: 0.07 | rmse_val: 1.1452
Epoch: 0033 | loss_train: 4.2317 loss_val: 3.6384 | f1_val: 0.06 | rmse_val: 1.2814
Epoch: 0034 | loss_train: 4.2024 loss_val: 3.3677 | f1_val: 0.08 | rmse_val: 1.1290
Epoch: 0035 | loss_train: 4.2442 loss_val: 5.2228 | f1_val: 0.07 | rmse_val: 1.8477
Epoch: 0036 | loss_train: 4.9731 loss_val: 3.4339 | f1_val: 0.08 | rmse_val: 1.0046
Epoch: 0037 | loss_train: 4.4826 loss_val: 3.4801 | f1_val: 0.06 | rmse_val: 1.1923
Epoch: 0038 | loss_train: 4.3955 loss_val: 4.4921 | f1_val: 0.08 | rmse_val: 1.1308
Epoch: 0039 | loss_train: 4.3728 loss_val: 4.4620 | f1_val: 0.07 | rmse_val: 1.3153
Epoch: 0040 | loss_train: 4.5543 loss_val: 4.5376 | f1_val: 0.08 | rmse_val: 1.0642
Epoch: 0041 | loss_train: 4.4279 loss_val: 5.6308 | f1_val: 0.07 | rmse_val: 0.9294
Epoch: 0042 | loss_train: 4.1966 loss_val: 3.7919 | f1_val: 0.06 | rmse_val: 1.1594
Epoch: 0043 | loss_train: 4.2422 loss_val: 4.6689 | f1_val: 0.04 | rmse_val: 1.2542
Epoch: 0044 | loss_train: 4.3139 loss_val: 3.5065 | f1_val: 0.06 | rmse_val: 1.0069
Epoch: 0045 | loss_train: 4.1870 loss_val: 4.4205 | f1_val: 0.08 | rmse_val: 1.2016
Epoch: 0046 | loss_train: 4.8497 loss_val: 3.6967 | f1_val: 0.08 | rmse_val: 1.0554
Epoch: 0047 | loss_train: 4.0524 loss_val: 5.3126 | f1_val: 0.06 | rmse_val: 1.1833
Epoch: 0048 | loss_train: 4.3114 loss_val: 5.8896 | f1_val: 0.06 | rmse_val: 1.0262
Epoch: 0049 | loss_train: 4.1797 loss_val: 3.8445 | f1_val: 0.05 | rmse_val: 1.2700
Epoch: 0050 | loss_train: 4.2797 loss_val: 4.8570 | f1_val: 0.07 | rmse_val: 1.0939
Epoch: 0051 | loss_train: 4.0527 loss_val: 3.9177 | f1_val: 0.06 | rmse_val: 1.0252
Epoch: 0052 | loss_train: 3.9218 loss_val: 3.5927 | f1_val: 0.04 | rmse_val: 1.1428
Epoch: 0053 | loss_train: 4.0423 loss_val: 4.8119 | f1_val: 0.06 | rmse_val: 1.1700
Epoch: 0054 | loss_train: 4.6533 loss_val: 5.2421 | f1_val: 0.07 | rmse_val: 1.0802
Epoch: 0055 | loss_train: 4.7389 loss_val: 4.7445 | f1_val: 0.06 | rmse_val: 1.1832
Epoch: 0056 | loss_train: 3.8688 loss_val: 3.3977 | f1_val: 0.06 | rmse_val: 1.1448
Epoch: 0057 | loss_train: 4.5289 loss_val: 3.7942 | f1_val: 0.06 | rmse_val: 1.1183
Epoch: 0058 | loss_train: 3.8528 loss_val: 3.4782 | f1_val: 0.06 | rmse_val: 1.0403
Epoch: 0059 | loss_train: 4.0985 loss_val: 3.8995 | f1_val: 0.08 | rmse_val: 0.9991
Epoch: 0060 | loss_train: 4.4198 loss_val: 3.9059 | f1_val: 0.05 | rmse_val: 1.1219
Epoch: 0061 | loss_train: 4.2128 loss_val: 5.4924 | f1_val: 0.07 | rmse_val: 0.9494
Epoch: 0062 | loss_train: 4.3758 loss_val: 3.7362 | f1_val: 0.06 | rmse_val: 1.0459
Epoch: 0063 | loss_train: 4.2102 loss_val: 4.7187 | f1_val: 0.07 | rmse_val: 1.0218
Epoch: 0064 | loss_train: 4.2181 loss_val: 4.2606 | f1_val: 0.06 | rmse_val: 1.1559
Optimization Finished!
Train cost: 728.4886s
Loading 9th epoch
f1_test: 0.06 | rmse_test: 1.1559

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/hops', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.07 | rmse_val: 3.3741
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.06 | rmse_val: 2.9943
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.07 | rmse_val: 1.7410
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.07 | rmse_val: 1.1327
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.06 | rmse_val: 0.9800
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.06 | rmse_val: 1.0872
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.06 | rmse_val: 1.1652
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.07 | rmse_val: 1.1518
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.08 | rmse_val: 1.2344
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.07 | rmse_val: 1.3389
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.07 | rmse_val: 1.3391
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.06 | rmse_val: 0.9978
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.06 | rmse_val: 1.2773
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.08 | rmse_val: 1.1648
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.08 | rmse_val: 1.2310
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.06 | rmse_val: 1.4026
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.10 | rmse_val: 1.1990
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.06 | rmse_val: 1.2226
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.06 | rmse_val: 1.1331
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.06 | rmse_val: 1.1435
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.06 | rmse_val: 1.1230
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.06 | rmse_val: 1.7135
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.06 | rmse_val: 1.7576
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.07 | rmse_val: 1.1399
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.07 | rmse_val: 1.2766
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.07 | rmse_val: 1.1888
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.07 | rmse_val: 1.6370
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.07 | rmse_val: 1.7470
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.06 | rmse_val: 1.6875
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.06 | rmse_val: 1.5689
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.07 | rmse_val: 1.0698
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.08 | rmse_val: 1.0978
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.06 | rmse_val: 1.2972
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.07 | rmse_val: 1.1596
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.07 | rmse_val: 1.8801
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.08 | rmse_val: 1.0040
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.09 | rmse_val: 1.1605
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.08 | rmse_val: 1.1127
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.07 | rmse_val: 1.3111
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.08 | rmse_val: 1.0541
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.07 | rmse_val: 0.9032
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.06 | rmse_val: 1.1433
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.05 | rmse_val: 1.2399
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.07 | rmse_val: 0.9962
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.08 | rmse_val: 1.1889
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.08 | rmse_val: 1.0389
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.06 | rmse_val: 1.1820
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.06 | rmse_val: 1.0117
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.07 | rmse_val: 1.2784
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.05 | rmse_val: 1.0770
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.07 | rmse_val: 1.0232
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.07 | rmse_val: 1.1284
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.07 | rmse_val: 1.1607
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.05 | rmse_val: 1.0688
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.06 | rmse_val: 1.1700
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.06 | rmse_val: 1.1267
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.07 | rmse_val: 1.1829
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.05 | rmse_val: 0.9879
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.06 | rmse_val: 0.9795
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.06 | rmse_val: 1.1594
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.06 | rmse_val: 0.8617
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.07 | rmse_val: 1.0771
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.06 | rmse_val: 0.9418
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.06 | rmse_val: 1.1806
Optimization Finished!
Train cost: 570.9452s
Loading 17th epoch
f1_test: 0.06 | rmse_test: 1.1806

>>> run.py: Namespace(dataset='LINUX', device=1, experiment='hops', log_path='log/nagphormer/LINUX/hops', path='data/nagphormer', plot_path='plots/nagphormer/LINUX/hops', pyg_path='data/pyg/LINUX')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=1, log_path='log/nagphormer/AIDS700nef/hops', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5826 loss_val: 78.2028 | f1_val: 0.05 | rmse_val: 7.4408
Epoch: 0002 | loss_train: 79.4730 loss_val: 91.0242 | f1_val: 0.07 | rmse_val: 7.4259
Epoch: 0003 | loss_train: 76.2573 loss_val: 67.8947 | f1_val: 0.06 | rmse_val: 6.7963
Epoch: 0004 | loss_train: 64.1982 loss_val: 58.2019 | f1_val: 0.03 | rmse_val: 5.9956
Epoch: 0005 | loss_train: 48.5970 loss_val: 43.3655 | f1_val: 0.06 | rmse_val: 4.3602
Epoch: 0006 | loss_train: 29.7560 loss_val: 15.0334 | f1_val: 0.07 | rmse_val: 1.8656
Epoch: 0007 | loss_train: 10.3700 loss_val: 4.6998 | f1_val: 0.07 | rmse_val: 1.5549
Epoch: 0008 | loss_train: 7.4769 loss_val: 3.9915 | f1_val: 0.07 | rmse_val: 2.2140
Epoch: 0009 | loss_train: 5.7136 loss_val: 7.1483 | f1_val: 0.04 | rmse_val: 1.2208
Epoch: 0010 | loss_train: 6.9042 loss_val: 8.7679 | f1_val: 0.09 | rmse_val: 1.6132
Epoch: 0011 | loss_train: 7.7457 loss_val: 6.8780 | f1_val: 0.04 | rmse_val: 1.6082
Epoch: 0012 | loss_train: 6.4020 loss_val: 7.7624 | f1_val: 0.05 | rmse_val: 1.0344
Epoch: 0013 | loss_train: 6.5371 loss_val: 7.8612 | f1_val: 0.07 | rmse_val: 1.6527
Epoch: 0014 | loss_train: 7.7536 loss_val: 10.2851 | f1_val: 0.05 | rmse_val: 1.5365
Epoch: 0015 | loss_train: 7.5403 loss_val: 7.9328 | f1_val: 0.06 | rmse_val: 1.2148
Epoch: 0016 | loss_train: 6.5734 loss_val: 4.2064 | f1_val: 0.05 | rmse_val: 1.2391
Epoch: 0017 | loss_train: 7.4925 loss_val: 7.0761 | f1_val: 0.05 | rmse_val: 1.7286
Epoch: 0018 | loss_train: 6.6485 loss_val: 6.6161 | f1_val: 0.05 | rmse_val: 1.1043
Epoch: 0019 | loss_train: 5.7238 loss_val: 9.4265 | f1_val: 0.06 | rmse_val: 0.8953
Epoch: 0020 | loss_train: 6.6025 loss_val: 8.9846 | f1_val: 0.06 | rmse_val: 1.9137
Epoch: 0021 | loss_train: 7.1913 loss_val: 5.8387 | f1_val: 0.07 | rmse_val: 1.3354
Epoch: 0022 | loss_train: 6.2606 loss_val: 8.0684 | f1_val: 0.03 | rmse_val: 1.8153
Epoch: 0023 | loss_train: 6.9055 loss_val: 3.7508 | f1_val: 0.10 | rmse_val: 1.4035
Epoch: 0024 | loss_train: 5.8636 loss_val: 7.1397 | f1_val: 0.06 | rmse_val: 1.7830
Epoch: 0025 | loss_train: 6.2672 loss_val: 7.0916 | f1_val: 0.07 | rmse_val: 1.3982
Epoch: 0026 | loss_train: 7.4849 loss_val: 8.8928 | f1_val: 0.08 | rmse_val: 0.6956
Epoch: 0027 | loss_train: 7.0253 loss_val: 8.1538 | f1_val: 0.08 | rmse_val: 1.8829
Epoch: 0028 | loss_train: 5.8668 loss_val: 9.1435 | f1_val: 0.06 | rmse_val: 1.4378
Epoch: 0029 | loss_train: 5.6782 loss_val: 7.7910 | f1_val: 0.05 | rmse_val: 1.2221
Epoch: 0030 | loss_train: 5.6546 loss_val: 7.3956 | f1_val: 0.05 | rmse_val: 1.1129
Epoch: 0031 | loss_train: 6.1173 loss_val: 7.6316 | f1_val: 0.05 | rmse_val: 0.8429
Epoch: 0032 | loss_train: 7.7667 loss_val: 7.5642 | f1_val: 0.06 | rmse_val: 2.1166
Epoch: 0033 | loss_train: 6.9915 loss_val: 8.6463 | f1_val: 0.07 | rmse_val: 1.4452
Epoch: 0034 | loss_train: 5.9983 loss_val: 4.8152 | f1_val: 0.06 | rmse_val: 0.7703
Epoch: 0035 | loss_train: 6.0064 loss_val: 6.6052 | f1_val: 0.04 | rmse_val: 2.1250
Epoch: 0036 | loss_train: 5.4138 loss_val: 6.1619 | f1_val: 0.04 | rmse_val: 1.1082
Epoch: 0037 | loss_train: 5.6102 loss_val: 7.8107 | f1_val: 0.04 | rmse_val: 1.2661
Epoch: 0038 | loss_train: 6.4648 loss_val: 4.5322 | f1_val: 0.06 | rmse_val: 1.7123
Epoch: 0039 | loss_train: 5.2631 loss_val: 7.7254 | f1_val: 0.05 | rmse_val: 2.1810
Epoch: 0040 | loss_train: 5.3814 loss_val: 6.8219 | f1_val: 0.06 | rmse_val: 1.2821
Epoch: 0041 | loss_train: 5.8380 loss_val: 7.5999 | f1_val: 0.05 | rmse_val: 0.9401
Epoch: 0042 | loss_train: 5.4241 loss_val: 5.6166 | f1_val: 0.07 | rmse_val: 0.9745
Epoch: 0043 | loss_train: 6.0018 loss_val: 5.3965 | f1_val: 0.06 | rmse_val: 1.0489
Epoch: 0044 | loss_train: 6.2279 loss_val: 9.2791 | f1_val: 0.06 | rmse_val: 2.5747
Epoch: 0045 | loss_train: 6.7185 loss_val: 8.7820 | f1_val: 0.05 | rmse_val: 2.6384
Epoch: 0046 | loss_train: 24.0246 loss_val: 14.8991 | f1_val: 0.04 | rmse_val: 1.7078
Epoch: 0047 | loss_train: 9.9574 loss_val: 6.4613 | f1_val: 0.06 | rmse_val: 1.3500
Epoch: 0048 | loss_train: 7.3971 loss_val: 10.1586 | f1_val: 0.05 | rmse_val: 1.7228
Epoch: 0049 | loss_train: 6.2881 loss_val: 4.8661 | f1_val: 0.06 | rmse_val: 1.0645
Epoch: 0050 | loss_train: 6.0859 loss_val: 4.3975 | f1_val: 0.06 | rmse_val: 1.5672
Epoch: 0051 | loss_train: 5.6349 loss_val: 4.9658 | f1_val: 0.06 | rmse_val: 1.4788
Epoch: 0052 | loss_train: 7.6495 loss_val: 7.5214 | f1_val: 0.04 | rmse_val: 0.9569
Epoch: 0053 | loss_train: 7.3545 loss_val: 8.6025 | f1_val: 0.08 | rmse_val: 1.1527
Optimization Finished!
Train cost: 273.8907s
Loading 23th epoch
f1_test: 0.08 | rmse_test: 1.1527

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=2, log_path='log/nagphormer/AIDS700nef/hops', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5641 loss_val: 78.2420 | f1_val: 0.05 | rmse_val: 7.4430
Epoch: 0002 | loss_train: 79.5004 loss_val: 91.2265 | f1_val: 0.05 | rmse_val: 7.4348
Epoch: 0003 | loss_train: 76.3936 loss_val: 68.1997 | f1_val: 0.07 | rmse_val: 6.8147
Epoch: 0004 | loss_train: 64.4386 loss_val: 58.5612 | f1_val: 0.02 | rmse_val: 6.0199
Epoch: 0005 | loss_train: 48.8390 loss_val: 43.5673 | f1_val: 0.05 | rmse_val: 4.3768
Epoch: 0006 | loss_train: 29.7942 loss_val: 15.0513 | f1_val: 0.05 | rmse_val: 1.8674
Epoch: 0007 | loss_train: 10.3394 loss_val: 4.7091 | f1_val: 0.04 | rmse_val: 1.5603
Epoch: 0008 | loss_train: 7.5085 loss_val: 4.0180 | f1_val: 0.06 | rmse_val: 2.2256
Epoch: 0009 | loss_train: 5.7349 loss_val: 6.9829 | f1_val: 0.06 | rmse_val: 1.2648
Epoch: 0010 | loss_train: 6.9270 loss_val: 8.8479 | f1_val: 0.08 | rmse_val: 1.5317
Epoch: 0011 | loss_train: 7.7490 loss_val: 6.8264 | f1_val: 0.05 | rmse_val: 1.6343
Epoch: 0012 | loss_train: 6.4314 loss_val: 7.7674 | f1_val: 0.05 | rmse_val: 1.0506
Epoch: 0013 | loss_train: 6.5542 loss_val: 7.8787 | f1_val: 0.06 | rmse_val: 1.6149
Epoch: 0014 | loss_train: 7.7773 loss_val: 10.3434 | f1_val: 0.05 | rmse_val: 1.5677
Epoch: 0015 | loss_train: 7.5441 loss_val: 7.9921 | f1_val: 0.06 | rmse_val: 1.2030
Epoch: 0016 | loss_train: 6.5767 loss_val: 4.2580 | f1_val: 0.04 | rmse_val: 1.2698
Epoch: 0017 | loss_train: 7.4963 loss_val: 7.0237 | f1_val: 0.05 | rmse_val: 1.7157
Epoch: 0018 | loss_train: 6.7341 loss_val: 6.6163 | f1_val: 0.06 | rmse_val: 1.1427
Epoch: 0019 | loss_train: 5.6578 loss_val: 9.5143 | f1_val: 0.04 | rmse_val: 0.8999
Epoch: 0020 | loss_train: 6.4674 loss_val: 8.7857 | f1_val: 0.07 | rmse_val: 1.7628
Epoch: 0021 | loss_train: 7.1250 loss_val: 5.6052 | f1_val: 0.07 | rmse_val: 1.3980
Epoch: 0022 | loss_train: 6.1109 loss_val: 7.6165 | f1_val: 0.05 | rmse_val: 1.6996
Epoch: 0023 | loss_train: 6.6958 loss_val: 3.6559 | f1_val: 0.07 | rmse_val: 1.2361
Epoch: 0024 | loss_train: 5.6767 loss_val: 6.6650 | f1_val: 0.05 | rmse_val: 1.7267
Epoch: 0025 | loss_train: 6.2174 loss_val: 6.7078 | f1_val: 0.07 | rmse_val: 1.6940
Epoch: 0026 | loss_train: 7.4259 loss_val: 8.5438 | f1_val: 0.08 | rmse_val: 0.7198
Epoch: 0027 | loss_train: 6.6400 loss_val: 7.7661 | f1_val: 0.07 | rmse_val: 1.7301
Epoch: 0028 | loss_train: 5.6011 loss_val: 8.7721 | f1_val: 0.07 | rmse_val: 1.3974
Epoch: 0029 | loss_train: 5.3656 loss_val: 7.2374 | f1_val: 0.04 | rmse_val: 1.2426
Epoch: 0030 | loss_train: 5.6447 loss_val: 7.3170 | f1_val: 0.06 | rmse_val: 1.0243
Epoch: 0031 | loss_train: 6.1705 loss_val: 7.1467 | f1_val: 0.04 | rmse_val: 0.8488
Epoch: 0032 | loss_train: 7.9919 loss_val: 7.3655 | f1_val: 0.05 | rmse_val: 2.1012
Epoch: 0033 | loss_train: 6.9896 loss_val: 8.6938 | f1_val: 0.08 | rmse_val: 1.4434
Epoch: 0034 | loss_train: 5.9421 loss_val: 4.8291 | f1_val: 0.03 | rmse_val: 0.7544
Epoch: 0035 | loss_train: 5.9796 loss_val: 6.0923 | f1_val: 0.06 | rmse_val: 2.0280
Epoch: 0036 | loss_train: 5.3333 loss_val: 5.8991 | f1_val: 0.05 | rmse_val: 1.1049
Epoch: 0037 | loss_train: 5.5116 loss_val: 7.3268 | f1_val: 0.05 | rmse_val: 1.2022
Epoch: 0038 | loss_train: 6.3598 loss_val: 4.1891 | f1_val: 0.06 | rmse_val: 1.6606
Epoch: 0039 | loss_train: 5.4367 loss_val: 7.5969 | f1_val: 0.06 | rmse_val: 2.1647
Epoch: 0040 | loss_train: 5.1776 loss_val: 6.5968 | f1_val: 0.06 | rmse_val: 1.5622
Epoch: 0041 | loss_train: 5.8226 loss_val: 7.4343 | f1_val: 0.05 | rmse_val: 0.9279
Epoch: 0042 | loss_train: 5.5494 loss_val: 5.1036 | f1_val: 0.07 | rmse_val: 1.2201
Epoch: 0043 | loss_train: 5.9807 loss_val: 5.3409 | f1_val: 0.05 | rmse_val: 1.1085
Epoch: 0044 | loss_train: 5.9273 loss_val: 8.4061 | f1_val: 0.05 | rmse_val: 2.3949
Epoch: 0045 | loss_train: 6.5147 loss_val: 9.2652 | f1_val: 0.07 | rmse_val: 2.7754
Epoch: 0046 | loss_train: 23.8635 loss_val: 15.5587 | f1_val: 0.04 | rmse_val: 1.8619
Epoch: 0047 | loss_train: 12.4840 loss_val: 7.9948 | f1_val: 0.07 | rmse_val: 2.4916
Epoch: 0048 | loss_train: 8.2318 loss_val: 10.1682 | f1_val: 0.05 | rmse_val: 1.1359
Epoch: 0049 | loss_train: 6.7221 loss_val: 5.8508 | f1_val: 0.07 | rmse_val: 1.6716
Epoch: 0050 | loss_train: 6.2427 loss_val: 4.5311 | f1_val: 0.07 | rmse_val: 1.2292
Epoch: 0051 | loss_train: 5.4533 loss_val: 5.5449 | f1_val: 0.08 | rmse_val: 1.6526
Epoch: 0052 | loss_train: 7.5951 loss_val: 8.9000 | f1_val: 0.05 | rmse_val: 0.8248
Epoch: 0053 | loss_train: 8.6714 loss_val: 7.7160 | f1_val: 0.07 | rmse_val: 1.9081
Optimization Finished!
Train cost: 279.1152s
Loading 10th epoch
f1_test: 0.07 | rmse_test: 1.9081

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/hops', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.05 | rmse_val: 7.4451
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 7.4450
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.08 | rmse_val: 6.8450
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.03 | rmse_val: 6.0641
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.05 | rmse_val: 4.4422
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.06 | rmse_val: 1.9609
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.06 | rmse_val: 1.4814
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.06 | rmse_val: 2.2505
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.07 | rmse_val: 1.4667
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.07 | rmse_val: 1.3041
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.6533
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.06 | rmse_val: 1.0961
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.06 | rmse_val: 1.5658
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.05 | rmse_val: 1.5932
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.05 | rmse_val: 1.1988
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.04 | rmse_val: 1.2835
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.05 | rmse_val: 1.7141
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.06 | rmse_val: 1.2771
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.05 | rmse_val: 0.9793
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.08 | rmse_val: 1.8279
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.07 | rmse_val: 1.3268
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.04 | rmse_val: 1.8338
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.06 | rmse_val: 1.1240
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.03 | rmse_val: 1.8121
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.07 | rmse_val: 1.6883
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.09 | rmse_val: 0.7529
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.07 | rmse_val: 1.7207
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.07 | rmse_val: 1.3494
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.04 | rmse_val: 0.8378
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.07 | rmse_val: 1.1355
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.06 | rmse_val: 0.8232
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.06 | rmse_val: 1.9919
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.07 | rmse_val: 1.4889
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.05 | rmse_val: 0.7824
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.05 | rmse_val: 2.1047
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.06 | rmse_val: 1.1257
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.04 | rmse_val: 1.1966
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.07 | rmse_val: 1.5883
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.06 | rmse_val: 2.1858
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.04 | rmse_val: 1.4801
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.07 | rmse_val: 0.8435
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.06 | rmse_val: 1.0060
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.06 | rmse_val: 1.1348
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.07 | rmse_val: 2.5679
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.07 | rmse_val: 2.5789
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.04 | rmse_val: 1.4546
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.08 | rmse_val: 1.9236
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.06 | rmse_val: 1.3427
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.08 | rmse_val: 1.5619
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.06 | rmse_val: 1.4774
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.05 | rmse_val: 1.6202
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.05 | rmse_val: 1.0127
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.05 | rmse_val: 1.4628
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.05 | rmse_val: 1.4804
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.08 | rmse_val: 1.2230
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.06 | rmse_val: 1.5972
Optimization Finished!
Train cost: 295.9149s
Loading 26th epoch
f1_test: 0.06 | rmse_test: 1.5972

>>> run.py: Namespace(dataset='AIDS700nef', device=1, experiment='hops', log_path='log/nagphormer/AIDS700nef/hops', path='data/nagphormer', plot_path='plots/nagphormer/AIDS700nef/hops', pyg_path='data/pyg/AIDS700nef')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/pe_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=1, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 0/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 149826

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/pe_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=2, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 9/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=2, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 149954
Epoch: 0001 | loss_train: 25.4380 loss_val: 20.0515 | f1_val: 0.06 | rmse_val: 3.2681
Epoch: 0002 | loss_train: 22.0528 loss_val: 15.1398 | f1_val: 0.05 | rmse_val: 2.4377
Epoch: 0003 | loss_train: 13.7410 loss_val: 8.0154 | f1_val: 0.05 | rmse_val: 1.0952
Epoch: 0004 | loss_train: 6.0716 loss_val: 6.7479 | f1_val: 0.06 | rmse_val: 1.7515
Epoch: 0005 | loss_train: 5.6634 loss_val: 4.3116 | f1_val: 0.04 | rmse_val: 1.2549
Epoch: 0006 | loss_train: 6.4677 loss_val: 4.6205 | f1_val: 0.04 | rmse_val: 1.3743
Epoch: 0007 | loss_train: 6.0497 loss_val: 6.2505 | f1_val: 0.05 | rmse_val: 1.2468
Epoch: 0008 | loss_train: 5.5217 loss_val: 6.4190 | f1_val: 0.06 | rmse_val: 1.1081
Epoch: 0009 | loss_train: 5.5888 loss_val: 6.7255 | f1_val: 0.05 | rmse_val: 1.3647
Epoch: 0010 | loss_train: 5.9252 loss_val: 6.2436 | f1_val: 0.05 | rmse_val: 1.5473
Epoch: 0011 | loss_train: 6.2761 loss_val: 6.3035 | f1_val: 0.05 | rmse_val: 1.6331
Epoch: 0012 | loss_train: 5.5678 loss_val: 3.8924 | f1_val: 0.04 | rmse_val: 1.1215
Epoch: 0013 | loss_train: 5.8513 loss_val: 5.4172 | f1_val: 0.06 | rmse_val: 1.3905
Epoch: 0014 | loss_train: 6.5822 loss_val: 5.2962 | f1_val: 0.06 | rmse_val: 1.3595
Epoch: 0015 | loss_train: 5.8441 loss_val: 5.1847 | f1_val: 0.06 | rmse_val: 1.2423
Epoch: 0016 | loss_train: 5.7357 loss_val: 6.9108 | f1_val: 0.04 | rmse_val: 1.2852
Epoch: 0017 | loss_train: 6.1539 loss_val: 4.5411 | f1_val: 0.05 | rmse_val: 1.1807
Epoch: 0018 | loss_train: 6.4058 loss_val: 5.3863 | f1_val: 0.06 | rmse_val: 1.1867
Epoch: 0019 | loss_train: 5.7993 loss_val: 5.8162 | f1_val: 0.05 | rmse_val: 1.4137
Epoch: 0020 | loss_train: 6.5062 loss_val: 4.5937 | f1_val: 0.04 | rmse_val: 1.1874
Epoch: 0021 | loss_train: 5.4881 loss_val: 5.6652 | f1_val: 0.05 | rmse_val: 1.0698
Epoch: 0022 | loss_train: 5.7821 loss_val: 7.8551 | f1_val: 0.04 | rmse_val: 1.8876
Epoch: 0023 | loss_train: 5.7436 loss_val: 6.0282 | f1_val: 0.05 | rmse_val: 1.9135
Epoch: 0024 | loss_train: 6.3194 loss_val: 6.7762 | f1_val: 0.06 | rmse_val: 1.2131
Epoch: 0025 | loss_train: 5.6489 loss_val: 5.6808 | f1_val: 0.06 | rmse_val: 1.0766
Epoch: 0026 | loss_train: 5.0535 loss_val: 5.9929 | f1_val: 0.05 | rmse_val: 1.2877
Epoch: 0027 | loss_train: 5.8818 loss_val: 5.4593 | f1_val: 0.06 | rmse_val: 1.6508
Epoch: 0028 | loss_train: 6.0313 loss_val: 7.2420 | f1_val: 0.03 | rmse_val: 1.9195
Epoch: 0029 | loss_train: 6.3727 loss_val: 5.2642 | f1_val: 0.06 | rmse_val: 1.8277
Epoch: 0030 | loss_train: 6.5173 loss_val: 4.6930 | f1_val: 0.05 | rmse_val: 1.9636
Epoch: 0031 | loss_train: 5.8008 loss_val: 4.7792 | f1_val: 0.04 | rmse_val: 1.2049
Epoch: 0032 | loss_train: 5.5194 loss_val: 5.2330 | f1_val: 0.06 | rmse_val: 1.4221
Epoch: 0033 | loss_train: 5.8217 loss_val: 4.1289 | f1_val: 0.04 | rmse_val: 1.4556
Epoch: 0034 | loss_train: 6.0421 loss_val: 3.9072 | f1_val: 0.06 | rmse_val: 1.5178
Epoch: 0035 | loss_train: 5.7368 loss_val: 7.1783 | f1_val: 0.05 | rmse_val: 2.1492
Epoch: 0036 | loss_train: 6.5716 loss_val: 4.9481 | f1_val: 0.04 | rmse_val: 1.0832
Epoch: 0037 | loss_train: 5.9232 loss_val: 4.3060 | f1_val: 0.05 | rmse_val: 1.4475
Epoch: 0038 | loss_train: 5.8050 loss_val: 4.7713 | f1_val: 0.06 | rmse_val: 1.1354
Epoch: 0039 | loss_train: 5.8293 loss_val: 6.3242 | f1_val: 0.04 | rmse_val: 1.4743
Epoch: 0040 | loss_train: 6.2640 loss_val: 6.0764 | f1_val: 0.05 | rmse_val: 1.2241
Epoch: 0041 | loss_train: 5.9750 loss_val: 6.4237 | f1_val: 0.07 | rmse_val: 1.0654
Epoch: 0042 | loss_train: 5.5568 loss_val: 4.3606 | f1_val: 0.06 | rmse_val: 1.3403
Epoch: 0043 | loss_train: 5.7843 loss_val: 5.4355 | f1_val: 0.05 | rmse_val: 1.3339
Epoch: 0044 | loss_train: 5.9272 loss_val: 4.4995 | f1_val: 0.04 | rmse_val: 1.1403
Epoch: 0045 | loss_train: 6.0168 loss_val: 5.0279 | f1_val: 0.05 | rmse_val: 1.1493
Epoch: 0046 | loss_train: 6.3902 loss_val: 4.8453 | f1_val: 0.05 | rmse_val: 1.1992
Epoch: 0047 | loss_train: 5.8255 loss_val: 6.3757 | f1_val: 0.05 | rmse_val: 1.3117
Epoch: 0048 | loss_train: 5.4049 loss_val: 6.3857 | f1_val: 0.06 | rmse_val: 1.0948
Epoch: 0049 | loss_train: 5.1962 loss_val: 4.0526 | f1_val: 0.05 | rmse_val: 1.3342
Epoch: 0050 | loss_train: 6.2150 loss_val: 5.5164 | f1_val: 0.05 | rmse_val: 1.3086
Epoch: 0051 | loss_train: 5.5760 loss_val: 4.6603 | f1_val: 0.06 | rmse_val: 1.1766
Epoch: 0052 | loss_train: 5.5902 loss_val: 3.7571 | f1_val: 0.06 | rmse_val: 1.1798
Epoch: 0053 | loss_train: 5.6294 loss_val: 7.3849 | f1_val: 0.05 | rmse_val: 1.2154
Epoch: 0054 | loss_train: 6.0476 loss_val: 6.0216 | f1_val: 0.07 | rmse_val: 1.0973
Epoch: 0055 | loss_train: 5.8910 loss_val: 4.7100 | f1_val: 0.07 | rmse_val: 1.1774
Epoch: 0056 | loss_train: 5.7062 loss_val: 5.1587 | f1_val: 0.05 | rmse_val: 1.3411
Epoch: 0057 | loss_train: 5.9192 loss_val: 5.0529 | f1_val: 0.06 | rmse_val: 1.3688
Epoch: 0058 | loss_train: 5.2516 loss_val: 4.4690 | f1_val: 0.06 | rmse_val: 1.0500
Epoch: 0059 | loss_train: 5.6066 loss_val: 4.1277 | f1_val: 0.04 | rmse_val: 1.1997
Epoch: 0060 | loss_train: 5.6329 loss_val: 4.1211 | f1_val: 0.06 | rmse_val: 0.9340
Epoch: 0061 | loss_train: 5.7693 loss_val: 5.8426 | f1_val: 0.05 | rmse_val: 1.1418
Epoch: 0062 | loss_train: 5.4533 loss_val: 4.5527 | f1_val: 0.05 | rmse_val: 1.0799
Epoch: 0063 | loss_train: 5.1776 loss_val: 4.9373 | f1_val: 0.04 | rmse_val: 1.2219
Epoch: 0064 | loss_train: 5.5922 loss_val: 5.5453 | f1_val: 0.05 | rmse_val: 1.3794
Epoch: 0065 | loss_train: 5.4881 loss_val: 4.4684 | f1_val: 0.06 | rmse_val: 0.9402
Epoch: 0066 | loss_train: 6.1882 loss_val: 5.6361 | f1_val: 0.05 | rmse_val: 1.3370
Epoch: 0067 | loss_train: 4.9490 loss_val: 5.7094 | f1_val: 0.05 | rmse_val: 1.0742
Epoch: 0068 | loss_train: 5.3552 loss_val: 6.7888 | f1_val: 0.05 | rmse_val: 1.0614
Epoch: 0069 | loss_train: 5.6050 loss_val: 5.5362 | f1_val: 0.05 | rmse_val: 1.4063
Epoch: 0070 | loss_train: 5.4383 loss_val: 4.8402 | f1_val: 0.04 | rmse_val: 1.1985
Epoch: 0071 | loss_train: 5.3172 loss_val: 4.8372 | f1_val: 0.03 | rmse_val: 1.2548
Epoch: 0072 | loss_train: 5.3442 loss_val: 4.1285 | f1_val: 0.05 | rmse_val: 1.0300
Epoch: 0073 | loss_train: 5.3005 loss_val: 5.7482 | f1_val: 0.06 | rmse_val: 1.2288
Epoch: 0074 | loss_train: 5.2408 loss_val: 5.2879 | f1_val: 0.05 | rmse_val: 1.1327
Epoch: 0075 | loss_train: 5.6536 loss_val: 6.0649 | f1_val: 0.05 | rmse_val: 1.1139
Epoch: 0076 | loss_train: 6.0935 loss_val: 4.6579 | f1_val: 0.05 | rmse_val: 1.1431
Epoch: 0077 | loss_train: 5.3329 loss_val: 4.3326 | f1_val: 0.04 | rmse_val: 1.0583
Epoch: 0078 | loss_train: 5.3796 loss_val: 5.4244 | f1_val: 0.04 | rmse_val: 1.2578
Epoch: 0079 | loss_train: 5.2531 loss_val: 5.3971 | f1_val: 0.05 | rmse_val: 1.1002
Epoch: 0080 | loss_train: 5.7967 loss_val: 3.0824 | f1_val: 0.05 | rmse_val: 1.5516
Epoch: 0081 | loss_train: 4.9986 loss_val: 4.7564 | f1_val: 0.05 | rmse_val: 1.1730
Epoch: 0082 | loss_train: 5.2378 loss_val: 5.7710 | f1_val: 0.05 | rmse_val: 1.2307
Epoch: 0083 | loss_train: 4.9867 loss_val: 5.6959 | f1_val: 0.04 | rmse_val: 1.1510
Epoch: 0084 | loss_train: 5.6231 loss_val: 4.8717 | f1_val: 0.06 | rmse_val: 1.1788
Epoch: 0085 | loss_train: 5.3833 loss_val: 4.8554 | f1_val: 0.05 | rmse_val: 1.1138
Epoch: 0086 | loss_train: 5.4044 loss_val: 5.6314 | f1_val: 0.03 | rmse_val: 1.1288
Epoch: 0087 | loss_train: 5.0709 loss_val: 4.2785 | f1_val: 0.05 | rmse_val: 1.0609
Epoch: 0088 | loss_train: 5.0295 loss_val: 4.3275 | f1_val: 0.05 | rmse_val: 1.0360
Epoch: 0089 | loss_train: 5.3736 loss_val: 4.8479 | f1_val: 0.06 | rmse_val: 1.3844
Epoch: 0090 | loss_train: 5.4807 loss_val: 5.7890 | f1_val: 0.05 | rmse_val: 1.2336
Epoch: 0091 | loss_train: 5.0258 loss_val: 4.1217 | f1_val: 0.05 | rmse_val: 1.0913
Epoch: 0092 | loss_train: 5.2955 loss_val: 5.7217 | f1_val: 0.05 | rmse_val: 1.2657
Epoch: 0093 | loss_train: 5.3125 loss_val: 4.5774 | f1_val: 0.05 | rmse_val: 1.0786
Epoch: 0094 | loss_train: 5.6051 loss_val: 6.3381 | f1_val: 0.04 | rmse_val: 1.4436
Epoch: 0095 | loss_train: 5.4871 loss_val: 4.9981 | f1_val: 0.06 | rmse_val: 0.9615
Epoch: 0096 | loss_train: 5.5958 loss_val: 4.1378 | f1_val: 0.04 | rmse_val: 1.1053
Epoch: 0097 | loss_train: 5.5860 loss_val: 4.8851 | f1_val: 0.07 | rmse_val: 1.1169
Epoch: 0098 | loss_train: 4.9391 loss_val: 3.3656 | f1_val: 0.05 | rmse_val: 1.0216
Epoch: 0099 | loss_train: 5.5106 loss_val: 4.6872 | f1_val: 0.06 | rmse_val: 1.0037
Epoch: 0100 | loss_train: 5.5925 loss_val: 5.2913 | f1_val: 0.05 | rmse_val: 1.6048
Epoch: 0101 | loss_train: 5.7497 loss_val: 5.8504 | f1_val: 0.06 | rmse_val: 1.0402
Epoch: 0102 | loss_train: 5.2630 loss_val: 4.0875 | f1_val: 0.06 | rmse_val: 1.2918
Epoch: 0103 | loss_train: 5.0517 loss_val: 4.3414 | f1_val: 0.06 | rmse_val: 1.1767
Epoch: 0104 | loss_train: 5.1657 loss_val: 4.5329 | f1_val: 0.05 | rmse_val: 1.0997
Epoch: 0105 | loss_train: 5.0189 loss_val: 4.2948 | f1_val: 0.05 | rmse_val: 1.1888
Epoch: 0106 | loss_train: 5.5391 loss_val: 4.1046 | f1_val: 0.05 | rmse_val: 1.0656
Epoch: 0107 | loss_train: 5.3838 loss_val: 3.7287 | f1_val: 0.04 | rmse_val: 1.1871
Epoch: 0108 | loss_train: 5.4608 loss_val: 5.5074 | f1_val: 0.05 | rmse_val: 1.0989
Epoch: 0109 | loss_train: 5.7538 loss_val: 4.4333 | f1_val: 0.06 | rmse_val: 1.1037
Epoch: 0110 | loss_train: 5.0796 loss_val: 3.9429 | f1_val: 0.05 | rmse_val: 1.0038
Optimization Finished!
Train cost: 1095.6595s
Loading 55th epoch
f1_test: 0.05 | rmse_test: 1.0038

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/pe_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.07 | rmse_val: 3.3741
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.06 | rmse_val: 2.9943
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.07 | rmse_val: 1.7410
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.07 | rmse_val: 1.1327
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.06 | rmse_val: 0.9800
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.06 | rmse_val: 1.0872
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.06 | rmse_val: 1.1652
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.07 | rmse_val: 1.1518
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.08 | rmse_val: 1.2344
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.07 | rmse_val: 1.3389
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.07 | rmse_val: 1.3391
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.06 | rmse_val: 0.9978
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.06 | rmse_val: 1.2773
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.08 | rmse_val: 1.1648
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.08 | rmse_val: 1.2310
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.06 | rmse_val: 1.4026
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.10 | rmse_val: 1.1990
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.06 | rmse_val: 1.2226
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.06 | rmse_val: 1.1331
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.06 | rmse_val: 1.1435
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.06 | rmse_val: 1.1230
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.06 | rmse_val: 1.7135
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.06 | rmse_val: 1.7576
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.07 | rmse_val: 1.1399
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.07 | rmse_val: 1.2766
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.07 | rmse_val: 1.1888
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.07 | rmse_val: 1.6370
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.07 | rmse_val: 1.7470
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.06 | rmse_val: 1.6875
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.06 | rmse_val: 1.5689
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.07 | rmse_val: 1.0698
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.08 | rmse_val: 1.0978
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.06 | rmse_val: 1.2972
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.07 | rmse_val: 1.1596
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.07 | rmse_val: 1.8801
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.08 | rmse_val: 1.0040
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.09 | rmse_val: 1.1605
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.08 | rmse_val: 1.1127
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.07 | rmse_val: 1.3111
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.08 | rmse_val: 1.0541
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.07 | rmse_val: 0.9032
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.06 | rmse_val: 1.1433
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.05 | rmse_val: 1.2399
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.07 | rmse_val: 0.9962
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.08 | rmse_val: 1.1889
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.08 | rmse_val: 1.0389
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.06 | rmse_val: 1.1820
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.06 | rmse_val: 1.0117
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.07 | rmse_val: 1.2784
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.05 | rmse_val: 1.0770
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.07 | rmse_val: 1.0232
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.07 | rmse_val: 1.1284
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.07 | rmse_val: 1.1607
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.05 | rmse_val: 1.0688
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.06 | rmse_val: 1.1700
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.06 | rmse_val: 1.1267
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.07 | rmse_val: 1.1829
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.05 | rmse_val: 0.9879
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.06 | rmse_val: 0.9795
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.06 | rmse_val: 1.1594
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.06 | rmse_val: 0.8617
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.07 | rmse_val: 1.0771
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.06 | rmse_val: 0.9418
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.06 | rmse_val: 1.1806
Optimization Finished!
Train cost: 567.2200s
Loading 17th epoch
f1_test: 0.06 | rmse_test: 1.1806

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/pe_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=4, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 204/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=4, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150210
Epoch: 0001 | loss_train: 11.8435 loss_val: 13.2408 | f1_val: 0.10 | rmse_val: 3.6645
Epoch: 0002 | loss_train: 11.7348 loss_val: 13.2540 | f1_val: 0.10 | rmse_val: 3.4249
Epoch: 0003 | loss_train: 10.6524 loss_val: 10.2441 | f1_val: 0.08 | rmse_val: 3.3175
Epoch: 0004 | loss_train: 10.0132 loss_val: 11.0525 | f1_val: 0.10 | rmse_val: 2.8545
Epoch: 0005 | loss_train: 7.9773 loss_val: 5.4983 | f1_val: 0.09 | rmse_val: 2.0777
Epoch: 0006 | loss_train: 3.4610 loss_val: 2.8600 | f1_val: 0.09 | rmse_val: 0.4728
Epoch: 0007 | loss_train: 2.5839 loss_val: 2.6045 | f1_val: 0.09 | rmse_val: 1.1775
Epoch: 0008 | loss_train: 2.2626 loss_val: 2.8898 | f1_val: 0.09 | rmse_val: 0.7626
Epoch: 0009 | loss_train: 2.2968 loss_val: 3.2760 | f1_val: 0.10 | rmse_val: 0.7406
Epoch: 0010 | loss_train: 2.1468 loss_val: 2.5778 | f1_val: 0.08 | rmse_val: 0.8464
Epoch: 0011 | loss_train: 2.2362 loss_val: 2.6896 | f1_val: 0.10 | rmse_val: 0.6844
Epoch: 0012 | loss_train: 2.3830 loss_val: 3.1295 | f1_val: 0.13 | rmse_val: 0.8614
Epoch: 0013 | loss_train: 2.2942 loss_val: 2.5512 | f1_val: 0.09 | rmse_val: 0.8122
Epoch: 0014 | loss_train: 2.5345 loss_val: 2.7098 | f1_val: 0.09 | rmse_val: 0.7229
Epoch: 0015 | loss_train: 2.4827 loss_val: 2.6899 | f1_val: 0.09 | rmse_val: 0.8175
Epoch: 0016 | loss_train: 2.5217 loss_val: 2.6097 | f1_val: 0.09 | rmse_val: 0.6349
Epoch: 0017 | loss_train: 2.5003 loss_val: 1.9788 | f1_val: 0.11 | rmse_val: 0.6865
Epoch: 0018 | loss_train: 2.5632 loss_val: 3.4974 | f1_val: 0.08 | rmse_val: 0.7470
Epoch: 0019 | loss_train: 2.3887 loss_val: 2.1159 | f1_val: 0.10 | rmse_val: 0.9131
Epoch: 0020 | loss_train: 2.4309 loss_val: 1.4342 | f1_val: 0.10 | rmse_val: 0.8147
Epoch: 0021 | loss_train: 2.3319 loss_val: 2.7299 | f1_val: 0.09 | rmse_val: 0.8573
Epoch: 0022 | loss_train: 2.4483 loss_val: 2.7910 | f1_val: 0.11 | rmse_val: 0.8616
Epoch: 0023 | loss_train: 2.3441 loss_val: 3.1091 | f1_val: 0.09 | rmse_val: 0.9873
Epoch: 0024 | loss_train: 2.4612 loss_val: 2.7269 | f1_val: 0.08 | rmse_val: 0.7079
Epoch: 0025 | loss_train: 2.1616 loss_val: 3.2671 | f1_val: 0.08 | rmse_val: 1.0956
Epoch: 0026 | loss_train: 2.2286 loss_val: 2.3122 | f1_val: 0.10 | rmse_val: 0.8079
Epoch: 0027 | loss_train: 2.3317 loss_val: 2.7414 | f1_val: 0.09 | rmse_val: 0.8390
Epoch: 0028 | loss_train: 2.3286 loss_val: 3.3310 | f1_val: 0.11 | rmse_val: 0.6706
Epoch: 0029 | loss_train: 2.2921 loss_val: 1.6999 | f1_val: 0.10 | rmse_val: 0.9385
Epoch: 0030 | loss_train: 2.5007 loss_val: 2.2024 | f1_val: 0.10 | rmse_val: 0.7783
Epoch: 0031 | loss_train: 2.2947 loss_val: 2.6579 | f1_val: 0.08 | rmse_val: 0.6569
Epoch: 0032 | loss_train: 2.3792 loss_val: 2.6152 | f1_val: 0.09 | rmse_val: 0.7064
Epoch: 0033 | loss_train: 2.3893 loss_val: 1.9020 | f1_val: 0.09 | rmse_val: 0.8397
Epoch: 0034 | loss_train: 2.3577 loss_val: 1.8063 | f1_val: 0.10 | rmse_val: 0.9737
Epoch: 0035 | loss_train: 2.4621 loss_val: 4.1836 | f1_val: 0.08 | rmse_val: 1.9968
Epoch: 0036 | loss_train: 2.9373 loss_val: 1.7810 | f1_val: 0.10 | rmse_val: 0.5986
Epoch: 0037 | loss_train: 2.2429 loss_val: 1.8877 | f1_val: 0.09 | rmse_val: 0.7224
Epoch: 0038 | loss_train: 2.3155 loss_val: 1.9606 | f1_val: 0.09 | rmse_val: 0.5787
Epoch: 0039 | loss_train: 2.4228 loss_val: 2.4984 | f1_val: 0.08 | rmse_val: 1.0842
Epoch: 0040 | loss_train: 2.5765 loss_val: 3.1340 | f1_val: 0.12 | rmse_val: 0.7164
Epoch: 0041 | loss_train: 2.1391 loss_val: 2.5470 | f1_val: 0.07 | rmse_val: 0.6073
Epoch: 0042 | loss_train: 2.2939 loss_val: 2.0646 | f1_val: 0.09 | rmse_val: 0.7959
Epoch: 0043 | loss_train: 2.6517 loss_val: 1.8322 | f1_val: 0.10 | rmse_val: 0.9760
Epoch: 0044 | loss_train: 2.9997 loss_val: 2.5375 | f1_val: 0.08 | rmse_val: 1.0645
Epoch: 0045 | loss_train: 2.9437 loss_val: 1.8755 | f1_val: 0.07 | rmse_val: 0.6491
Epoch: 0046 | loss_train: 2.7666 loss_val: 1.9527 | f1_val: 0.08 | rmse_val: 0.8228
Epoch: 0047 | loss_train: 2.4648 loss_val: 3.3475 | f1_val: 0.08 | rmse_val: 0.8919
Epoch: 0048 | loss_train: 2.4408 loss_val: 3.1460 | f1_val: 0.10 | rmse_val: 0.8873
Epoch: 0049 | loss_train: 2.3691 loss_val: 1.9544 | f1_val: 0.10 | rmse_val: 0.8173
Epoch: 0050 | loss_train: 2.5526 loss_val: 3.7318 | f1_val: 0.09 | rmse_val: 0.7297
Optimization Finished!
Train cost: 312.5272s
Loading 12th epoch
f1_test: 0.09 | rmse_test: 0.7297

>>> run.py: Namespace(dataset='LINUX', device=1, experiment='pe_dim', log_path='log/nagphormer/LINUX/pe_dim', path='data/nagphormer', plot_path='plots/nagphormer/LINUX/pe_dim', pyg_path='data/pyg/LINUX')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/pe_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=1, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 2/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=30, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153538
Epoch: 0001 | loss_train: 94.3163 loss_val: 99.4356 | f1_val: 0.04 | rmse_val: 7.8117
Epoch: 0002 | loss_train: 84.8179 loss_val: 98.3591 | f1_val: 0.05 | rmse_val: 7.5663
Epoch: 0003 | loss_train: 79.2042 loss_val: 77.8207 | f1_val: 0.07 | rmse_val: 6.8976
Epoch: 0004 | loss_train: 68.0568 loss_val: 67.4384 | f1_val: 0.05 | rmse_val: 5.7434
Epoch: 0005 | loss_train: 47.8143 loss_val: 43.7006 | f1_val: 0.06 | rmse_val: 3.5958
Epoch: 0006 | loss_train: 25.9864 loss_val: 7.9071 | f1_val: 0.05 | rmse_val: 1.2361
Epoch: 0007 | loss_train: 10.7244 loss_val: 9.1350 | f1_val: 0.06 | rmse_val: 2.7011
Epoch: 0008 | loss_train: 10.2413 loss_val: 7.6376 | f1_val: 0.04 | rmse_val: 1.8656
Epoch: 0009 | loss_train: 8.5579 loss_val: 9.3208 | f1_val: 0.05 | rmse_val: 1.4661
Epoch: 0010 | loss_train: 9.0958 loss_val: 10.4109 | f1_val: 0.07 | rmse_val: 1.8773
Epoch: 0011 | loss_train: 10.7969 loss_val: 9.2238 | f1_val: 0.05 | rmse_val: 2.0164
Epoch: 0012 | loss_train: 8.4169 loss_val: 11.8129 | f1_val: 0.07 | rmse_val: 1.5300
Epoch: 0013 | loss_train: 8.9635 loss_val: 8.8678 | f1_val: 0.06 | rmse_val: 1.8709
Epoch: 0014 | loss_train: 10.5081 loss_val: 14.8902 | f1_val: 0.05 | rmse_val: 1.9699
Epoch: 0015 | loss_train: 9.9420 loss_val: 9.9511 | f1_val: 0.07 | rmse_val: 1.4306
Epoch: 0016 | loss_train: 9.4617 loss_val: 7.4736 | f1_val: 0.08 | rmse_val: 1.4341
Epoch: 0017 | loss_train: 10.1237 loss_val: 8.5891 | f1_val: 0.07 | rmse_val: 2.0508
Epoch: 0018 | loss_train: 9.2286 loss_val: 10.3675 | f1_val: 0.07 | rmse_val: 1.5322
Epoch: 0019 | loss_train: 8.7022 loss_val: 11.7392 | f1_val: 0.05 | rmse_val: 1.0428
Epoch: 0020 | loss_train: 8.7666 loss_val: 10.8989 | f1_val: 0.07 | rmse_val: 1.2007
Epoch: 0021 | loss_train: 9.3486 loss_val: 10.8662 | f1_val: 0.05 | rmse_val: 1.4197
Epoch: 0022 | loss_train: 8.5491 loss_val: 13.2322 | f1_val: 0.06 | rmse_val: 1.9024
Epoch: 0023 | loss_train: 8.6050 loss_val: 9.5084 | f1_val: 0.07 | rmse_val: 2.9612
Epoch: 0024 | loss_train: 11.0860 loss_val: 9.6823 | f1_val: 0.07 | rmse_val: 2.3399
Epoch: 0025 | loss_train: 9.3862 loss_val: 12.7357 | f1_val: 0.06 | rmse_val: 1.1079
Epoch: 0026 | loss_train: 11.4938 loss_val: 9.2943 | f1_val: 0.05 | rmse_val: 1.6120
Epoch: 0027 | loss_train: 10.1062 loss_val: 10.6356 | f1_val: 0.05 | rmse_val: 2.5188
Epoch: 0028 | loss_train: 9.1060 loss_val: 12.4958 | f1_val: 0.06 | rmse_val: 1.1850
Epoch: 0029 | loss_train: 8.2387 loss_val: 9.9666 | f1_val: 0.05 | rmse_val: 2.0548
Epoch: 0030 | loss_train: 7.9670 loss_val: 10.0556 | f1_val: 0.05 | rmse_val: 1.4840
Epoch: 0031 | loss_train: 7.5237 loss_val: 7.7344 | f1_val: 0.06 | rmse_val: 0.9616
Epoch: 0032 | loss_train: 9.7610 loss_val: 10.5620 | f1_val: 0.09 | rmse_val: 2.4286
Epoch: 0033 | loss_train: 9.6253 loss_val: 11.0339 | f1_val: 0.07 | rmse_val: 1.6531
Epoch: 0034 | loss_train: 8.7020 loss_val: 10.0765 | f1_val: 0.07 | rmse_val: 1.0794
Epoch: 0035 | loss_train: 8.4145 loss_val: 10.2614 | f1_val: 0.07 | rmse_val: 2.0975
Epoch: 0036 | loss_train: 7.3668 loss_val: 9.7373 | f1_val: 0.08 | rmse_val: 1.5023
Epoch: 0037 | loss_train: 7.3544 loss_val: 10.5828 | f1_val: 0.05 | rmse_val: 1.5867
Epoch: 0038 | loss_train: 8.9135 loss_val: 6.3920 | f1_val: 0.05 | rmse_val: 1.7296
Epoch: 0039 | loss_train: 7.2440 loss_val: 9.4078 | f1_val: 0.06 | rmse_val: 1.9190
Epoch: 0040 | loss_train: 7.6202 loss_val: 12.2804 | f1_val: 0.06 | rmse_val: 1.7898
Epoch: 0041 | loss_train: 7.9939 loss_val: 9.2166 | f1_val: 0.06 | rmse_val: 1.4550
Epoch: 0042 | loss_train: 8.3705 loss_val: 8.5713 | f1_val: 0.06 | rmse_val: 1.6587
Epoch: 0043 | loss_train: 9.1584 loss_val: 9.7732 | f1_val: 0.05 | rmse_val: 1.2431
Epoch: 0044 | loss_train: 9.2184 loss_val: 11.6125 | f1_val: 0.07 | rmse_val: 2.6871
Epoch: 0045 | loss_train: 9.5148 loss_val: 8.0743 | f1_val: 0.07 | rmse_val: 1.4493
Epoch: 0046 | loss_train: 7.9598 loss_val: 8.6669 | f1_val: 0.05 | rmse_val: 1.1271
Epoch: 0047 | loss_train: 7.4661 loss_val: 6.6401 | f1_val: 0.04 | rmse_val: 1.1195
Epoch: 0048 | loss_train: 8.7919 loss_val: 9.4190 | f1_val: 0.07 | rmse_val: 1.6994
Epoch: 0049 | loss_train: 7.6456 loss_val: 8.7111 | f1_val: 0.07 | rmse_val: 1.5780
Epoch: 0050 | loss_train: 7.6858 loss_val: 7.8030 | f1_val: 0.06 | rmse_val: 1.2089
Epoch: 0051 | loss_train: 7.4183 loss_val: 9.0625 | f1_val: 0.07 | rmse_val: 1.4958
Epoch: 0052 | loss_train: 7.6091 loss_val: 9.8344 | f1_val: 0.06 | rmse_val: 0.7858
Epoch: 0053 | loss_train: 10.4223 loss_val: 9.5704 | f1_val: 0.08 | rmse_val: 1.7951
Epoch: 0054 | loss_train: 7.8159 loss_val: 11.7581 | f1_val: 0.06 | rmse_val: 2.0162
Epoch: 0055 | loss_train: 8.2747 loss_val: 12.6114 | f1_val: 0.06 | rmse_val: 0.9477
Epoch: 0056 | loss_train: 10.5707 loss_val: 10.7023 | f1_val: 0.07 | rmse_val: 2.1957
Epoch: 0057 | loss_train: 8.5185 loss_val: 9.6908 | f1_val: 0.06 | rmse_val: 1.9136
Epoch: 0058 | loss_train: 6.9369 loss_val: 13.6740 | f1_val: 0.05 | rmse_val: 0.9939
Epoch: 0059 | loss_train: 8.1248 loss_val: 8.9875 | f1_val: 0.06 | rmse_val: 2.4849
Epoch: 0060 | loss_train: 6.8467 loss_val: 9.7096 | f1_val: 0.07 | rmse_val: 1.2372
Epoch: 0061 | loss_train: 6.4359 loss_val: 7.4255 | f1_val: 0.04 | rmse_val: 1.9629
Epoch: 0062 | loss_train: 7.6992 loss_val: 7.4880 | f1_val: 0.07 | rmse_val: 1.5939
Epoch: 0063 | loss_train: 6.8688 loss_val: 7.1454 | f1_val: 0.08 | rmse_val: 1.6468
Epoch: 0064 | loss_train: 7.4487 loss_val: 8.3756 | f1_val: 0.06 | rmse_val: 1.2911
Epoch: 0065 | loss_train: 7.2159 loss_val: 14.2224 | f1_val: 0.06 | rmse_val: 1.9737
Epoch: 0066 | loss_train: 6.4559 loss_val: 7.8162 | f1_val: 0.06 | rmse_val: 1.4747
Epoch: 0067 | loss_train: 5.6888 loss_val: 7.1498 | f1_val: 0.08 | rmse_val: 1.3387
Epoch: 0068 | loss_train: 6.8605 loss_val: 8.6137 | f1_val: 0.04 | rmse_val: 1.8081
Optimization Finished!
Train cost: 342.7014s
Loading 32th epoch
f1_test: 0.04 | rmse_test: 1.8081

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/pe_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=2, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 12/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=31, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153666
Epoch: 0001 | loss_train: 84.9907 loss_val: 78.6469 | f1_val: 0.04 | rmse_val: 7.4148
Epoch: 0002 | loss_train: 80.3519 loss_val: 90.1980 | f1_val: 0.05 | rmse_val: 7.3222
Epoch: 0003 | loss_train: 75.9616 loss_val: 66.5241 | f1_val: 0.07 | rmse_val: 6.6856
Epoch: 0004 | loss_train: 62.8265 loss_val: 55.9579 | f1_val: 0.06 | rmse_val: 5.6867
Epoch: 0005 | loss_train: 46.8740 loss_val: 40.5144 | f1_val: 0.05 | rmse_val: 3.9629
Epoch: 0006 | loss_train: 28.0128 loss_val: 13.0978 | f1_val: 0.05 | rmse_val: 1.4853
Epoch: 0007 | loss_train: 9.6046 loss_val: 4.9285 | f1_val: 0.04 | rmse_val: 1.7754
Epoch: 0008 | loss_train: 7.9100 loss_val: 3.9954 | f1_val: 0.06 | rmse_val: 2.0447
Epoch: 0009 | loss_train: 5.8697 loss_val: 6.2617 | f1_val: 0.04 | rmse_val: 1.3623
Epoch: 0010 | loss_train: 7.2018 loss_val: 9.1200 | f1_val: 0.06 | rmse_val: 1.2956
Epoch: 0011 | loss_train: 8.0412 loss_val: 6.7081 | f1_val: 0.03 | rmse_val: 1.6476
Epoch: 0012 | loss_train: 6.6623 loss_val: 7.7514 | f1_val: 0.06 | rmse_val: 1.3650
Epoch: 0013 | loss_train: 6.8677 loss_val: 7.8560 | f1_val: 0.07 | rmse_val: 1.7095
Epoch: 0014 | loss_train: 7.9868 loss_val: 10.3627 | f1_val: 0.06 | rmse_val: 1.5679
Epoch: 0015 | loss_train: 7.8359 loss_val: 7.8802 | f1_val: 0.06 | rmse_val: 1.3598
Epoch: 0016 | loss_train: 6.8005 loss_val: 4.2867 | f1_val: 0.06 | rmse_val: 1.2223
Epoch: 0017 | loss_train: 7.9557 loss_val: 7.2041 | f1_val: 0.05 | rmse_val: 1.7865
Epoch: 0018 | loss_train: 6.9299 loss_val: 6.6020 | f1_val: 0.05 | rmse_val: 1.4304
Epoch: 0019 | loss_train: 5.7892 loss_val: 8.3160 | f1_val: 0.07 | rmse_val: 0.6655
Epoch: 0020 | loss_train: 6.6187 loss_val: 8.9703 | f1_val: 0.06 | rmse_val: 1.9191
Epoch: 0021 | loss_train: 7.5277 loss_val: 5.8511 | f1_val: 0.07 | rmse_val: 1.4284
Epoch: 0022 | loss_train: 6.6108 loss_val: 8.2792 | f1_val: 0.08 | rmse_val: 1.7083
Epoch: 0023 | loss_train: 7.2418 loss_val: 3.8118 | f1_val: 0.06 | rmse_val: 1.3700
Epoch: 0024 | loss_train: 6.9888 loss_val: 6.0611 | f1_val: 0.07 | rmse_val: 1.3717
Epoch: 0025 | loss_train: 6.4200 loss_val: 9.3420 | f1_val: 0.05 | rmse_val: 0.7836
Epoch: 0026 | loss_train: 9.1000 loss_val: 8.1819 | f1_val: 0.06 | rmse_val: 1.2945
Epoch: 0027 | loss_train: 7.1415 loss_val: 9.3306 | f1_val: 0.07 | rmse_val: 2.2206
Epoch: 0028 | loss_train: 6.9578 loss_val: 9.3284 | f1_val: 0.06 | rmse_val: 0.7854
Epoch: 0029 | loss_train: 6.2088 loss_val: 8.2193 | f1_val: 0.08 | rmse_val: 1.7675
Epoch: 0030 | loss_train: 5.8545 loss_val: 7.7636 | f1_val: 0.07 | rmse_val: 1.1719
Epoch: 0031 | loss_train: 6.0090 loss_val: 7.0454 | f1_val: 0.04 | rmse_val: 0.7860
Epoch: 0032 | loss_train: 7.7027 loss_val: 7.9214 | f1_val: 0.07 | rmse_val: 2.0884
Epoch: 0033 | loss_train: 7.7890 loss_val: 8.8408 | f1_val: 0.09 | rmse_val: 1.2216
Epoch: 0034 | loss_train: 6.4043 loss_val: 5.0835 | f1_val: 0.07 | rmse_val: 0.9621
Epoch: 0035 | loss_train: 6.8634 loss_val: 6.6417 | f1_val: 0.06 | rmse_val: 2.2421
Epoch: 0036 | loss_train: 6.1479 loss_val: 6.2139 | f1_val: 0.06 | rmse_val: 1.2627
Epoch: 0037 | loss_train: 6.1372 loss_val: 8.1038 | f1_val: 0.05 | rmse_val: 1.5704
Epoch: 0038 | loss_train: 6.7648 loss_val: 4.7809 | f1_val: 0.07 | rmse_val: 1.5072
Epoch: 0039 | loss_train: 5.5431 loss_val: 7.1092 | f1_val: 0.06 | rmse_val: 1.8115
Epoch: 0040 | loss_train: 5.5104 loss_val: 6.4278 | f1_val: 0.05 | rmse_val: 1.4232
Epoch: 0041 | loss_train: 6.2163 loss_val: 7.7529 | f1_val: 0.05 | rmse_val: 0.8362
Epoch: 0042 | loss_train: 5.6778 loss_val: 5.5117 | f1_val: 0.06 | rmse_val: 0.8870
Epoch: 0043 | loss_train: 6.0644 loss_val: 5.3784 | f1_val: 0.05 | rmse_val: 0.9564
Epoch: 0044 | loss_train: 5.9211 loss_val: 8.3706 | f1_val: 0.07 | rmse_val: 2.1872
Epoch: 0045 | loss_train: 6.6027 loss_val: 9.0968 | f1_val: 0.08 | rmse_val: 2.7989
Epoch: 0046 | loss_train: 22.3564 loss_val: 10.3431 | f1_val: 0.03 | rmse_val: 1.4770
Epoch: 0047 | loss_train: 8.8235 loss_val: 6.8620 | f1_val: 0.07 | rmse_val: 1.3718
Epoch: 0048 | loss_train: 8.6805 loss_val: 8.7998 | f1_val: 0.05 | rmse_val: 2.1116
Epoch: 0049 | loss_train: 6.9857 loss_val: 6.5241 | f1_val: 0.06 | rmse_val: 1.0753
Epoch: 0050 | loss_train: 6.8137 loss_val: 4.5232 | f1_val: 0.06 | rmse_val: 1.3702
Epoch: 0051 | loss_train: 5.4898 loss_val: 6.1806 | f1_val: 0.07 | rmse_val: 1.7459
Epoch: 0052 | loss_train: 7.7486 loss_val: 9.7549 | f1_val: 0.06 | rmse_val: 0.7566
Epoch: 0053 | loss_train: 9.5907 loss_val: 9.1494 | f1_val: 0.06 | rmse_val: 1.8461
Epoch: 0054 | loss_train: 6.4422 loss_val: 7.8605 | f1_val: 0.07 | rmse_val: 1.2551
Epoch: 0055 | loss_train: 7.3792 loss_val: 9.4321 | f1_val: 0.05 | rmse_val: 1.2891
Epoch: 0056 | loss_train: 7.0624 loss_val: 4.9091 | f1_val: 0.07 | rmse_val: 1.6440
Epoch: 0057 | loss_train: 6.9396 loss_val: 5.3737 | f1_val: 0.05 | rmse_val: 1.6532
Epoch: 0058 | loss_train: 4.8798 loss_val: 6.8768 | f1_val: 0.05 | rmse_val: 0.9702
Epoch: 0059 | loss_train: 7.2074 loss_val: 5.7336 | f1_val: 0.05 | rmse_val: 1.5776
Epoch: 0060 | loss_train: 6.0962 loss_val: 9.4613 | f1_val: 0.05 | rmse_val: 1.3267
Epoch: 0061 | loss_train: 5.2518 loss_val: 4.6208 | f1_val: 0.07 | rmse_val: 1.6229
Epoch: 0062 | loss_train: 5.9944 loss_val: 3.7462 | f1_val: 0.06 | rmse_val: 1.5171
Epoch: 0063 | loss_train: 6.0229 loss_val: 4.7065 | f1_val: 0.05 | rmse_val: 1.2770
Epoch: 0064 | loss_train: 6.5653 loss_val: 4.5900 | f1_val: 0.06 | rmse_val: 1.1794
Epoch: 0065 | loss_train: 5.8188 loss_val: 8.4233 | f1_val: 0.05 | rmse_val: 1.4116
Epoch: 0066 | loss_train: 5.7713 loss_val: 6.7883 | f1_val: 0.07 | rmse_val: 1.2966
Epoch: 0067 | loss_train: 5.4954 loss_val: 5.0451 | f1_val: 0.07 | rmse_val: 1.2864
Epoch: 0068 | loss_train: 5.6597 loss_val: 8.1901 | f1_val: 0.07 | rmse_val: 1.6243
Epoch: 0069 | loss_train: 5.5238 loss_val: 7.3864 | f1_val: 0.05 | rmse_val: 1.4476
Epoch: 0070 | loss_train: 6.5492 loss_val: 6.5468 | f1_val: 0.05 | rmse_val: 1.4289
Epoch: 0071 | loss_train: 5.7855 loss_val: 5.4472 | f1_val: 0.05 | rmse_val: 1.5253
Epoch: 0072 | loss_train: 6.4793 loss_val: 7.1678 | f1_val: 0.05 | rmse_val: 1.4905
Epoch: 0073 | loss_train: 5.6223 loss_val: 4.7338 | f1_val: 0.06 | rmse_val: 1.2948
Epoch: 0074 | loss_train: 5.7039 loss_val: 3.9130 | f1_val: 0.05 | rmse_val: 1.4063
Epoch: 0075 | loss_train: 5.3973 loss_val: 6.3339 | f1_val: 0.06 | rmse_val: 1.2306
Epoch: 0076 | loss_train: 5.3198 loss_val: 4.6545 | f1_val: 0.05 | rmse_val: 1.4152
Epoch: 0077 | loss_train: 6.6333 loss_val: 6.3646 | f1_val: 0.07 | rmse_val: 1.7680
Epoch: 0078 | loss_train: 6.0630 loss_val: 4.5417 | f1_val: 0.05 | rmse_val: 1.2203
Epoch: 0079 | loss_train: 6.7211 loss_val: 6.5216 | f1_val: 0.07 | rmse_val: 1.2856
Epoch: 0080 | loss_train: 5.5956 loss_val: 6.9015 | f1_val: 0.06 | rmse_val: 1.4773
Epoch: 0081 | loss_train: 5.9700 loss_val: 5.1079 | f1_val: 0.06 | rmse_val: 1.2890
Epoch: 0082 | loss_train: 5.8117 loss_val: 4.4858 | f1_val: 0.05 | rmse_val: 1.1359
Epoch: 0083 | loss_train: 5.9346 loss_val: 8.2247 | f1_val: 0.05 | rmse_val: 1.3964
Epoch: 0084 | loss_train: 6.4204 loss_val: 4.9309 | f1_val: 0.07 | rmse_val: 1.2897
Epoch: 0085 | loss_train: 5.6376 loss_val: 5.4168 | f1_val: 0.05 | rmse_val: 1.2053
Epoch: 0086 | loss_train: 6.2649 loss_val: 6.6000 | f1_val: 0.07 | rmse_val: 1.2403
Epoch: 0087 | loss_train: 5.3236 loss_val: 8.2568 | f1_val: 0.07 | rmse_val: 1.3368
Epoch: 0088 | loss_train: 4.8578 loss_val: 7.2247 | f1_val: 0.07 | rmse_val: 1.2447
Epoch: 0089 | loss_train: 5.3761 loss_val: 10.7587 | f1_val: 0.05 | rmse_val: 1.2693
Epoch: 0090 | loss_train: 4.9767 loss_val: 5.0305 | f1_val: 0.06 | rmse_val: 1.2890
Epoch: 0091 | loss_train: 6.1024 loss_val: 7.7519 | f1_val: 0.06 | rmse_val: 1.5473
Epoch: 0092 | loss_train: 5.1618 loss_val: 6.1326 | f1_val: 0.06 | rmse_val: 1.4459
Optimization Finished!
Train cost: 481.5627s
Loading 33th epoch
f1_test: 0.06 | rmse_test: 1.4459

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/pe_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.05 | rmse_val: 7.4451
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 7.4450
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.08 | rmse_val: 6.8450
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.03 | rmse_val: 6.0641
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.05 | rmse_val: 4.4422
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.06 | rmse_val: 1.9609
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.06 | rmse_val: 1.4814
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.06 | rmse_val: 2.2505
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.07 | rmse_val: 1.4667
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.07 | rmse_val: 1.3041
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.6533
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.06 | rmse_val: 1.0961
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.06 | rmse_val: 1.5658
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.05 | rmse_val: 1.5932
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.05 | rmse_val: 1.1988
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.04 | rmse_val: 1.2835
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.05 | rmse_val: 1.7141
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.06 | rmse_val: 1.2771
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.05 | rmse_val: 0.9793
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.08 | rmse_val: 1.8279
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.07 | rmse_val: 1.3268
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.04 | rmse_val: 1.8338
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.06 | rmse_val: 1.1240
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.03 | rmse_val: 1.8121
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.07 | rmse_val: 1.6883
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.09 | rmse_val: 0.7529
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.07 | rmse_val: 1.7207
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.07 | rmse_val: 1.3494
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.04 | rmse_val: 0.8378
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.07 | rmse_val: 1.1355
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.06 | rmse_val: 0.8232
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.06 | rmse_val: 1.9919
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.07 | rmse_val: 1.4889
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.05 | rmse_val: 0.7824
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.05 | rmse_val: 2.1047
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.06 | rmse_val: 1.1257
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.04 | rmse_val: 1.1966
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.07 | rmse_val: 1.5883
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.06 | rmse_val: 2.1858
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.04 | rmse_val: 1.4801
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.07 | rmse_val: 0.8435
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.06 | rmse_val: 1.0060
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.06 | rmse_val: 1.1348
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.07 | rmse_val: 2.5679
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.07 | rmse_val: 2.5789
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.04 | rmse_val: 1.4546
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.08 | rmse_val: 1.9236
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.06 | rmse_val: 1.3427
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.08 | rmse_val: 1.5619
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.06 | rmse_val: 1.4774
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.05 | rmse_val: 1.6202
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.05 | rmse_val: 1.0127
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.05 | rmse_val: 1.4628
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.05 | rmse_val: 1.4804
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.08 | rmse_val: 1.2230
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.06 | rmse_val: 1.5972
Optimization Finished!
Train cost: 237.2246s
Loading 26th epoch
f1_test: 0.06 | rmse_test: 1.5972

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/pe_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=4, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 36/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=33, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153922
Epoch: 0001 | loss_train: 70.5153 loss_val: 57.7820 | f1_val: 0.07 | rmse_val: 6.9577
Epoch: 0002 | loss_train: 68.3309 loss_val: 78.3592 | f1_val: 0.03 | rmse_val: 6.6224
Epoch: 0003 | loss_train: 71.3829 loss_val: 56.0778 | f1_val: 0.04 | rmse_val: 6.5714
Epoch: 0004 | loss_train: 64.8841 loss_val: 48.1360 | f1_val: 0.04 | rmse_val: 6.0052
Epoch: 0005 | loss_train: 55.8171 loss_val: 39.9897 | f1_val: 0.05 | rmse_val: 5.3734
Epoch: 0006 | loss_train: 43.9315 loss_val: 29.4119 | f1_val: 0.04 | rmse_val: 4.1789
Epoch: 0007 | loss_train: 30.5044 loss_val: 18.4765 | f1_val: 0.05 | rmse_val: 2.7583
Epoch: 0008 | loss_train: 15.8601 loss_val: 7.3925 | f1_val: 0.05 | rmse_val: 0.9833
Epoch: 0009 | loss_train: 5.3165 loss_val: 3.2204 | f1_val: 0.05 | rmse_val: 1.6834
Epoch: 0010 | loss_train: 6.0027 loss_val: 6.3032 | f1_val: 0.05 | rmse_val: 2.3756
Epoch: 0011 | loss_train: 6.6764 loss_val: 3.7501 | f1_val: 0.03 | rmse_val: 1.5198
Epoch: 0012 | loss_train: 5.0692 loss_val: 6.5613 | f1_val: 0.06 | rmse_val: 0.8755
Epoch: 0013 | loss_train: 5.5243 loss_val: 4.0694 | f1_val: 0.05 | rmse_val: 1.3651
Epoch: 0014 | loss_train: 6.5627 loss_val: 7.8657 | f1_val: 0.05 | rmse_val: 1.5444
Epoch: 0015 | loss_train: 6.1684 loss_val: 3.2866 | f1_val: 0.04 | rmse_val: 1.3887
Epoch: 0016 | loss_train: 5.2235 loss_val: 2.6064 | f1_val: 0.06 | rmse_val: 1.0790
Epoch: 0017 | loss_train: 6.0462 loss_val: 5.4856 | f1_val: 0.05 | rmse_val: 1.2106
Epoch: 0018 | loss_train: 5.7074 loss_val: 2.6528 | f1_val: 0.06 | rmse_val: 1.7178
Epoch: 0019 | loss_train: 4.7192 loss_val: 3.4358 | f1_val: 0.04 | rmse_val: 1.1649
Epoch: 0020 | loss_train: 5.3045 loss_val: 4.1973 | f1_val: 0.05 | rmse_val: 1.2970
Epoch: 0021 | loss_train: 6.2397 loss_val: 3.4725 | f1_val: 0.05 | rmse_val: 1.7160
Epoch: 0022 | loss_train: 4.9865 loss_val: 5.1847 | f1_val: 0.05 | rmse_val: 1.1776
Epoch: 0023 | loss_train: 5.8870 loss_val: 2.3401 | f1_val: 0.05 | rmse_val: 1.4308
Epoch: 0024 | loss_train: 5.3762 loss_val: 4.5087 | f1_val: 0.05 | rmse_val: 1.4386
Epoch: 0025 | loss_train: 4.7029 loss_val: 4.1170 | f1_val: 0.06 | rmse_val: 0.7198
Epoch: 0026 | loss_train: 6.4871 loss_val: 3.4803 | f1_val: 0.05 | rmse_val: 0.7161
Epoch: 0027 | loss_train: 5.1851 loss_val: 6.3609 | f1_val: 0.05 | rmse_val: 2.0759
Epoch: 0028 | loss_train: 5.3318 loss_val: 5.1568 | f1_val: 0.06 | rmse_val: 0.8877
Epoch: 0029 | loss_train: 4.4646 loss_val: 4.7669 | f1_val: 0.06 | rmse_val: 1.2253
Epoch: 0030 | loss_train: 4.8341 loss_val: 6.0663 | f1_val: 0.05 | rmse_val: 1.5108
Epoch: 0031 | loss_train: 4.5347 loss_val: 3.8684 | f1_val: 0.05 | rmse_val: 0.9864
Epoch: 0032 | loss_train: 4.7523 loss_val: 5.3544 | f1_val: 0.07 | rmse_val: 1.9476
Epoch: 0033 | loss_train: 6.3298 loss_val: 4.6349 | f1_val: 0.05 | rmse_val: 0.9273
Epoch: 0034 | loss_train: 4.8334 loss_val: 2.5162 | f1_val: 0.07 | rmse_val: 0.4926
Epoch: 0035 | loss_train: 4.8220 loss_val: 5.8673 | f1_val: 0.05 | rmse_val: 2.0445
Epoch: 0036 | loss_train: 4.9448 loss_val: 5.3286 | f1_val: 0.07 | rmse_val: 0.6276
Epoch: 0037 | loss_train: 5.1462 loss_val: 7.9015 | f1_val: 0.06 | rmse_val: 1.9093
Epoch: 0038 | loss_train: 5.6810 loss_val: 3.8103 | f1_val: 0.04 | rmse_val: 1.1389
Epoch: 0039 | loss_train: 3.9746 loss_val: 5.5061 | f1_val: 0.07 | rmse_val: 1.4506
Epoch: 0040 | loss_train: 4.0968 loss_val: 4.5095 | f1_val: 0.06 | rmse_val: 1.1857
Epoch: 0041 | loss_train: 4.4794 loss_val: 4.4211 | f1_val: 0.06 | rmse_val: 0.9256
Epoch: 0042 | loss_train: 4.1337 loss_val: 2.9874 | f1_val: 0.05 | rmse_val: 1.1947
Epoch: 0043 | loss_train: 4.8072 loss_val: 3.6203 | f1_val: 0.04 | rmse_val: 0.9424
Epoch: 0044 | loss_train: 4.5503 loss_val: 6.0191 | f1_val: 0.06 | rmse_val: 1.8818
Epoch: 0045 | loss_train: 5.3404 loss_val: 4.1440 | f1_val: 0.06 | rmse_val: 0.8241
Epoch: 0046 | loss_train: 5.5279 loss_val: 4.1725 | f1_val: 0.09 | rmse_val: 1.0307
Epoch: 0047 | loss_train: 5.0943 loss_val: 4.1645 | f1_val: 0.08 | rmse_val: 1.3181
Epoch: 0048 | loss_train: 5.4482 loss_val: 2.8578 | f1_val: 0.06 | rmse_val: 1.2114
Epoch: 0049 | loss_train: 4.6532 loss_val: 4.6390 | f1_val: 0.06 | rmse_val: 1.4281
Epoch: 0050 | loss_train: 5.2855 loss_val: 2.9140 | f1_val: 0.04 | rmse_val: 0.5492
Epoch: 0051 | loss_train: 4.0813 loss_val: 3.8442 | f1_val: 0.04 | rmse_val: 0.9269
Epoch: 0052 | loss_train: 5.0535 loss_val: 4.9134 | f1_val: 0.06 | rmse_val: 1.2620
Epoch: 0053 | loss_train: 4.9839 loss_val: 5.0386 | f1_val: 0.06 | rmse_val: 1.5185
Epoch: 0054 | loss_train: 4.4026 loss_val: 5.7375 | f1_val: 0.08 | rmse_val: 0.9921
Epoch: 0055 | loss_train: 5.8489 loss_val: 6.3285 | f1_val: 0.05 | rmse_val: 0.6566
Epoch: 0056 | loss_train: 5.6585 loss_val: 4.0856 | f1_val: 0.07 | rmse_val: 1.9717
Epoch: 0057 | loss_train: 5.8230 loss_val: 2.7823 | f1_val: 0.05 | rmse_val: 1.2131
Epoch: 0058 | loss_train: 3.9115 loss_val: 2.4739 | f1_val: 0.06 | rmse_val: 0.5110
Epoch: 0059 | loss_train: 5.3773 loss_val: 4.8202 | f1_val: 0.03 | rmse_val: 1.8905
Epoch: 0060 | loss_train: 4.3949 loss_val: 4.2057 | f1_val: 0.06 | rmse_val: 0.7908
Epoch: 0061 | loss_train: 3.8740 loss_val: 4.1402 | f1_val: 0.05 | rmse_val: 1.7559
Epoch: 0062 | loss_train: 5.1297 loss_val: 3.2274 | f1_val: 0.04 | rmse_val: 0.9337
Epoch: 0063 | loss_train: 4.3782 loss_val: 3.4390 | f1_val: 0.04 | rmse_val: 1.1553
Epoch: 0064 | loss_train: 4.7894 loss_val: 3.3994 | f1_val: 0.07 | rmse_val: 0.8605
Epoch: 0065 | loss_train: 4.0338 loss_val: 8.4489 | f1_val: 0.05 | rmse_val: 1.2461
Epoch: 0066 | loss_train: 4.1958 loss_val: 5.2290 | f1_val: 0.06 | rmse_val: 1.1440
Epoch: 0067 | loss_train: 4.2488 loss_val: 3.5572 | f1_val: 0.06 | rmse_val: 0.9188
Epoch: 0068 | loss_train: 4.3435 loss_val: 2.8325 | f1_val: 0.06 | rmse_val: 1.1570
Epoch: 0069 | loss_train: 3.9219 loss_val: 5.3675 | f1_val: 0.06 | rmse_val: 0.9612
Epoch: 0070 | loss_train: 4.0226 loss_val: 2.9456 | f1_val: 0.05 | rmse_val: 1.1460
Epoch: 0071 | loss_train: 3.7244 loss_val: 2.7040 | f1_val: 0.07 | rmse_val: 0.9899
Epoch: 0072 | loss_train: 4.8040 loss_val: 4.6097 | f1_val: 0.05 | rmse_val: 1.4096
Epoch: 0073 | loss_train: 4.0879 loss_val: 3.8189 | f1_val: 0.06 | rmse_val: 0.9425
Epoch: 0074 | loss_train: 4.1843 loss_val: 2.5464 | f1_val: 0.07 | rmse_val: 0.9977
Epoch: 0075 | loss_train: 3.8307 loss_val: 3.9501 | f1_val: 0.06 | rmse_val: 1.3716
Epoch: 0076 | loss_train: 3.9147 loss_val: 2.5747 | f1_val: 0.05 | rmse_val: 0.9444
Optimization Finished!
Train cost: 211.1017s
Loading 46th epoch
f1_test: 0.05 | rmse_test: 0.9444

>>> run.py: Namespace(dataset='AIDS700nef', device=1, experiment='pe_dim', log_path='log/nagphormer/AIDS700nef/pe_dim', path='data/nagphormer', plot_path='plots/nagphormer/AIDS700nef/pe_dim', pyg_path='data/pyg/AIDS700nef')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/n_layers', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.07 | rmse_val: 3.3741
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.06 | rmse_val: 2.9943
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.07 | rmse_val: 1.7410
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.07 | rmse_val: 1.1327
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.06 | rmse_val: 0.9800
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.06 | rmse_val: 1.0872
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.06 | rmse_val: 1.1652
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.07 | rmse_val: 1.1518
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.08 | rmse_val: 1.2344
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.07 | rmse_val: 1.3389
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.07 | rmse_val: 1.3391
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.06 | rmse_val: 0.9978
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.06 | rmse_val: 1.2773
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.08 | rmse_val: 1.1648
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.08 | rmse_val: 1.2310
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.06 | rmse_val: 1.4026
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.10 | rmse_val: 1.1990
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.06 | rmse_val: 1.2226
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.06 | rmse_val: 1.1331
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.06 | rmse_val: 1.1435
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.06 | rmse_val: 1.1230
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.06 | rmse_val: 1.7135
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.06 | rmse_val: 1.7576
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.07 | rmse_val: 1.1399
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.07 | rmse_val: 1.2766
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.07 | rmse_val: 1.1888
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.07 | rmse_val: 1.6370
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.07 | rmse_val: 1.7470
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.06 | rmse_val: 1.6875
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.06 | rmse_val: 1.5689
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.07 | rmse_val: 1.0698
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.08 | rmse_val: 1.0978
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.06 | rmse_val: 1.2972
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.07 | rmse_val: 1.1596
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.07 | rmse_val: 1.8801
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.08 | rmse_val: 1.0040
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.09 | rmse_val: 1.1605
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.08 | rmse_val: 1.1127
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.07 | rmse_val: 1.3111
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.08 | rmse_val: 1.0541
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.07 | rmse_val: 0.9032
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.06 | rmse_val: 1.1433
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.05 | rmse_val: 1.2399
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.07 | rmse_val: 0.9962
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.08 | rmse_val: 1.1889
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.08 | rmse_val: 1.0389
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.06 | rmse_val: 1.1820
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.06 | rmse_val: 1.0117
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.07 | rmse_val: 1.2784
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.05 | rmse_val: 1.0770
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.07 | rmse_val: 1.0232
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.07 | rmse_val: 1.1284
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.07 | rmse_val: 1.1607
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.05 | rmse_val: 1.0688
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.06 | rmse_val: 1.1700
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.06 | rmse_val: 1.1267
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.07 | rmse_val: 1.1829
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.05 | rmse_val: 0.9879
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.06 | rmse_val: 0.9795
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.06 | rmse_val: 1.1594
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.06 | rmse_val: 0.8617
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.07 | rmse_val: 1.0771
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.06 | rmse_val: 0.9418
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.06 | rmse_val: 1.1806
Optimization Finished!
Train cost: 371.3437s
Loading 17th epoch
f1_test: 0.06 | rmse_test: 1.1806

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/n_layers', n_heads=8, n_layers=2, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 282562
Epoch: 0001 | loss_train: 20.4889 loss_val: 19.6336 | f1_val: 0.06 | rmse_val: 3.4825
Epoch: 0002 | loss_train: 19.5613 loss_val: 16.4629 | f1_val: 0.07 | rmse_val: 3.0692
Epoch: 0003 | loss_train: 15.1348 loss_val: 13.0875 | f1_val: 0.06 | rmse_val: 1.8575
Epoch: 0004 | loss_train: 6.5788 loss_val: 3.9803 | f1_val: 0.07 | rmse_val: 1.1655
Epoch: 0005 | loss_train: 4.2943 loss_val: 4.2381 | f1_val: 0.07 | rmse_val: 1.0288
Epoch: 0006 | loss_train: 4.2968 loss_val: 4.0252 | f1_val: 0.06 | rmse_val: 1.0707
Epoch: 0007 | loss_train: 4.3611 loss_val: 4.1337 | f1_val: 0.07 | rmse_val: 1.1218
Epoch: 0008 | loss_train: 3.7890 loss_val: 5.0123 | f1_val: 0.07 | rmse_val: 1.0490
Epoch: 0009 | loss_train: 4.2730 loss_val: 5.8740 | f1_val: 0.08 | rmse_val: 1.1756
Epoch: 0010 | loss_train: 4.1285 loss_val: 5.3436 | f1_val: 0.07 | rmse_val: 1.3954
Epoch: 0011 | loss_train: 4.5443 loss_val: 4.4416 | f1_val: 0.06 | rmse_val: 1.2504
Epoch: 0012 | loss_train: 4.2966 loss_val: 3.9297 | f1_val: 0.08 | rmse_val: 1.0625
Epoch: 0013 | loss_train: 4.3913 loss_val: 4.2979 | f1_val: 0.05 | rmse_val: 0.9874
Epoch: 0014 | loss_train: 4.4511 loss_val: 4.4343 | f1_val: 0.07 | rmse_val: 1.1102
Epoch: 0015 | loss_train: 4.2965 loss_val: 3.9660 | f1_val: 0.07 | rmse_val: 1.1150
Epoch: 0016 | loss_train: 4.3053 loss_val: 5.2752 | f1_val: 0.09 | rmse_val: 1.2544
Epoch: 0017 | loss_train: 4.5487 loss_val: 4.0070 | f1_val: 0.06 | rmse_val: 1.1363
Epoch: 0018 | loss_train: 4.9192 loss_val: 5.3816 | f1_val: 0.05 | rmse_val: 1.0946
Epoch: 0019 | loss_train: 4.4615 loss_val: 4.8880 | f1_val: 0.07 | rmse_val: 0.9428
Epoch: 0020 | loss_train: 4.3462 loss_val: 4.1626 | f1_val: 0.07 | rmse_val: 1.1036
Epoch: 0021 | loss_train: 4.1138 loss_val: 4.9240 | f1_val: 0.05 | rmse_val: 1.0451
Epoch: 0022 | loss_train: 4.3959 loss_val: 6.0260 | f1_val: 0.08 | rmse_val: 1.5670
Epoch: 0023 | loss_train: 4.0163 loss_val: 5.2450 | f1_val: 0.05 | rmse_val: 1.6874
Epoch: 0024 | loss_train: 4.7214 loss_val: 3.9097 | f1_val: 0.05 | rmse_val: 1.0229
Epoch: 0025 | loss_train: 4.0693 loss_val: 5.8538 | f1_val: 0.06 | rmse_val: 1.4779
Epoch: 0026 | loss_train: 3.9450 loss_val: 3.9693 | f1_val: 0.07 | rmse_val: 1.3120
Epoch: 0027 | loss_train: 4.5676 loss_val: 4.5539 | f1_val: 0.06 | rmse_val: 1.7248
Epoch: 0028 | loss_train: 4.6762 loss_val: 6.0864 | f1_val: 0.05 | rmse_val: 1.5608
Epoch: 0029 | loss_train: 4.4482 loss_val: 3.8681 | f1_val: 0.08 | rmse_val: 1.4970
Epoch: 0030 | loss_train: 5.0183 loss_val: 3.6195 | f1_val: 0.06 | rmse_val: 1.2902
Epoch: 0031 | loss_train: 4.3883 loss_val: 4.0699 | f1_val: 0.08 | rmse_val: 0.9293
Epoch: 0032 | loss_train: 4.0713 loss_val: 4.3959 | f1_val: 0.06 | rmse_val: 1.0758
Epoch: 0033 | loss_train: 4.2019 loss_val: 3.6253 | f1_val: 0.08 | rmse_val: 1.1055
Epoch: 0034 | loss_train: 4.2099 loss_val: 3.3634 | f1_val: 0.06 | rmse_val: 1.2565
Epoch: 0035 | loss_train: 4.2998 loss_val: 5.1692 | f1_val: 0.06 | rmse_val: 1.8425
Epoch: 0036 | loss_train: 5.0586 loss_val: 3.4352 | f1_val: 0.06 | rmse_val: 0.9120
Epoch: 0037 | loss_train: 4.5076 loss_val: 3.4223 | f1_val: 0.06 | rmse_val: 1.0998
Epoch: 0038 | loss_train: 4.3935 loss_val: 4.6087 | f1_val: 0.06 | rmse_val: 0.9756
Epoch: 0039 | loss_train: 4.3607 loss_val: 4.4223 | f1_val: 0.06 | rmse_val: 1.2778
Epoch: 0040 | loss_train: 4.5720 loss_val: 4.5584 | f1_val: 0.07 | rmse_val: 1.0256
Epoch: 0041 | loss_train: 4.4208 loss_val: 5.7495 | f1_val: 0.07 | rmse_val: 0.9443
Epoch: 0042 | loss_train: 4.1932 loss_val: 3.7834 | f1_val: 0.06 | rmse_val: 1.0574
Epoch: 0043 | loss_train: 4.2457 loss_val: 4.6406 | f1_val: 0.07 | rmse_val: 1.1871
Epoch: 0044 | loss_train: 4.3069 loss_val: 3.5101 | f1_val: 0.07 | rmse_val: 1.0789
Epoch: 0045 | loss_train: 4.1889 loss_val: 4.4438 | f1_val: 0.07 | rmse_val: 0.9395
Epoch: 0046 | loss_train: 4.8489 loss_val: 3.7369 | f1_val: 0.04 | rmse_val: 0.9702
Epoch: 0047 | loss_train: 4.0456 loss_val: 5.2547 | f1_val: 0.05 | rmse_val: 1.1055
Epoch: 0048 | loss_train: 4.2941 loss_val: 5.9230 | f1_val: 0.05 | rmse_val: 1.0177
Epoch: 0049 | loss_train: 4.1613 loss_val: 3.8239 | f1_val: 0.06 | rmse_val: 1.1752
Epoch: 0050 | loss_train: 4.2820 loss_val: 4.8707 | f1_val: 0.06 | rmse_val: 1.0091
Epoch: 0051 | loss_train: 4.0474 loss_val: 3.9277 | f1_val: 0.05 | rmse_val: 0.9761
Epoch: 0052 | loss_train: 3.9106 loss_val: 3.6197 | f1_val: 0.07 | rmse_val: 0.9543
Epoch: 0053 | loss_train: 4.0377 loss_val: 4.7442 | f1_val: 0.06 | rmse_val: 0.9037
Epoch: 0054 | loss_train: 4.6497 loss_val: 5.2071 | f1_val: 0.07 | rmse_val: 1.0836
Epoch: 0055 | loss_train: 4.7335 loss_val: 4.7299 | f1_val: 0.06 | rmse_val: 1.0496
Epoch: 0056 | loss_train: 3.8610 loss_val: 3.3397 | f1_val: 0.07 | rmse_val: 1.0974
Epoch: 0057 | loss_train: 4.5135 loss_val: 3.7894 | f1_val: 0.06 | rmse_val: 1.1355
Epoch: 0058 | loss_train: 3.8468 loss_val: 3.4596 | f1_val: 0.06 | rmse_val: 1.1416
Epoch: 0059 | loss_train: 4.0928 loss_val: 3.9662 | f1_val: 0.07 | rmse_val: 1.0619
Epoch: 0060 | loss_train: 4.4072 loss_val: 3.8977 | f1_val: 0.07 | rmse_val: 1.0007
Epoch: 0061 | loss_train: 4.2160 loss_val: 5.6115 | f1_val: 0.07 | rmse_val: 0.9464
Epoch: 0062 | loss_train: 4.3685 loss_val: 3.7587 | f1_val: 0.04 | rmse_val: 1.0248
Epoch: 0063 | loss_train: 4.2031 loss_val: 4.7534 | f1_val: 0.06 | rmse_val: 0.9825
Epoch: 0064 | loss_train: 4.2105 loss_val: 4.2777 | f1_val: 0.05 | rmse_val: 1.1216
Epoch: 0065 | loss_train: 4.3740 loss_val: 3.8555 | f1_val: 0.06 | rmse_val: 1.1357
Epoch: 0066 | loss_train: 4.1964 loss_val: 4.5659 | f1_val: 0.06 | rmse_val: 0.9831
Epoch: 0067 | loss_train: 4.0734 loss_val: 6.0653 | f1_val: 0.06 | rmse_val: 1.0407
Epoch: 0068 | loss_train: 4.3954 loss_val: 4.8021 | f1_val: 0.07 | rmse_val: 1.0532
Epoch: 0069 | loss_train: 4.6592 loss_val: 4.3212 | f1_val: 0.05 | rmse_val: 0.9667
Epoch: 0070 | loss_train: 4.1768 loss_val: 4.7412 | f1_val: 0.05 | rmse_val: 1.1074
Epoch: 0071 | loss_train: 4.1827 loss_val: 4.3878 | f1_val: 0.07 | rmse_val: 1.1165
Epoch: 0072 | loss_train: 3.8312 loss_val: 4.3018 | f1_val: 0.08 | rmse_val: 0.8491
Epoch: 0073 | loss_train: 4.2701 loss_val: 5.7605 | f1_val: 0.06 | rmse_val: 0.9600
Epoch: 0074 | loss_train: 3.9355 loss_val: 4.6288 | f1_val: 0.08 | rmse_val: 0.9993
Epoch: 0075 | loss_train: 4.3028 loss_val: 5.6421 | f1_val: 0.06 | rmse_val: 1.0904
Epoch: 0076 | loss_train: 4.2193 loss_val: 4.3348 | f1_val: 0.08 | rmse_val: 0.9727
Epoch: 0077 | loss_train: 3.9866 loss_val: 3.5431 | f1_val: 0.07 | rmse_val: 1.0987
Epoch: 0078 | loss_train: 4.3607 loss_val: 5.0555 | f1_val: 0.07 | rmse_val: 1.0642
Epoch: 0079 | loss_train: 4.3884 loss_val: 5.1123 | f1_val: 0.07 | rmse_val: 1.0647
Epoch: 0080 | loss_train: 4.2132 loss_val: 2.9021 | f1_val: 0.07 | rmse_val: 1.0980
Epoch: 0081 | loss_train: 3.8802 loss_val: 3.5734 | f1_val: 0.07 | rmse_val: 0.9815
Epoch: 0082 | loss_train: 4.0742 loss_val: 4.2095 | f1_val: 0.06 | rmse_val: 1.2842
Epoch: 0083 | loss_train: 3.8164 loss_val: 4.9605 | f1_val: 0.07 | rmse_val: 1.0035
Epoch: 0084 | loss_train: 4.5124 loss_val: 3.8017 | f1_val: 0.06 | rmse_val: 1.0872
Epoch: 0085 | loss_train: 4.2537 loss_val: 4.4523 | f1_val: 0.07 | rmse_val: 0.9220
Epoch: 0086 | loss_train: 4.0347 loss_val: 6.0450 | f1_val: 0.06 | rmse_val: 1.0266
Epoch: 0087 | loss_train: 4.2932 loss_val: 3.9701 | f1_val: 0.07 | rmse_val: 1.1082
Epoch: 0088 | loss_train: 3.6614 loss_val: 3.8751 | f1_val: 0.06 | rmse_val: 1.0326
Epoch: 0089 | loss_train: 4.5145 loss_val: 3.6021 | f1_val: 0.05 | rmse_val: 1.0884
Epoch: 0090 | loss_train: 4.1489 loss_val: 4.9959 | f1_val: 0.07 | rmse_val: 0.9710
Epoch: 0091 | loss_train: 4.3824 loss_val: 4.2437 | f1_val: 0.07 | rmse_val: 1.0794
Epoch: 0092 | loss_train: 3.9412 loss_val: 4.5557 | f1_val: 0.07 | rmse_val: 1.0263
Epoch: 0093 | loss_train: 4.1796 loss_val: 3.6234 | f1_val: 0.05 | rmse_val: 1.0678
Epoch: 0094 | loss_train: 4.3461 loss_val: 4.8599 | f1_val: 0.06 | rmse_val: 1.0250
Epoch: 0095 | loss_train: 4.3145 loss_val: 3.9644 | f1_val: 0.07 | rmse_val: 1.0873
Epoch: 0096 | loss_train: 4.2177 loss_val: 3.7764 | f1_val: 0.07 | rmse_val: 0.9377
Epoch: 0097 | loss_train: 4.8615 loss_val: 4.5214 | f1_val: 0.08 | rmse_val: 0.9998
Epoch: 0098 | loss_train: 3.7927 loss_val: 3.1151 | f1_val: 0.06 | rmse_val: 1.1843
Epoch: 0099 | loss_train: 4.2570 loss_val: 4.7169 | f1_val: 0.07 | rmse_val: 0.9351
Epoch: 0100 | loss_train: 4.4326 loss_val: 4.6208 | f1_val: 0.08 | rmse_val: 1.0249
Epoch: 0101 | loss_train: 4.2075 loss_val: 4.9755 | f1_val: 0.07 | rmse_val: 1.1385
Epoch: 0102 | loss_train: 4.2891 loss_val: 4.3614 | f1_val: 0.06 | rmse_val: 1.1962
Epoch: 0103 | loss_train: 4.1922 loss_val: 4.6787 | f1_val: 0.06 | rmse_val: 1.0762
Epoch: 0104 | loss_train: 3.8837 loss_val: 4.2722 | f1_val: 0.08 | rmse_val: 1.0984
Epoch: 0105 | loss_train: 3.9188 loss_val: 3.5782 | f1_val: 0.06 | rmse_val: 1.0883
Epoch: 0106 | loss_train: 4.1195 loss_val: 3.3930 | f1_val: 0.05 | rmse_val: 0.8973
Epoch: 0107 | loss_train: 4.4425 loss_val: 3.8802 | f1_val: 0.07 | rmse_val: 1.1390
Epoch: 0108 | loss_train: 4.2487 loss_val: 4.7261 | f1_val: 0.09 | rmse_val: 1.1865
Epoch: 0109 | loss_train: 5.0207 loss_val: 4.7066 | f1_val: 0.06 | rmse_val: 0.9496
Epoch: 0110 | loss_train: 4.2195 loss_val: 4.5172 | f1_val: 0.07 | rmse_val: 1.0868
Optimization Finished!
Train cost: 884.1295s
Loading 16th epoch
f1_test: 0.07 | rmse_test: 1.0868

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/n_layers', n_heads=8, n_layers=3, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 415042
Epoch: 0001 | loss_train: 20.1191 loss_val: 19.4086 | f1_val: 0.06 | rmse_val: 3.3258
Epoch: 0002 | loss_train: 19.3907 loss_val: 16.3656 | f1_val: 0.05 | rmse_val: 3.2833
Epoch: 0003 | loss_train: 14.9947 loss_val: 12.7985 | f1_val: 0.05 | rmse_val: 1.8292
Epoch: 0004 | loss_train: 6.1924 loss_val: 4.1979 | f1_val: 0.07 | rmse_val: 1.3236
Epoch: 0005 | loss_train: 4.3666 loss_val: 4.2683 | f1_val: 0.06 | rmse_val: 0.8409
Epoch: 0006 | loss_train: 4.2684 loss_val: 3.9991 | f1_val: 0.07 | rmse_val: 0.9235
Epoch: 0007 | loss_train: 4.3591 loss_val: 4.0769 | f1_val: 0.06 | rmse_val: 1.1657
Epoch: 0008 | loss_train: 3.8194 loss_val: 4.9434 | f1_val: 0.07 | rmse_val: 1.1409
Epoch: 0009 | loss_train: 4.2820 loss_val: 5.8178 | f1_val: 0.08 | rmse_val: 1.1475
Epoch: 0010 | loss_train: 4.1244 loss_val: 5.2962 | f1_val: 0.07 | rmse_val: 1.2249
Epoch: 0011 | loss_train: 4.5541 loss_val: 4.4253 | f1_val: 0.06 | rmse_val: 1.1890
Epoch: 0012 | loss_train: 4.2898 loss_val: 3.7804 | f1_val: 0.08 | rmse_val: 1.0921
Epoch: 0013 | loss_train: 4.4732 loss_val: 4.3088 | f1_val: 0.07 | rmse_val: 1.1313
Epoch: 0014 | loss_train: 4.5376 loss_val: 4.4545 | f1_val: 0.05 | rmse_val: 1.0159
Epoch: 0015 | loss_train: 4.3296 loss_val: 3.9764 | f1_val: 0.07 | rmse_val: 1.2428
Epoch: 0016 | loss_train: 4.3570 loss_val: 5.2526 | f1_val: 0.08 | rmse_val: 1.3562
Epoch: 0017 | loss_train: 4.5477 loss_val: 4.0068 | f1_val: 0.05 | rmse_val: 1.1426
Epoch: 0018 | loss_train: 4.9237 loss_val: 5.3777 | f1_val: 0.06 | rmse_val: 1.1232
Epoch: 0019 | loss_train: 4.4609 loss_val: 4.8998 | f1_val: 0.06 | rmse_val: 0.8996
Epoch: 0020 | loss_train: 4.3570 loss_val: 4.1816 | f1_val: 0.05 | rmse_val: 0.9225
Epoch: 0021 | loss_train: 4.1200 loss_val: 5.0090 | f1_val: 0.05 | rmse_val: 0.9507
Epoch: 0022 | loss_train: 4.4161 loss_val: 5.9522 | f1_val: 0.06 | rmse_val: 1.5690
Epoch: 0023 | loss_train: 4.0024 loss_val: 5.3082 | f1_val: 0.06 | rmse_val: 1.6808
Epoch: 0024 | loss_train: 4.8367 loss_val: 3.9573 | f1_val: 0.07 | rmse_val: 1.1849
Epoch: 0025 | loss_train: 4.2123 loss_val: 5.5519 | f1_val: 0.06 | rmse_val: 1.2214
Epoch: 0026 | loss_train: 3.8580 loss_val: 4.1205 | f1_val: 0.07 | rmse_val: 0.9999
Epoch: 0027 | loss_train: 4.4326 loss_val: 4.3855 | f1_val: 0.05 | rmse_val: 1.3540
Epoch: 0028 | loss_train: 4.4653 loss_val: 6.0378 | f1_val: 0.06 | rmse_val: 1.5062
Epoch: 0029 | loss_train: 4.3081 loss_val: 3.9923 | f1_val: 0.06 | rmse_val: 1.6049
Epoch: 0030 | loss_train: 5.0123 loss_val: 3.7054 | f1_val: 0.06 | rmse_val: 1.6499
Epoch: 0031 | loss_train: 4.6995 loss_val: 4.2029 | f1_val: 0.04 | rmse_val: 1.2234
Epoch: 0032 | loss_train: 4.0881 loss_val: 4.4085 | f1_val: 0.06 | rmse_val: 1.0530
Epoch: 0033 | loss_train: 4.2544 loss_val: 3.6386 | f1_val: 0.08 | rmse_val: 1.2462
Epoch: 0034 | loss_train: 4.2612 loss_val: 3.4200 | f1_val: 0.07 | rmse_val: 1.3822
Epoch: 0035 | loss_train: 4.4247 loss_val: 5.6362 | f1_val: 0.09 | rmse_val: 1.8281
Epoch: 0036 | loss_train: 5.2585 loss_val: 3.3582 | f1_val: 0.06 | rmse_val: 1.0970
Epoch: 0037 | loss_train: 4.6186 loss_val: 3.4053 | f1_val: 0.09 | rmse_val: 1.0972
Epoch: 0038 | loss_train: 4.4296 loss_val: 4.6642 | f1_val: 0.07 | rmse_val: 0.8614
Epoch: 0039 | loss_train: 4.3635 loss_val: 4.4218 | f1_val: 0.06 | rmse_val: 1.3155
Epoch: 0040 | loss_train: 4.6098 loss_val: 4.5416 | f1_val: 0.06 | rmse_val: 0.9379
Epoch: 0041 | loss_train: 4.4335 loss_val: 5.8515 | f1_val: 0.05 | rmse_val: 0.7911
Epoch: 0042 | loss_train: 4.2065 loss_val: 3.7997 | f1_val: 0.05 | rmse_val: 1.1156
Epoch: 0043 | loss_train: 4.2582 loss_val: 4.6424 | f1_val: 0.07 | rmse_val: 1.1586
Epoch: 0044 | loss_train: 4.3169 loss_val: 3.5131 | f1_val: 0.07 | rmse_val: 1.0135
Epoch: 0045 | loss_train: 4.2211 loss_val: 4.4654 | f1_val: 0.06 | rmse_val: 1.0987
Epoch: 0046 | loss_train: 4.8562 loss_val: 3.8376 | f1_val: 0.07 | rmse_val: 0.8915
Epoch: 0047 | loss_train: 4.0708 loss_val: 5.2644 | f1_val: 0.07 | rmse_val: 1.1348
Epoch: 0048 | loss_train: 4.2893 loss_val: 5.9853 | f1_val: 0.06 | rmse_val: 0.9045
Epoch: 0049 | loss_train: 4.1449 loss_val: 3.8280 | f1_val: 0.06 | rmse_val: 1.2373
Epoch: 0050 | loss_train: 4.2929 loss_val: 4.8999 | f1_val: 0.05 | rmse_val: 0.9840
Epoch: 0051 | loss_train: 4.0527 loss_val: 3.9282 | f1_val: 0.07 | rmse_val: 0.9849
Epoch: 0052 | loss_train: 3.9150 loss_val: 3.6416 | f1_val: 0.07 | rmse_val: 1.1305
Epoch: 0053 | loss_train: 4.0420 loss_val: 4.7293 | f1_val: 0.06 | rmse_val: 1.1302
Epoch: 0054 | loss_train: 4.6573 loss_val: 5.1979 | f1_val: 0.06 | rmse_val: 1.0108
Epoch: 0055 | loss_train: 4.7391 loss_val: 4.7271 | f1_val: 0.05 | rmse_val: 1.0623
Epoch: 0056 | loss_train: 3.8688 loss_val: 3.3173 | f1_val: 0.08 | rmse_val: 1.1701
Epoch: 0057 | loss_train: 4.5162 loss_val: 3.7913 | f1_val: 0.08 | rmse_val: 1.1486
Epoch: 0058 | loss_train: 3.8516 loss_val: 3.4643 | f1_val: 0.06 | rmse_val: 1.1045
Epoch: 0059 | loss_train: 4.0980 loss_val: 4.0038 | f1_val: 0.05 | rmse_val: 0.9545
Epoch: 0060 | loss_train: 4.4137 loss_val: 3.9035 | f1_val: 0.06 | rmse_val: 1.1171
Epoch: 0061 | loss_train: 4.2194 loss_val: 5.6752 | f1_val: 0.07 | rmse_val: 1.0953
Epoch: 0062 | loss_train: 4.3756 loss_val: 3.7772 | f1_val: 0.06 | rmse_val: 1.1336
Epoch: 0063 | loss_train: 4.2106 loss_val: 4.7764 | f1_val: 0.05 | rmse_val: 1.0573
Epoch: 0064 | loss_train: 4.2171 loss_val: 4.2828 | f1_val: 0.07 | rmse_val: 1.0581
Epoch: 0065 | loss_train: 4.3805 loss_val: 3.8484 | f1_val: 0.07 | rmse_val: 1.1443
Epoch: 0066 | loss_train: 4.2066 loss_val: 4.5876 | f1_val: 0.04 | rmse_val: 0.9862
Epoch: 0067 | loss_train: 4.0741 loss_val: 6.0721 | f1_val: 0.05 | rmse_val: 1.1982
Epoch: 0068 | loss_train: 4.4075 loss_val: 4.7988 | f1_val: 0.07 | rmse_val: 1.0738
Epoch: 0069 | loss_train: 4.6698 loss_val: 4.2986 | f1_val: 0.06 | rmse_val: 0.9909
Epoch: 0070 | loss_train: 4.1811 loss_val: 4.7383 | f1_val: 0.05 | rmse_val: 1.1001
Epoch: 0071 | loss_train: 4.1909 loss_val: 4.4296 | f1_val: 0.06 | rmse_val: 1.0838
Epoch: 0072 | loss_train: 3.8351 loss_val: 4.3439 | f1_val: 0.08 | rmse_val: 0.8652
Epoch: 0073 | loss_train: 4.2748 loss_val: 5.7756 | f1_val: 0.06 | rmse_val: 1.0853
Epoch: 0074 | loss_train: 3.9434 loss_val: 4.6508 | f1_val: 0.06 | rmse_val: 1.0856
Epoch: 0075 | loss_train: 4.3082 loss_val: 5.6620 | f1_val: 0.09 | rmse_val: 0.9885
Epoch: 0076 | loss_train: 4.2276 loss_val: 4.3461 | f1_val: 0.06 | rmse_val: 1.0366
Epoch: 0077 | loss_train: 3.9927 loss_val: 3.5679 | f1_val: 0.05 | rmse_val: 0.9257
Epoch: 0078 | loss_train: 4.3668 loss_val: 5.0401 | f1_val: 0.06 | rmse_val: 0.9993
Epoch: 0079 | loss_train: 4.3937 loss_val: 5.1137 | f1_val: 0.08 | rmse_val: 1.0816
Epoch: 0080 | loss_train: 4.2193 loss_val: 2.9111 | f1_val: 0.07 | rmse_val: 1.1565
Epoch: 0081 | loss_train: 3.8866 loss_val: 3.5988 | f1_val: 0.05 | rmse_val: 0.9706
Epoch: 0082 | loss_train: 4.0789 loss_val: 4.2064 | f1_val: 0.05 | rmse_val: 1.1294
Epoch: 0083 | loss_train: 3.8184 loss_val: 4.9749 | f1_val: 0.07 | rmse_val: 0.9979
Epoch: 0084 | loss_train: 4.5188 loss_val: 3.8248 | f1_val: 0.05 | rmse_val: 1.0365
Epoch: 0085 | loss_train: 4.2587 loss_val: 4.4264 | f1_val: 0.06 | rmse_val: 0.9927
Epoch: 0086 | loss_train: 4.0388 loss_val: 6.0863 | f1_val: 0.05 | rmse_val: 0.9162
Epoch: 0087 | loss_train: 4.2982 loss_val: 3.9729 | f1_val: 0.07 | rmse_val: 1.1016
Epoch: 0088 | loss_train: 3.6687 loss_val: 3.9064 | f1_val: 0.08 | rmse_val: 0.9354
Epoch: 0089 | loss_train: 4.5208 loss_val: 3.6275 | f1_val: 0.05 | rmse_val: 1.0334
Epoch: 0090 | loss_train: 4.1520 loss_val: 4.9776 | f1_val: 0.04 | rmse_val: 1.1319
Epoch: 0091 | loss_train: 4.3855 loss_val: 4.2328 | f1_val: 0.06 | rmse_val: 1.0589
Epoch: 0092 | loss_train: 3.9454 loss_val: 4.5373 | f1_val: 0.06 | rmse_val: 1.1436
Epoch: 0093 | loss_train: 4.1865 loss_val: 3.6260 | f1_val: 0.08 | rmse_val: 0.9882
Epoch: 0094 | loss_train: 4.3573 loss_val: 4.8584 | f1_val: 0.08 | rmse_val: 0.9816
Epoch: 0095 | loss_train: 4.3172 loss_val: 3.9690 | f1_val: 0.06 | rmse_val: 1.0317
Epoch: 0096 | loss_train: 4.2222 loss_val: 3.7861 | f1_val: 0.06 | rmse_val: 0.9973
Epoch: 0097 | loss_train: 4.8667 loss_val: 4.5163 | f1_val: 0.07 | rmse_val: 1.2036
Epoch: 0098 | loss_train: 3.8017 loss_val: 3.1161 | f1_val: 0.08 | rmse_val: 1.2202
Epoch: 0099 | loss_train: 4.2598 loss_val: 4.7502 | f1_val: 0.06 | rmse_val: 1.1283
Epoch: 0100 | loss_train: 4.4392 loss_val: 4.6367 | f1_val: 0.06 | rmse_val: 1.0163
Epoch: 0101 | loss_train: 4.2139 loss_val: 4.9848 | f1_val: 0.07 | rmse_val: 1.0889
Epoch: 0102 | loss_train: 4.2959 loss_val: 4.3727 | f1_val: 0.07 | rmse_val: 1.1618
Epoch: 0103 | loss_train: 4.1974 loss_val: 4.6875 | f1_val: 0.06 | rmse_val: 1.0341
Epoch: 0104 | loss_train: 3.8906 loss_val: 4.2898 | f1_val: 0.06 | rmse_val: 1.0744
Epoch: 0105 | loss_train: 3.9242 loss_val: 3.5734 | f1_val: 0.06 | rmse_val: 1.0692
Epoch: 0106 | loss_train: 4.1258 loss_val: 3.4013 | f1_val: 0.06 | rmse_val: 1.0247
Epoch: 0107 | loss_train: 4.4507 loss_val: 3.8905 | f1_val: 0.06 | rmse_val: 0.9529
Epoch: 0108 | loss_train: 4.2550 loss_val: 4.7209 | f1_val: 0.06 | rmse_val: 1.2020
Epoch: 0109 | loss_train: 5.0259 loss_val: 4.7318 | f1_val: 0.07 | rmse_val: 0.8588
Epoch: 0110 | loss_train: 4.2276 loss_val: 4.5265 | f1_val: 0.07 | rmse_val: 0.9656
Optimization Finished!
Train cost: 1152.9075s
Loading 35th epoch
f1_test: 0.07 | rmse_test: 0.9656

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/n_layers', n_heads=8, n_layers=4, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 547522
Epoch: 0001 | loss_train: 21.2887 loss_val: 20.8299 | f1_val: 0.04 | rmse_val: 3.5359
Epoch: 0002 | loss_train: 20.9048 loss_val: 18.1487 | f1_val: 0.08 | rmse_val: 3.3648
Epoch: 0003 | loss_train: 16.8970 loss_val: 15.5534 | f1_val: 0.08 | rmse_val: 2.4123
Epoch: 0004 | loss_train: 7.7753 loss_val: 3.9312 | f1_val: 0.06 | rmse_val: 1.0073
Epoch: 0005 | loss_train: 4.2526 loss_val: 4.8357 | f1_val: 0.06 | rmse_val: 0.7267
Epoch: 0006 | loss_train: 4.2666 loss_val: 4.3472 | f1_val: 0.06 | rmse_val: 0.9176
Epoch: 0007 | loss_train: 4.3472 loss_val: 4.2755 | f1_val: 0.07 | rmse_val: 1.1388
Epoch: 0008 | loss_train: 3.8091 loss_val: 5.0943 | f1_val: 0.04 | rmse_val: 1.1786
Epoch: 0009 | loss_train: 4.3082 loss_val: 5.9454 | f1_val: 0.06 | rmse_val: 1.1587
Epoch: 0010 | loss_train: 4.1242 loss_val: 5.3922 | f1_val: 0.05 | rmse_val: 1.2042
Epoch: 0011 | loss_train: 4.5417 loss_val: 4.4752 | f1_val: 0.06 | rmse_val: 1.2837
Epoch: 0012 | loss_train: 4.2850 loss_val: 3.8844 | f1_val: 0.06 | rmse_val: 1.1744
Epoch: 0013 | loss_train: 4.4133 loss_val: 4.3170 | f1_val: 0.06 | rmse_val: 1.1882
Epoch: 0014 | loss_train: 4.4719 loss_val: 4.4388 | f1_val: 0.07 | rmse_val: 1.0609
Epoch: 0015 | loss_train: 4.2940 loss_val: 3.9645 | f1_val: 0.07 | rmse_val: 0.9550
Epoch: 0016 | loss_train: 4.2930 loss_val: 5.2963 | f1_val: 0.06 | rmse_val: 1.2198
Epoch: 0017 | loss_train: 4.5617 loss_val: 4.0054 | f1_val: 0.07 | rmse_val: 1.2015
Epoch: 0018 | loss_train: 4.9154 loss_val: 5.3812 | f1_val: 0.05 | rmse_val: 1.0427
Epoch: 0019 | loss_train: 4.4345 loss_val: 4.8815 | f1_val: 0.06 | rmse_val: 0.9100
Epoch: 0020 | loss_train: 4.3366 loss_val: 4.1968 | f1_val: 0.07 | rmse_val: 1.0129
Epoch: 0021 | loss_train: 4.1043 loss_val: 4.9885 | f1_val: 0.08 | rmse_val: 0.9015
Epoch: 0022 | loss_train: 4.3934 loss_val: 5.9398 | f1_val: 0.09 | rmse_val: 1.4959
Epoch: 0023 | loss_train: 3.9898 loss_val: 5.2452 | f1_val: 0.06 | rmse_val: 1.6617
Epoch: 0024 | loss_train: 4.7351 loss_val: 3.9051 | f1_val: 0.06 | rmse_val: 1.0455
Epoch: 0025 | loss_train: 4.0562 loss_val: 5.5500 | f1_val: 0.07 | rmse_val: 1.1462
Epoch: 0026 | loss_train: 3.8519 loss_val: 4.0424 | f1_val: 0.07 | rmse_val: 1.1303
Epoch: 0027 | loss_train: 4.4332 loss_val: 4.4098 | f1_val: 0.05 | rmse_val: 1.4569
Epoch: 0028 | loss_train: 4.4718 loss_val: 6.0545 | f1_val: 0.06 | rmse_val: 1.6190
Epoch: 0029 | loss_train: 4.2833 loss_val: 4.0297 | f1_val: 0.08 | rmse_val: 1.6725
Epoch: 0030 | loss_train: 4.9649 loss_val: 3.7584 | f1_val: 0.07 | rmse_val: 1.7436
Epoch: 0031 | loss_train: 4.7453 loss_val: 4.2658 | f1_val: 0.06 | rmse_val: 1.2338
Epoch: 0032 | loss_train: 4.0774 loss_val: 4.4118 | f1_val: 0.08 | rmse_val: 1.1679
Epoch: 0033 | loss_train: 4.2759 loss_val: 3.6847 | f1_val: 0.06 | rmse_val: 1.3360
Epoch: 0034 | loss_train: 4.2747 loss_val: 3.4980 | f1_val: 0.06 | rmse_val: 1.4350
Epoch: 0035 | loss_train: 4.4732 loss_val: 6.0434 | f1_val: 0.06 | rmse_val: 2.1855
Epoch: 0036 | loss_train: 5.3524 loss_val: 3.3227 | f1_val: 0.08 | rmse_val: 1.1046
Epoch: 0037 | loss_train: 4.7103 loss_val: 3.4225 | f1_val: 0.06 | rmse_val: 1.3486
Epoch: 0038 | loss_train: 4.4588 loss_val: 4.6823 | f1_val: 0.06 | rmse_val: 0.9474
Epoch: 0039 | loss_train: 4.3470 loss_val: 4.4133 | f1_val: 0.07 | rmse_val: 1.4247
Epoch: 0040 | loss_train: 4.6060 loss_val: 4.5112 | f1_val: 0.07 | rmse_val: 1.0438
Epoch: 0041 | loss_train: 4.4258 loss_val: 5.8918 | f1_val: 0.06 | rmse_val: 0.8777
Epoch: 0042 | loss_train: 4.2009 loss_val: 3.8121 | f1_val: 0.05 | rmse_val: 1.0928
Epoch: 0043 | loss_train: 4.2494 loss_val: 4.6366 | f1_val: 0.07 | rmse_val: 1.1248
Epoch: 0044 | loss_train: 4.3029 loss_val: 3.4939 | f1_val: 0.07 | rmse_val: 1.0835
Epoch: 0045 | loss_train: 4.2337 loss_val: 4.4376 | f1_val: 0.06 | rmse_val: 0.9206
Epoch: 0046 | loss_train: 4.8458 loss_val: 3.9170 | f1_val: 0.07 | rmse_val: 0.8290
Epoch: 0047 | loss_train: 4.0792 loss_val: 5.2781 | f1_val: 0.06 | rmse_val: 1.2458
Epoch: 0048 | loss_train: 4.2679 loss_val: 5.9883 | f1_val: 0.06 | rmse_val: 0.9227
Epoch: 0049 | loss_train: 4.1161 loss_val: 3.8253 | f1_val: 0.06 | rmse_val: 1.2759
Epoch: 0050 | loss_train: 4.2833 loss_val: 4.9012 | f1_val: 0.08 | rmse_val: 1.0478
Epoch: 0051 | loss_train: 4.0457 loss_val: 3.9206 | f1_val: 0.07 | rmse_val: 0.8662
Epoch: 0052 | loss_train: 3.9010 loss_val: 3.6398 | f1_val: 0.06 | rmse_val: 1.0616
Epoch: 0053 | loss_train: 4.0298 loss_val: 4.7229 | f1_val: 0.05 | rmse_val: 1.1297
Epoch: 0054 | loss_train: 4.6473 loss_val: 5.1886 | f1_val: 0.05 | rmse_val: 1.0347
Epoch: 0055 | loss_train: 4.7263 loss_val: 4.7181 | f1_val: 0.08 | rmse_val: 1.0221
Epoch: 0056 | loss_train: 3.8528 loss_val: 3.3200 | f1_val: 0.04 | rmse_val: 0.9638
Epoch: 0057 | loss_train: 4.5022 loss_val: 3.7835 | f1_val: 0.06 | rmse_val: 1.0702
Epoch: 0058 | loss_train: 3.8381 loss_val: 3.4570 | f1_val: 0.05 | rmse_val: 1.1218
Epoch: 0059 | loss_train: 4.0853 loss_val: 4.0058 | f1_val: 0.05 | rmse_val: 0.9030
Epoch: 0060 | loss_train: 4.3985 loss_val: 3.8881 | f1_val: 0.06 | rmse_val: 1.0980
Epoch: 0061 | loss_train: 4.2144 loss_val: 5.7077 | f1_val: 0.07 | rmse_val: 1.0118
Epoch: 0062 | loss_train: 4.3663 loss_val: 3.7650 | f1_val: 0.06 | rmse_val: 1.0742
Epoch: 0063 | loss_train: 4.1986 loss_val: 4.7721 | f1_val: 0.06 | rmse_val: 0.8921
Epoch: 0064 | loss_train: 4.2005 loss_val: 4.2852 | f1_val: 0.05 | rmse_val: 1.0654
Epoch: 0065 | loss_train: 4.3637 loss_val: 3.8456 | f1_val: 0.07 | rmse_val: 1.0563
Epoch: 0066 | loss_train: 4.1980 loss_val: 4.5982 | f1_val: 0.07 | rmse_val: 0.9068
Epoch: 0067 | loss_train: 4.0535 loss_val: 6.0426 | f1_val: 0.08 | rmse_val: 1.1457
Epoch: 0068 | loss_train: 4.4013 loss_val: 4.7815 | f1_val: 0.07 | rmse_val: 0.9313
Epoch: 0069 | loss_train: 4.6589 loss_val: 4.2985 | f1_val: 0.07 | rmse_val: 1.0609
Epoch: 0070 | loss_train: 4.1669 loss_val: 4.7360 | f1_val: 0.07 | rmse_val: 1.0653
Epoch: 0071 | loss_train: 4.1791 loss_val: 4.4563 | f1_val: 0.05 | rmse_val: 1.0179
Epoch: 0072 | loss_train: 3.8173 loss_val: 4.3542 | f1_val: 0.07 | rmse_val: 0.8681
Epoch: 0073 | loss_train: 4.2597 loss_val: 5.7446 | f1_val: 0.07 | rmse_val: 1.0678
Epoch: 0074 | loss_train: 3.9365 loss_val: 4.6557 | f1_val: 0.07 | rmse_val: 1.0435
Epoch: 0075 | loss_train: 4.2982 loss_val: 5.6497 | f1_val: 0.06 | rmse_val: 1.0859
Epoch: 0076 | loss_train: 4.2154 loss_val: 4.3423 | f1_val: 0.05 | rmse_val: 1.0426
Epoch: 0077 | loss_train: 3.9777 loss_val: 3.5669 | f1_val: 0.05 | rmse_val: 0.9880
Epoch: 0078 | loss_train: 4.3477 loss_val: 5.0277 | f1_val: 0.06 | rmse_val: 1.0765
Epoch: 0079 | loss_train: 4.3793 loss_val: 5.1236 | f1_val: 0.07 | rmse_val: 1.1124
Epoch: 0080 | loss_train: 4.2042 loss_val: 2.9076 | f1_val: 0.08 | rmse_val: 1.2213
Epoch: 0081 | loss_train: 3.8763 loss_val: 3.5960 | f1_val: 0.07 | rmse_val: 1.1284
Epoch: 0082 | loss_train: 4.0676 loss_val: 4.2097 | f1_val: 0.06 | rmse_val: 1.0224
Epoch: 0083 | loss_train: 3.8076 loss_val: 4.9700 | f1_val: 0.06 | rmse_val: 1.1813
Epoch: 0084 | loss_train: 4.5039 loss_val: 3.8155 | f1_val: 0.07 | rmse_val: 1.0520
Epoch: 0085 | loss_train: 4.2448 loss_val: 4.4170 | f1_val: 0.07 | rmse_val: 0.8553
Epoch: 0086 | loss_train: 4.0265 loss_val: 6.0872 | f1_val: 0.08 | rmse_val: 0.9101
Epoch: 0087 | loss_train: 4.2824 loss_val: 3.9647 | f1_val: 0.06 | rmse_val: 1.1317
Epoch: 0088 | loss_train: 3.6592 loss_val: 3.9210 | f1_val: 0.06 | rmse_val: 0.9919
Epoch: 0089 | loss_train: 4.5080 loss_val: 3.6189 | f1_val: 0.06 | rmse_val: 0.9883
Epoch: 0090 | loss_train: 4.1389 loss_val: 4.9947 | f1_val: 0.06 | rmse_val: 1.0072
Epoch: 0091 | loss_train: 4.3740 loss_val: 4.2079 | f1_val: 0.05 | rmse_val: 0.9524
Epoch: 0092 | loss_train: 3.9308 loss_val: 4.5189 | f1_val: 0.05 | rmse_val: 0.9321
Epoch: 0093 | loss_train: 4.1731 loss_val: 3.6117 | f1_val: 0.07 | rmse_val: 1.2019
Epoch: 0094 | loss_train: 4.3425 loss_val: 4.8395 | f1_val: 0.06 | rmse_val: 1.0988
Epoch: 0095 | loss_train: 4.3053 loss_val: 3.9612 | f1_val: 0.06 | rmse_val: 0.9904
Epoch: 0096 | loss_train: 4.2030 loss_val: 3.7809 | f1_val: 0.07 | rmse_val: 0.9565
Epoch: 0097 | loss_train: 4.8458 loss_val: 4.5222 | f1_val: 0.08 | rmse_val: 1.0928
Epoch: 0098 | loss_train: 3.7890 loss_val: 3.1102 | f1_val: 0.09 | rmse_val: 1.1456
Epoch: 0099 | loss_train: 4.2366 loss_val: 4.7517 | f1_val: 0.07 | rmse_val: 0.9452
Epoch: 0100 | loss_train: 4.4121 loss_val: 4.5400 | f1_val: 0.06 | rmse_val: 1.0162
Epoch: 0101 | loss_train: 4.2033 loss_val: 4.9536 | f1_val: 0.05 | rmse_val: 1.0180
Epoch: 0102 | loss_train: 4.2802 loss_val: 4.3603 | f1_val: 0.05 | rmse_val: 1.0994
Epoch: 0103 | loss_train: 4.1734 loss_val: 4.6725 | f1_val: 0.06 | rmse_val: 0.9902
Epoch: 0104 | loss_train: 3.8725 loss_val: 4.2534 | f1_val: 0.08 | rmse_val: 1.0527
Epoch: 0105 | loss_train: 3.9178 loss_val: 3.5546 | f1_val: 0.07 | rmse_val: 1.0647
Epoch: 0106 | loss_train: 4.0929 loss_val: 3.3661 | f1_val: 0.05 | rmse_val: 0.9127
Epoch: 0107 | loss_train: 4.4023 loss_val: 3.8279 | f1_val: 0.06 | rmse_val: 1.1521
Epoch: 0108 | loss_train: 4.2345 loss_val: 4.7079 | f1_val: 0.05 | rmse_val: 1.2721
Epoch: 0109 | loss_train: 5.0111 loss_val: 4.6337 | f1_val: 0.06 | rmse_val: 0.8932
Epoch: 0110 | loss_train: 4.1638 loss_val: 4.4480 | f1_val: 0.04 | rmse_val: 0.9370
Optimization Finished!
Train cost: 1458.3609s
Loading 22th epoch
f1_test: 0.04 | rmse_test: 0.9370

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/n_layers', n_heads=8, n_layers=5, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 680002
Epoch: 0001 | loss_train: 19.1191 loss_val: 18.4662 | f1_val: 0.07 | rmse_val: 3.1580
Epoch: 0002 | loss_train: 18.6341 loss_val: 15.7178 | f1_val: 0.07 | rmse_val: 3.1093
Epoch: 0003 | loss_train: 14.5036 loss_val: 12.7023 | f1_val: 0.05 | rmse_val: 1.7104
Epoch: 0004 | loss_train: 6.1196 loss_val: 4.2279 | f1_val: 0.07 | rmse_val: 1.3479
Epoch: 0005 | loss_train: 4.3819 loss_val: 4.1856 | f1_val: 0.06 | rmse_val: 1.0071
Epoch: 0006 | loss_train: 4.3562 loss_val: 4.0841 | f1_val: 0.06 | rmse_val: 1.0709
Epoch: 0007 | loss_train: 4.3814 loss_val: 4.1733 | f1_val: 0.06 | rmse_val: 0.9636
Epoch: 0008 | loss_train: 3.8042 loss_val: 5.0436 | f1_val: 0.06 | rmse_val: 1.0166
Epoch: 0009 | loss_train: 4.3034 loss_val: 5.8723 | f1_val: 0.07 | rmse_val: 1.0984
Epoch: 0010 | loss_train: 4.1575 loss_val: 5.3413 | f1_val: 0.04 | rmse_val: 1.3532
Epoch: 0011 | loss_train: 4.5756 loss_val: 4.4531 | f1_val: 0.07 | rmse_val: 1.2173
Epoch: 0012 | loss_train: 4.3097 loss_val: 3.8328 | f1_val: 0.08 | rmse_val: 1.0795
Epoch: 0013 | loss_train: 4.4798 loss_val: 4.3203 | f1_val: 0.07 | rmse_val: 1.2397
Epoch: 0014 | loss_train: 4.5419 loss_val: 4.4314 | f1_val: 0.06 | rmse_val: 1.0340
Epoch: 0015 | loss_train: 4.3284 loss_val: 3.9756 | f1_val: 0.06 | rmse_val: 1.0866
Epoch: 0016 | loss_train: 4.3353 loss_val: 5.3032 | f1_val: 0.05 | rmse_val: 1.3586
Epoch: 0017 | loss_train: 4.5774 loss_val: 4.0276 | f1_val: 0.07 | rmse_val: 1.1466
Epoch: 0018 | loss_train: 4.9397 loss_val: 5.3864 | f1_val: 0.05 | rmse_val: 1.0655
Epoch: 0019 | loss_train: 4.4594 loss_val: 4.8976 | f1_val: 0.06 | rmse_val: 0.9465
Epoch: 0020 | loss_train: 4.3723 loss_val: 4.1794 | f1_val: 0.06 | rmse_val: 1.0778
Epoch: 0021 | loss_train: 4.1359 loss_val: 5.0099 | f1_val: 0.07 | rmse_val: 1.0659
Epoch: 0022 | loss_train: 4.4293 loss_val: 5.9417 | f1_val: 0.06 | rmse_val: 1.5923
Epoch: 0023 | loss_train: 4.0317 loss_val: 5.2746 | f1_val: 0.07 | rmse_val: 1.8500
Epoch: 0024 | loss_train: 4.8295 loss_val: 3.9367 | f1_val: 0.07 | rmse_val: 1.0553
Epoch: 0025 | loss_train: 4.1715 loss_val: 5.5585 | f1_val: 0.08 | rmse_val: 1.0661
Epoch: 0026 | loss_train: 3.9125 loss_val: 4.0752 | f1_val: 0.08 | rmse_val: 0.9675
Epoch: 0027 | loss_train: 4.4609 loss_val: 4.4080 | f1_val: 0.06 | rmse_val: 1.6037
Epoch: 0028 | loss_train: 4.5001 loss_val: 6.0575 | f1_val: 0.06 | rmse_val: 1.5993
Epoch: 0029 | loss_train: 4.3378 loss_val: 3.9989 | f1_val: 0.06 | rmse_val: 1.4995
Epoch: 0030 | loss_train: 5.0428 loss_val: 3.7247 | f1_val: 0.07 | rmse_val: 1.7071
Epoch: 0031 | loss_train: 4.7451 loss_val: 4.2127 | f1_val: 0.07 | rmse_val: 1.1769
Epoch: 0032 | loss_train: 4.1033 loss_val: 4.4240 | f1_val: 0.05 | rmse_val: 1.0131
Epoch: 0033 | loss_train: 4.2758 loss_val: 3.6511 | f1_val: 0.06 | rmse_val: 1.3157
Epoch: 0034 | loss_train: 4.2868 loss_val: 3.4359 | f1_val: 0.05 | rmse_val: 1.2730
Epoch: 0035 | loss_train: 4.4626 loss_val: 5.7378 | f1_val: 0.06 | rmse_val: 2.0009
Epoch: 0036 | loss_train: 5.3215 loss_val: 3.3627 | f1_val: 0.07 | rmse_val: 0.9475
Epoch: 0037 | loss_train: 4.6755 loss_val: 3.4075 | f1_val: 0.06 | rmse_val: 1.0880
Epoch: 0038 | loss_train: 4.4603 loss_val: 4.7041 | f1_val: 0.07 | rmse_val: 0.8888
Epoch: 0039 | loss_train: 4.3830 loss_val: 4.4365 | f1_val: 0.05 | rmse_val: 1.3057
Epoch: 0040 | loss_train: 4.6337 loss_val: 4.5573 | f1_val: 0.06 | rmse_val: 0.9067
Epoch: 0041 | loss_train: 4.4488 loss_val: 5.9167 | f1_val: 0.06 | rmse_val: 0.9198
Epoch: 0042 | loss_train: 4.2194 loss_val: 3.8319 | f1_val: 0.05 | rmse_val: 1.1188
Epoch: 0043 | loss_train: 4.2720 loss_val: 4.6710 | f1_val: 0.06 | rmse_val: 1.1436
Epoch: 0044 | loss_train: 4.3349 loss_val: 3.5332 | f1_val: 0.07 | rmse_val: 1.1632
Epoch: 0045 | loss_train: 4.2468 loss_val: 4.5074 | f1_val: 0.06 | rmse_val: 1.0051
Epoch: 0046 | loss_train: 4.8706 loss_val: 3.9062 | f1_val: 0.05 | rmse_val: 0.9937
Epoch: 0047 | loss_train: 4.0937 loss_val: 5.2815 | f1_val: 0.08 | rmse_val: 1.1379
Epoch: 0048 | loss_train: 4.2992 loss_val: 6.0404 | f1_val: 0.07 | rmse_val: 0.9825
Epoch: 0049 | loss_train: 4.1517 loss_val: 3.8486 | f1_val: 0.08 | rmse_val: 1.1997
Epoch: 0050 | loss_train: 4.3079 loss_val: 4.9286 | f1_val: 0.05 | rmse_val: 0.9894
Epoch: 0051 | loss_train: 4.0644 loss_val: 3.9380 | f1_val: 0.05 | rmse_val: 0.9869
Epoch: 0052 | loss_train: 3.9300 loss_val: 3.6664 | f1_val: 0.06 | rmse_val: 0.9429
Epoch: 0053 | loss_train: 4.0549 loss_val: 4.7211 | f1_val: 0.08 | rmse_val: 1.0020
Epoch: 0054 | loss_train: 4.6707 loss_val: 5.2017 | f1_val: 0.05 | rmse_val: 0.9274
Epoch: 0055 | loss_train: 4.7544 loss_val: 4.7367 | f1_val: 0.05 | rmse_val: 1.1893
Epoch: 0056 | loss_train: 3.8824 loss_val: 3.3014 | f1_val: 0.05 | rmse_val: 1.0209
Epoch: 0057 | loss_train: 4.5301 loss_val: 3.8034 | f1_val: 0.06 | rmse_val: 1.1462
Epoch: 0058 | loss_train: 3.8657 loss_val: 3.4812 | f1_val: 0.06 | rmse_val: 1.0580
Epoch: 0059 | loss_train: 4.1106 loss_val: 4.0378 | f1_val: 0.05 | rmse_val: 0.9283
Epoch: 0060 | loss_train: 4.4290 loss_val: 3.9225 | f1_val: 0.08 | rmse_val: 1.1441
Epoch: 0061 | loss_train: 4.2252 loss_val: 5.7278 | f1_val: 0.06 | rmse_val: 1.0233
Epoch: 0062 | loss_train: 4.3889 loss_val: 3.8035 | f1_val: 0.05 | rmse_val: 1.0201
Epoch: 0063 | loss_train: 4.2265 loss_val: 4.8029 | f1_val: 0.06 | rmse_val: 1.1320
Epoch: 0064 | loss_train: 4.2336 loss_val: 4.2916 | f1_val: 0.05 | rmse_val: 1.1075
Epoch: 0065 | loss_train: 4.3980 loss_val: 3.8485 | f1_val: 0.08 | rmse_val: 1.0531
Epoch: 0066 | loss_train: 4.2216 loss_val: 4.6029 | f1_val: 0.06 | rmse_val: 0.9154
Epoch: 0067 | loss_train: 4.0883 loss_val: 6.1031 | f1_val: 0.06 | rmse_val: 1.0378
Epoch: 0068 | loss_train: 4.4198 loss_val: 4.8097 | f1_val: 0.06 | rmse_val: 1.0441
Epoch: 0069 | loss_train: 4.6865 loss_val: 4.2838 | f1_val: 0.08 | rmse_val: 1.0558
Epoch: 0070 | loss_train: 4.1948 loss_val: 4.7448 | f1_val: 0.06 | rmse_val: 1.1528
Epoch: 0071 | loss_train: 4.2070 loss_val: 4.4623 | f1_val: 0.06 | rmse_val: 1.1002
Epoch: 0072 | loss_train: 3.8502 loss_val: 4.3793 | f1_val: 0.08 | rmse_val: 0.9876
Epoch: 0073 | loss_train: 4.2876 loss_val: 5.8104 | f1_val: 0.06 | rmse_val: 0.9918
Epoch: 0074 | loss_train: 3.9563 loss_val: 4.6756 | f1_val: 0.06 | rmse_val: 0.9907
Epoch: 0075 | loss_train: 4.3195 loss_val: 5.6908 | f1_val: 0.06 | rmse_val: 0.9269
Epoch: 0076 | loss_train: 4.2443 loss_val: 4.3656 | f1_val: 0.06 | rmse_val: 0.9553
Epoch: 0077 | loss_train: 4.0080 loss_val: 3.5987 | f1_val: 0.05 | rmse_val: 1.1634
Epoch: 0078 | loss_train: 4.3852 loss_val: 5.0396 | f1_val: 0.05 | rmse_val: 1.1108
Epoch: 0079 | loss_train: 4.4072 loss_val: 5.1231 | f1_val: 0.06 | rmse_val: 1.0215
Epoch: 0080 | loss_train: 4.2343 loss_val: 2.9300 | f1_val: 0.06 | rmse_val: 1.0257
Epoch: 0081 | loss_train: 3.9068 loss_val: 3.6335 | f1_val: 0.06 | rmse_val: 1.0061
Epoch: 0082 | loss_train: 4.0907 loss_val: 4.2083 | f1_val: 0.05 | rmse_val: 1.0812
Epoch: 0083 | loss_train: 3.8285 loss_val: 5.0009 | f1_val: 0.04 | rmse_val: 0.9914
Epoch: 0084 | loss_train: 4.5334 loss_val: 3.8549 | f1_val: 0.05 | rmse_val: 0.8426
Epoch: 0085 | loss_train: 4.2726 loss_val: 4.4133 | f1_val: 0.07 | rmse_val: 0.8911
Epoch: 0086 | loss_train: 4.0511 loss_val: 6.1271 | f1_val: 0.06 | rmse_val: 0.9549
Epoch: 0087 | loss_train: 4.3112 loss_val: 3.9846 | f1_val: 0.06 | rmse_val: 1.0173
Epoch: 0088 | loss_train: 3.6803 loss_val: 3.9351 | f1_val: 0.08 | rmse_val: 0.9380
Epoch: 0089 | loss_train: 4.5347 loss_val: 3.6546 | f1_val: 0.05 | rmse_val: 1.0189
Epoch: 0090 | loss_train: 4.1630 loss_val: 4.9590 | f1_val: 0.06 | rmse_val: 1.0160
Epoch: 0091 | loss_train: 4.3965 loss_val: 4.2401 | f1_val: 0.06 | rmse_val: 1.0636
Epoch: 0092 | loss_train: 3.9562 loss_val: 4.5340 | f1_val: 0.06 | rmse_val: 1.0111
Epoch: 0093 | loss_train: 4.1999 loss_val: 3.6381 | f1_val: 0.07 | rmse_val: 1.0959
Epoch: 0094 | loss_train: 4.3783 loss_val: 4.8698 | f1_val: 0.06 | rmse_val: 0.9145
Epoch: 0095 | loss_train: 4.3277 loss_val: 3.9804 | f1_val: 0.07 | rmse_val: 1.0187
Epoch: 0096 | loss_train: 4.2348 loss_val: 3.8001 | f1_val: 0.06 | rmse_val: 0.9764
Epoch: 0097 | loss_train: 4.8797 loss_val: 4.5125 | f1_val: 0.05 | rmse_val: 1.0721
Epoch: 0098 | loss_train: 3.8154 loss_val: 3.1250 | f1_val: 0.08 | rmse_val: 1.0945
Epoch: 0099 | loss_train: 4.2725 loss_val: 4.7870 | f1_val: 0.06 | rmse_val: 0.9375
Epoch: 0100 | loss_train: 4.4532 loss_val: 4.6522 | f1_val: 0.05 | rmse_val: 0.9725
Epoch: 0101 | loss_train: 4.2293 loss_val: 5.0093 | f1_val: 0.06 | rmse_val: 1.0650
Epoch: 0102 | loss_train: 4.3140 loss_val: 4.3995 | f1_val: 0.08 | rmse_val: 1.0583
Epoch: 0103 | loss_train: 4.2112 loss_val: 4.7090 | f1_val: 0.07 | rmse_val: 1.0723
Epoch: 0104 | loss_train: 3.9035 loss_val: 4.3142 | f1_val: 0.05 | rmse_val: 1.1314
Epoch: 0105 | loss_train: 3.9347 loss_val: 3.5754 | f1_val: 0.05 | rmse_val: 1.1586
Epoch: 0106 | loss_train: 4.1411 loss_val: 3.4198 | f1_val: 0.07 | rmse_val: 0.9820
Epoch: 0107 | loss_train: 4.4676 loss_val: 3.9108 | f1_val: 0.06 | rmse_val: 1.0246
Epoch: 0108 | loss_train: 4.2674 loss_val: 4.7274 | f1_val: 0.05 | rmse_val: 1.2051
Epoch: 0109 | loss_train: 5.0385 loss_val: 4.7584 | f1_val: 0.06 | rmse_val: 0.9922
Epoch: 0110 | loss_train: 4.2428 loss_val: 4.5378 | f1_val: 0.06 | rmse_val: 1.0021
Epoch: 0111 | loss_train: 4.2519 loss_val: 3.1726 | f1_val: 0.06 | rmse_val: 1.0697
Epoch: 0112 | loss_train: 3.9958 loss_val: 4.0569 | f1_val: 0.07 | rmse_val: 1.0255
Epoch: 0113 | loss_train: 4.3672 loss_val: 3.0111 | f1_val: 0.06 | rmse_val: 0.9671
Epoch: 0114 | loss_train: 3.9072 loss_val: 4.1993 | f1_val: 0.04 | rmse_val: 1.0245
Epoch: 0115 | loss_train: 3.9237 loss_val: 4.2210 | f1_val: 0.07 | rmse_val: 1.1896
Epoch: 0116 | loss_train: 3.8748 loss_val: 4.0007 | f1_val: 0.08 | rmse_val: 0.9177
Epoch: 0117 | loss_train: 4.7075 loss_val: 5.8221 | f1_val: 0.08 | rmse_val: 0.7684
Epoch: 0118 | loss_train: 4.0129 loss_val: 4.5413 | f1_val: 0.08 | rmse_val: 0.8328
Optimization Finished!
Train cost: 1922.4998s
Loading 88th epoch
f1_test: 0.08 | rmse_test: 0.8328

>>> run.py: Namespace(dataset='LINUX', device=1, experiment='n_layers', log_path='log/nagphormer/LINUX/n_layers', path='data/nagphormer', plot_path='plots/nagphormer/LINUX/n_layers', pyg_path='data/pyg/LINUX')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/n_layers', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.05 | rmse_val: 7.4451
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 7.4450
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.08 | rmse_val: 6.8450
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.03 | rmse_val: 6.0641
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.05 | rmse_val: 4.4422
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.06 | rmse_val: 1.9609
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.06 | rmse_val: 1.4814
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.06 | rmse_val: 2.2505
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.07 | rmse_val: 1.4667
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.07 | rmse_val: 1.3041
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.6533
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.06 | rmse_val: 1.0961
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.06 | rmse_val: 1.5658
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.05 | rmse_val: 1.5932
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.05 | rmse_val: 1.1988
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.04 | rmse_val: 1.2835
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.05 | rmse_val: 1.7141
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.06 | rmse_val: 1.2771
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.05 | rmse_val: 0.9793
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.08 | rmse_val: 1.8279
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.07 | rmse_val: 1.3268
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.04 | rmse_val: 1.8338
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.06 | rmse_val: 1.1240
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.03 | rmse_val: 1.8121
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.07 | rmse_val: 1.6883
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.09 | rmse_val: 0.7529
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.07 | rmse_val: 1.7207
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.07 | rmse_val: 1.3494
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.04 | rmse_val: 0.8378
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.07 | rmse_val: 1.1355
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.06 | rmse_val: 0.8232
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.06 | rmse_val: 1.9919
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.07 | rmse_val: 1.4889
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.05 | rmse_val: 0.7824
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.05 | rmse_val: 2.1047
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.06 | rmse_val: 1.1257
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.04 | rmse_val: 1.1966
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.07 | rmse_val: 1.5883
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.06 | rmse_val: 2.1858
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.04 | rmse_val: 1.4801
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.07 | rmse_val: 0.8435
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.06 | rmse_val: 1.0060
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.06 | rmse_val: 1.1348
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.07 | rmse_val: 2.5679
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.07 | rmse_val: 2.5789
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.04 | rmse_val: 1.4546
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.08 | rmse_val: 1.9236
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.06 | rmse_val: 1.3427
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.08 | rmse_val: 1.5619
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.06 | rmse_val: 1.4774
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.05 | rmse_val: 1.6202
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.05 | rmse_val: 1.0127
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.05 | rmse_val: 1.4628
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.05 | rmse_val: 1.4804
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.08 | rmse_val: 1.2230
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.06 | rmse_val: 1.5972
Optimization Finished!
Train cost: 184.1822s
Loading 26th epoch
f1_test: 0.06 | rmse_test: 1.5972

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/n_layers', n_heads=8, n_layers=2, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 286274
Epoch: 0001 | loss_train: 84.2991 loss_val: 79.5978 | f1_val: 0.06 | rmse_val: 7.5956
Epoch: 0002 | loss_train: 80.4715 loss_val: 91.6962 | f1_val: 0.04 | rmse_val: 7.3312
Epoch: 0003 | loss_train: 76.6969 loss_val: 68.8048 | f1_val: 0.06 | rmse_val: 6.9103
Epoch: 0004 | loss_train: 64.5460 loss_val: 58.5072 | f1_val: 0.05 | rmse_val: 5.9569
Epoch: 0005 | loss_train: 47.2663 loss_val: 41.1170 | f1_val: 0.06 | rmse_val: 4.1883
Epoch: 0006 | loss_train: 26.2493 loss_val: 12.0190 | f1_val: 0.05 | rmse_val: 1.3624
Epoch: 0007 | loss_train: 8.6447 loss_val: 5.5274 | f1_val: 0.06 | rmse_val: 2.1738
Epoch: 0008 | loss_train: 7.9875 loss_val: 3.4699 | f1_val: 0.05 | rmse_val: 1.6177
Epoch: 0009 | loss_train: 5.7983 loss_val: 6.6213 | f1_val: 0.08 | rmse_val: 1.1650
Epoch: 0010 | loss_train: 6.5617 loss_val: 9.0586 | f1_val: 0.07 | rmse_val: 1.3687
Epoch: 0011 | loss_train: 7.6991 loss_val: 7.3499 | f1_val: 0.06 | rmse_val: 1.5489
Epoch: 0012 | loss_train: 6.4160 loss_val: 7.8371 | f1_val: 0.06 | rmse_val: 1.0661
Epoch: 0013 | loss_train: 6.5468 loss_val: 7.9401 | f1_val: 0.07 | rmse_val: 1.6313
Epoch: 0014 | loss_train: 7.7408 loss_val: 10.0794 | f1_val: 0.08 | rmse_val: 1.4128
Epoch: 0015 | loss_train: 7.5437 loss_val: 8.0480 | f1_val: 0.06 | rmse_val: 1.2920
Epoch: 0016 | loss_train: 6.5361 loss_val: 4.0180 | f1_val: 0.05 | rmse_val: 1.0623
Epoch: 0017 | loss_train: 7.5645 loss_val: 6.7252 | f1_val: 0.05 | rmse_val: 1.3553
Epoch: 0018 | loss_train: 6.6777 loss_val: 6.4496 | f1_val: 0.06 | rmse_val: 1.5013
Epoch: 0019 | loss_train: 5.4431 loss_val: 9.4986 | f1_val: 0.06 | rmse_val: 0.7589
Epoch: 0020 | loss_train: 6.2076 loss_val: 8.6034 | f1_val: 0.05 | rmse_val: 1.4424
Epoch: 0021 | loss_train: 6.6880 loss_val: 5.2445 | f1_val: 0.05 | rmse_val: 1.1323
Epoch: 0022 | loss_train: 5.8689 loss_val: 6.5962 | f1_val: 0.06 | rmse_val: 1.4455
Epoch: 0023 | loss_train: 6.3792 loss_val: 3.4970 | f1_val: 0.06 | rmse_val: 1.2389
Epoch: 0024 | loss_train: 5.5804 loss_val: 6.9737 | f1_val: 0.04 | rmse_val: 2.0511
Epoch: 0025 | loss_train: 6.5219 loss_val: 6.9064 | f1_val: 0.08 | rmse_val: 1.4585
Epoch: 0026 | loss_train: 7.5845 loss_val: 8.7034 | f1_val: 0.06 | rmse_val: 0.7095
Epoch: 0027 | loss_train: 6.6162 loss_val: 7.5997 | f1_val: 0.05 | rmse_val: 1.6694
Epoch: 0028 | loss_train: 5.5964 loss_val: 8.7121 | f1_val: 0.06 | rmse_val: 1.3298
Epoch: 0029 | loss_train: 5.4579 loss_val: 7.5022 | f1_val: 0.05 | rmse_val: 1.3893
Epoch: 0030 | loss_train: 5.6296 loss_val: 7.2545 | f1_val: 0.05 | rmse_val: 0.9387
Epoch: 0031 | loss_train: 5.7206 loss_val: 6.9507 | f1_val: 0.06 | rmse_val: 0.8335
Epoch: 0032 | loss_train: 7.6062 loss_val: 7.3380 | f1_val: 0.08 | rmse_val: 1.9030
Epoch: 0033 | loss_train: 6.9147 loss_val: 8.3769 | f1_val: 0.06 | rmse_val: 1.6547
Epoch: 0034 | loss_train: 6.1705 loss_val: 4.4201 | f1_val: 0.06 | rmse_val: 0.7782
Epoch: 0035 | loss_train: 5.4585 loss_val: 5.4914 | f1_val: 0.05 | rmse_val: 1.2375
Epoch: 0036 | loss_train: 5.4291 loss_val: 5.8117 | f1_val: 0.08 | rmse_val: 0.9677
Epoch: 0037 | loss_train: 5.6041 loss_val: 7.2972 | f1_val: 0.08 | rmse_val: 1.1494
Epoch: 0038 | loss_train: 6.3013 loss_val: 4.3561 | f1_val: 0.07 | rmse_val: 1.5100
Epoch: 0039 | loss_train: 5.2983 loss_val: 6.9319 | f1_val: 0.06 | rmse_val: 1.9171
Epoch: 0040 | loss_train: 5.0812 loss_val: 6.9062 | f1_val: 0.07 | rmse_val: 1.2964
Epoch: 0041 | loss_train: 5.7453 loss_val: 7.0867 | f1_val: 0.05 | rmse_val: 0.8443
Epoch: 0042 | loss_train: 5.4061 loss_val: 5.9589 | f1_val: 0.08 | rmse_val: 0.8875
Epoch: 0043 | loss_train: 6.0735 loss_val: 5.7854 | f1_val: 0.08 | rmse_val: 0.9193
Epoch: 0044 | loss_train: 6.2463 loss_val: 8.7511 | f1_val: 0.06 | rmse_val: 2.3235
Epoch: 0045 | loss_train: 6.7455 loss_val: 8.1919 | f1_val: 0.04 | rmse_val: 2.6576
Epoch: 0046 | loss_train: 24.0210 loss_val: 11.6822 | f1_val: 0.05 | rmse_val: 1.4467
Epoch: 0047 | loss_train: 8.6692 loss_val: 7.3347 | f1_val: 0.07 | rmse_val: 1.7483
Epoch: 0048 | loss_train: 6.7690 loss_val: 10.0106 | f1_val: 0.05 | rmse_val: 1.3758
Epoch: 0049 | loss_train: 7.1263 loss_val: 7.0776 | f1_val: 0.06 | rmse_val: 1.5232
Epoch: 0050 | loss_train: 6.5227 loss_val: 6.6332 | f1_val: 0.06 | rmse_val: 1.4518
Epoch: 0051 | loss_train: 6.4499 loss_val: 4.8425 | f1_val: 0.06 | rmse_val: 1.6381
Epoch: 0052 | loss_train: 7.1820 loss_val: 10.0974 | f1_val: 0.06 | rmse_val: 1.0769
Epoch: 0053 | loss_train: 7.2866 loss_val: 9.0556 | f1_val: 0.08 | rmse_val: 1.6064
Epoch: 0054 | loss_train: 5.9210 loss_val: 8.7008 | f1_val: 0.05 | rmse_val: 1.2201
Epoch: 0055 | loss_train: 7.1695 loss_val: 10.2885 | f1_val: 0.06 | rmse_val: 1.2377
Epoch: 0056 | loss_train: 7.5192 loss_val: 5.6230 | f1_val: 0.06 | rmse_val: 1.4986
Epoch: 0057 | loss_train: 7.1634 loss_val: 5.8727 | f1_val: 0.07 | rmse_val: 1.5496
Epoch: 0058 | loss_train: 5.4121 loss_val: 7.9195 | f1_val: 0.06 | rmse_val: 0.9463
Epoch: 0059 | loss_train: 7.1607 loss_val: 6.5729 | f1_val: 0.05 | rmse_val: 1.2966
Epoch: 0060 | loss_train: 6.7800 loss_val: 10.5056 | f1_val: 0.05 | rmse_val: 1.3243
Epoch: 0061 | loss_train: 5.7127 loss_val: 6.9272 | f1_val: 0.06 | rmse_val: 1.2414
Epoch: 0062 | loss_train: 6.4945 loss_val: 6.4175 | f1_val: 0.07 | rmse_val: 1.2157
Epoch: 0063 | loss_train: 6.3047 loss_val: 7.1851 | f1_val: 0.07 | rmse_val: 1.3201
Epoch: 0064 | loss_train: 6.2780 loss_val: 5.6825 | f1_val: 0.06 | rmse_val: 1.1252
Epoch: 0065 | loss_train: 6.3413 loss_val: 6.9587 | f1_val: 0.06 | rmse_val: 1.4721
Epoch: 0066 | loss_train: 6.0015 loss_val: 7.6463 | f1_val: 0.05 | rmse_val: 1.6276
Epoch: 0067 | loss_train: 6.1189 loss_val: 5.2364 | f1_val: 0.07 | rmse_val: 1.2767
Optimization Finished!
Train cost: 325.2049s
Loading 37th epoch
f1_test: 0.07 | rmse_test: 1.2767

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/n_layers', n_heads=8, n_layers=3, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 418754
Epoch: 0001 | loss_train: 84.3293 loss_val: 79.8537 | f1_val: 0.08 | rmse_val: 7.6192
Epoch: 0002 | loss_train: 80.8631 loss_val: 92.4156 | f1_val: 0.04 | rmse_val: 7.3891
Epoch: 0003 | loss_train: 77.4757 loss_val: 69.8740 | f1_val: 0.05 | rmse_val: 6.8318
Epoch: 0004 | loss_train: 65.4735 loss_val: 59.7657 | f1_val: 0.07 | rmse_val: 5.9465
Epoch: 0005 | loss_train: 48.0160 loss_val: 42.4181 | f1_val: 0.05 | rmse_val: 4.1556
Epoch: 0006 | loss_train: 26.9014 loss_val: 13.1340 | f1_val: 0.05 | rmse_val: 1.4891
Epoch: 0007 | loss_train: 8.9892 loss_val: 5.3710 | f1_val: 0.06 | rmse_val: 1.7405
Epoch: 0008 | loss_train: 7.8062 loss_val: 3.6417 | f1_val: 0.04 | rmse_val: 1.6151
Epoch: 0009 | loss_train: 5.8148 loss_val: 6.7220 | f1_val: 0.05 | rmse_val: 1.2652
Epoch: 0010 | loss_train: 6.5497 loss_val: 9.7124 | f1_val: 0.05 | rmse_val: 1.3448
Epoch: 0011 | loss_train: 7.6720 loss_val: 8.0060 | f1_val: 0.06 | rmse_val: 1.4115
Epoch: 0012 | loss_train: 6.4028 loss_val: 7.8912 | f1_val: 0.08 | rmse_val: 1.0664
Epoch: 0013 | loss_train: 6.5668 loss_val: 8.1991 | f1_val: 0.06 | rmse_val: 1.7075
Epoch: 0014 | loss_train: 7.7372 loss_val: 10.1445 | f1_val: 0.06 | rmse_val: 1.2676
Epoch: 0015 | loss_train: 7.5297 loss_val: 8.4134 | f1_val: 0.06 | rmse_val: 1.3569
Epoch: 0016 | loss_train: 6.7340 loss_val: 5.6512 | f1_val: 0.06 | rmse_val: 1.4550
Epoch: 0017 | loss_train: 7.6083 loss_val: 6.6223 | f1_val: 0.07 | rmse_val: 1.2997
Epoch: 0018 | loss_train: 6.9429 loss_val: 6.9801 | f1_val: 0.06 | rmse_val: 1.3042
Epoch: 0019 | loss_train: 5.8117 loss_val: 8.6204 | f1_val: 0.06 | rmse_val: 0.7740
Epoch: 0020 | loss_train: 6.5189 loss_val: 9.1159 | f1_val: 0.09 | rmse_val: 1.7058
Epoch: 0021 | loss_train: 7.3876 loss_val: 6.3991 | f1_val: 0.06 | rmse_val: 1.0733
Epoch: 0022 | loss_train: 6.4182 loss_val: 8.6010 | f1_val: 0.02 | rmse_val: 1.4669
Epoch: 0023 | loss_train: 7.2386 loss_val: 4.0563 | f1_val: 0.07 | rmse_val: 1.5642
Epoch: 0024 | loss_train: 6.4053 loss_val: 7.2803 | f1_val: 0.06 | rmse_val: 1.7300
Epoch: 0025 | loss_train: 6.6256 loss_val: 10.5099 | f1_val: 0.09 | rmse_val: 0.8241
Epoch: 0026 | loss_train: 8.9134 loss_val: 8.4581 | f1_val: 0.08 | rmse_val: 1.2438
Epoch: 0027 | loss_train: 7.1826 loss_val: 9.0766 | f1_val: 0.06 | rmse_val: 2.1048
Epoch: 0028 | loss_train: 6.8844 loss_val: 10.0284 | f1_val: 0.07 | rmse_val: 0.7444
Epoch: 0029 | loss_train: 6.5839 loss_val: 9.1273 | f1_val: 0.03 | rmse_val: 1.8124
Epoch: 0030 | loss_train: 6.0242 loss_val: 8.2857 | f1_val: 0.06 | rmse_val: 1.1562
Epoch: 0031 | loss_train: 5.7124 loss_val: 7.7673 | f1_val: 0.07 | rmse_val: 0.9051
Epoch: 0032 | loss_train: 7.2562 loss_val: 7.1312 | f1_val: 0.07 | rmse_val: 1.3417
Epoch: 0033 | loss_train: 7.4559 loss_val: 8.8618 | f1_val: 0.08 | rmse_val: 1.1292
Epoch: 0034 | loss_train: 6.1668 loss_val: 5.0102 | f1_val: 0.08 | rmse_val: 0.8359
Epoch: 0035 | loss_train: 6.6120 loss_val: 5.7580 | f1_val: 0.07 | rmse_val: 1.7847
Epoch: 0036 | loss_train: 5.5024 loss_val: 6.3125 | f1_val: 0.07 | rmse_val: 1.2760
Epoch: 0037 | loss_train: 5.6376 loss_val: 7.0024 | f1_val: 0.05 | rmse_val: 0.7576
Epoch: 0038 | loss_train: 6.7969 loss_val: 5.3470 | f1_val: 0.09 | rmse_val: 0.6620
Epoch: 0039 | loss_train: 7.1010 loss_val: 7.2996 | f1_val: 0.07 | rmse_val: 1.7882
Epoch: 0040 | loss_train: 5.2225 loss_val: 6.8660 | f1_val: 0.05 | rmse_val: 1.6392
Epoch: 0041 | loss_train: 5.6893 loss_val: 7.3790 | f1_val: 0.06 | rmse_val: 1.3271
Epoch: 0042 | loss_train: 5.6801 loss_val: 5.5198 | f1_val: 0.07 | rmse_val: 1.0948
Epoch: 0043 | loss_train: 6.1814 loss_val: 5.7225 | f1_val: 0.04 | rmse_val: 1.0041
Epoch: 0044 | loss_train: 6.3743 loss_val: 8.0863 | f1_val: 0.04 | rmse_val: 2.0727
Epoch: 0045 | loss_train: 6.5866 loss_val: 8.9720 | f1_val: 0.06 | rmse_val: 2.9345
Epoch: 0046 | loss_train: 24.6713 loss_val: 13.9661 | f1_val: 0.05 | rmse_val: 1.4312
Epoch: 0047 | loss_train: 9.4400 loss_val: 7.7027 | f1_val: 0.08 | rmse_val: 1.8710
Epoch: 0048 | loss_train: 6.5962 loss_val: 9.9258 | f1_val: 0.08 | rmse_val: 1.3580
Epoch: 0049 | loss_train: 7.4664 loss_val: 6.1876 | f1_val: 0.08 | rmse_val: 1.2111
Epoch: 0050 | loss_train: 6.6395 loss_val: 6.4271 | f1_val: 0.04 | rmse_val: 1.2921
Epoch: 0051 | loss_train: 6.1126 loss_val: 5.3003 | f1_val: 0.07 | rmse_val: 1.6668
Epoch: 0052 | loss_train: 6.5954 loss_val: 9.3546 | f1_val: 0.06 | rmse_val: 0.8993
Epoch: 0053 | loss_train: 7.7716 loss_val: 9.1343 | f1_val: 0.04 | rmse_val: 1.5426
Epoch: 0054 | loss_train: 6.0137 loss_val: 8.3713 | f1_val: 0.07 | rmse_val: 1.1226
Epoch: 0055 | loss_train: 7.1107 loss_val: 8.9882 | f1_val: 0.07 | rmse_val: 0.9675
Optimization Finished!
Train cost: 354.4811s
Loading 25th epoch
f1_test: 0.07 | rmse_test: 0.9675

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/n_layers', n_heads=8, n_layers=4, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 551234
Epoch: 0001 | loss_train: 84.2537 loss_val: 79.5476 | f1_val: 0.05 | rmse_val: 7.3617
Epoch: 0002 | loss_train: 80.6171 loss_val: 92.2750 | f1_val: 0.06 | rmse_val: 7.4970
Epoch: 0003 | loss_train: 77.3948 loss_val: 69.4195 | f1_val: 0.06 | rmse_val: 6.8420
Epoch: 0004 | loss_train: 65.0366 loss_val: 58.1448 | f1_val: 0.06 | rmse_val: 5.9726
Epoch: 0005 | loss_train: 45.9511 loss_val: 37.9660 | f1_val: 0.03 | rmse_val: 3.7761
Epoch: 0006 | loss_train: 23.2328 loss_val: 9.2254 | f1_val: 0.06 | rmse_val: 0.9314
Epoch: 0007 | loss_train: 7.8710 loss_val: 5.8857 | f1_val: 0.06 | rmse_val: 2.4614
Epoch: 0008 | loss_train: 8.0159 loss_val: 4.3048 | f1_val: 0.08 | rmse_val: 1.0264
Epoch: 0009 | loss_train: 6.1246 loss_val: 5.1504 | f1_val: 0.06 | rmse_val: 1.7214
Epoch: 0010 | loss_train: 6.4379 loss_val: 8.6497 | f1_val: 0.06 | rmse_val: 1.2811
Epoch: 0011 | loss_train: 7.7801 loss_val: 6.6649 | f1_val: 0.06 | rmse_val: 1.5996
Epoch: 0012 | loss_train: 6.4730 loss_val: 7.9054 | f1_val: 0.05 | rmse_val: 1.0624
Epoch: 0013 | loss_train: 6.5657 loss_val: 7.6407 | f1_val: 0.06 | rmse_val: 1.7536
Epoch: 0014 | loss_train: 7.6588 loss_val: 9.7254 | f1_val: 0.06 | rmse_val: 1.1859
Epoch: 0015 | loss_train: 7.4751 loss_val: 7.3139 | f1_val: 0.07 | rmse_val: 1.4812
Epoch: 0016 | loss_train: 8.0831 loss_val: 4.5089 | f1_val: 0.06 | rmse_val: 1.6233
Epoch: 0017 | loss_train: 7.5911 loss_val: 6.3960 | f1_val: 0.04 | rmse_val: 1.3065
Epoch: 0018 | loss_train: 6.9208 loss_val: 6.8274 | f1_val: 0.06 | rmse_val: 1.2837
Epoch: 0019 | loss_train: 5.9587 loss_val: 7.7535 | f1_val: 0.06 | rmse_val: 0.9676
Epoch: 0020 | loss_train: 6.5435 loss_val: 8.6325 | f1_val: 0.05 | rmse_val: 1.9174
Epoch: 0021 | loss_train: 7.5077 loss_val: 6.1156 | f1_val: 0.07 | rmse_val: 1.3996
Epoch: 0022 | loss_train: 6.4869 loss_val: 8.1201 | f1_val: 0.06 | rmse_val: 1.8151
Epoch: 0023 | loss_train: 7.2493 loss_val: 4.1071 | f1_val: 0.06 | rmse_val: 1.4723
Epoch: 0024 | loss_train: 6.4983 loss_val: 7.0762 | f1_val: 0.06 | rmse_val: 1.8710
Epoch: 0025 | loss_train: 6.7613 loss_val: 10.2228 | f1_val: 0.09 | rmse_val: 0.9592
Epoch: 0026 | loss_train: 8.9746 loss_val: 8.3859 | f1_val: 0.05 | rmse_val: 1.3486
Epoch: 0027 | loss_train: 7.5358 loss_val: 8.9817 | f1_val: 0.05 | rmse_val: 2.1401
Epoch: 0028 | loss_train: 7.1937 loss_val: 9.4882 | f1_val: 0.06 | rmse_val: 1.3930
Epoch: 0029 | loss_train: 6.2196 loss_val: 9.3597 | f1_val: 0.07 | rmse_val: 1.1932
Epoch: 0030 | loss_train: 6.2971 loss_val: 8.6562 | f1_val: 0.10 | rmse_val: 1.3810
Epoch: 0031 | loss_train: 6.1499 loss_val: 6.8019 | f1_val: 0.06 | rmse_val: 1.1096
Epoch: 0032 | loss_train: 7.0776 loss_val: 8.1765 | f1_val: 0.08 | rmse_val: 2.1993
Epoch: 0033 | loss_train: 8.3779 loss_val: 9.9638 | f1_val: 0.05 | rmse_val: 0.9408
Epoch: 0034 | loss_train: 6.8312 loss_val: 5.1470 | f1_val: 0.07 | rmse_val: 1.0258
Epoch: 0035 | loss_train: 7.8140 loss_val: 7.6820 | f1_val: 0.04 | rmse_val: 2.3567
Epoch: 0036 | loss_train: 6.6755 loss_val: 8.0783 | f1_val: 0.05 | rmse_val: 1.0621
Epoch: 0037 | loss_train: 6.3779 loss_val: 8.4684 | f1_val: 0.05 | rmse_val: 1.3761
Epoch: 0038 | loss_train: 7.2796 loss_val: 6.5973 | f1_val: 0.07 | rmse_val: 1.6829
Epoch: 0039 | loss_train: 6.0863 loss_val: 8.1135 | f1_val: 0.05 | rmse_val: 1.7177
Epoch: 0040 | loss_train: 6.1347 loss_val: 8.2218 | f1_val: 0.06 | rmse_val: 1.1533
Epoch: 0041 | loss_train: 6.7144 loss_val: 8.0848 | f1_val: 0.07 | rmse_val: 1.1943
Epoch: 0042 | loss_train: 6.4247 loss_val: 6.3756 | f1_val: 0.04 | rmse_val: 1.0991
Epoch: 0043 | loss_train: 6.6041 loss_val: 6.4427 | f1_val: 0.07 | rmse_val: 1.2464
Epoch: 0044 | loss_train: 6.9279 loss_val: 8.5410 | f1_val: 0.07 | rmse_val: 2.2175
Epoch: 0045 | loss_train: 7.9055 loss_val: 8.4843 | f1_val: 0.07 | rmse_val: 2.5112
Epoch: 0046 | loss_train: 20.3636 loss_val: 16.2012 | f1_val: 0.06 | rmse_val: 1.8749
Epoch: 0047 | loss_train: 9.8708 loss_val: 7.4270 | f1_val: 0.05 | rmse_val: 1.8389
Epoch: 0048 | loss_train: 8.1277 loss_val: 9.3943 | f1_val: 0.06 | rmse_val: 1.6196
Epoch: 0049 | loss_train: 7.2662 loss_val: 7.2544 | f1_val: 0.06 | rmse_val: 1.4322
Epoch: 0050 | loss_train: 7.2534 loss_val: 6.6364 | f1_val: 0.07 | rmse_val: 1.2652
Epoch: 0051 | loss_train: 6.6296 loss_val: 4.8345 | f1_val: 0.07 | rmse_val: 1.5547
Epoch: 0052 | loss_train: 7.8184 loss_val: 10.9678 | f1_val: 0.08 | rmse_val: 0.9747
Epoch: 0053 | loss_train: 7.7329 loss_val: 9.2007 | f1_val: 0.06 | rmse_val: 1.5300
Epoch: 0054 | loss_train: 6.4748 loss_val: 8.6904 | f1_val: 0.04 | rmse_val: 1.1944
Epoch: 0055 | loss_train: 7.6140 loss_val: 10.3768 | f1_val: 0.06 | rmse_val: 1.1184
Epoch: 0056 | loss_train: 7.7787 loss_val: 5.6114 | f1_val: 0.05 | rmse_val: 1.5134
Epoch: 0057 | loss_train: 7.5273 loss_val: 5.8836 | f1_val: 0.08 | rmse_val: 1.5014
Epoch: 0058 | loss_train: 5.5302 loss_val: 8.0617 | f1_val: 0.06 | rmse_val: 0.9984
Epoch: 0059 | loss_train: 7.9176 loss_val: 6.5918 | f1_val: 0.06 | rmse_val: 1.5313
Epoch: 0060 | loss_train: 7.0994 loss_val: 10.5100 | f1_val: 0.06 | rmse_val: 1.1480
Optimization Finished!
Train cost: 475.5789s
Loading 30th epoch
f1_test: 0.06 | rmse_test: 1.1480

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/n_layers', n_heads=8, n_layers=5, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 683714
Epoch: 0001 | loss_train: 84.4781 loss_val: 79.9824 | f1_val: 0.05 | rmse_val: 7.4308
Epoch: 0002 | loss_train: 81.3909 loss_val: 93.7835 | f1_val: 0.05 | rmse_val: 7.3611
Epoch: 0003 | loss_train: 79.3025 loss_val: 72.1050 | f1_val: 0.04 | rmse_val: 7.1024
Epoch: 0004 | loss_train: 68.7966 loss_val: 63.8655 | f1_val: 0.05 | rmse_val: 6.4130
Epoch: 0005 | loss_train: 53.2072 loss_val: 48.4909 | f1_val: 0.05 | rmse_val: 4.6394
Epoch: 0006 | loss_train: 32.6043 loss_val: 17.5476 | f1_val: 0.07 | rmse_val: 2.1586
Epoch: 0007 | loss_train: 11.0711 loss_val: 4.5765 | f1_val: 0.04 | rmse_val: 1.5370
Epoch: 0008 | loss_train: 7.4929 loss_val: 3.4335 | f1_val: 0.06 | rmse_val: 1.6979
Epoch: 0009 | loss_train: 5.8203 loss_val: 6.1613 | f1_val: 0.07 | rmse_val: 1.2298
Epoch: 0010 | loss_train: 6.5527 loss_val: 9.0424 | f1_val: 0.04 | rmse_val: 1.5164
Epoch: 0011 | loss_train: 7.7311 loss_val: 7.6904 | f1_val: 0.05 | rmse_val: 1.5075
Epoch: 0012 | loss_train: 6.3666 loss_val: 7.7225 | f1_val: 0.03 | rmse_val: 1.0441
Epoch: 0013 | loss_train: 6.5906 loss_val: 7.9621 | f1_val: 0.06 | rmse_val: 1.4133
Epoch: 0014 | loss_train: 7.2894 loss_val: 8.9991 | f1_val: 0.08 | rmse_val: 1.0169
Epoch: 0015 | loss_train: 9.3419 loss_val: 7.8451 | f1_val: 0.08 | rmse_val: 1.3945
Epoch: 0016 | loss_train: 6.4429 loss_val: 4.2488 | f1_val: 0.07 | rmse_val: 1.0650
Epoch: 0017 | loss_train: 7.6650 loss_val: 6.6807 | f1_val: 0.06 | rmse_val: 1.6068
Epoch: 0018 | loss_train: 6.8314 loss_val: 6.7295 | f1_val: 0.05 | rmse_val: 1.4996
Epoch: 0019 | loss_train: 5.9242 loss_val: 8.1238 | f1_val: 0.04 | rmse_val: 0.8739
Epoch: 0020 | loss_train: 6.5501 loss_val: 8.8350 | f1_val: 0.07 | rmse_val: 1.6591
Epoch: 0021 | loss_train: 7.5301 loss_val: 6.4400 | f1_val: 0.08 | rmse_val: 1.3729
Epoch: 0022 | loss_train: 6.5096 loss_val: 8.4417 | f1_val: 0.07 | rmse_val: 1.7923
Epoch: 0023 | loss_train: 7.9358 loss_val: 4.1026 | f1_val: 0.06 | rmse_val: 1.5042
Epoch: 0024 | loss_train: 6.5299 loss_val: 6.9789 | f1_val: 0.07 | rmse_val: 1.6581
Epoch: 0025 | loss_train: 6.6095 loss_val: 9.9097 | f1_val: 0.06 | rmse_val: 0.7361
Epoch: 0026 | loss_train: 8.7205 loss_val: 8.3526 | f1_val: 0.06 | rmse_val: 1.5400
Epoch: 0027 | loss_train: 6.7876 loss_val: 8.5169 | f1_val: 0.07 | rmse_val: 1.7310
Epoch: 0028 | loss_train: 6.6820 loss_val: 9.7423 | f1_val: 0.07 | rmse_val: 1.0709
Epoch: 0029 | loss_train: 6.4932 loss_val: 9.2869 | f1_val: 0.08 | rmse_val: 1.6856
Epoch: 0030 | loss_train: 6.2299 loss_val: 8.7284 | f1_val: 0.07 | rmse_val: 1.3871
Epoch: 0031 | loss_train: 6.1763 loss_val: 6.7363 | f1_val: 0.07 | rmse_val: 0.8939
Epoch: 0032 | loss_train: 7.1279 loss_val: 7.9539 | f1_val: 0.04 | rmse_val: 2.0600
Epoch: 0033 | loss_train: 8.0274 loss_val: 9.6859 | f1_val: 0.07 | rmse_val: 0.9918
Epoch: 0034 | loss_train: 6.5712 loss_val: 5.2228 | f1_val: 0.06 | rmse_val: 0.9535
Epoch: 0035 | loss_train: 7.3981 loss_val: 6.9298 | f1_val: 0.08 | rmse_val: 2.3220
Epoch: 0036 | loss_train: 6.0628 loss_val: 6.3710 | f1_val: 0.07 | rmse_val: 1.0367
Epoch: 0037 | loss_train: 5.5517 loss_val: 7.1989 | f1_val: 0.06 | rmse_val: 0.9215
Epoch: 0038 | loss_train: 6.5393 loss_val: 5.2778 | f1_val: 0.06 | rmse_val: 0.8298
Epoch: 0039 | loss_train: 6.2323 loss_val: 7.4651 | f1_val: 0.05 | rmse_val: 1.1693
Epoch: 0040 | loss_train: 5.3212 loss_val: 6.6721 | f1_val: 0.05 | rmse_val: 1.4172
Epoch: 0041 | loss_train: 5.7245 loss_val: 7.8616 | f1_val: 0.06 | rmse_val: 1.1341
Epoch: 0042 | loss_train: 5.5432 loss_val: 5.3354 | f1_val: 0.06 | rmse_val: 0.9258
Epoch: 0043 | loss_train: 6.1154 loss_val: 5.5077 | f1_val: 0.08 | rmse_val: 0.8606
Epoch: 0044 | loss_train: 6.0404 loss_val: 7.7217 | f1_val: 0.07 | rmse_val: 1.8352
Epoch: 0045 | loss_train: 6.6428 loss_val: 8.7066 | f1_val: 0.09 | rmse_val: 2.9036
Epoch: 0046 | loss_train: 23.0735 loss_val: 7.2259 | f1_val: 0.09 | rmse_val: 0.9953
Epoch: 0047 | loss_train: 6.8824 loss_val: 8.1264 | f1_val: 0.05 | rmse_val: 1.2449
Epoch: 0048 | loss_train: 8.5974 loss_val: 7.3935 | f1_val: 0.07 | rmse_val: 1.3407
Epoch: 0049 | loss_train: 6.2511 loss_val: 5.0925 | f1_val: 0.06 | rmse_val: 1.0432
Epoch: 0050 | loss_train: 7.8426 loss_val: 5.7225 | f1_val: 0.06 | rmse_val: 0.8597
Epoch: 0051 | loss_train: 6.1551 loss_val: 5.4349 | f1_val: 0.06 | rmse_val: 1.7808
Epoch: 0052 | loss_train: 7.2730 loss_val: 10.4004 | f1_val: 0.05 | rmse_val: 0.8106
Epoch: 0053 | loss_train: 7.3421 loss_val: 6.9760 | f1_val: 0.07 | rmse_val: 1.1218
Epoch: 0054 | loss_train: 6.0967 loss_val: 8.9237 | f1_val: 0.08 | rmse_val: 1.6643
Epoch: 0055 | loss_train: 7.4062 loss_val: 8.3732 | f1_val: 0.08 | rmse_val: 0.7193
Epoch: 0056 | loss_train: 8.1094 loss_val: 6.9979 | f1_val: 0.07 | rmse_val: 1.1851
Epoch: 0057 | loss_train: 6.8626 loss_val: 5.6708 | f1_val: 0.04 | rmse_val: 1.2970
Epoch: 0058 | loss_train: 5.2353 loss_val: 7.7482 | f1_val: 0.05 | rmse_val: 0.7265
Epoch: 0059 | loss_train: 7.4299 loss_val: 6.7983 | f1_val: 0.06 | rmse_val: 2.1464
Epoch: 0060 | loss_train: 6.8115 loss_val: 9.4282 | f1_val: 0.07 | rmse_val: 0.9581
Epoch: 0061 | loss_train: 6.1992 loss_val: 6.4659 | f1_val: 0.05 | rmse_val: 1.5010
Epoch: 0062 | loss_train: 7.0082 loss_val: 6.5839 | f1_val: 0.07 | rmse_val: 1.4910
Epoch: 0063 | loss_train: 6.4495 loss_val: 7.2165 | f1_val: 0.07 | rmse_val: 1.2073
Epoch: 0064 | loss_train: 7.0835 loss_val: 4.7907 | f1_val: 0.07 | rmse_val: 1.1572
Epoch: 0065 | loss_train: 6.1338 loss_val: 6.8314 | f1_val: 0.06 | rmse_val: 1.0683
Epoch: 0066 | loss_train: 5.9427 loss_val: 7.5713 | f1_val: 0.07 | rmse_val: 1.2721
Epoch: 0067 | loss_train: 5.8864 loss_val: 4.8610 | f1_val: 0.05 | rmse_val: 1.0462
Epoch: 0068 | loss_train: 6.4417 loss_val: 8.5842 | f1_val: 0.05 | rmse_val: 1.4762
Epoch: 0069 | loss_train: 6.2180 loss_val: 6.8556 | f1_val: 0.04 | rmse_val: 1.2861
Epoch: 0070 | loss_train: 6.9804 loss_val: 6.8175 | f1_val: 0.03 | rmse_val: 1.2532
Epoch: 0071 | loss_train: 7.1676 loss_val: 6.4090 | f1_val: 0.05 | rmse_val: 1.5202
Epoch: 0072 | loss_train: 7.1516 loss_val: 8.8324 | f1_val: 0.05 | rmse_val: 1.3803
Epoch: 0073 | loss_train: 6.2377 loss_val: 4.4090 | f1_val: 0.06 | rmse_val: 1.3708
Epoch: 0074 | loss_train: 6.2510 loss_val: 4.7179 | f1_val: 0.05 | rmse_val: 1.3096
Epoch: 0075 | loss_train: 5.9604 loss_val: 7.7341 | f1_val: 0.06 | rmse_val: 1.4465
Optimization Finished!
Train cost: 723.5605s
Loading 45th epoch
f1_test: 0.06 | rmse_test: 1.4465

>>> run.py: Namespace(dataset='AIDS700nef', device=1, experiment='n_layers', log_path='log/nagphormer/AIDS700nef/n_layers', path='data/nagphormer', plot_path='plots/nagphormer/AIDS700nef/n_layers', pyg_path='data/pyg/AIDS700nef')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=32, hops=3, log_path='log/nagphormer/LINUX/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=32, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=32, out_features=32, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=32, out_features=64, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=64, out_features=32, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=32, out_features=16, bias=True)
  (attn_layer): Linear(in_features=64, out_features=1, bias=True)
  (Linear1): Linear(in_features=16, out_features=32, bias=True)
)
total params: 9874
Epoch: 0001 | loss_train: 21.0046 loss_val: 20.8907 | f1_val: 0.06 | rmse_val: 3.4122
Epoch: 0002 | loss_train: 21.4949 loss_val: 19.7760 | f1_val: 0.06 | rmse_val: 3.5569
Epoch: 0003 | loss_train: 20.9164 loss_val: 24.7910 | f1_val: 0.05 | rmse_val: 3.4588
Epoch: 0004 | loss_train: 20.0875 loss_val: 18.6578 | f1_val: 0.06 | rmse_val: 3.2473
Epoch: 0005 | loss_train: 17.9436 loss_val: 17.7663 | f1_val: 0.06 | rmse_val: 2.9723
Epoch: 0006 | loss_train: 13.0537 loss_val: 13.4475 | f1_val: 0.06 | rmse_val: 2.2556
Epoch: 0007 | loss_train: 9.1893 loss_val: 6.7109 | f1_val: 0.06 | rmse_val: 1.0325
Epoch: 0008 | loss_train: 4.7892 loss_val: 5.1231 | f1_val: 0.05 | rmse_val: 1.0328
Epoch: 0009 | loss_train: 4.2487 loss_val: 5.8168 | f1_val: 0.06 | rmse_val: 1.3156
Epoch: 0010 | loss_train: 4.1017 loss_val: 5.0308 | f1_val: 0.08 | rmse_val: 1.1251
Epoch: 0011 | loss_train: 4.4172 loss_val: 4.4365 | f1_val: 0.07 | rmse_val: 1.1736
Epoch: 0012 | loss_train: 4.2573 loss_val: 3.7200 | f1_val: 0.07 | rmse_val: 1.0825
Epoch: 0013 | loss_train: 4.3883 loss_val: 4.1208 | f1_val: 0.05 | rmse_val: 0.9444
Epoch: 0014 | loss_train: 4.3741 loss_val: 4.3595 | f1_val: 0.07 | rmse_val: 0.8981
Epoch: 0015 | loss_train: 4.3249 loss_val: 3.9554 | f1_val: 0.06 | rmse_val: 1.1957
Epoch: 0016 | loss_train: 4.3244 loss_val: 4.9351 | f1_val: 0.07 | rmse_val: 0.9662
Epoch: 0017 | loss_train: 4.4787 loss_val: 3.8764 | f1_val: 0.05 | rmse_val: 1.0309
Epoch: 0018 | loss_train: 4.8757 loss_val: 5.1916 | f1_val: 0.06 | rmse_val: 1.1637
Epoch: 0019 | loss_train: 4.3910 loss_val: 4.8148 | f1_val: 0.05 | rmse_val: 1.3402
Epoch: 0020 | loss_train: 4.3658 loss_val: 4.4453 | f1_val: 0.07 | rmse_val: 0.9459
Epoch: 0021 | loss_train: 4.0585 loss_val: 4.9722 | f1_val: 0.04 | rmse_val: 1.2135
Epoch: 0022 | loss_train: 4.4200 loss_val: 5.2788 | f1_val: 0.07 | rmse_val: 1.0904
Epoch: 0023 | loss_train: 3.6801 loss_val: 4.4197 | f1_val: 0.06 | rmse_val: 1.1859
Epoch: 0024 | loss_train: 4.3332 loss_val: 3.8889 | f1_val: 0.05 | rmse_val: 1.0782
Epoch: 0025 | loss_train: 3.8057 loss_val: 5.5858 | f1_val: 0.05 | rmse_val: 1.2314
Epoch: 0026 | loss_train: 3.7306 loss_val: 4.1413 | f1_val: 0.06 | rmse_val: 0.9069
Epoch: 0027 | loss_train: 4.3985 loss_val: 4.4358 | f1_val: 0.07 | rmse_val: 1.1489
Epoch: 0028 | loss_train: 4.3402 loss_val: 5.8833 | f1_val: 0.05 | rmse_val: 1.0731
Epoch: 0029 | loss_train: 4.0197 loss_val: 3.8997 | f1_val: 0.05 | rmse_val: 1.0360
Epoch: 0030 | loss_train: 4.6380 loss_val: 3.7421 | f1_val: 0.06 | rmse_val: 1.1642
Epoch: 0031 | loss_train: 4.0998 loss_val: 4.0675 | f1_val: 0.06 | rmse_val: 0.9562
Epoch: 0032 | loss_train: 4.0923 loss_val: 4.4959 | f1_val: 0.05 | rmse_val: 1.2887
Epoch: 0033 | loss_train: 4.2190 loss_val: 3.8579 | f1_val: 0.06 | rmse_val: 0.9011
Epoch: 0034 | loss_train: 4.0252 loss_val: 3.3504 | f1_val: 0.04 | rmse_val: 1.1383
Epoch: 0035 | loss_train: 4.1610 loss_val: 4.1478 | f1_val: 0.07 | rmse_val: 1.3225
Epoch: 0036 | loss_train: 4.6352 loss_val: 3.3210 | f1_val: 0.06 | rmse_val: 0.9671
Epoch: 0037 | loss_train: 4.4209 loss_val: 3.4403 | f1_val: 0.06 | rmse_val: 1.1146
Epoch: 0038 | loss_train: 4.3973 loss_val: 4.5335 | f1_val: 0.07 | rmse_val: 1.0188
Epoch: 0039 | loss_train: 4.3840 loss_val: 4.3819 | f1_val: 0.06 | rmse_val: 0.9948
Epoch: 0040 | loss_train: 4.4057 loss_val: 4.5191 | f1_val: 0.05 | rmse_val: 1.0779
Epoch: 0041 | loss_train: 4.3874 loss_val: 5.5555 | f1_val: 0.06 | rmse_val: 0.9700
Epoch: 0042 | loss_train: 4.1680 loss_val: 3.7477 | f1_val: 0.05 | rmse_val: 1.0697
Epoch: 0043 | loss_train: 4.2276 loss_val: 4.5676 | f1_val: 0.05 | rmse_val: 1.1775
Epoch: 0044 | loss_train: 4.2882 loss_val: 3.4512 | f1_val: 0.06 | rmse_val: 1.0901
Epoch: 0045 | loss_train: 4.1004 loss_val: 4.3257 | f1_val: 0.06 | rmse_val: 0.9285
Epoch: 0046 | loss_train: 4.8063 loss_val: 3.5391 | f1_val: 0.05 | rmse_val: 1.0555
Epoch: 0047 | loss_train: 3.9574 loss_val: 5.1123 | f1_val: 0.06 | rmse_val: 1.0965
Epoch: 0048 | loss_train: 4.2657 loss_val: 5.6412 | f1_val: 0.06 | rmse_val: 1.1222
Epoch: 0049 | loss_train: 4.1509 loss_val: 3.8132 | f1_val: 0.05 | rmse_val: 1.1123
Epoch: 0050 | loss_train: 4.2039 loss_val: 4.7980 | f1_val: 0.06 | rmse_val: 1.0677
Epoch: 0051 | loss_train: 4.0398 loss_val: 3.8841 | f1_val: 0.07 | rmse_val: 1.0325
Epoch: 0052 | loss_train: 3.8989 loss_val: 3.6008 | f1_val: 0.06 | rmse_val: 1.0842
Epoch: 0053 | loss_train: 4.0147 loss_val: 4.7980 | f1_val: 0.07 | rmse_val: 1.1802
Epoch: 0054 | loss_train: 4.6364 loss_val: 5.1853 | f1_val: 0.07 | rmse_val: 1.1008
Epoch: 0055 | loss_train: 4.7102 loss_val: 4.7034 | f1_val: 0.08 | rmse_val: 1.1851
Epoch: 0056 | loss_train: 3.8465 loss_val: 3.4163 | f1_val: 0.08 | rmse_val: 0.9163
Epoch: 0057 | loss_train: 4.4968 loss_val: 3.7585 | f1_val: 0.07 | rmse_val: 1.0346
Epoch: 0058 | loss_train: 3.8283 loss_val: 3.4337 | f1_val: 0.07 | rmse_val: 1.2432
Epoch: 0059 | loss_train: 4.0739 loss_val: 3.8092 | f1_val: 0.06 | rmse_val: 1.0278
Epoch: 0060 | loss_train: 4.3710 loss_val: 3.8557 | f1_val: 0.05 | rmse_val: 1.1497
Epoch: 0061 | loss_train: 4.1906 loss_val: 5.4613 | f1_val: 0.07 | rmse_val: 1.1370
Epoch: 0062 | loss_train: 4.3475 loss_val: 3.6837 | f1_val: 0.09 | rmse_val: 1.0078
Epoch: 0063 | loss_train: 4.1657 loss_val: 4.6673 | f1_val: 0.06 | rmse_val: 1.0730
Epoch: 0064 | loss_train: 4.1846 loss_val: 4.2389 | f1_val: 0.06 | rmse_val: 0.9877
Epoch: 0065 | loss_train: 4.3260 loss_val: 3.8487 | f1_val: 0.05 | rmse_val: 1.1817
Epoch: 0066 | loss_train: 4.1616 loss_val: 4.4652 | f1_val: 0.06 | rmse_val: 0.9200
Epoch: 0067 | loss_train: 4.0455 loss_val: 6.0073 | f1_val: 0.08 | rmse_val: 1.1693
Epoch: 0068 | loss_train: 4.3484 loss_val: 4.7845 | f1_val: 0.07 | rmse_val: 1.0639
Epoch: 0069 | loss_train: 4.6102 loss_val: 4.3624 | f1_val: 0.05 | rmse_val: 1.1225
Epoch: 0070 | loss_train: 4.1482 loss_val: 4.6927 | f1_val: 0.07 | rmse_val: 1.0772
Epoch: 0071 | loss_train: 4.1366 loss_val: 4.2608 | f1_val: 0.06 | rmse_val: 1.0603
Epoch: 0072 | loss_train: 3.7822 loss_val: 4.1272 | f1_val: 0.06 | rmse_val: 0.9158
Epoch: 0073 | loss_train: 4.2273 loss_val: 5.6343 | f1_val: 0.06 | rmse_val: 0.9966
Epoch: 0074 | loss_train: 3.8959 loss_val: 4.5321 | f1_val: 0.07 | rmse_val: 1.0620
Epoch: 0075 | loss_train: 4.2547 loss_val: 5.5141 | f1_val: 0.07 | rmse_val: 1.1376
Epoch: 0076 | loss_train: 4.1611 loss_val: 4.2568 | f1_val: 0.06 | rmse_val: 1.1099
Epoch: 0077 | loss_train: 3.9437 loss_val: 3.4291 | f1_val: 0.06 | rmse_val: 1.0437
Epoch: 0078 | loss_train: 4.2948 loss_val: 5.0249 | f1_val: 0.05 | rmse_val: 1.0633
Epoch: 0079 | loss_train: 4.3364 loss_val: 5.0496 | f1_val: 0.07 | rmse_val: 0.9820
Epoch: 0080 | loss_train: 4.1524 loss_val: 2.8191 | f1_val: 0.07 | rmse_val: 1.1200
Epoch: 0081 | loss_train: 3.8092 loss_val: 3.4384 | f1_val: 0.06 | rmse_val: 0.9773
Epoch: 0082 | loss_train: 4.0157 loss_val: 4.1698 | f1_val: 0.07 | rmse_val: 1.1614
Epoch: 0083 | loss_train: 3.7463 loss_val: 4.8007 | f1_val: 0.05 | rmse_val: 1.0771
Epoch: 0084 | loss_train: 4.4386 loss_val: 3.7022 | f1_val: 0.06 | rmse_val: 1.0783
Epoch: 0085 | loss_train: 4.1694 loss_val: 4.4921 | f1_val: 0.05 | rmse_val: 1.0269
Epoch: 0086 | loss_train: 3.9624 loss_val: 5.7629 | f1_val: 0.05 | rmse_val: 0.9499
Epoch: 0087 | loss_train: 4.1979 loss_val: 3.7984 | f1_val: 0.07 | rmse_val: 1.0961
Epoch: 0088 | loss_train: 3.5838 loss_val: 3.7762 | f1_val: 0.08 | rmse_val: 0.9927
Epoch: 0089 | loss_train: 4.4355 loss_val: 3.3314 | f1_val: 0.05 | rmse_val: 1.1476
Epoch: 0090 | loss_train: 4.0540 loss_val: 4.9937 | f1_val: 0.07 | rmse_val: 0.9047
Epoch: 0091 | loss_train: 4.2927 loss_val: 4.1577 | f1_val: 0.06 | rmse_val: 1.1032
Epoch: 0092 | loss_train: 3.8427 loss_val: 4.6364 | f1_val: 0.05 | rmse_val: 0.9113
Epoch: 0093 | loss_train: 4.0412 loss_val: 3.4683 | f1_val: 0.07 | rmse_val: 1.0957
Epoch: 0094 | loss_train: 4.2273 loss_val: 4.6522 | f1_val: 0.06 | rmse_val: 1.0044
Epoch: 0095 | loss_train: 4.2012 loss_val: 3.8478 | f1_val: 0.07 | rmse_val: 1.0829
Epoch: 0096 | loss_train: 4.0767 loss_val: 3.5454 | f1_val: 0.05 | rmse_val: 0.9921
Epoch: 0097 | loss_train: 4.6818 loss_val: 4.3784 | f1_val: 0.07 | rmse_val: 1.0955
Epoch: 0098 | loss_train: 3.6045 loss_val: 2.8916 | f1_val: 0.05 | rmse_val: 0.9066
Epoch: 0099 | loss_train: 4.1635 loss_val: 4.4201 | f1_val: 0.07 | rmse_val: 1.0366
Epoch: 0100 | loss_train: 4.2926 loss_val: 4.3241 | f1_val: 0.06 | rmse_val: 1.0937
Epoch: 0101 | loss_train: 4.0925 loss_val: 4.5265 | f1_val: 0.06 | rmse_val: 1.0920
Epoch: 0102 | loss_train: 4.1392 loss_val: 4.0634 | f1_val: 0.06 | rmse_val: 1.2531
Epoch: 0103 | loss_train: 4.0452 loss_val: 4.3404 | f1_val: 0.07 | rmse_val: 1.0001
Epoch: 0104 | loss_train: 3.7077 loss_val: 3.9067 | f1_val: 0.06 | rmse_val: 1.2965
Epoch: 0105 | loss_train: 3.8427 loss_val: 3.3840 | f1_val: 0.07 | rmse_val: 1.0415
Epoch: 0106 | loss_train: 3.9252 loss_val: 3.0582 | f1_val: 0.07 | rmse_val: 0.9847
Epoch: 0107 | loss_train: 4.1972 loss_val: 3.5008 | f1_val: 0.05 | rmse_val: 1.1179
Epoch: 0108 | loss_train: 3.9806 loss_val: 4.3377 | f1_val: 0.06 | rmse_val: 1.1850
Epoch: 0109 | loss_train: 4.6997 loss_val: 4.2765 | f1_val: 0.05 | rmse_val: 1.0156
Epoch: 0110 | loss_train: 3.8887 loss_val: 3.8891 | f1_val: 0.05 | rmse_val: 0.8890
Optimization Finished!
Train cost: 582.6065s
Loading 62th epoch
f1_test: 0.05 | rmse_test: 0.8890

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=64, hops=3, log_path='log/nagphormer/LINUX/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=64, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=64, out_features=64, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=64, out_features=128, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=128, out_features=64, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=64, out_features=32, bias=True)
  (attn_layer): Linear(in_features=128, out_features=1, bias=True)
  (Linear1): Linear(in_features=32, out_features=64, bias=True)
)
total params: 38178
Epoch: 0001 | loss_train: 21.0651 loss_val: 20.8474 | f1_val: 0.04 | rmse_val: 3.4700
Epoch: 0002 | loss_train: 21.2648 loss_val: 19.2601 | f1_val: 0.04 | rmse_val: 3.4070
Epoch: 0003 | loss_train: 20.0553 loss_val: 23.0543 | f1_val: 0.07 | rmse_val: 3.1749
Epoch: 0004 | loss_train: 17.7282 loss_val: 14.5773 | f1_val: 0.07 | rmse_val: 2.6866
Epoch: 0005 | loss_train: 12.2557 loss_val: 9.0634 | f1_val: 0.06 | rmse_val: 1.3728
Epoch: 0006 | loss_train: 5.5160 loss_val: 4.3909 | f1_val: 0.05 | rmse_val: 0.9695
Epoch: 0007 | loss_train: 4.3922 loss_val: 4.3718 | f1_val: 0.07 | rmse_val: 1.1586
Epoch: 0008 | loss_train: 3.8266 loss_val: 5.2616 | f1_val: 0.07 | rmse_val: 0.9382
Epoch: 0009 | loss_train: 4.2836 loss_val: 5.9654 | f1_val: 0.06 | rmse_val: 1.2239
Epoch: 0010 | loss_train: 4.0964 loss_val: 5.2741 | f1_val: 0.06 | rmse_val: 1.1364
Epoch: 0011 | loss_train: 4.4055 loss_val: 4.6026 | f1_val: 0.07 | rmse_val: 1.0277
Epoch: 0012 | loss_train: 4.2332 loss_val: 3.9245 | f1_val: 0.05 | rmse_val: 1.0575
Epoch: 0013 | loss_train: 4.3420 loss_val: 4.3300 | f1_val: 0.06 | rmse_val: 1.0487
Epoch: 0014 | loss_train: 4.4012 loss_val: 4.4475 | f1_val: 0.10 | rmse_val: 0.9344
Epoch: 0015 | loss_train: 4.3275 loss_val: 4.1303 | f1_val: 0.06 | rmse_val: 1.3047
Epoch: 0016 | loss_train: 4.3446 loss_val: 5.1691 | f1_val: 0.05 | rmse_val: 1.1028
Epoch: 0017 | loss_train: 4.4952 loss_val: 4.0083 | f1_val: 0.06 | rmse_val: 0.9277
Epoch: 0018 | loss_train: 4.8933 loss_val: 5.3596 | f1_val: 0.07 | rmse_val: 1.2964
Epoch: 0019 | loss_train: 4.4265 loss_val: 4.8995 | f1_val: 0.07 | rmse_val: 1.3168
Epoch: 0020 | loss_train: 4.3757 loss_val: 4.2756 | f1_val: 0.06 | rmse_val: 1.0077
Epoch: 0021 | loss_train: 4.0481 loss_val: 5.0321 | f1_val: 0.08 | rmse_val: 1.0951
Epoch: 0022 | loss_train: 4.4235 loss_val: 5.4453 | f1_val: 0.08 | rmse_val: 1.1875
Epoch: 0023 | loss_train: 3.8641 loss_val: 4.4367 | f1_val: 0.06 | rmse_val: 1.1261
Epoch: 0024 | loss_train: 4.4384 loss_val: 3.9204 | f1_val: 0.05 | rmse_val: 1.1346
Epoch: 0025 | loss_train: 3.8324 loss_val: 5.8034 | f1_val: 0.06 | rmse_val: 1.4041
Epoch: 0026 | loss_train: 3.8253 loss_val: 4.0209 | f1_val: 0.09 | rmse_val: 1.0747
Epoch: 0027 | loss_train: 4.4822 loss_val: 4.4445 | f1_val: 0.08 | rmse_val: 1.0955
Epoch: 0028 | loss_train: 4.3936 loss_val: 5.8690 | f1_val: 0.06 | rmse_val: 1.1030
Epoch: 0029 | loss_train: 4.0496 loss_val: 3.8846 | f1_val: 0.05 | rmse_val: 1.0326
Epoch: 0030 | loss_train: 4.6799 loss_val: 3.6860 | f1_val: 0.05 | rmse_val: 1.2724
Epoch: 0031 | loss_train: 4.1383 loss_val: 4.0584 | f1_val: 0.07 | rmse_val: 0.9655
Epoch: 0032 | loss_train: 4.0927 loss_val: 4.5064 | f1_val: 0.05 | rmse_val: 1.1847
Epoch: 0033 | loss_train: 4.2983 loss_val: 3.6589 | f1_val: 0.07 | rmse_val: 0.8362
Epoch: 0034 | loss_train: 4.0890 loss_val: 3.3391 | f1_val: 0.07 | rmse_val: 0.9437
Epoch: 0035 | loss_train: 4.1491 loss_val: 4.6481 | f1_val: 0.06 | rmse_val: 1.4358
Epoch: 0036 | loss_train: 4.8052 loss_val: 3.4312 | f1_val: 0.05 | rmse_val: 0.8238
Epoch: 0037 | loss_train: 4.4040 loss_val: 3.4578 | f1_val: 0.06 | rmse_val: 1.1461
Epoch: 0038 | loss_train: 4.3764 loss_val: 4.5641 | f1_val: 0.05 | rmse_val: 0.9252
Epoch: 0039 | loss_train: 4.3461 loss_val: 4.3837 | f1_val: 0.06 | rmse_val: 1.3382
Epoch: 0040 | loss_train: 4.4748 loss_val: 4.5471 | f1_val: 0.06 | rmse_val: 0.9345
Epoch: 0041 | loss_train: 4.3962 loss_val: 5.5248 | f1_val: 0.04 | rmse_val: 1.0206
Epoch: 0042 | loss_train: 4.1622 loss_val: 3.7426 | f1_val: 0.06 | rmse_val: 1.0436
Epoch: 0043 | loss_train: 4.2162 loss_val: 4.5670 | f1_val: 0.06 | rmse_val: 1.1540
Epoch: 0044 | loss_train: 4.2589 loss_val: 3.4278 | f1_val: 0.05 | rmse_val: 1.0623
Epoch: 0045 | loss_train: 4.1746 loss_val: 4.1955 | f1_val: 0.05 | rmse_val: 1.1517
Epoch: 0046 | loss_train: 4.8313 loss_val: 3.6324 | f1_val: 0.04 | rmse_val: 0.9530
Epoch: 0047 | loss_train: 4.0107 loss_val: 5.1562 | f1_val: 0.07 | rmse_val: 1.1079
Epoch: 0048 | loss_train: 4.2694 loss_val: 5.7042 | f1_val: 0.06 | rmse_val: 1.0259
Epoch: 0049 | loss_train: 4.1532 loss_val: 3.7578 | f1_val: 0.05 | rmse_val: 1.1882
Epoch: 0050 | loss_train: 4.2423 loss_val: 4.7928 | f1_val: 0.06 | rmse_val: 1.1641
Epoch: 0051 | loss_train: 4.0151 loss_val: 3.8617 | f1_val: 0.07 | rmse_val: 0.8848
Epoch: 0052 | loss_train: 3.8688 loss_val: 3.5701 | f1_val: 0.06 | rmse_val: 1.0420
Epoch: 0053 | loss_train: 3.9845 loss_val: 4.7565 | f1_val: 0.05 | rmse_val: 1.0166
Epoch: 0054 | loss_train: 4.5984 loss_val: 5.1571 | f1_val: 0.05 | rmse_val: 1.1264
Epoch: 0055 | loss_train: 4.6754 loss_val: 4.6599 | f1_val: 0.06 | rmse_val: 1.2086
Epoch: 0056 | loss_train: 3.7867 loss_val: 3.3734 | f1_val: 0.06 | rmse_val: 0.9592
Epoch: 0057 | loss_train: 4.4031 loss_val: 3.6905 | f1_val: 0.06 | rmse_val: 1.2009
Epoch: 0058 | loss_train: 3.7519 loss_val: 3.3253 | f1_val: 0.07 | rmse_val: 1.0965
Epoch: 0059 | loss_train: 3.9895 loss_val: 3.7340 | f1_val: 0.06 | rmse_val: 1.0579
Epoch: 0060 | loss_train: 4.2679 loss_val: 3.7839 | f1_val: 0.07 | rmse_val: 1.0985
Epoch: 0061 | loss_train: 4.1056 loss_val: 5.4120 | f1_val: 0.04 | rmse_val: 0.9424
Epoch: 0062 | loss_train: 4.2552 loss_val: 3.5577 | f1_val: 0.05 | rmse_val: 1.0908
Epoch: 0063 | loss_train: 4.0677 loss_val: 4.4279 | f1_val: 0.06 | rmse_val: 1.0206
Epoch: 0064 | loss_train: 4.0278 loss_val: 4.0491 | f1_val: 0.05 | rmse_val: 1.1579
Epoch: 0065 | loss_train: 4.2035 loss_val: 3.6685 | f1_val: 0.07 | rmse_val: 0.9687
Epoch: 0066 | loss_train: 4.0955 loss_val: 4.3210 | f1_val: 0.06 | rmse_val: 0.9587
Epoch: 0067 | loss_train: 3.8563 loss_val: 5.7090 | f1_val: 0.06 | rmse_val: 1.1563
Epoch: 0068 | loss_train: 4.2589 loss_val: 4.5696 | f1_val: 0.06 | rmse_val: 0.8823
Epoch: 0069 | loss_train: 4.4525 loss_val: 4.5679 | f1_val: 0.06 | rmse_val: 1.0991
Epoch: 0070 | loss_train: 3.9239 loss_val: 4.5088 | f1_val: 0.07 | rmse_val: 1.0656
Epoch: 0071 | loss_train: 3.8636 loss_val: 3.9368 | f1_val: 0.04 | rmse_val: 1.1062
Epoch: 0072 | loss_train: 3.5415 loss_val: 3.7678 | f1_val: 0.08 | rmse_val: 0.7735
Epoch: 0073 | loss_train: 4.0286 loss_val: 5.1678 | f1_val: 0.05 | rmse_val: 1.1053
Epoch: 0074 | loss_train: 3.6989 loss_val: 4.3994 | f1_val: 0.07 | rmse_val: 0.8012
Epoch: 0075 | loss_train: 3.9338 loss_val: 5.0272 | f1_val: 0.05 | rmse_val: 1.1080
Epoch: 0076 | loss_train: 4.0391 loss_val: 3.8094 | f1_val: 0.05 | rmse_val: 1.0262
Epoch: 0077 | loss_train: 3.6822 loss_val: 2.9606 | f1_val: 0.07 | rmse_val: 1.0642
Epoch: 0078 | loss_train: 3.9265 loss_val: 4.9560 | f1_val: 0.04 | rmse_val: 0.9935
Epoch: 0079 | loss_train: 3.9877 loss_val: 4.4976 | f1_val: 0.06 | rmse_val: 1.2037
Epoch: 0080 | loss_train: 3.7045 loss_val: 2.2720 | f1_val: 0.05 | rmse_val: 0.9893
Epoch: 0081 | loss_train: 3.6060 loss_val: 3.1205 | f1_val: 0.05 | rmse_val: 1.2475
Epoch: 0082 | loss_train: 3.6851 loss_val: 3.9203 | f1_val: 0.04 | rmse_val: 0.8399
Epoch: 0083 | loss_train: 3.3913 loss_val: 4.1916 | f1_val: 0.05 | rmse_val: 1.0432
Epoch: 0084 | loss_train: 4.1598 loss_val: 2.8272 | f1_val: 0.09 | rmse_val: 0.9946
Epoch: 0085 | loss_train: 3.8329 loss_val: 4.4646 | f1_val: 0.07 | rmse_val: 0.8833
Epoch: 0086 | loss_train: 3.6167 loss_val: 4.6347 | f1_val: 0.06 | rmse_val: 0.9473
Epoch: 0087 | loss_train: 3.9523 loss_val: 3.4046 | f1_val: 0.06 | rmse_val: 1.1858
Epoch: 0088 | loss_train: 3.5875 loss_val: 3.5458 | f1_val: 0.07 | rmse_val: 0.8104
Epoch: 0089 | loss_train: 4.1264 loss_val: 3.0352 | f1_val: 0.05 | rmse_val: 1.0671
Epoch: 0090 | loss_train: 3.7354 loss_val: 4.7094 | f1_val: 0.06 | rmse_val: 0.9782
Epoch: 0091 | loss_train: 3.9218 loss_val: 3.8102 | f1_val: 0.08 | rmse_val: 1.0379
Epoch: 0092 | loss_train: 3.8262 loss_val: 4.4215 | f1_val: 0.06 | rmse_val: 0.7861
Epoch: 0093 | loss_train: 3.7609 loss_val: 3.2107 | f1_val: 0.06 | rmse_val: 0.7995
Epoch: 0094 | loss_train: 3.7486 loss_val: 4.5884 | f1_val: 0.06 | rmse_val: 1.2178
Epoch: 0095 | loss_train: 3.8746 loss_val: 3.9143 | f1_val: 0.06 | rmse_val: 0.7220
Epoch: 0096 | loss_train: 3.7457 loss_val: 2.7993 | f1_val: 0.06 | rmse_val: 0.6963
Epoch: 0097 | loss_train: 4.3140 loss_val: 4.3531 | f1_val: 0.07 | rmse_val: 0.9186
Epoch: 0098 | loss_train: 3.4425 loss_val: 2.6137 | f1_val: 0.06 | rmse_val: 0.7117
Epoch: 0099 | loss_train: 3.9952 loss_val: 3.7345 | f1_val: 0.06 | rmse_val: 0.7682
Epoch: 0100 | loss_train: 3.9813 loss_val: 3.8432 | f1_val: 0.06 | rmse_val: 1.1678
Epoch: 0101 | loss_train: 4.0232 loss_val: 3.9764 | f1_val: 0.06 | rmse_val: 0.8213
Epoch: 0102 | loss_train: 3.7610 loss_val: 3.4847 | f1_val: 0.06 | rmse_val: 1.0904
Epoch: 0103 | loss_train: 3.6390 loss_val: 4.0901 | f1_val: 0.05 | rmse_val: 0.7436
Epoch: 0104 | loss_train: 3.3853 loss_val: 3.2692 | f1_val: 0.05 | rmse_val: 1.2112
Epoch: 0105 | loss_train: 3.6299 loss_val: 2.8398 | f1_val: 0.07 | rmse_val: 0.8619
Epoch: 0106 | loss_train: 3.7621 loss_val: 2.7514 | f1_val: 0.07 | rmse_val: 0.8060
Epoch: 0107 | loss_train: 3.7129 loss_val: 3.3817 | f1_val: 0.04 | rmse_val: 0.9626
Epoch: 0108 | loss_train: 3.6396 loss_val: 4.0548 | f1_val: 0.08 | rmse_val: 0.9100
Epoch: 0109 | loss_train: 4.2069 loss_val: 3.4784 | f1_val: 0.08 | rmse_val: 0.7782
Epoch: 0110 | loss_train: 3.6096 loss_val: 3.5475 | f1_val: 0.06 | rmse_val: 0.8061
Optimization Finished!
Train cost: 589.3323s
Loading 14th epoch
f1_test: 0.06 | rmse_test: 0.8061

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.07 | rmse_val: 3.3741
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.06 | rmse_val: 2.9943
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.07 | rmse_val: 1.7410
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.07 | rmse_val: 1.1327
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.06 | rmse_val: 0.9800
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.06 | rmse_val: 1.0872
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.06 | rmse_val: 1.1652
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.07 | rmse_val: 1.1518
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.08 | rmse_val: 1.2344
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.07 | rmse_val: 1.3389
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.07 | rmse_val: 1.3391
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.06 | rmse_val: 0.9978
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.06 | rmse_val: 1.2773
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.08 | rmse_val: 1.1648
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.08 | rmse_val: 1.2310
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.06 | rmse_val: 1.4026
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.10 | rmse_val: 1.1990
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.06 | rmse_val: 1.2226
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.06 | rmse_val: 1.1331
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.06 | rmse_val: 1.1435
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.06 | rmse_val: 1.1230
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.06 | rmse_val: 1.7135
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.06 | rmse_val: 1.7576
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.07 | rmse_val: 1.1399
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.07 | rmse_val: 1.2766
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.07 | rmse_val: 1.1888
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.07 | rmse_val: 1.6370
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.07 | rmse_val: 1.7470
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.06 | rmse_val: 1.6875
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.06 | rmse_val: 1.5689
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.07 | rmse_val: 1.0698
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.08 | rmse_val: 1.0978
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.06 | rmse_val: 1.2972
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.07 | rmse_val: 1.1596
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.07 | rmse_val: 1.8801
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.08 | rmse_val: 1.0040
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.09 | rmse_val: 1.1605
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.08 | rmse_val: 1.1127
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.07 | rmse_val: 1.3111
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.08 | rmse_val: 1.0541
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.07 | rmse_val: 0.9032
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.06 | rmse_val: 1.1433
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.05 | rmse_val: 1.2399
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.07 | rmse_val: 0.9962
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.08 | rmse_val: 1.1889
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.08 | rmse_val: 1.0389
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.06 | rmse_val: 1.1820
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.06 | rmse_val: 1.0117
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.07 | rmse_val: 1.2784
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.05 | rmse_val: 1.0770
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.07 | rmse_val: 1.0232
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.07 | rmse_val: 1.1284
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.07 | rmse_val: 1.1607
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.05 | rmse_val: 1.0688
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.06 | rmse_val: 1.1700
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.06 | rmse_val: 1.1267
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.07 | rmse_val: 1.1829
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.05 | rmse_val: 0.9879
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.06 | rmse_val: 0.9795
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.06 | rmse_val: 1.1594
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.06 | rmse_val: 0.8617
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.07 | rmse_val: 1.0771
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.06 | rmse_val: 0.9418
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.06 | rmse_val: 1.1806
Optimization Finished!
Train cost: 343.5002s
Loading 17th epoch
f1_test: 0.06 | rmse_test: 1.1806

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=256, hops=3, log_path='log/nagphormer/LINUX/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=256, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=256, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=512, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=256, out_features=128, bias=True)
  (attn_layer): Linear(in_features=512, out_features=1, bias=True)
  (Linear1): Linear(in_features=128, out_features=256, bias=True)
)
total params: 595074
Epoch: 0001 | loss_train: 19.3056 loss_val: 16.3491 | f1_val: 0.06 | rmse_val: 3.1420
Epoch: 0002 | loss_train: 14.1486 loss_val: 7.6585 | f1_val: 0.06 | rmse_val: 1.6665
Epoch: 0003 | loss_train: 5.9760 loss_val: 4.4650 | f1_val: 0.06 | rmse_val: 1.6670
Epoch: 0004 | loss_train: 4.2407 loss_val: 3.8770 | f1_val: 0.06 | rmse_val: 0.9892
Epoch: 0005 | loss_train: 4.0949 loss_val: 4.2428 | f1_val: 0.06 | rmse_val: 1.3375
Epoch: 0006 | loss_train: 4.3896 loss_val: 3.9783 | f1_val: 0.08 | rmse_val: 1.0683
Epoch: 0007 | loss_train: 4.3608 loss_val: 4.1092 | f1_val: 0.06 | rmse_val: 0.9759
Epoch: 0008 | loss_train: 3.7733 loss_val: 4.9120 | f1_val: 0.05 | rmse_val: 1.1496
Epoch: 0009 | loss_train: 4.2805 loss_val: 5.9621 | f1_val: 0.07 | rmse_val: 1.3106
Epoch: 0010 | loss_train: 4.1336 loss_val: 5.5427 | f1_val: 0.05 | rmse_val: 1.5311
Epoch: 0011 | loss_train: 4.5945 loss_val: 4.4974 | f1_val: 0.06 | rmse_val: 1.7545
Epoch: 0012 | loss_train: 4.2821 loss_val: 3.8408 | f1_val: 0.07 | rmse_val: 1.0915
Epoch: 0013 | loss_train: 4.4615 loss_val: 4.3860 | f1_val: 0.06 | rmse_val: 1.1862
Epoch: 0014 | loss_train: 4.3906 loss_val: 4.5662 | f1_val: 0.08 | rmse_val: 1.2557
Epoch: 0015 | loss_train: 4.3640 loss_val: 4.1310 | f1_val: 0.07 | rmse_val: 1.1413
Epoch: 0016 | loss_train: 4.3595 loss_val: 5.1319 | f1_val: 0.05 | rmse_val: 1.3164
Epoch: 0017 | loss_train: 4.5427 loss_val: 4.0347 | f1_val: 0.07 | rmse_val: 1.0823
Epoch: 0018 | loss_train: 4.9108 loss_val: 5.3732 | f1_val: 0.07 | rmse_val: 1.1670
Epoch: 0019 | loss_train: 4.4947 loss_val: 4.7628 | f1_val: 0.07 | rmse_val: 1.0827
Epoch: 0020 | loss_train: 4.3908 loss_val: 3.9619 | f1_val: 0.07 | rmse_val: 1.3053
Epoch: 0021 | loss_train: 4.1097 loss_val: 5.0149 | f1_val: 0.05 | rmse_val: 0.9231
Epoch: 0022 | loss_train: 4.5838 loss_val: 6.2750 | f1_val: 0.08 | rmse_val: 1.7025
Epoch: 0023 | loss_train: 4.0072 loss_val: 5.3596 | f1_val: 0.07 | rmse_val: 1.7770
Epoch: 0024 | loss_train: 4.6839 loss_val: 4.3436 | f1_val: 0.05 | rmse_val: 0.7510
Epoch: 0025 | loss_train: 3.8705 loss_val: 5.5069 | f1_val: 0.08 | rmse_val: 0.8635
Epoch: 0026 | loss_train: 3.6860 loss_val: 3.9736 | f1_val: 0.05 | rmse_val: 1.1936
Epoch: 0027 | loss_train: 4.3894 loss_val: 4.4471 | f1_val: 0.05 | rmse_val: 1.4680
Epoch: 0028 | loss_train: 4.3624 loss_val: 5.6960 | f1_val: 0.06 | rmse_val: 1.3279
Epoch: 0029 | loss_train: 4.0658 loss_val: 3.7757 | f1_val: 0.05 | rmse_val: 1.5535
Epoch: 0030 | loss_train: 4.6853 loss_val: 3.7513 | f1_val: 0.07 | rmse_val: 1.6880
Epoch: 0031 | loss_train: 4.5562 loss_val: 4.1503 | f1_val: 0.06 | rmse_val: 1.1226
Epoch: 0032 | loss_train: 4.1152 loss_val: 4.2828 | f1_val: 0.05 | rmse_val: 1.1273
Epoch: 0033 | loss_train: 4.2473 loss_val: 3.9526 | f1_val: 0.06 | rmse_val: 1.5800
Epoch: 0034 | loss_train: 4.2468 loss_val: 4.0476 | f1_val: 0.05 | rmse_val: 1.7572
Epoch: 0035 | loss_train: 4.5746 loss_val: 7.5891 | f1_val: 0.06 | rmse_val: 2.5820
Epoch: 0036 | loss_train: 5.4260 loss_val: 3.3913 | f1_val: 0.05 | rmse_val: 1.4262
Epoch: 0037 | loss_train: 4.7600 loss_val: 3.9828 | f1_val: 0.08 | rmse_val: 1.5121
Epoch: 0038 | loss_train: 4.6108 loss_val: 4.2654 | f1_val: 0.06 | rmse_val: 1.3606
Epoch: 0039 | loss_train: 4.4714 loss_val: 4.5588 | f1_val: 0.07 | rmse_val: 1.5884
Epoch: 0040 | loss_train: 4.7192 loss_val: 4.4721 | f1_val: 0.07 | rmse_val: 1.1904
Epoch: 0041 | loss_train: 4.4406 loss_val: 5.7207 | f1_val: 0.06 | rmse_val: 0.8795
Epoch: 0042 | loss_train: 4.1895 loss_val: 3.7414 | f1_val: 0.07 | rmse_val: 1.1429
Epoch: 0043 | loss_train: 4.2130 loss_val: 4.5614 | f1_val: 0.06 | rmse_val: 1.2657
Epoch: 0044 | loss_train: 4.2961 loss_val: 3.4132 | f1_val: 0.07 | rmse_val: 1.1699
Epoch: 0045 | loss_train: 4.2013 loss_val: 4.3578 | f1_val: 0.06 | rmse_val: 0.9449
Epoch: 0046 | loss_train: 4.8108 loss_val: 3.7529 | f1_val: 0.07 | rmse_val: 0.8074
Epoch: 0047 | loss_train: 4.0574 loss_val: 5.3263 | f1_val: 0.05 | rmse_val: 1.3200
Epoch: 0048 | loss_train: 4.2448 loss_val: 5.8481 | f1_val: 0.07 | rmse_val: 0.9373
Epoch: 0049 | loss_train: 4.0818 loss_val: 3.7955 | f1_val: 0.05 | rmse_val: 1.4313
Epoch: 0050 | loss_train: 4.2472 loss_val: 4.7668 | f1_val: 0.07 | rmse_val: 0.9802
Epoch: 0051 | loss_train: 4.0151 loss_val: 3.9375 | f1_val: 0.06 | rmse_val: 0.9675
Epoch: 0052 | loss_train: 3.8554 loss_val: 3.5516 | f1_val: 0.06 | rmse_val: 1.1009
Epoch: 0053 | loss_train: 3.9464 loss_val: 4.7370 | f1_val: 0.06 | rmse_val: 1.1366
Epoch: 0054 | loss_train: 4.5839 loss_val: 5.1373 | f1_val: 0.07 | rmse_val: 0.9456
Epoch: 0055 | loss_train: 4.6878 loss_val: 4.6101 | f1_val: 0.08 | rmse_val: 1.1076
Epoch: 0056 | loss_train: 3.7964 loss_val: 3.3628 | f1_val: 0.06 | rmse_val: 1.1206
Epoch: 0057 | loss_train: 4.4244 loss_val: 3.6809 | f1_val: 0.07 | rmse_val: 1.2135
Epoch: 0058 | loss_train: 3.7174 loss_val: 3.3028 | f1_val: 0.04 | rmse_val: 1.1750
Epoch: 0059 | loss_train: 4.0566 loss_val: 3.7490 | f1_val: 0.06 | rmse_val: 1.1250
Epoch: 0060 | loss_train: 4.2997 loss_val: 3.7810 | f1_val: 0.07 | rmse_val: 0.9607
Epoch: 0061 | loss_train: 4.1203 loss_val: 5.4394 | f1_val: 0.06 | rmse_val: 1.0532
Epoch: 0062 | loss_train: 4.2641 loss_val: 3.6188 | f1_val: 0.07 | rmse_val: 1.1171
Epoch: 0063 | loss_train: 4.1515 loss_val: 4.5851 | f1_val: 0.07 | rmse_val: 1.0605
Epoch: 0064 | loss_train: 4.0343 loss_val: 4.2268 | f1_val: 0.08 | rmse_val: 1.1755
Epoch: 0065 | loss_train: 4.2052 loss_val: 3.6785 | f1_val: 0.07 | rmse_val: 0.9149
Epoch: 0066 | loss_train: 4.0990 loss_val: 4.2597 | f1_val: 0.06 | rmse_val: 0.9618
Epoch: 0067 | loss_train: 3.9003 loss_val: 5.7885 | f1_val: 0.06 | rmse_val: 1.1930
Epoch: 0068 | loss_train: 4.2407 loss_val: 4.3177 | f1_val: 0.06 | rmse_val: 1.0469
Epoch: 0069 | loss_train: 4.3962 loss_val: 4.4641 | f1_val: 0.07 | rmse_val: 1.2164
Epoch: 0070 | loss_train: 3.9791 loss_val: 4.7006 | f1_val: 0.07 | rmse_val: 1.1602
Epoch: 0071 | loss_train: 3.9463 loss_val: 3.8906 | f1_val: 0.06 | rmse_val: 1.0301
Epoch: 0072 | loss_train: 3.6999 loss_val: 4.1288 | f1_val: 0.06 | rmse_val: 0.8103
Epoch: 0073 | loss_train: 4.0761 loss_val: 4.9673 | f1_val: 0.07 | rmse_val: 1.1676
Epoch: 0074 | loss_train: 3.7828 loss_val: 4.5414 | f1_val: 0.05 | rmse_val: 1.0892
Epoch: 0075 | loss_train: 4.0452 loss_val: 4.9351 | f1_val: 0.06 | rmse_val: 1.1671
Epoch: 0076 | loss_train: 4.1001 loss_val: 3.8619 | f1_val: 0.06 | rmse_val: 0.8938
Epoch: 0077 | loss_train: 3.7926 loss_val: 3.1097 | f1_val: 0.06 | rmse_val: 1.1249
Epoch: 0078 | loss_train: 4.0611 loss_val: 4.8886 | f1_val: 0.06 | rmse_val: 0.9000
Epoch: 0079 | loss_train: 4.0787 loss_val: 4.6992 | f1_val: 0.06 | rmse_val: 1.2859
Epoch: 0080 | loss_train: 3.8710 loss_val: 2.4779 | f1_val: 0.05 | rmse_val: 1.1828
Epoch: 0081 | loss_train: 3.7082 loss_val: 3.0894 | f1_val: 0.06 | rmse_val: 1.1422
Epoch: 0082 | loss_train: 3.8108 loss_val: 3.8848 | f1_val: 0.06 | rmse_val: 0.9830
Epoch: 0083 | loss_train: 3.4950 loss_val: 4.3918 | f1_val: 0.06 | rmse_val: 0.9408
Epoch: 0084 | loss_train: 4.1397 loss_val: 3.0685 | f1_val: 0.07 | rmse_val: 0.9311
Epoch: 0085 | loss_train: 3.8943 loss_val: 4.1518 | f1_val: 0.07 | rmse_val: 1.0042
Epoch: 0086 | loss_train: 3.7820 loss_val: 4.8444 | f1_val: 0.06 | rmse_val: 1.1802
Epoch: 0087 | loss_train: 4.0262 loss_val: 3.3411 | f1_val: 0.08 | rmse_val: 1.0234
Epoch: 0088 | loss_train: 3.5020 loss_val: 3.6102 | f1_val: 0.07 | rmse_val: 0.7066
Epoch: 0089 | loss_train: 4.0359 loss_val: 2.8585 | f1_val: 0.07 | rmse_val: 1.2075
Epoch: 0090 | loss_train: 3.7754 loss_val: 4.5780 | f1_val: 0.06 | rmse_val: 0.9978
Epoch: 0091 | loss_train: 3.9732 loss_val: 3.9880 | f1_val: 0.07 | rmse_val: 0.9819
Epoch: 0092 | loss_train: 3.7372 loss_val: 4.3625 | f1_val: 0.05 | rmse_val: 0.8158
Epoch: 0093 | loss_train: 3.7929 loss_val: 3.0754 | f1_val: 0.06 | rmse_val: 0.9083
Epoch: 0094 | loss_train: 3.8909 loss_val: 4.5888 | f1_val: 0.05 | rmse_val: 1.4014
Epoch: 0095 | loss_train: 3.9840 loss_val: 3.7196 | f1_val: 0.06 | rmse_val: 0.8188
Epoch: 0096 | loss_train: 3.9783 loss_val: 3.1785 | f1_val: 0.04 | rmse_val: 0.7770
Epoch: 0097 | loss_train: 4.3834 loss_val: 3.8104 | f1_val: 0.07 | rmse_val: 0.9270
Epoch: 0098 | loss_train: 3.4909 loss_val: 2.3345 | f1_val: 0.07 | rmse_val: 0.7227
Epoch: 0099 | loss_train: 3.9704 loss_val: 4.2351 | f1_val: 0.06 | rmse_val: 0.8996
Epoch: 0100 | loss_train: 3.9331 loss_val: 4.0431 | f1_val: 0.07 | rmse_val: 1.1708
Epoch: 0101 | loss_train: 3.9695 loss_val: 4.1412 | f1_val: 0.06 | rmse_val: 0.8719
Epoch: 0102 | loss_train: 3.7066 loss_val: 3.4305 | f1_val: 0.07 | rmse_val: 1.2491
Epoch: 0103 | loss_train: 3.8599 loss_val: 4.0434 | f1_val: 0.06 | rmse_val: 0.6966
Epoch: 0104 | loss_train: 3.4771 loss_val: 3.2958 | f1_val: 0.07 | rmse_val: 1.1120
Epoch: 0105 | loss_train: 3.6705 loss_val: 2.9144 | f1_val: 0.07 | rmse_val: 0.9273
Epoch: 0106 | loss_train: 3.6690 loss_val: 2.6481 | f1_val: 0.07 | rmse_val: 0.7679
Epoch: 0107 | loss_train: 3.6970 loss_val: 3.3873 | f1_val: 0.06 | rmse_val: 1.0528
Epoch: 0108 | loss_train: 3.7162 loss_val: 4.0501 | f1_val: 0.05 | rmse_val: 1.0010
Epoch: 0109 | loss_train: 4.4533 loss_val: 3.7029 | f1_val: 0.05 | rmse_val: 0.7783
Epoch: 0110 | loss_train: 3.7128 loss_val: 3.2070 | f1_val: 0.05 | rmse_val: 0.9625
Epoch: 0111 | loss_train: 3.6105 loss_val: 2.3902 | f1_val: 0.08 | rmse_val: 1.0006
Epoch: 0112 | loss_train: 3.5314 loss_val: 3.0254 | f1_val: 0.07 | rmse_val: 0.7803
Epoch: 0113 | loss_train: 3.8956 loss_val: 2.4739 | f1_val: 0.07 | rmse_val: 0.9006
Epoch: 0114 | loss_train: 3.5705 loss_val: 3.6987 | f1_val: 0.06 | rmse_val: 0.6698
Epoch: 0115 | loss_train: 3.4336 loss_val: 2.9267 | f1_val: 0.07 | rmse_val: 0.8709
Epoch: 0116 | loss_train: 3.4728 loss_val: 3.0538 | f1_val: 0.07 | rmse_val: 0.8904
Epoch: 0117 | loss_train: 4.1024 loss_val: 4.9609 | f1_val: 0.06 | rmse_val: 0.8066
Epoch: 0118 | loss_train: 3.7064 loss_val: 4.1888 | f1_val: 0.06 | rmse_val: 0.8350
Epoch: 0119 | loss_train: 3.9609 loss_val: 3.0675 | f1_val: 0.07 | rmse_val: 0.7508
Epoch: 0120 | loss_train: 3.8792 loss_val: 3.7831 | f1_val: 0.07 | rmse_val: 0.6951
Epoch: 0121 | loss_train: 3.8594 loss_val: 2.8965 | f1_val: 0.05 | rmse_val: 1.0591
Epoch: 0122 | loss_train: 3.8516 loss_val: 3.0923 | f1_val: 0.06 | rmse_val: 0.8455
Epoch: 0123 | loss_train: 3.4760 loss_val: 5.1577 | f1_val: 0.07 | rmse_val: 0.8819
Epoch: 0124 | loss_train: 3.8167 loss_val: 3.2304 | f1_val: 0.08 | rmse_val: 1.0845
Epoch: 0125 | loss_train: 3.5386 loss_val: 4.4797 | f1_val: 0.06 | rmse_val: 1.1234
Epoch: 0126 | loss_train: 4.1637 loss_val: 2.9860 | f1_val: 0.06 | rmse_val: 1.0260
Epoch: 0127 | loss_train: 3.9602 loss_val: 2.7930 | f1_val: 0.07 | rmse_val: 0.7097
Epoch: 0128 | loss_train: 3.7316 loss_val: 2.8999 | f1_val: 0.05 | rmse_val: 0.9836
Optimization Finished!
Train cost: 783.5836s
Loading 6th epoch
f1_test: 0.05 | rmse_test: 0.9836

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/LINUX/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=512, bias=True)
)
total params: 2369794
Epoch: 0001 | loss_train: 16.4264 loss_val: 8.3641 | f1_val: 0.07 | rmse_val: 1.7657
Epoch: 0002 | loss_train: 6.1343 loss_val: 4.8785 | f1_val: 0.06 | rmse_val: 1.5942
Epoch: 0003 | loss_train: 4.5406 loss_val: 4.3803 | f1_val: 0.05 | rmse_val: 1.2331
Epoch: 0004 | loss_train: 4.0448 loss_val: 4.1911 | f1_val: 0.05 | rmse_val: 1.3694
Epoch: 0005 | loss_train: 4.0853 loss_val: 4.1374 | f1_val: 0.06 | rmse_val: 1.1712
Epoch: 0006 | loss_train: 4.3135 loss_val: 3.8752 | f1_val: 0.06 | rmse_val: 1.3285
Epoch: 0007 | loss_train: 4.3964 loss_val: 4.2746 | f1_val: 0.05 | rmse_val: 1.2175
Epoch: 0008 | loss_train: 3.8101 loss_val: 4.9196 | f1_val: 0.06 | rmse_val: 1.4976
Epoch: 0009 | loss_train: 4.4090 loss_val: 5.7170 | f1_val: 0.07 | rmse_val: 1.2172
Epoch: 0010 | loss_train: 4.0914 loss_val: 4.8543 | f1_val: 0.05 | rmse_val: 0.8981
Epoch: 0011 | loss_train: 4.8257 loss_val: 4.5982 | f1_val: 0.07 | rmse_val: 1.6417
Epoch: 0012 | loss_train: 4.2238 loss_val: 3.8950 | f1_val: 0.07 | rmse_val: 1.0588
Epoch: 0013 | loss_train: 4.5350 loss_val: 5.2551 | f1_val: 0.05 | rmse_val: 0.8371
Epoch: 0014 | loss_train: 4.5702 loss_val: 4.5302 | f1_val: 0.05 | rmse_val: 1.4000
Epoch: 0015 | loss_train: 4.3807 loss_val: 4.0146 | f1_val: 0.07 | rmse_val: 0.9352
Epoch: 0016 | loss_train: 4.2501 loss_val: 5.0495 | f1_val: 0.07 | rmse_val: 0.9462
Epoch: 0017 | loss_train: 4.4349 loss_val: 3.9523 | f1_val: 0.08 | rmse_val: 0.9311
Epoch: 0018 | loss_train: 4.9600 loss_val: 5.0819 | f1_val: 0.07 | rmse_val: 1.0499
Epoch: 0019 | loss_train: 4.3716 loss_val: 5.0952 | f1_val: 0.07 | rmse_val: 1.9130
Epoch: 0020 | loss_train: 4.1940 loss_val: 4.5008 | f1_val: 0.05 | rmse_val: 0.6459
Epoch: 0021 | loss_train: 4.2039 loss_val: 4.6037 | f1_val: 0.06 | rmse_val: 0.8566
Epoch: 0022 | loss_train: 4.5720 loss_val: 8.6052 | f1_val: 0.06 | rmse_val: 2.3198
Epoch: 0023 | loss_train: 3.9231 loss_val: 4.1758 | f1_val: 0.06 | rmse_val: 1.1231
Epoch: 0024 | loss_train: 4.2504 loss_val: 3.7169 | f1_val: 0.06 | rmse_val: 0.9512
Epoch: 0025 | loss_train: 3.6985 loss_val: 5.7174 | f1_val: 0.07 | rmse_val: 0.6008
Epoch: 0026 | loss_train: 3.7874 loss_val: 4.0458 | f1_val: 0.07 | rmse_val: 1.6392
Epoch: 0027 | loss_train: 4.8999 loss_val: 4.4010 | f1_val: 0.05 | rmse_val: 1.3281
Epoch: 0028 | loss_train: 4.1549 loss_val: 5.7075 | f1_val: 0.06 | rmse_val: 0.9593
Epoch: 0029 | loss_train: 3.9071 loss_val: 3.7261 | f1_val: 0.06 | rmse_val: 0.9184
Epoch: 0030 | loss_train: 4.4646 loss_val: 3.9350 | f1_val: 0.06 | rmse_val: 2.0859
Epoch: 0031 | loss_train: 4.8279 loss_val: 4.1596 | f1_val: 0.05 | rmse_val: 0.6366
Epoch: 0032 | loss_train: 4.3713 loss_val: 3.8864 | f1_val: 0.05 | rmse_val: 0.8448
Epoch: 0033 | loss_train: 4.1006 loss_val: 3.2885 | f1_val: 0.05 | rmse_val: 1.0475
Epoch: 0034 | loss_train: 3.8463 loss_val: 2.8965 | f1_val: 0.07 | rmse_val: 0.9677
Epoch: 0035 | loss_train: 3.7873 loss_val: 4.5336 | f1_val: 0.04 | rmse_val: 1.7932
Epoch: 0036 | loss_train: 4.3035 loss_val: 3.2495 | f1_val: 0.08 | rmse_val: 0.8131
Epoch: 0037 | loss_train: 4.1512 loss_val: 3.7498 | f1_val: 0.06 | rmse_val: 1.2298
Epoch: 0038 | loss_train: 4.2615 loss_val: 4.0481 | f1_val: 0.06 | rmse_val: 0.6075
Epoch: 0039 | loss_train: 3.7532 loss_val: 4.2466 | f1_val: 0.05 | rmse_val: 1.7043
Epoch: 0040 | loss_train: 4.3466 loss_val: 3.7406 | f1_val: 0.07 | rmse_val: 0.9878
Epoch: 0041 | loss_train: 4.2294 loss_val: 5.1165 | f1_val: 0.07 | rmse_val: 0.7969
Epoch: 0042 | loss_train: 4.0215 loss_val: 3.1656 | f1_val: 0.08 | rmse_val: 0.8340
Epoch: 0043 | loss_train: 3.7103 loss_val: 3.7835 | f1_val: 0.05 | rmse_val: 0.6952
Epoch: 0044 | loss_train: 3.9551 loss_val: 2.9363 | f1_val: 0.05 | rmse_val: 0.9074
Epoch: 0045 | loss_train: 3.8151 loss_val: 3.7301 | f1_val: 0.06 | rmse_val: 1.2385
Epoch: 0046 | loss_train: 4.5038 loss_val: 3.2094 | f1_val: 0.06 | rmse_val: 0.5071
Epoch: 0047 | loss_train: 3.8019 loss_val: 4.8270 | f1_val: 0.06 | rmse_val: 0.7675
Epoch: 0048 | loss_train: 4.4198 loss_val: 5.6171 | f1_val: 0.07 | rmse_val: 0.7187
Epoch: 0049 | loss_train: 3.8499 loss_val: 3.3079 | f1_val: 0.06 | rmse_val: 1.2449
Epoch: 0050 | loss_train: 4.0211 loss_val: 4.1267 | f1_val: 0.04 | rmse_val: 1.0309
Epoch: 0051 | loss_train: 3.8180 loss_val: 3.2034 | f1_val: 0.06 | rmse_val: 0.8441
Epoch: 0052 | loss_train: 3.5291 loss_val: 3.1826 | f1_val: 0.08 | rmse_val: 0.8317
Epoch: 0053 | loss_train: 3.7073 loss_val: 4.6269 | f1_val: 0.05 | rmse_val: 1.0313
Epoch: 0054 | loss_train: 4.2866 loss_val: 4.4250 | f1_val: 0.05 | rmse_val: 0.9969
Epoch: 0055 | loss_train: 4.4123 loss_val: 4.1407 | f1_val: 0.05 | rmse_val: 0.9676
Epoch: 0056 | loss_train: 3.6643 loss_val: 3.8361 | f1_val: 0.06 | rmse_val: 0.8031
Epoch: 0057 | loss_train: 4.1221 loss_val: 3.0989 | f1_val: 0.06 | rmse_val: 1.1104
Epoch: 0058 | loss_train: 3.4379 loss_val: 2.8561 | f1_val: 0.05 | rmse_val: 1.0272
Epoch: 0059 | loss_train: 3.8355 loss_val: 3.1679 | f1_val: 0.06 | rmse_val: 0.8180
Epoch: 0060 | loss_train: 3.8212 loss_val: 3.4499 | f1_val: 0.06 | rmse_val: 1.0443
Epoch: 0061 | loss_train: 4.0472 loss_val: 4.4863 | f1_val: 0.08 | rmse_val: 0.8506
Epoch: 0062 | loss_train: 3.8305 loss_val: 2.9866 | f1_val: 0.07 | rmse_val: 1.0493
Epoch: 0063 | loss_train: 3.7802 loss_val: 3.8929 | f1_val: 0.06 | rmse_val: 0.8119
Epoch: 0064 | loss_train: 3.6074 loss_val: 4.0764 | f1_val: 0.07 | rmse_val: 1.1382
Epoch: 0065 | loss_train: 3.8460 loss_val: 3.3448 | f1_val: 0.06 | rmse_val: 0.6288
Epoch: 0066 | loss_train: 3.7500 loss_val: 4.1887 | f1_val: 0.06 | rmse_val: 1.0593
Epoch: 0067 | loss_train: 3.3921 loss_val: 4.4527 | f1_val: 0.05 | rmse_val: 0.9623
Epoch: 0068 | loss_train: 4.0148 loss_val: 3.4996 | f1_val: 0.09 | rmse_val: 0.7345
Epoch: 0069 | loss_train: 3.8316 loss_val: 4.6099 | f1_val: 0.07 | rmse_val: 0.9346
Epoch: 0070 | loss_train: 3.5153 loss_val: 4.2627 | f1_val: 0.07 | rmse_val: 0.8484
Epoch: 0071 | loss_train: 3.6513 loss_val: 3.4661 | f1_val: 0.05 | rmse_val: 0.8162
Epoch: 0072 | loss_train: 3.2611 loss_val: 3.2582 | f1_val: 0.06 | rmse_val: 0.7029
Epoch: 0073 | loss_train: 3.8911 loss_val: 4.0617 | f1_val: 0.07 | rmse_val: 0.9453
Epoch: 0074 | loss_train: 3.4360 loss_val: 3.5453 | f1_val: 0.05 | rmse_val: 0.6136
Epoch: 0075 | loss_train: 3.6798 loss_val: 4.2485 | f1_val: 0.08 | rmse_val: 0.9367
Epoch: 0076 | loss_train: 3.5127 loss_val: 3.5723 | f1_val: 0.06 | rmse_val: 0.7710
Epoch: 0077 | loss_train: 3.5296 loss_val: 2.9647 | f1_val: 0.08 | rmse_val: 0.8700
Epoch: 0078 | loss_train: 3.6881 loss_val: 4.7562 | f1_val: 0.07 | rmse_val: 0.9085
Epoch: 0079 | loss_train: 3.6762 loss_val: 3.8716 | f1_val: 0.08 | rmse_val: 0.9581
Epoch: 0080 | loss_train: 3.5937 loss_val: 2.1795 | f1_val: 0.07 | rmse_val: 0.9565
Epoch: 0081 | loss_train: 3.5074 loss_val: 2.5949 | f1_val: 0.08 | rmse_val: 0.9085
Epoch: 0082 | loss_train: 3.4531 loss_val: 4.2312 | f1_val: 0.07 | rmse_val: 0.8321
Epoch: 0083 | loss_train: 3.2873 loss_val: 3.3707 | f1_val: 0.06 | rmse_val: 0.7931
Epoch: 0084 | loss_train: 3.8220 loss_val: 2.7888 | f1_val: 0.06 | rmse_val: 0.7821
Epoch: 0085 | loss_train: 3.5351 loss_val: 4.2082 | f1_val: 0.06 | rmse_val: 0.7873
Epoch: 0086 | loss_train: 3.7529 loss_val: 4.7968 | f1_val: 0.06 | rmse_val: 0.8197
Epoch: 0087 | loss_train: 3.7860 loss_val: 3.2050 | f1_val: 0.08 | rmse_val: 0.8110
Epoch: 0088 | loss_train: 3.2397 loss_val: 2.9759 | f1_val: 0.07 | rmse_val: 0.5990
Epoch: 0089 | loss_train: 3.9492 loss_val: 2.9852 | f1_val: 0.06 | rmse_val: 1.0612
Epoch: 0090 | loss_train: 3.6007 loss_val: 4.7261 | f1_val: 0.09 | rmse_val: 0.8755
Epoch: 0091 | loss_train: 3.9075 loss_val: 3.6857 | f1_val: 0.08 | rmse_val: 0.9840
Epoch: 0092 | loss_train: 3.6451 loss_val: 4.8214 | f1_val: 0.06 | rmse_val: 0.6941
Epoch: 0093 | loss_train: 3.8733 loss_val: 3.0477 | f1_val: 0.06 | rmse_val: 0.8903
Epoch: 0094 | loss_train: 3.6454 loss_val: 4.1856 | f1_val: 0.08 | rmse_val: 0.9254
Epoch: 0095 | loss_train: 3.8415 loss_val: 3.5562 | f1_val: 0.05 | rmse_val: 0.8030
Epoch: 0096 | loss_train: 3.8355 loss_val: 3.1259 | f1_val: 0.07 | rmse_val: 0.8075
Epoch: 0097 | loss_train: 4.2535 loss_val: 4.2341 | f1_val: 0.05 | rmse_val: 0.8618
Epoch: 0098 | loss_train: 3.4704 loss_val: 2.5588 | f1_val: 0.03 | rmse_val: 0.8305
Epoch: 0099 | loss_train: 3.9628 loss_val: 3.9800 | f1_val: 0.06 | rmse_val: 0.7327
Epoch: 0100 | loss_train: 3.8475 loss_val: 4.3235 | f1_val: 0.06 | rmse_val: 0.9598
Epoch: 0101 | loss_train: 3.8125 loss_val: 3.9031 | f1_val: 0.08 | rmse_val: 0.7275
Epoch: 0102 | loss_train: 3.5734 loss_val: 3.2718 | f1_val: 0.06 | rmse_val: 1.1810
Epoch: 0103 | loss_train: 3.6995 loss_val: 4.1713 | f1_val: 0.07 | rmse_val: 0.7275
Epoch: 0104 | loss_train: 3.3819 loss_val: 3.3212 | f1_val: 0.07 | rmse_val: 0.7852
Epoch: 0105 | loss_train: 3.8561 loss_val: 3.1200 | f1_val: 0.05 | rmse_val: 0.8324
Epoch: 0106 | loss_train: 3.5501 loss_val: 2.8131 | f1_val: 0.06 | rmse_val: 0.6174
Epoch: 0107 | loss_train: 3.7552 loss_val: 3.5671 | f1_val: 0.05 | rmse_val: 0.9109
Epoch: 0108 | loss_train: 3.6249 loss_val: 3.7119 | f1_val: 0.08 | rmse_val: 0.8323
Epoch: 0109 | loss_train: 4.2424 loss_val: 4.0047 | f1_val: 0.07 | rmse_val: 0.6189
Epoch: 0110 | loss_train: 3.5933 loss_val: 3.6665 | f1_val: 0.06 | rmse_val: 0.8958
Epoch: 0111 | loss_train: 3.7260 loss_val: 2.1776 | f1_val: 0.07 | rmse_val: 0.7661
Epoch: 0112 | loss_train: 3.6079 loss_val: 3.2186 | f1_val: 0.06 | rmse_val: 0.7454
Epoch: 0113 | loss_train: 3.6200 loss_val: 2.4314 | f1_val: 0.06 | rmse_val: 0.8669
Epoch: 0114 | loss_train: 3.7855 loss_val: 3.3554 | f1_val: 0.06 | rmse_val: 0.5716
Epoch: 0115 | loss_train: 3.5144 loss_val: 2.9292 | f1_val: 0.05 | rmse_val: 0.9558
Epoch: 0116 | loss_train: 3.4092 loss_val: 3.2549 | f1_val: 0.07 | rmse_val: 0.6361
Epoch: 0117 | loss_train: 4.0204 loss_val: 5.1137 | f1_val: 0.06 | rmse_val: 0.7598
Epoch: 0118 | loss_train: 3.6109 loss_val: 4.5867 | f1_val: 0.08 | rmse_val: 0.6182
Epoch: 0119 | loss_train: 3.9550 loss_val: 2.8758 | f1_val: 0.05 | rmse_val: 0.7744
Epoch: 0120 | loss_train: 3.6544 loss_val: 3.8833 | f1_val: 0.07 | rmse_val: 0.7262
Epoch: 0121 | loss_train: 3.8277 loss_val: 3.2196 | f1_val: 0.05 | rmse_val: 0.8923
Epoch: 0122 | loss_train: 3.8208 loss_val: 3.5047 | f1_val: 0.05 | rmse_val: 0.7362
Epoch: 0123 | loss_train: 3.3162 loss_val: 5.6140 | f1_val: 0.06 | rmse_val: 0.8148
Epoch: 0124 | loss_train: 3.7608 loss_val: 3.4763 | f1_val: 0.06 | rmse_val: 0.7915
Epoch: 0125 | loss_train: 3.4186 loss_val: 3.7109 | f1_val: 0.06 | rmse_val: 0.8391
Epoch: 0126 | loss_train: 4.2072 loss_val: 3.0082 | f1_val: 0.06 | rmse_val: 0.6170
Epoch: 0127 | loss_train: 3.8057 loss_val: 2.8788 | f1_val: 0.07 | rmse_val: 0.8471
Epoch: 0128 | loss_train: 3.4317 loss_val: 3.0018 | f1_val: 0.08 | rmse_val: 0.9985
Epoch: 0129 | loss_train: 3.9155 loss_val: 4.2078 | f1_val: 0.08 | rmse_val: 0.8393
Epoch: 0130 | loss_train: 3.3946 loss_val: 2.5754 | f1_val: 0.06 | rmse_val: 0.8669
Epoch: 0131 | loss_train: 3.5677 loss_val: 3.2662 | f1_val: 0.06 | rmse_val: 0.7508
Epoch: 0132 | loss_train: 3.5994 loss_val: 3.4700 | f1_val: 0.07 | rmse_val: 0.6963
Epoch: 0133 | loss_train: 3.4826 loss_val: 4.0803 | f1_val: 0.08 | rmse_val: 0.9080
Epoch: 0134 | loss_train: 3.9354 loss_val: 4.0220 | f1_val: 0.07 | rmse_val: 0.5498
Epoch: 0135 | loss_train: 3.9185 loss_val: 3.5443 | f1_val: 0.05 | rmse_val: 0.8095
Epoch: 0136 | loss_train: 3.6667 loss_val: 2.9808 | f1_val: 0.07 | rmse_val: 0.7859
Epoch: 0137 | loss_train: 3.7919 loss_val: 3.7841 | f1_val: 0.05 | rmse_val: 0.8835
Epoch: 0138 | loss_train: 3.5058 loss_val: 3.4083 | f1_val: 0.06 | rmse_val: 0.8144
Epoch: 0139 | loss_train: 3.5948 loss_val: 3.1186 | f1_val: 0.07 | rmse_val: 0.9934
Epoch: 0140 | loss_train: 3.9302 loss_val: 4.2281 | f1_val: 0.07 | rmse_val: 0.7181
Epoch: 0141 | loss_train: 3.5225 loss_val: 3.0462 | f1_val: 0.08 | rmse_val: 0.7280
Optimization Finished!
Train cost: 882.2323s
Loading 90th epoch
f1_test: 0.08 | rmse_test: 0.7280

>>> run.py: Namespace(dataset='LINUX', device=1, experiment='hidden_dim', log_path='log/nagphormer/LINUX/hidden_dim', path='data/nagphormer', plot_path='plots/nagphormer/LINUX/hidden_dim', pyg_path='data/pyg/LINUX')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=32, hops=3, log_path='log/nagphormer/AIDS700nef/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=32, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=32, out_features=32, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=32, out_features=64, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=64, out_features=32, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=32, out_features=16, bias=True)
  (attn_layer): Linear(in_features=64, out_features=1, bias=True)
  (Linear1): Linear(in_features=16, out_features=32, bias=True)
)
total params: 10802
Epoch: 0001 | loss_train: 84.7832 loss_val: 81.1365 | f1_val: 0.05 | rmse_val: 7.6632
Epoch: 0002 | loss_train: 83.3697 loss_val: 97.4165 | f1_val: 0.06 | rmse_val: 7.6557
Epoch: 0003 | loss_train: 84.3156 loss_val: 79.4907 | f1_val: 0.06 | rmse_val: 7.3398
Epoch: 0004 | loss_train: 80.8588 loss_val: 82.7160 | f1_val: 0.05 | rmse_val: 7.6050
Epoch: 0005 | loss_train: 81.1179 loss_val: 90.8261 | f1_val: 0.06 | rmse_val: 7.6142
Epoch: 0006 | loss_train: 83.5052 loss_val: 81.4233 | f1_val: 0.05 | rmse_val: 7.3082
Epoch: 0007 | loss_train: 76.4943 loss_val: 74.7312 | f1_val: 0.06 | rmse_val: 7.1880
Epoch: 0008 | loss_train: 74.4067 loss_val: 69.8805 | f1_val: 0.07 | rmse_val: 7.0551
Epoch: 0009 | loss_train: 69.0055 loss_val: 76.4892 | f1_val: 0.06 | rmse_val: 6.6629
Epoch: 0010 | loss_train: 61.2509 loss_val: 67.2569 | f1_val: 0.05 | rmse_val: 6.0439
Epoch: 0011 | loss_train: 52.9591 loss_val: 57.9198 | f1_val: 0.05 | rmse_val: 5.1330
Epoch: 0012 | loss_train: 37.9964 loss_val: 33.6422 | f1_val: 0.05 | rmse_val: 3.9798
Epoch: 0013 | loss_train: 28.3109 loss_val: 26.0664 | f1_val: 0.08 | rmse_val: 2.7187
Epoch: 0014 | loss_train: 18.2281 loss_val: 13.1624 | f1_val: 0.06 | rmse_val: 1.3641
Epoch: 0015 | loss_train: 10.6722 loss_val: 10.0197 | f1_val: 0.05 | rmse_val: 1.0499
Epoch: 0016 | loss_train: 7.1348 loss_val: 3.8620 | f1_val: 0.07 | rmse_val: 1.5243
Epoch: 0017 | loss_train: 7.5855 loss_val: 6.4431 | f1_val: 0.06 | rmse_val: 1.3765
Epoch: 0018 | loss_train: 6.8691 loss_val: 5.5856 | f1_val: 0.06 | rmse_val: 1.8937
Epoch: 0019 | loss_train: 5.9817 loss_val: 7.0841 | f1_val: 0.07 | rmse_val: 0.9630
Epoch: 0020 | loss_train: 6.4041 loss_val: 7.8654 | f1_val: 0.07 | rmse_val: 1.6693
Epoch: 0021 | loss_train: 7.3645 loss_val: 5.2388 | f1_val: 0.05 | rmse_val: 1.5986
Epoch: 0022 | loss_train: 6.3197 loss_val: 7.3795 | f1_val: 0.05 | rmse_val: 1.3553
Epoch: 0023 | loss_train: 7.2258 loss_val: 4.3945 | f1_val: 0.07 | rmse_val: 1.7833
Epoch: 0024 | loss_train: 6.6336 loss_val: 6.2253 | f1_val: 0.08 | rmse_val: 1.5132
Epoch: 0025 | loss_train: 6.1543 loss_val: 7.2628 | f1_val: 0.06 | rmse_val: 1.4786
Epoch: 0026 | loss_train: 7.9132 loss_val: 7.5352 | f1_val: 0.04 | rmse_val: 1.2880
Epoch: 0027 | loss_train: 6.2902 loss_val: 7.7522 | f1_val: 0.06 | rmse_val: 1.6148
Epoch: 0028 | loss_train: 6.3874 loss_val: 8.7576 | f1_val: 0.07 | rmse_val: 1.3553
Epoch: 0029 | loss_train: 5.8989 loss_val: 8.1742 | f1_val: 0.04 | rmse_val: 1.3900
Epoch: 0030 | loss_train: 5.9105 loss_val: 7.6631 | f1_val: 0.08 | rmse_val: 1.4392
Epoch: 0031 | loss_train: 5.7188 loss_val: 6.6474 | f1_val: 0.07 | rmse_val: 1.1156
Epoch: 0032 | loss_train: 6.3866 loss_val: 6.9731 | f1_val: 0.05 | rmse_val: 1.8254
Epoch: 0033 | loss_train: 7.7243 loss_val: 8.3630 | f1_val: 0.08 | rmse_val: 1.2137
Epoch: 0034 | loss_train: 5.9834 loss_val: 3.7662 | f1_val: 0.05 | rmse_val: 1.0439
Epoch: 0035 | loss_train: 6.2302 loss_val: 5.1917 | f1_val: 0.06 | rmse_val: 1.8306
Epoch: 0036 | loss_train: 5.7861 loss_val: 6.7027 | f1_val: 0.08 | rmse_val: 1.0317
Epoch: 0037 | loss_train: 6.2403 loss_val: 8.1890 | f1_val: 0.08 | rmse_val: 1.6847
Epoch: 0038 | loss_train: 6.5985 loss_val: 5.3006 | f1_val: 0.07 | rmse_val: 1.2336
Epoch: 0039 | loss_train: 5.4525 loss_val: 7.3177 | f1_val: 0.06 | rmse_val: 1.5877
Epoch: 0040 | loss_train: 5.6598 loss_val: 7.4493 | f1_val: 0.05 | rmse_val: 0.9865
Epoch: 0041 | loss_train: 5.9832 loss_val: 7.2130 | f1_val: 0.07 | rmse_val: 1.3703
Epoch: 0042 | loss_train: 5.5100 loss_val: 5.6294 | f1_val: 0.06 | rmse_val: 0.9644
Epoch: 0043 | loss_train: 6.2221 loss_val: 5.5804 | f1_val: 0.05 | rmse_val: 1.3000
Epoch: 0044 | loss_train: 5.6067 loss_val: 7.1431 | f1_val: 0.06 | rmse_val: 1.5213
Epoch: 0045 | loss_train: 6.5711 loss_val: 6.4972 | f1_val: 0.06 | rmse_val: 1.4581
Epoch: 0046 | loss_train: 6.9417 loss_val: 6.8422 | f1_val: 0.05 | rmse_val: 1.1783
Epoch: 0047 | loss_train: 6.0988 loss_val: 5.6997 | f1_val: 0.05 | rmse_val: 1.4803
Epoch: 0048 | loss_train: 6.9038 loss_val: 8.7998 | f1_val: 0.06 | rmse_val: 1.1229
Epoch: 0049 | loss_train: 6.1312 loss_val: 5.2787 | f1_val: 0.05 | rmse_val: 1.3322
Epoch: 0050 | loss_train: 6.1771 loss_val: 4.6794 | f1_val: 0.06 | rmse_val: 1.0942
Epoch: 0051 | loss_train: 5.5784 loss_val: 4.2673 | f1_val: 0.06 | rmse_val: 1.4446
Epoch: 0052 | loss_train: 6.1643 loss_val: 8.3601 | f1_val: 0.07 | rmse_val: 0.7001
Epoch: 0053 | loss_train: 6.8288 loss_val: 7.6984 | f1_val: 0.05 | rmse_val: 1.5243
Epoch: 0054 | loss_train: 5.4273 loss_val: 7.3724 | f1_val: 0.08 | rmse_val: 1.2533
Epoch: 0055 | loss_train: 6.7511 loss_val: 8.9680 | f1_val: 0.07 | rmse_val: 1.0095
Epoch: 0056 | loss_train: 6.9280 loss_val: 5.2501 | f1_val: 0.06 | rmse_val: 1.2230
Epoch: 0057 | loss_train: 6.5261 loss_val: 4.9810 | f1_val: 0.06 | rmse_val: 1.4218
Epoch: 0058 | loss_train: 4.7519 loss_val: 5.8649 | f1_val: 0.08 | rmse_val: 1.2024
Epoch: 0059 | loss_train: 7.0025 loss_val: 5.8239 | f1_val: 0.06 | rmse_val: 1.3363
Epoch: 0060 | loss_train: 6.0313 loss_val: 9.5936 | f1_val: 0.06 | rmse_val: 1.4960
Epoch: 0061 | loss_train: 5.0134 loss_val: 5.2526 | f1_val: 0.07 | rmse_val: 1.2850
Epoch: 0062 | loss_train: 5.8332 loss_val: 4.5928 | f1_val: 0.05 | rmse_val: 1.1195
Epoch: 0063 | loss_train: 5.6217 loss_val: 5.5687 | f1_val: 0.07 | rmse_val: 1.4549
Epoch: 0064 | loss_train: 6.2840 loss_val: 4.4215 | f1_val: 0.06 | rmse_val: 1.0349
Epoch: 0065 | loss_train: 5.6273 loss_val: 7.6414 | f1_val: 0.04 | rmse_val: 1.1871
Epoch: 0066 | loss_train: 5.4491 loss_val: 5.8249 | f1_val: 0.08 | rmse_val: 1.5190
Epoch: 0067 | loss_train: 5.2685 loss_val: 4.3458 | f1_val: 0.08 | rmse_val: 0.9458
Epoch: 0068 | loss_train: 5.6541 loss_val: 8.0578 | f1_val: 0.06 | rmse_val: 1.3464
Epoch: 0069 | loss_train: 5.4523 loss_val: 6.1905 | f1_val: 0.08 | rmse_val: 1.3678
Epoch: 0070 | loss_train: 5.9705 loss_val: 5.8068 | f1_val: 0.05 | rmse_val: 1.2116
Epoch: 0071 | loss_train: 5.8288 loss_val: 4.5020 | f1_val: 0.03 | rmse_val: 1.5316
Epoch: 0072 | loss_train: 6.3606 loss_val: 6.5220 | f1_val: 0.05 | rmse_val: 1.1955
Epoch: 0073 | loss_train: 5.5281 loss_val: 4.4320 | f1_val: 0.05 | rmse_val: 1.1451
Epoch: 0074 | loss_train: 5.6047 loss_val: 4.0585 | f1_val: 0.07 | rmse_val: 1.3837
Epoch: 0075 | loss_train: 5.2391 loss_val: 6.2856 | f1_val: 0.04 | rmse_val: 1.2282
Epoch: 0076 | loss_train: 4.8631 loss_val: 4.2559 | f1_val: 0.06 | rmse_val: 1.1243
Epoch: 0077 | loss_train: 6.2599 loss_val: 6.3217 | f1_val: 0.07 | rmse_val: 1.4542
Epoch: 0078 | loss_train: 6.0670 loss_val: 3.9286 | f1_val: 0.10 | rmse_val: 1.1635
Epoch: 0079 | loss_train: 6.4871 loss_val: 6.0425 | f1_val: 0.06 | rmse_val: 0.8531
Epoch: 0080 | loss_train: 5.7049 loss_val: 6.0255 | f1_val: 0.07 | rmse_val: 1.1662
Epoch: 0081 | loss_train: 5.3506 loss_val: 4.6041 | f1_val: 0.05 | rmse_val: 1.1699
Epoch: 0082 | loss_train: 5.9040 loss_val: 4.0196 | f1_val: 0.08 | rmse_val: 1.1476
Epoch: 0083 | loss_train: 5.9799 loss_val: 7.5225 | f1_val: 0.08 | rmse_val: 1.1678
Epoch: 0084 | loss_train: 6.3772 loss_val: 5.0977 | f1_val: 0.06 | rmse_val: 1.1010
Epoch: 0085 | loss_train: 5.3466 loss_val: 4.9112 | f1_val: 0.06 | rmse_val: 1.3171
Epoch: 0086 | loss_train: 6.0530 loss_val: 5.8766 | f1_val: 0.06 | rmse_val: 1.2119
Epoch: 0087 | loss_train: 5.1985 loss_val: 7.7190 | f1_val: 0.05 | rmse_val: 1.1273
Epoch: 0088 | loss_train: 4.6508 loss_val: 5.7178 | f1_val: 0.06 | rmse_val: 1.2067
Epoch: 0089 | loss_train: 5.6415 loss_val: 9.3468 | f1_val: 0.07 | rmse_val: 1.1241
Epoch: 0090 | loss_train: 4.9179 loss_val: 5.0122 | f1_val: 0.06 | rmse_val: 1.1767
Epoch: 0091 | loss_train: 5.6243 loss_val: 6.4476 | f1_val: 0.07 | rmse_val: 1.2994
Epoch: 0092 | loss_train: 5.0407 loss_val: 5.9241 | f1_val: 0.06 | rmse_val: 1.2745
Epoch: 0093 | loss_train: 5.7161 loss_val: 4.9623 | f1_val: 0.04 | rmse_val: 1.4002
Epoch: 0094 | loss_train: 4.9906 loss_val: 6.5189 | f1_val: 0.08 | rmse_val: 1.3581
Epoch: 0095 | loss_train: 5.7312 loss_val: 8.5567 | f1_val: 0.08 | rmse_val: 1.1305
Epoch: 0096 | loss_train: 6.4522 loss_val: 4.5335 | f1_val: 0.08 | rmse_val: 1.3214
Epoch: 0097 | loss_train: 5.4029 loss_val: 5.7399 | f1_val: 0.05 | rmse_val: 1.3652
Epoch: 0098 | loss_train: 5.5790 loss_val: 7.4576 | f1_val: 0.09 | rmse_val: 1.4135
Epoch: 0099 | loss_train: 4.9899 loss_val: 6.6719 | f1_val: 0.05 | rmse_val: 1.4170
Epoch: 0100 | loss_train: 5.5531 loss_val: 6.9627 | f1_val: 0.05 | rmse_val: 1.2136
Epoch: 0101 | loss_train: 4.9126 loss_val: 9.5624 | f1_val: 0.07 | rmse_val: 1.2377
Epoch: 0102 | loss_train: 6.6429 loss_val: 6.2156 | f1_val: 0.06 | rmse_val: 1.1553
Epoch: 0103 | loss_train: 4.9892 loss_val: 5.7836 | f1_val: 0.09 | rmse_val: 1.3022
Epoch: 0104 | loss_train: 6.3823 loss_val: 4.8008 | f1_val: 0.06 | rmse_val: 1.5037
Epoch: 0105 | loss_train: 5.3612 loss_val: 8.8097 | f1_val: 0.06 | rmse_val: 1.0275
Epoch: 0106 | loss_train: 5.6498 loss_val: 4.5834 | f1_val: 0.07 | rmse_val: 1.1075
Epoch: 0107 | loss_train: 5.3372 loss_val: 6.7865 | f1_val: 0.05 | rmse_val: 1.0417
Epoch: 0108 | loss_train: 6.1204 loss_val: 4.0966 | f1_val: 0.06 | rmse_val: 0.9976
Optimization Finished!
Train cost: 355.0725s
Loading 78th epoch
f1_test: 0.06 | rmse_test: 0.9976

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=64, hops=3, log_path='log/nagphormer/AIDS700nef/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=64, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=64, out_features=64, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=64, out_features=128, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=128, out_features=64, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=64, out_features=32, bias=True)
  (attn_layer): Linear(in_features=128, out_features=1, bias=True)
  (Linear1): Linear(in_features=32, out_features=64, bias=True)
)
total params: 40034
Epoch: 0001 | loss_train: 82.4414 loss_val: 78.4750 | f1_val: 0.04 | rmse_val: 7.5075
Epoch: 0002 | loss_train: 80.0738 loss_val: 92.8407 | f1_val: 0.06 | rmse_val: 7.2635
Epoch: 0003 | loss_train: 79.0423 loss_val: 73.2472 | f1_val: 0.05 | rmse_val: 7.1023
Epoch: 0004 | loss_train: 72.4582 loss_val: 71.9306 | f1_val: 0.04 | rmse_val: 7.0335
Epoch: 0005 | loss_train: 67.5910 loss_val: 72.6233 | f1_val: 0.06 | rmse_val: 6.3684
Epoch: 0006 | loss_train: 62.4475 loss_val: 54.6678 | f1_val: 0.04 | rmse_val: 5.6767
Epoch: 0007 | loss_train: 46.8716 loss_val: 36.6684 | f1_val: 0.07 | rmse_val: 4.8005
Epoch: 0008 | loss_train: 31.3500 loss_val: 20.3933 | f1_val: 0.05 | rmse_val: 2.8431
Epoch: 0009 | loss_train: 15.3123 loss_val: 12.8376 | f1_val: 0.06 | rmse_val: 1.1218
Epoch: 0010 | loss_train: 7.7709 loss_val: 9.3880 | f1_val: 0.04 | rmse_val: 1.4931
Epoch: 0011 | loss_train: 7.9366 loss_val: 6.7244 | f1_val: 0.05 | rmse_val: 1.9093
Epoch: 0012 | loss_train: 6.9124 loss_val: 7.7304 | f1_val: 0.08 | rmse_val: 1.3672
Epoch: 0013 | loss_train: 6.5674 loss_val: 8.3459 | f1_val: 0.07 | rmse_val: 1.3708
Epoch: 0014 | loss_train: 7.6824 loss_val: 10.4789 | f1_val: 0.06 | rmse_val: 1.5818
Epoch: 0015 | loss_train: 7.5498 loss_val: 7.9217 | f1_val: 0.06 | rmse_val: 1.6070
Epoch: 0016 | loss_train: 6.4686 loss_val: 4.3963 | f1_val: 0.05 | rmse_val: 1.4218
Epoch: 0017 | loss_train: 7.5093 loss_val: 6.7847 | f1_val: 0.06 | rmse_val: 1.6158
Epoch: 0018 | loss_train: 6.9435 loss_val: 6.5302 | f1_val: 0.05 | rmse_val: 1.4799
Epoch: 0019 | loss_train: 5.9268 loss_val: 7.2889 | f1_val: 0.05 | rmse_val: 0.9917
Epoch: 0020 | loss_train: 6.2347 loss_val: 8.5602 | f1_val: 0.07 | rmse_val: 1.4099
Epoch: 0021 | loss_train: 7.4925 loss_val: 5.7586 | f1_val: 0.06 | rmse_val: 1.7719
Epoch: 0022 | loss_train: 6.3696 loss_val: 8.4929 | f1_val: 0.05 | rmse_val: 1.4232
Epoch: 0023 | loss_train: 7.3193 loss_val: 4.3554 | f1_val: 0.07 | rmse_val: 2.1605
Epoch: 0024 | loss_train: 6.5117 loss_val: 6.8641 | f1_val: 0.04 | rmse_val: 1.2762
Epoch: 0025 | loss_train: 6.1669 loss_val: 7.9690 | f1_val: 0.07 | rmse_val: 1.2294
Epoch: 0026 | loss_train: 7.9942 loss_val: 8.4453 | f1_val: 0.07 | rmse_val: 1.1287
Epoch: 0027 | loss_train: 6.4917 loss_val: 8.5781 | f1_val: 0.05 | rmse_val: 1.7767
Epoch: 0028 | loss_train: 6.3256 loss_val: 9.6117 | f1_val: 0.09 | rmse_val: 1.2435
Epoch: 0029 | loss_train: 6.1677 loss_val: 9.1309 | f1_val: 0.06 | rmse_val: 1.6403
Epoch: 0030 | loss_train: 6.0287 loss_val: 8.4985 | f1_val: 0.05 | rmse_val: 1.2361
Epoch: 0031 | loss_train: 5.9627 loss_val: 7.2240 | f1_val: 0.07 | rmse_val: 0.9018
Epoch: 0032 | loss_train: 6.6096 loss_val: 7.8102 | f1_val: 0.05 | rmse_val: 2.0076
Epoch: 0033 | loss_train: 7.7888 loss_val: 9.3441 | f1_val: 0.07 | rmse_val: 0.9714
Epoch: 0034 | loss_train: 6.3295 loss_val: 4.0113 | f1_val: 0.05 | rmse_val: 1.1808
Epoch: 0035 | loss_train: 6.7183 loss_val: 6.1739 | f1_val: 0.07 | rmse_val: 1.7830
Epoch: 0036 | loss_train: 6.0264 loss_val: 7.7485 | f1_val: 0.06 | rmse_val: 0.7678
Epoch: 0037 | loss_train: 6.4688 loss_val: 9.0958 | f1_val: 0.07 | rmse_val: 1.8949
Epoch: 0038 | loss_train: 6.6801 loss_val: 5.2676 | f1_val: 0.07 | rmse_val: 1.4855
Epoch: 0039 | loss_train: 5.3930 loss_val: 7.4926 | f1_val: 0.07 | rmse_val: 1.6176
Epoch: 0040 | loss_train: 5.3947 loss_val: 6.4405 | f1_val: 0.05 | rmse_val: 1.0389
Epoch: 0041 | loss_train: 5.7217 loss_val: 8.0490 | f1_val: 0.07 | rmse_val: 0.9080
Epoch: 0042 | loss_train: 5.3791 loss_val: 5.1606 | f1_val: 0.05 | rmse_val: 1.1094
Epoch: 0043 | loss_train: 6.1116 loss_val: 5.4760 | f1_val: 0.05 | rmse_val: 1.2809
Epoch: 0044 | loss_train: 5.7457 loss_val: 7.8189 | f1_val: 0.08 | rmse_val: 2.1381
Epoch: 0045 | loss_train: 6.6073 loss_val: 6.8998 | f1_val: 0.08 | rmse_val: 1.8669
Epoch: 0046 | loss_train: 10.8456 loss_val: 7.7085 | f1_val: 0.06 | rmse_val: 1.0077
Epoch: 0047 | loss_train: 6.8815 loss_val: 6.1036 | f1_val: 0.07 | rmse_val: 1.3413
Epoch: 0048 | loss_train: 7.0335 loss_val: 8.8274 | f1_val: 0.05 | rmse_val: 1.4996
Epoch: 0049 | loss_train: 6.1129 loss_val: 5.1178 | f1_val: 0.06 | rmse_val: 1.4413
Epoch: 0050 | loss_train: 6.0961 loss_val: 3.8118 | f1_val: 0.07 | rmse_val: 0.8307
Epoch: 0051 | loss_train: 5.3377 loss_val: 4.9822 | f1_val: 0.08 | rmse_val: 1.2996
Epoch: 0052 | loss_train: 6.2511 loss_val: 7.4072 | f1_val: 0.08 | rmse_val: 1.0060
Epoch: 0053 | loss_train: 6.2084 loss_val: 7.2980 | f1_val: 0.06 | rmse_val: 1.6793
Epoch: 0054 | loss_train: 5.3614 loss_val: 7.8799 | f1_val: 0.08 | rmse_val: 1.3390
Epoch: 0055 | loss_train: 6.6925 loss_val: 9.2083 | f1_val: 0.07 | rmse_val: 1.0581
Epoch: 0056 | loss_train: 7.3446 loss_val: 5.8566 | f1_val: 0.06 | rmse_val: 1.5244
Epoch: 0057 | loss_train: 6.5144 loss_val: 5.3038 | f1_val: 0.05 | rmse_val: 1.5944
Epoch: 0058 | loss_train: 4.6562 loss_val: 6.5105 | f1_val: 0.04 | rmse_val: 1.1120
Epoch: 0059 | loss_train: 6.9467 loss_val: 5.3814 | f1_val: 0.09 | rmse_val: 1.4730
Epoch: 0060 | loss_train: 5.7484 loss_val: 9.3873 | f1_val: 0.06 | rmse_val: 1.2681
Epoch: 0061 | loss_train: 4.9730 loss_val: 5.3216 | f1_val: 0.07 | rmse_val: 1.7430
Epoch: 0062 | loss_train: 5.7469 loss_val: 4.8890 | f1_val: 0.06 | rmse_val: 1.4990
Epoch: 0063 | loss_train: 5.6272 loss_val: 5.7461 | f1_val: 0.05 | rmse_val: 1.2780
Epoch: 0064 | loss_train: 6.0863 loss_val: 4.0958 | f1_val: 0.04 | rmse_val: 0.8770
Epoch: 0065 | loss_train: 5.3796 loss_val: 8.5431 | f1_val: 0.05 | rmse_val: 1.1756
Epoch: 0066 | loss_train: 5.4172 loss_val: 6.0310 | f1_val: 0.04 | rmse_val: 1.2912
Epoch: 0067 | loss_train: 5.0002 loss_val: 4.2248 | f1_val: 0.06 | rmse_val: 1.0472
Epoch: 0068 | loss_train: 5.5518 loss_val: 7.5472 | f1_val: 0.09 | rmse_val: 1.5294
Epoch: 0069 | loss_train: 5.0964 loss_val: 5.8107 | f1_val: 0.07 | rmse_val: 1.4897
Epoch: 0070 | loss_train: 5.6817 loss_val: 5.1813 | f1_val: 0.07 | rmse_val: 1.2943
Epoch: 0071 | loss_train: 5.6153 loss_val: 4.0174 | f1_val: 0.06 | rmse_val: 1.3279
Epoch: 0072 | loss_train: 6.2770 loss_val: 6.1448 | f1_val: 0.05 | rmse_val: 1.2811
Epoch: 0073 | loss_train: 5.3326 loss_val: 4.5375 | f1_val: 0.08 | rmse_val: 1.3445
Epoch: 0074 | loss_train: 5.4099 loss_val: 4.1350 | f1_val: 0.07 | rmse_val: 1.3442
Epoch: 0075 | loss_train: 5.1757 loss_val: 6.0208 | f1_val: 0.08 | rmse_val: 1.2998
Epoch: 0076 | loss_train: 4.8293 loss_val: 4.3573 | f1_val: 0.06 | rmse_val: 1.3566
Epoch: 0077 | loss_train: 6.0825 loss_val: 6.3982 | f1_val: 0.06 | rmse_val: 1.6360
Epoch: 0078 | loss_train: 5.8244 loss_val: 3.8052 | f1_val: 0.09 | rmse_val: 0.9165
Epoch: 0079 | loss_train: 6.4711 loss_val: 5.7565 | f1_val: 0.06 | rmse_val: 1.0593
Epoch: 0080 | loss_train: 5.5287 loss_val: 6.3250 | f1_val: 0.08 | rmse_val: 1.0691
Epoch: 0081 | loss_train: 5.1484 loss_val: 4.5908 | f1_val: 0.07 | rmse_val: 1.2327
Epoch: 0082 | loss_train: 5.7430 loss_val: 3.9868 | f1_val: 0.07 | rmse_val: 1.0741
Epoch: 0083 | loss_train: 5.7861 loss_val: 7.8844 | f1_val: 0.07 | rmse_val: 1.2464
Epoch: 0084 | loss_train: 6.1742 loss_val: 5.0231 | f1_val: 0.07 | rmse_val: 1.1625
Epoch: 0085 | loss_train: 5.0953 loss_val: 5.0105 | f1_val: 0.08 | rmse_val: 1.2159
Epoch: 0086 | loss_train: 5.9559 loss_val: 6.0185 | f1_val: 0.07 | rmse_val: 1.4525
Epoch: 0087 | loss_train: 4.9711 loss_val: 7.4844 | f1_val: 0.07 | rmse_val: 1.3419
Epoch: 0088 | loss_train: 4.5299 loss_val: 5.5165 | f1_val: 0.05 | rmse_val: 1.2457
Epoch: 0089 | loss_train: 5.4836 loss_val: 9.3552 | f1_val: 0.06 | rmse_val: 1.2962
Epoch: 0090 | loss_train: 4.8221 loss_val: 5.0993 | f1_val: 0.07 | rmse_val: 1.1126
Epoch: 0091 | loss_train: 5.4612 loss_val: 6.3495 | f1_val: 0.09 | rmse_val: 1.4425
Epoch: 0092 | loss_train: 4.7997 loss_val: 5.8941 | f1_val: 0.09 | rmse_val: 1.4364
Epoch: 0093 | loss_train: 5.6447 loss_val: 4.6506 | f1_val: 0.06 | rmse_val: 1.4119
Epoch: 0094 | loss_train: 4.8506 loss_val: 6.3683 | f1_val: 0.05 | rmse_val: 1.0202
Epoch: 0095 | loss_train: 5.4132 loss_val: 7.9839 | f1_val: 0.05 | rmse_val: 1.0675
Epoch: 0096 | loss_train: 6.1222 loss_val: 4.5078 | f1_val: 0.05 | rmse_val: 1.2976
Epoch: 0097 | loss_train: 5.3132 loss_val: 5.8467 | f1_val: 0.06 | rmse_val: 1.1975
Epoch: 0098 | loss_train: 5.2868 loss_val: 6.6845 | f1_val: 0.06 | rmse_val: 1.2468
Epoch: 0099 | loss_train: 4.8515 loss_val: 6.7587 | f1_val: 0.06 | rmse_val: 1.4996
Epoch: 0100 | loss_train: 5.2215 loss_val: 7.1224 | f1_val: 0.06 | rmse_val: 1.2831
Epoch: 0101 | loss_train: 4.8018 loss_val: 9.8848 | f1_val: 0.06 | rmse_val: 1.2117
Epoch: 0102 | loss_train: 6.4223 loss_val: 6.6422 | f1_val: 0.04 | rmse_val: 1.3036
Epoch: 0103 | loss_train: 4.9172 loss_val: 5.6185 | f1_val: 0.05 | rmse_val: 1.5638
Epoch: 0104 | loss_train: 6.2766 loss_val: 4.8378 | f1_val: 0.04 | rmse_val: 1.3012
Epoch: 0105 | loss_train: 5.2228 loss_val: 8.8884 | f1_val: 0.07 | rmse_val: 1.0671
Epoch: 0106 | loss_train: 5.5173 loss_val: 4.7036 | f1_val: 0.08 | rmse_val: 1.0278
Epoch: 0107 | loss_train: 5.1325 loss_val: 6.3441 | f1_val: 0.05 | rmse_val: 1.2476
Epoch: 0108 | loss_train: 5.8326 loss_val: 4.3560 | f1_val: 0.06 | rmse_val: 1.2037
Optimization Finished!
Train cost: 345.6702s
Loading 59th epoch
f1_test: 0.06 | rmse_test: 1.2037

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.05 | rmse_val: 7.4451
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 7.4450
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.08 | rmse_val: 6.8450
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.03 | rmse_val: 6.0641
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.05 | rmse_val: 4.4422
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.06 | rmse_val: 1.9609
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.06 | rmse_val: 1.4814
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.06 | rmse_val: 2.2505
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.07 | rmse_val: 1.4667
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.07 | rmse_val: 1.3041
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.6533
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.06 | rmse_val: 1.0961
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.06 | rmse_val: 1.5658
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.05 | rmse_val: 1.5932
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.05 | rmse_val: 1.1988
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.04 | rmse_val: 1.2835
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.05 | rmse_val: 1.7141
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.06 | rmse_val: 1.2771
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.05 | rmse_val: 0.9793
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.08 | rmse_val: 1.8279
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.07 | rmse_val: 1.3268
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.04 | rmse_val: 1.8338
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.06 | rmse_val: 1.1240
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.03 | rmse_val: 1.8121
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.07 | rmse_val: 1.6883
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.09 | rmse_val: 0.7529
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.07 | rmse_val: 1.7207
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.07 | rmse_val: 1.3494
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.04 | rmse_val: 0.8378
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.07 | rmse_val: 1.1355
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.06 | rmse_val: 0.8232
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.06 | rmse_val: 1.9919
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.07 | rmse_val: 1.4889
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.05 | rmse_val: 0.7824
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.05 | rmse_val: 2.1047
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.06 | rmse_val: 1.1257
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.04 | rmse_val: 1.1966
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.07 | rmse_val: 1.5883
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.06 | rmse_val: 2.1858
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.04 | rmse_val: 1.4801
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.07 | rmse_val: 0.8435
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.06 | rmse_val: 1.0060
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.06 | rmse_val: 1.1348
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.07 | rmse_val: 2.5679
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.07 | rmse_val: 2.5789
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.04 | rmse_val: 1.4546
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.08 | rmse_val: 1.9236
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.06 | rmse_val: 1.3427
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.08 | rmse_val: 1.5619
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.06 | rmse_val: 1.4774
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.05 | rmse_val: 1.6202
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.05 | rmse_val: 1.0127
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.05 | rmse_val: 1.4628
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.05 | rmse_val: 1.4804
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.08 | rmse_val: 1.2230
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.06 | rmse_val: 1.5972
Optimization Finished!
Train cost: 181.6365s
Loading 26th epoch
f1_test: 0.06 | rmse_test: 1.5972

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=256, hops=3, log_path='log/nagphormer/AIDS700nef/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=256, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=256, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=512, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=256, out_features=128, bias=True)
  (attn_layer): Linear(in_features=512, out_features=1, bias=True)
  (Linear1): Linear(in_features=128, out_features=256, bias=True)
)
total params: 602498
Epoch: 0001 | loss_train: 83.7752 loss_val: 75.4847 | f1_val: 0.05 | rmse_val: 7.2845
Epoch: 0002 | loss_train: 72.0016 loss_val: 73.0202 | f1_val: 0.04 | rmse_val: 6.1837
Epoch: 0003 | loss_train: 51.8470 loss_val: 33.4256 | f1_val: 0.08 | rmse_val: 4.0858
Epoch: 0004 | loss_train: 20.1969 loss_val: 7.7119 | f1_val: 0.06 | rmse_val: 1.0513
Epoch: 0005 | loss_train: 7.8936 loss_val: 7.4092 | f1_val: 0.05 | rmse_val: 2.8462
Epoch: 0006 | loss_train: 8.6248 loss_val: 6.0941 | f1_val: 0.06 | rmse_val: 1.4593
Epoch: 0007 | loss_train: 6.7032 loss_val: 4.5308 | f1_val: 0.06 | rmse_val: 1.3302
Epoch: 0008 | loss_train: 6.6561 loss_val: 3.4634 | f1_val: 0.05 | rmse_val: 1.6156
Epoch: 0009 | loss_train: 5.6968 loss_val: 5.7857 | f1_val: 0.07 | rmse_val: 1.4230
Epoch: 0010 | loss_train: 6.5102 loss_val: 8.7206 | f1_val: 0.06 | rmse_val: 1.7201
Epoch: 0011 | loss_train: 7.6297 loss_val: 7.7189 | f1_val: 0.06 | rmse_val: 1.4301
Epoch: 0012 | loss_train: 6.2172 loss_val: 6.9937 | f1_val: 0.06 | rmse_val: 1.1495
Epoch: 0013 | loss_train: 6.8157 loss_val: 7.9529 | f1_val: 0.05 | rmse_val: 1.5110
Epoch: 0014 | loss_train: 7.4704 loss_val: 9.2152 | f1_val: 0.07 | rmse_val: 0.6819
Epoch: 0015 | loss_train: 7.4671 loss_val: 6.7170 | f1_val: 0.06 | rmse_val: 1.5754
Epoch: 0016 | loss_train: 5.6614 loss_val: 4.4662 | f1_val: 0.08 | rmse_val: 1.7444
Epoch: 0017 | loss_train: 7.0033 loss_val: 8.9905 | f1_val: 0.05 | rmse_val: 1.7316
Epoch: 0018 | loss_train: 6.3157 loss_val: 5.3765 | f1_val: 0.06 | rmse_val: 1.5351
Epoch: 0019 | loss_train: 4.9208 loss_val: 6.6987 | f1_val: 0.06 | rmse_val: 1.0978
Epoch: 0020 | loss_train: 5.2523 loss_val: 9.1803 | f1_val: 0.06 | rmse_val: 1.3137
Epoch: 0021 | loss_train: 5.9604 loss_val: 5.3209 | f1_val: 0.06 | rmse_val: 0.7932
Epoch: 0022 | loss_train: 6.5514 loss_val: 6.6630 | f1_val: 0.06 | rmse_val: 1.2187
Epoch: 0023 | loss_train: 6.4530 loss_val: 3.5064 | f1_val: 0.06 | rmse_val: 1.3687
Epoch: 0024 | loss_train: 5.5083 loss_val: 8.1550 | f1_val: 0.04 | rmse_val: 2.4670
Epoch: 0025 | loss_train: 6.7076 loss_val: 6.7808 | f1_val: 0.07 | rmse_val: 1.6106
Epoch: 0026 | loss_train: 7.1592 loss_val: 7.9200 | f1_val: 0.07 | rmse_val: 0.9441
Epoch: 0027 | loss_train: 6.3504 loss_val: 6.9129 | f1_val: 0.05 | rmse_val: 1.2091
Epoch: 0028 | loss_train: 5.7995 loss_val: 8.9344 | f1_val: 0.06 | rmse_val: 1.1246
Epoch: 0029 | loss_train: 5.7530 loss_val: 7.4801 | f1_val: 0.06 | rmse_val: 0.7671
Epoch: 0030 | loss_train: 6.0377 loss_val: 7.6999 | f1_val: 0.06 | rmse_val: 0.7413
Epoch: 0031 | loss_train: 6.7663 loss_val: 8.1386 | f1_val: 0.06 | rmse_val: 1.1495
Epoch: 0032 | loss_train: 8.4336 loss_val: 6.9203 | f1_val: 0.08 | rmse_val: 1.2533
Epoch: 0033 | loss_train: 6.9050 loss_val: 8.6090 | f1_val: 0.07 | rmse_val: 1.3433
Epoch: 0034 | loss_train: 6.0914 loss_val: 4.1950 | f1_val: 0.06 | rmse_val: 1.4105
Epoch: 0035 | loss_train: 5.3306 loss_val: 7.5963 | f1_val: 0.07 | rmse_val: 1.9788
Epoch: 0036 | loss_train: 6.2810 loss_val: 6.4388 | f1_val: 0.04 | rmse_val: 0.7451
Epoch: 0037 | loss_train: 7.3905 loss_val: 7.4720 | f1_val: 0.06 | rmse_val: 0.8319
Epoch: 0038 | loss_train: 7.4716 loss_val: 5.4506 | f1_val: 0.06 | rmse_val: 0.7509
Optimization Finished!
Train cost: 146.0586s
Loading 3th epoch
f1_test: 0.06 | rmse_test: 0.7509

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/AIDS700nef/hidden_dim', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=10, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=512, bias=True)
)
total params: 2384642
Epoch: 0001 | loss_train: 77.2434 loss_val: 55.8366 | f1_val: 0.06 | rmse_val: 5.9608
Epoch: 0002 | loss_train: 40.8035 loss_val: 19.7932 | f1_val: 0.05 | rmse_val: 1.7904
Epoch: 0003 | loss_train: 9.4044 loss_val: 12.9257 | f1_val: 0.05 | rmse_val: 3.3796
Epoch: 0004 | loss_train: 7.8744 loss_val: 8.2340 | f1_val: 0.05 | rmse_val: 0.8275
Epoch: 0005 | loss_train: 6.7657 loss_val: 6.6127 | f1_val: 0.06 | rmse_val: 2.0823
Epoch: 0006 | loss_train: 8.1353 loss_val: 5.9257 | f1_val: 0.05 | rmse_val: 1.1292
Epoch: 0007 | loss_train: 6.6041 loss_val: 4.5303 | f1_val: 0.05 | rmse_val: 1.5133
Epoch: 0008 | loss_train: 6.3677 loss_val: 3.5304 | f1_val: 0.08 | rmse_val: 1.5083
Epoch: 0009 | loss_train: 5.1777 loss_val: 7.1976 | f1_val: 0.09 | rmse_val: 0.8697
Epoch: 0010 | loss_train: 7.4706 loss_val: 8.9052 | f1_val: 0.07 | rmse_val: 1.1980
Epoch: 0011 | loss_train: 7.2258 loss_val: 6.0960 | f1_val: 0.06 | rmse_val: 1.6793
Epoch: 0012 | loss_train: 5.5879 loss_val: 6.2867 | f1_val: 0.06 | rmse_val: 1.3842
Epoch: 0013 | loss_train: 7.2274 loss_val: 11.0708 | f1_val: 0.07 | rmse_val: 3.1068
Epoch: 0014 | loss_train: 10.8606 loss_val: 9.1855 | f1_val: 0.08 | rmse_val: 1.2693
Epoch: 0015 | loss_train: 7.5090 loss_val: 8.2717 | f1_val: 0.07 | rmse_val: 1.0106
Epoch: 0016 | loss_train: 6.4967 loss_val: 3.5775 | f1_val: 0.06 | rmse_val: 1.3279
Epoch: 0017 | loss_train: 7.0821 loss_val: 10.0495 | f1_val: 0.07 | rmse_val: 1.7865
Epoch: 0018 | loss_train: 6.6152 loss_val: 5.0760 | f1_val: 0.06 | rmse_val: 1.8329
Epoch: 0019 | loss_train: 5.6176 loss_val: 6.2281 | f1_val: 0.08 | rmse_val: 0.9487
Epoch: 0020 | loss_train: 5.3266 loss_val: 9.2751 | f1_val: 0.05 | rmse_val: 1.2573
Epoch: 0021 | loss_train: 5.8932 loss_val: 4.8841 | f1_val: 0.06 | rmse_val: 1.4253
Epoch: 0022 | loss_train: 5.8690 loss_val: 7.0561 | f1_val: 0.07 | rmse_val: 1.6359
Epoch: 0023 | loss_train: 6.1972 loss_val: 3.5744 | f1_val: 0.07 | rmse_val: 1.4517
Epoch: 0024 | loss_train: 5.3257 loss_val: 7.7911 | f1_val: 0.07 | rmse_val: 2.1691
Epoch: 0025 | loss_train: 6.2361 loss_val: 6.6932 | f1_val: 0.07 | rmse_val: 1.6988
Epoch: 0026 | loss_train: 7.2829 loss_val: 7.6524 | f1_val: 0.05 | rmse_val: 0.6979
Epoch: 0027 | loss_train: 7.0975 loss_val: 7.5836 | f1_val: 0.07 | rmse_val: 0.6737
Epoch: 0028 | loss_train: 6.2710 loss_val: 8.7363 | f1_val: 0.06 | rmse_val: 0.9696
Epoch: 0029 | loss_train: 5.9083 loss_val: 7.8889 | f1_val: 0.06 | rmse_val: 0.8320
Epoch: 0030 | loss_train: 6.2120 loss_val: 7.7891 | f1_val: 0.07 | rmse_val: 0.9087
Epoch: 0031 | loss_train: 6.7685 loss_val: 8.3884 | f1_val: 0.07 | rmse_val: 1.2757
Epoch: 0032 | loss_train: 9.3962 loss_val: 7.2132 | f1_val: 0.06 | rmse_val: 1.0163
Epoch: 0033 | loss_train: 6.9861 loss_val: 9.3947 | f1_val: 0.06 | rmse_val: 1.4947
Epoch: 0034 | loss_train: 6.7252 loss_val: 4.3167 | f1_val: 0.06 | rmse_val: 1.1725
Epoch: 0035 | loss_train: 5.7214 loss_val: 7.7786 | f1_val: 0.07 | rmse_val: 2.0228
Epoch: 0036 | loss_train: 6.7913 loss_val: 7.9078 | f1_val: 0.08 | rmse_val: 0.8370
Epoch: 0037 | loss_train: 8.6224 loss_val: 7.7089 | f1_val: 0.07 | rmse_val: 0.6524
Epoch: 0038 | loss_train: 7.3916 loss_val: 8.5054 | f1_val: 0.08 | rmse_val: 1.1202
Epoch: 0039 | loss_train: 9.7710 loss_val: 7.1418 | f1_val: 0.06 | rmse_val: 1.1881
Optimization Finished!
Train cost: 147.6114s
Loading 9th epoch
f1_test: 0.06 | rmse_test: 1.1881

>>> run.py: Namespace(dataset='AIDS700nef', device=1, experiment='hidden_dim', log_path='log/nagphormer/AIDS700nef/hidden_dim', path='data/nagphormer', plot_path='plots/nagphormer/AIDS700nef/hidden_dim', pyg_path='data/pyg/AIDS700nef')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/top_k', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=1, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.14 | rmse_val: 1.1523
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.05 | rmse_val: 1.1701
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.08 | rmse_val: 1.0444
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.05 | rmse_val: 2.8725
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.02 | rmse_val: 2.6884
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.02 | rmse_val: 2.8292
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.05 | rmse_val: 2.7328
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.03 | rmse_val: 2.9777
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.03 | rmse_val: 3.1154
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.03 | rmse_val: 3.2951
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.03 | rmse_val: 2.9656
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.05 | rmse_val: 2.7414
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.05 | rmse_val: 2.8871
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.06 | rmse_val: 2.8938
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.03 | rmse_val: 2.7124
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.03 | rmse_val: 3.1691
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.06 | rmse_val: 2.9331
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.02 | rmse_val: 2.9795
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.02 | rmse_val: 2.8479
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.05 | rmse_val: 2.8782
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.03 | rmse_val: 3.1047
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.03 | rmse_val: 3.5846
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.02 | rmse_val: 3.7367
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.03 | rmse_val: 2.9022
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.06 | rmse_val: 3.0475
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.05 | rmse_val: 2.9537
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.06 | rmse_val: 3.5534
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.03 | rmse_val: 3.6106
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.03 | rmse_val: 3.5686
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.05 | rmse_val: 3.4003
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.08 | rmse_val: 2.8431
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.08 | rmse_val: 2.9317
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.05 | rmse_val: 3.1578
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.06 | rmse_val: 3.0753
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.05 | rmse_val: 3.7519
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.03 | rmse_val: 2.6328
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.03 | rmse_val: 3.0499
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.03 | rmse_val: 2.7468
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.06 | rmse_val: 3.2482
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.05 | rmse_val: 2.7466
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.06 | rmse_val: 2.6265
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.03 | rmse_val: 2.8311
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.06 | rmse_val: 2.7319
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.02 | rmse_val: 2.9447
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.06 | rmse_val: 2.4931
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.05 | rmse_val: 2.5530
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.06 | rmse_val: 2.9099
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.06 | rmse_val: 2.4761
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.05 | rmse_val: 2.9000
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.05 | rmse_val: 2.6421
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.03 | rmse_val: 2.6567
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.05 | rmse_val: 2.8004
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.02 | rmse_val: 2.7474
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.05 | rmse_val: 2.6706
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.02 | rmse_val: 2.9747
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.03 | rmse_val: 2.7986
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.08 | rmse_val: 2.8797
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.02 | rmse_val: 2.7127
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.03 | rmse_val: 2.6055
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.08 | rmse_val: 2.9013
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.06 | rmse_val: 2.5653
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.03 | rmse_val: 2.8308
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.06 | rmse_val: 2.6702
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.05 | rmse_val: 2.8124
Optimization Finished!
Train cost: 348.5657s
Loading 1th epoch
f1_test: 0.05 | rmse_test: 2.8124

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/top_k', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=5, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.05 | rmse_val: 2.3768
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.03 | rmse_val: 1.9729
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.06 | rmse_val: 1.1717
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.04 | rmse_val: 1.7646
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.04 | rmse_val: 1.3797
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.04 | rmse_val: 1.8052
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.04 | rmse_val: 1.6915
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.05 | rmse_val: 1.7753
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.04 | rmse_val: 1.9747
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.05 | rmse_val: 2.0933
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.03 | rmse_val: 2.0494
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.05 | rmse_val: 1.5706
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.04 | rmse_val: 1.8810
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.06 | rmse_val: 1.8295
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.04 | rmse_val: 1.7023
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.04 | rmse_val: 2.1100
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.09 | rmse_val: 1.7417
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.04 | rmse_val: 1.7647
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.04 | rmse_val: 1.6410
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.04 | rmse_val: 1.7739
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.03 | rmse_val: 1.8655
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.04 | rmse_val: 2.4729
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.03 | rmse_val: 2.6314
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.05 | rmse_val: 1.7538
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.04 | rmse_val: 1.9645
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.04 | rmse_val: 1.8508
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.05 | rmse_val: 2.4116
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.05 | rmse_val: 2.4434
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.04 | rmse_val: 2.3982
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.03 | rmse_val: 2.3465
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.04 | rmse_val: 1.6985
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.06 | rmse_val: 1.7735
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.04 | rmse_val: 2.0113
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.05 | rmse_val: 1.8529
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.04 | rmse_val: 2.6659
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.04 | rmse_val: 1.5190
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.04 | rmse_val: 1.8983
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.06 | rmse_val: 1.6866
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.05 | rmse_val: 2.1167
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.06 | rmse_val: 1.6444
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.05 | rmse_val: 1.4332
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.04 | rmse_val: 1.7283
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.04 | rmse_val: 1.8098
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.05 | rmse_val: 1.7246
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.05 | rmse_val: 1.6035
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.06 | rmse_val: 1.4584
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.04 | rmse_val: 1.7801
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.05 | rmse_val: 1.3709
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.05 | rmse_val: 1.8384
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.03 | rmse_val: 1.6120
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.04 | rmse_val: 1.5209
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.03 | rmse_val: 1.7485
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.03 | rmse_val: 1.7490
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.03 | rmse_val: 1.6492
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.03 | rmse_val: 1.8232
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.03 | rmse_val: 1.6823
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.05 | rmse_val: 1.8549
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.03 | rmse_val: 1.6350
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.02 | rmse_val: 1.6234
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.04 | rmse_val: 1.8020
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.04 | rmse_val: 1.5072
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.04 | rmse_val: 1.7725
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.04 | rmse_val: 1.5565
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.03 | rmse_val: 1.7305
Optimization Finished!
Train cost: 352.2217s
Loading 17th epoch
f1_test: 0.03 | rmse_test: 1.7305

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/top_k', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=15, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.09 | rmse_val: 4.2557
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.09 | rmse_val: 3.8520
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.10 | rmse_val: 2.4692
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.10 | rmse_val: 0.7499
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.08 | rmse_val: 0.7488
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.09 | rmse_val: 0.6821
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.10 | rmse_val: 0.8316
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.10 | rmse_val: 0.8215
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.11 | rmse_val: 0.8110
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.10 | rmse_val: 0.8523
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.10 | rmse_val: 0.8707
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.08 | rmse_val: 0.7182
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.10 | rmse_val: 0.9516
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.11 | rmse_val: 0.8223
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.11 | rmse_val: 0.9046
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.09 | rmse_val: 0.9496
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.13 | rmse_val: 0.8793
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.09 | rmse_val: 0.8675
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.10 | rmse_val: 0.9056
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.09 | rmse_val: 0.8070
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.10 | rmse_val: 0.7372
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.09 | rmse_val: 1.1668
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.09 | rmse_val: 1.1651
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.10 | rmse_val: 0.7954
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.10 | rmse_val: 0.9016
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.09 | rmse_val: 0.8025
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.09 | rmse_val: 1.1196
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.11 | rmse_val: 1.1238
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.09 | rmse_val: 1.1181
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.09 | rmse_val: 1.0600
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.10 | rmse_val: 0.7244
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.12 | rmse_val: 0.7823
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.09 | rmse_val: 0.8583
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.11 | rmse_val: 0.8329
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.11 | rmse_val: 1.2505
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.10 | rmse_val: 0.7948
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.11 | rmse_val: 0.8036
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.10 | rmse_val: 0.8559
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.10 | rmse_val: 0.8845
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.11 | rmse_val: 0.8361
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.11 | rmse_val: 0.7522
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.11 | rmse_val: 0.8157
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.08 | rmse_val: 0.9022
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.11 | rmse_val: 0.6993
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.11 | rmse_val: 0.9557
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.11 | rmse_val: 0.8754
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.10 | rmse_val: 0.8485
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.11 | rmse_val: 0.8065
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.10 | rmse_val: 0.9038
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.09 | rmse_val: 0.8403
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.11 | rmse_val: 0.7800
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.11 | rmse_val: 0.7685
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.10 | rmse_val: 0.7847
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.10 | rmse_val: 0.7787
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.10 | rmse_val: 0.7642
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.10 | rmse_val: 0.8706
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.10 | rmse_val: 0.7946
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.08 | rmse_val: 0.8219
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.09 | rmse_val: 0.7532
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.10 | rmse_val: 0.7500
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.08 | rmse_val: 0.6722
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.10 | rmse_val: 0.7193
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.10 | rmse_val: 0.7034
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.09 | rmse_val: 0.8100
Optimization Finished!
Train cost: 354.3100s
Loading 17th epoch
f1_test: 0.09 | rmse_test: 0.8100

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='LINUX', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/LINUX/top_k', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/LINUX', seed=3407, split=1.0, top_k=20, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 63/800
TransformerModel(
  (att_embeddings_nope): Linear(in_features=3, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 150082
Epoch: 0001 | loss_train: 19.6795 loss_val: 18.6021 | f1_val: 0.10 | rmse_val: 4.2863
Epoch: 0002 | loss_train: 18.3024 loss_val: 14.6734 | f1_val: 0.09 | rmse_val: 3.8967
Epoch: 0003 | loss_train: 13.6742 loss_val: 12.0244 | f1_val: 0.10 | rmse_val: 2.5279
Epoch: 0004 | loss_train: 6.4100 loss_val: 4.2518 | f1_val: 0.10 | rmse_val: 0.7365
Epoch: 0005 | loss_train: 4.2721 loss_val: 4.3190 | f1_val: 0.08 | rmse_val: 0.7275
Epoch: 0006 | loss_train: 4.2919 loss_val: 4.1009 | f1_val: 0.10 | rmse_val: 0.6694
Epoch: 0007 | loss_train: 4.3621 loss_val: 4.2255 | f1_val: 0.10 | rmse_val: 0.8105
Epoch: 0008 | loss_train: 3.7940 loss_val: 5.0252 | f1_val: 0.10 | rmse_val: 0.8064
Epoch: 0009 | loss_train: 4.2851 loss_val: 5.9412 | f1_val: 0.11 | rmse_val: 0.7811
Epoch: 0010 | loss_train: 4.1349 loss_val: 5.5041 | f1_val: 0.11 | rmse_val: 0.8201
Epoch: 0011 | loss_train: 4.5597 loss_val: 4.4289 | f1_val: 0.11 | rmse_val: 0.8376
Epoch: 0012 | loss_train: 4.3141 loss_val: 3.8723 | f1_val: 0.09 | rmse_val: 0.7064
Epoch: 0013 | loss_train: 4.4081 loss_val: 4.3706 | f1_val: 0.10 | rmse_val: 0.9109
Epoch: 0014 | loss_train: 4.4677 loss_val: 4.5565 | f1_val: 0.12 | rmse_val: 0.8008
Epoch: 0015 | loss_train: 4.3164 loss_val: 4.0397 | f1_val: 0.12 | rmse_val: 0.8775
Epoch: 0016 | loss_train: 4.3335 loss_val: 5.4356 | f1_val: 0.09 | rmse_val: 0.9353
Epoch: 0017 | loss_train: 4.5592 loss_val: 4.0758 | f1_val: 0.14 | rmse_val: 0.8620
Epoch: 0018 | loss_train: 4.9254 loss_val: 5.3848 | f1_val: 0.10 | rmse_val: 0.8472
Epoch: 0019 | loss_train: 4.4499 loss_val: 4.8520 | f1_val: 0.10 | rmse_val: 0.8806
Epoch: 0020 | loss_train: 4.3531 loss_val: 4.0400 | f1_val: 0.09 | rmse_val: 0.7860
Epoch: 0021 | loss_train: 4.1115 loss_val: 4.7962 | f1_val: 0.10 | rmse_val: 0.7054
Epoch: 0022 | loss_train: 4.4205 loss_val: 6.2276 | f1_val: 0.09 | rmse_val: 1.1452
Epoch: 0023 | loss_train: 4.0164 loss_val: 5.4687 | f1_val: 0.09 | rmse_val: 1.1280
Epoch: 0024 | loss_train: 4.7478 loss_val: 4.0029 | f1_val: 0.11 | rmse_val: 0.7781
Epoch: 0025 | loss_train: 4.1121 loss_val: 5.7177 | f1_val: 0.11 | rmse_val: 0.8719
Epoch: 0026 | loss_train: 3.6853 loss_val: 3.9594 | f1_val: 0.10 | rmse_val: 0.7901
Epoch: 0027 | loss_train: 4.5117 loss_val: 4.5665 | f1_val: 0.09 | rmse_val: 1.0873
Epoch: 0028 | loss_train: 4.5603 loss_val: 6.2317 | f1_val: 0.11 | rmse_val: 1.1026
Epoch: 0029 | loss_train: 4.3795 loss_val: 3.9924 | f1_val: 0.09 | rmse_val: 1.0804
Epoch: 0030 | loss_train: 5.0498 loss_val: 3.6445 | f1_val: 0.09 | rmse_val: 1.0033
Epoch: 0031 | loss_train: 4.4724 loss_val: 4.0975 | f1_val: 0.11 | rmse_val: 0.7262
Epoch: 0032 | loss_train: 4.0802 loss_val: 4.4103 | f1_val: 0.12 | rmse_val: 0.7576
Epoch: 0033 | loss_train: 4.2053 loss_val: 3.6404 | f1_val: 0.10 | rmse_val: 0.8360
Epoch: 0034 | loss_train: 4.2066 loss_val: 3.3841 | f1_val: 0.12 | rmse_val: 0.8202
Epoch: 0035 | loss_train: 4.2920 loss_val: 5.2988 | f1_val: 0.11 | rmse_val: 1.2073
Epoch: 0036 | loss_train: 5.0373 loss_val: 3.4323 | f1_val: 0.11 | rmse_val: 0.7847
Epoch: 0037 | loss_train: 4.5029 loss_val: 3.4546 | f1_val: 0.11 | rmse_val: 0.7866
Epoch: 0038 | loss_train: 4.3970 loss_val: 4.5181 | f1_val: 0.11 | rmse_val: 0.8342
Epoch: 0039 | loss_train: 4.3718 loss_val: 4.4573 | f1_val: 0.10 | rmse_val: 0.8494
Epoch: 0040 | loss_train: 4.5688 loss_val: 4.5403 | f1_val: 0.12 | rmse_val: 0.8145
Epoch: 0041 | loss_train: 4.4272 loss_val: 5.6727 | f1_val: 0.12 | rmse_val: 0.7410
Epoch: 0042 | loss_train: 4.1988 loss_val: 3.7890 | f1_val: 0.11 | rmse_val: 0.7940
Epoch: 0043 | loss_train: 4.2450 loss_val: 4.6645 | f1_val: 0.09 | rmse_val: 0.8485
Epoch: 0044 | loss_train: 4.3102 loss_val: 3.5116 | f1_val: 0.12 | rmse_val: 0.6723
Epoch: 0045 | loss_train: 4.1904 loss_val: 4.4348 | f1_val: 0.12 | rmse_val: 0.9307
Epoch: 0046 | loss_train: 4.8507 loss_val: 3.7178 | f1_val: 0.12 | rmse_val: 0.8562
Epoch: 0047 | loss_train: 4.0527 loss_val: 5.3152 | f1_val: 0.10 | rmse_val: 0.8222
Epoch: 0048 | loss_train: 4.3044 loss_val: 5.9033 | f1_val: 0.11 | rmse_val: 0.7790
Epoch: 0049 | loss_train: 4.1651 loss_val: 3.8454 | f1_val: 0.11 | rmse_val: 0.8604
Epoch: 0050 | loss_train: 4.2805 loss_val: 4.8572 | f1_val: 0.09 | rmse_val: 0.8403
Epoch: 0051 | loss_train: 4.0504 loss_val: 3.9138 | f1_val: 0.11 | rmse_val: 0.7694
Epoch: 0052 | loss_train: 3.9136 loss_val: 3.5927 | f1_val: 0.12 | rmse_val: 0.7427
Epoch: 0053 | loss_train: 4.0389 loss_val: 4.7987 | f1_val: 0.11 | rmse_val: 0.7697
Epoch: 0054 | loss_train: 4.6467 loss_val: 5.2283 | f1_val: 0.10 | rmse_val: 0.7578
Epoch: 0055 | loss_train: 4.7298 loss_val: 4.7234 | f1_val: 0.10 | rmse_val: 0.7193
Epoch: 0056 | loss_train: 3.8525 loss_val: 3.3968 | f1_val: 0.10 | rmse_val: 0.8596
Epoch: 0057 | loss_train: 4.4932 loss_val: 3.7929 | f1_val: 0.11 | rmse_val: 0.7792
Epoch: 0058 | loss_train: 3.8356 loss_val: 3.4540 | f1_val: 0.09 | rmse_val: 0.7879
Epoch: 0059 | loss_train: 4.0792 loss_val: 3.8557 | f1_val: 0.09 | rmse_val: 0.7310
Epoch: 0060 | loss_train: 4.3898 loss_val: 3.8840 | f1_val: 0.10 | rmse_val: 0.7279
Epoch: 0061 | loss_train: 4.1862 loss_val: 5.5709 | f1_val: 0.09 | rmse_val: 0.6554
Epoch: 0062 | loss_train: 4.3517 loss_val: 3.6654 | f1_val: 0.10 | rmse_val: 0.7071
Epoch: 0063 | loss_train: 4.1964 loss_val: 4.7273 | f1_val: 0.10 | rmse_val: 0.6949
Epoch: 0064 | loss_train: 4.1639 loss_val: 4.2441 | f1_val: 0.09 | rmse_val: 0.7872
Optimization Finished!
Train cost: 357.7923s
Loading 17th epoch
f1_test: 0.09 | rmse_test: 0.7872

>>> run.py: Namespace(dataset='LINUX', device=1, experiment='top_k', log_path='log/nagphormer/LINUX/top_k', path='data/nagphormer', plot_path='plots/nagphormer/LINUX/top_k', pyg_path='data/pyg/LINUX')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/top_k', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=1, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.06 | rmse_val: 5.2944
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 4.8319
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.06 | rmse_val: 4.3429
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.03 | rmse_val: 3.6263
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.03 | rmse_val: 2.4933
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.14 | rmse_val: 1.1218
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.03 | rmse_val: 3.0639
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.08 | rmse_val: 3.5168
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.06 | rmse_val: 2.8356
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.00 | rmse_val: 2.4417
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 2.8188
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.06 | rmse_val: 2.6515
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.06 | rmse_val: 2.6062
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.11 | rmse_val: 2.8606
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.08 | rmse_val: 2.5529
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.03 | rmse_val: 2.4033
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.03 | rmse_val: 2.9065
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.11 | rmse_val: 2.3138
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.08 | rmse_val: 2.0763
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.06 | rmse_val: 3.4376
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.08 | rmse_val: 2.7909
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.06 | rmse_val: 3.2480
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.11 | rmse_val: 2.4210
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.06 | rmse_val: 3.3293
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.06 | rmse_val: 2.9373
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.08 | rmse_val: 1.6809
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.06 | rmse_val: 3.2583
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.08 | rmse_val: 2.7558
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.11 | rmse_val: 1.6559
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.03 | rmse_val: 2.2578
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.03 | rmse_val: 1.6535
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.08 | rmse_val: 3.6593
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.14 | rmse_val: 2.8084
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.03 | rmse_val: 1.6725
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.11 | rmse_val: 3.5353
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.11 | rmse_val: 2.3440
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.06 | rmse_val: 2.3720
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.08 | rmse_val: 3.0105
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.08 | rmse_val: 3.3142
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.06 | rmse_val: 2.6916
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.06 | rmse_val: 1.9332
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.03 | rmse_val: 2.3719
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.14 | rmse_val: 2.3356
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.14 | rmse_val: 4.0620
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.06 | rmse_val: 3.9694
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.03 | rmse_val: 1.2441
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.11 | rmse_val: 3.3345
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.06 | rmse_val: 2.4984
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.11 | rmse_val: 2.9568
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.11 | rmse_val: 2.7344
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.11 | rmse_val: 2.5947
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.03 | rmse_val: 2.1720
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.08 | rmse_val: 3.1349
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.11 | rmse_val: 2.9414
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.03 | rmse_val: 2.3918
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.00 | rmse_val: 2.7715
Epoch: 0057 | loss_train: 6.4586 loss_val: 5.4660 | f1_val: 0.11 | rmse_val: 2.7084
Epoch: 0058 | loss_train: 4.8283 loss_val: 6.8235 | f1_val: 0.03 | rmse_val: 2.3058
Epoch: 0059 | loss_train: 6.5769 loss_val: 6.0143 | f1_val: 0.11 | rmse_val: 3.1932
Epoch: 0060 | loss_train: 5.6123 loss_val: 10.2062 | f1_val: 0.06 | rmse_val: 2.5309
Epoch: 0061 | loss_train: 5.1132 loss_val: 5.4029 | f1_val: 0.06 | rmse_val: 3.1372
Epoch: 0062 | loss_train: 5.6445 loss_val: 4.6614 | f1_val: 0.06 | rmse_val: 2.8167
Epoch: 0063 | loss_train: 5.6194 loss_val: 6.3115 | f1_val: 0.08 | rmse_val: 2.9832
Epoch: 0064 | loss_train: 5.6664 loss_val: 3.8491 | f1_val: 0.03 | rmse_val: 2.1857
Epoch: 0065 | loss_train: 5.3443 loss_val: 9.9153 | f1_val: 0.08 | rmse_val: 2.8651
Epoch: 0066 | loss_train: 5.6556 loss_val: 6.5864 | f1_val: 0.11 | rmse_val: 3.0476
Epoch: 0067 | loss_train: 4.8656 loss_val: 4.4539 | f1_val: 0.14 | rmse_val: 2.4626
Epoch: 0068 | loss_train: 5.6325 loss_val: 7.2655 | f1_val: 0.06 | rmse_val: 2.9188
Epoch: 0069 | loss_train: 5.1119 loss_val: 6.2931 | f1_val: 0.08 | rmse_val: 2.7602
Epoch: 0070 | loss_train: 5.6306 loss_val: 4.6644 | f1_val: 0.11 | rmse_val: 2.4440
Epoch: 0071 | loss_train: 5.5429 loss_val: 5.1603 | f1_val: 0.14 | rmse_val: 2.6329
Epoch: 0072 | loss_train: 6.0333 loss_val: 6.2190 | f1_val: 0.08 | rmse_val: 2.7129
Epoch: 0073 | loss_train: 5.2044 loss_val: 4.0643 | f1_val: 0.11 | rmse_val: 3.0628
Epoch: 0074 | loss_train: 5.2564 loss_val: 4.3062 | f1_val: 0.06 | rmse_val: 2.7111
Epoch: 0075 | loss_train: 5.2343 loss_val: 6.6083 | f1_val: 0.03 | rmse_val: 3.0032
Epoch: 0076 | loss_train: 4.8221 loss_val: 4.4840 | f1_val: 0.06 | rmse_val: 2.6514
Epoch: 0077 | loss_train: 6.1510 loss_val: 6.8465 | f1_val: 0.06 | rmse_val: 3.3293
Epoch: 0078 | loss_train: 5.9551 loss_val: 4.1323 | f1_val: 0.08 | rmse_val: 2.4134
Epoch: 0079 | loss_train: 6.4302 loss_val: 5.8552 | f1_val: 0.08 | rmse_val: 2.2853
Epoch: 0080 | loss_train: 5.4841 loss_val: 6.8529 | f1_val: 0.03 | rmse_val: 2.8879
Epoch: 0081 | loss_train: 5.2083 loss_val: 4.8126 | f1_val: 0.14 | rmse_val: 2.4366
Epoch: 0082 | loss_train: 5.3006 loss_val: 4.0387 | f1_val: 0.06 | rmse_val: 3.0528
Epoch: 0083 | loss_train: 5.7320 loss_val: 8.3461 | f1_val: 0.06 | rmse_val: 3.3588
Epoch: 0084 | loss_train: 6.0837 loss_val: 5.3687 | f1_val: 0.11 | rmse_val: 2.9422
Epoch: 0085 | loss_train: 5.0678 loss_val: 5.3541 | f1_val: 0.03 | rmse_val: 2.5953
Epoch: 0086 | loss_train: 5.7829 loss_val: 6.5790 | f1_val: 0.11 | rmse_val: 3.2012
Epoch: 0087 | loss_train: 4.7180 loss_val: 7.3458 | f1_val: 0.08 | rmse_val: 2.8572
Epoch: 0088 | loss_train: 4.3950 loss_val: 5.4440 | f1_val: 0.17 | rmse_val: 2.8930
Epoch: 0089 | loss_train: 5.3899 loss_val: 9.9183 | f1_val: 0.08 | rmse_val: 2.8485
Epoch: 0090 | loss_train: 4.8888 loss_val: 5.5783 | f1_val: 0.14 | rmse_val: 2.6593
Epoch: 0091 | loss_train: 5.5032 loss_val: 6.0579 | f1_val: 0.03 | rmse_val: 2.9539
Epoch: 0092 | loss_train: 4.8921 loss_val: 5.8506 | f1_val: 0.00 | rmse_val: 2.8193
Epoch: 0093 | loss_train: 5.4622 loss_val: 4.6248 | f1_val: 0.08 | rmse_val: 2.6802
Epoch: 0094 | loss_train: 4.7527 loss_val: 7.0229 | f1_val: 0.06 | rmse_val: 2.6740
Epoch: 0095 | loss_train: 5.4412 loss_val: 8.5326 | f1_val: 0.11 | rmse_val: 2.9160
Epoch: 0096 | loss_train: 5.8304 loss_val: 4.5461 | f1_val: 0.06 | rmse_val: 2.5410
Epoch: 0097 | loss_train: 4.9453 loss_val: 5.9050 | f1_val: 0.00 | rmse_val: 2.9794
Epoch: 0098 | loss_train: 4.9524 loss_val: 6.7195 | f1_val: 0.11 | rmse_val: 2.7169
Epoch: 0099 | loss_train: 4.7822 loss_val: 7.0907 | f1_val: 0.06 | rmse_val: 2.9230
Epoch: 0100 | loss_train: 5.2401 loss_val: 6.6101 | f1_val: 0.03 | rmse_val: 3.1023
Epoch: 0101 | loss_train: 4.9416 loss_val: 9.4143 | f1_val: 0.06 | rmse_val: 2.8053
Epoch: 0102 | loss_train: 6.2550 loss_val: 6.3816 | f1_val: 0.00 | rmse_val: 3.0350
Epoch: 0103 | loss_train: 4.5276 loss_val: 5.1826 | f1_val: 0.08 | rmse_val: 3.0196
Epoch: 0104 | loss_train: 6.1766 loss_val: 5.6232 | f1_val: 0.06 | rmse_val: 3.0310
Epoch: 0105 | loss_train: 5.1470 loss_val: 9.1044 | f1_val: 0.03 | rmse_val: 2.5264
Epoch: 0106 | loss_train: 5.5326 loss_val: 4.5460 | f1_val: 0.11 | rmse_val: 2.7249
Epoch: 0107 | loss_train: 4.8522 loss_val: 6.1944 | f1_val: 0.08 | rmse_val: 2.8457
Epoch: 0108 | loss_train: 5.7731 loss_val: 4.3698 | f1_val: 0.11 | rmse_val: 3.0708
Epoch: 0109 | loss_train: 5.0224 loss_val: 7.5776 | f1_val: 0.08 | rmse_val: 2.3844
Epoch: 0110 | loss_train: 5.3680 loss_val: 6.0639 | f1_val: 0.11 | rmse_val: 2.7293
Epoch: 0111 | loss_train: 6.0819 loss_val: 6.2871 | f1_val: 0.03 | rmse_val: 2.6830
Epoch: 0112 | loss_train: 4.9394 loss_val: 6.6746 | f1_val: 0.11 | rmse_val: 2.4908
Epoch: 0113 | loss_train: 4.6755 loss_val: 7.8063 | f1_val: 0.08 | rmse_val: 2.6481
Epoch: 0114 | loss_train: 5.6897 loss_val: 7.5143 | f1_val: 0.06 | rmse_val: 2.7770
Epoch: 0115 | loss_train: 5.5707 loss_val: 3.9992 | f1_val: 0.08 | rmse_val: 2.7126
Epoch: 0116 | loss_train: 5.3986 loss_val: 6.4003 | f1_val: 0.03 | rmse_val: 2.6205
Epoch: 0117 | loss_train: 4.9784 loss_val: 8.7627 | f1_val: 0.11 | rmse_val: 2.7528
Epoch: 0118 | loss_train: 4.6359 loss_val: 6.1690 | f1_val: 0.08 | rmse_val: 2.5876
Optimization Finished!
Train cost: 396.6338s
Loading 88th epoch
f1_test: 0.08 | rmse_test: 2.5876

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/top_k', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=5, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.03 | rmse_val: 6.5315
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.04 | rmse_val: 6.4218
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.05 | rmse_val: 5.9113
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.03 | rmse_val: 5.1221
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.06 | rmse_val: 3.6240
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.05 | rmse_val: 1.4637
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.04 | rmse_val: 1.9435
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.05 | rmse_val: 2.6906
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.07 | rmse_val: 1.9419
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.05 | rmse_val: 1.6641
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.9899
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.06 | rmse_val: 1.4612
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.06 | rmse_val: 1.9314
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.06 | rmse_val: 1.9691
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.05 | rmse_val: 1.5949
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.03 | rmse_val: 1.6223
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.04 | rmse_val: 2.1667
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.06 | rmse_val: 1.7540
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.04 | rmse_val: 1.2971
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.09 | rmse_val: 2.3053
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.05 | rmse_val: 1.7257
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.04 | rmse_val: 2.2652
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.05 | rmse_val: 1.6178
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.03 | rmse_val: 2.4252
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.05 | rmse_val: 2.1733
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.06 | rmse_val: 1.0598
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.07 | rmse_val: 2.1549
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.07 | rmse_val: 1.8973
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.03 | rmse_val: 1.0305
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.05 | rmse_val: 1.4791
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.04 | rmse_val: 0.9640
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.06 | rmse_val: 2.5515
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.07 | rmse_val: 1.8927
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.03 | rmse_val: 0.9096
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.05 | rmse_val: 2.6973
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.05 | rmse_val: 1.5270
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.04 | rmse_val: 1.6179
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.08 | rmse_val: 2.0408
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.04 | rmse_val: 2.6397
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.04 | rmse_val: 1.9643
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.06 | rmse_val: 1.2148
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.04 | rmse_val: 1.3017
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.06 | rmse_val: 1.5223
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.06 | rmse_val: 3.2309
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.07 | rmse_val: 3.1487
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.02 | rmse_val: 1.1706
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.07 | rmse_val: 2.3803
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.06 | rmse_val: 1.7757
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.09 | rmse_val: 1.9660
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.05 | rmse_val: 1.9086
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.05 | rmse_val: 2.0903
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.04 | rmse_val: 1.3876
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.06 | rmse_val: 2.0152
Optimization Finished!
Train cost: 182.4380s
Loading 20th epoch
f1_test: 0.06 | rmse_test: 2.0152

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/top_k', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=15, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.06 | rmse_val: 8.3619
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 8.3549
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.07 | rmse_val: 7.7824
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.04 | rmse_val: 6.9612
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.06 | rmse_val: 5.2453
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.08 | rmse_val: 2.7150
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.07 | rmse_val: 1.1144
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.07 | rmse_val: 1.7750
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.09 | rmse_val: 1.1534
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.08 | rmse_val: 0.9937
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.2870
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.08 | rmse_val: 0.8061
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.07 | rmse_val: 1.1724
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.08 | rmse_val: 1.1886
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.07 | rmse_val: 0.9405
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.05 | rmse_val: 1.0035
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.07 | rmse_val: 1.2575
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.07 | rmse_val: 0.9538
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.07 | rmse_val: 0.9921
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.07 | rmse_val: 1.4636
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.06 | rmse_val: 1.0245
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.06 | rmse_val: 1.3510
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.08 | rmse_val: 0.7652
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.04 | rmse_val: 1.3647
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.09 | rmse_val: 1.2456
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.09 | rmse_val: 0.7930
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.08 | rmse_val: 1.2543
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.10 | rmse_val: 0.9168
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.06 | rmse_val: 0.7813
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.08 | rmse_val: 0.9454
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.09 | rmse_val: 1.2618
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.07 | rmse_val: 1.4937
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.09 | rmse_val: 0.9488
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.07 | rmse_val: 0.9861
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.07 | rmse_val: 1.5976
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.08 | rmse_val: 0.8635
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.06 | rmse_val: 0.9380
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.09 | rmse_val: 1.1303
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.08 | rmse_val: 1.7655
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.05 | rmse_val: 1.0120
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.08 | rmse_val: 0.6209
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.09 | rmse_val: 0.9002
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.09 | rmse_val: 0.9690
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.10 | rmse_val: 2.0112
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.08 | rmse_val: 2.0046
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.05 | rmse_val: 1.8991
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.09 | rmse_val: 1.4040
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.07 | rmse_val: 1.0460
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.10 | rmse_val: 1.1317
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.08 | rmse_val: 1.0684
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.08 | rmse_val: 1.2289
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.08 | rmse_val: 0.8531
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.06 | rmse_val: 1.0280
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.06 | rmse_val: 1.0830
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.10 | rmse_val: 0.8318
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.08 | rmse_val: 1.1019
Epoch: 0057 | loss_train: 6.4586 loss_val: 5.4660 | f1_val: 0.11 | rmse_val: 1.0239
Epoch: 0058 | loss_train: 4.8283 loss_val: 6.8235 | f1_val: 0.09 | rmse_val: 0.8371
Epoch: 0059 | loss_train: 6.5769 loss_val: 6.0143 | f1_val: 0.08 | rmse_val: 1.2295
Epoch: 0060 | loss_train: 5.6123 loss_val: 10.2062 | f1_val: 0.05 | rmse_val: 0.9148
Epoch: 0061 | loss_train: 5.1132 loss_val: 5.4029 | f1_val: 0.09 | rmse_val: 1.4419
Epoch: 0062 | loss_train: 5.6445 loss_val: 4.6614 | f1_val: 0.08 | rmse_val: 1.0572
Epoch: 0063 | loss_train: 5.6194 loss_val: 6.3115 | f1_val: 0.08 | rmse_val: 1.0009
Epoch: 0064 | loss_train: 5.6664 loss_val: 3.8491 | f1_val: 0.06 | rmse_val: 0.7815
Epoch: 0065 | loss_train: 5.3443 loss_val: 9.9153 | f1_val: 0.09 | rmse_val: 1.1143
Epoch: 0066 | loss_train: 5.6556 loss_val: 6.5864 | f1_val: 0.11 | rmse_val: 1.0352
Epoch: 0067 | loss_train: 4.8656 loss_val: 4.4539 | f1_val: 0.10 | rmse_val: 0.9910
Epoch: 0068 | loss_train: 5.6325 loss_val: 7.2655 | f1_val: 0.10 | rmse_val: 1.2333
Epoch: 0069 | loss_train: 5.1119 loss_val: 6.2931 | f1_val: 0.07 | rmse_val: 1.2042
Epoch: 0070 | loss_train: 5.6306 loss_val: 4.6644 | f1_val: 0.09 | rmse_val: 1.0246
Epoch: 0071 | loss_train: 5.5429 loss_val: 5.1603 | f1_val: 0.08 | rmse_val: 1.0505
Epoch: 0072 | loss_train: 6.0333 loss_val: 6.2190 | f1_val: 0.07 | rmse_val: 1.1376
Epoch: 0073 | loss_train: 5.2044 loss_val: 4.0643 | f1_val: 0.08 | rmse_val: 1.1481
Epoch: 0074 | loss_train: 5.2564 loss_val: 4.3062 | f1_val: 0.08 | rmse_val: 0.9656
Epoch: 0075 | loss_train: 5.2343 loss_val: 6.6083 | f1_val: 0.08 | rmse_val: 1.1007
Epoch: 0076 | loss_train: 4.8221 loss_val: 4.4840 | f1_val: 0.07 | rmse_val: 0.9362
Epoch: 0077 | loss_train: 6.1510 loss_val: 6.8465 | f1_val: 0.06 | rmse_val: 1.3293
Epoch: 0078 | loss_train: 5.9551 loss_val: 4.1323 | f1_val: 0.07 | rmse_val: 0.6875
Epoch: 0079 | loss_train: 6.4302 loss_val: 5.8552 | f1_val: 0.08 | rmse_val: 0.7566
Epoch: 0080 | loss_train: 5.4841 loss_val: 6.8529 | f1_val: 0.06 | rmse_val: 1.1144
Epoch: 0081 | loss_train: 5.2083 loss_val: 4.8126 | f1_val: 0.08 | rmse_val: 0.9803
Epoch: 0082 | loss_train: 5.3006 loss_val: 4.0387 | f1_val: 0.10 | rmse_val: 0.9313
Epoch: 0083 | loss_train: 5.7320 loss_val: 8.3461 | f1_val: 0.10 | rmse_val: 1.0995
Epoch: 0084 | loss_train: 6.0837 loss_val: 5.3687 | f1_val: 0.08 | rmse_val: 1.0450
Epoch: 0085 | loss_train: 5.0678 loss_val: 5.3541 | f1_val: 0.09 | rmse_val: 0.8619
Epoch: 0086 | loss_train: 5.7829 loss_val: 6.5790 | f1_val: 0.08 | rmse_val: 1.1201
Epoch: 0087 | loss_train: 4.7180 loss_val: 7.3458 | f1_val: 0.07 | rmse_val: 1.0231
Epoch: 0088 | loss_train: 4.3950 loss_val: 5.4440 | f1_val: 0.07 | rmse_val: 1.0095
Epoch: 0089 | loss_train: 5.3899 loss_val: 9.9183 | f1_val: 0.08 | rmse_val: 1.0687
Epoch: 0090 | loss_train: 4.8888 loss_val: 5.5783 | f1_val: 0.10 | rmse_val: 0.8153
Epoch: 0091 | loss_train: 5.5032 loss_val: 6.0579 | f1_val: 0.06 | rmse_val: 1.1190
Epoch: 0092 | loss_train: 4.8921 loss_val: 5.8506 | f1_val: 0.08 | rmse_val: 1.1765
Epoch: 0093 | loss_train: 5.4622 loss_val: 4.6248 | f1_val: 0.07 | rmse_val: 1.0811
Epoch: 0094 | loss_train: 4.7527 loss_val: 7.0229 | f1_val: 0.07 | rmse_val: 0.9427
Epoch: 0095 | loss_train: 5.4412 loss_val: 8.5326 | f1_val: 0.10 | rmse_val: 0.9068
Epoch: 0096 | loss_train: 5.8304 loss_val: 4.5461 | f1_val: 0.08 | rmse_val: 1.0232
Optimization Finished!
Train cost: 344.8734s
Loading 66th epoch
f1_test: 0.08 | rmse_test: 1.0232

>>> train.py: Namespace(attention_dropout=0.1, batch_size=32, dataset='AIDS700nef', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/AIDS700nef/top_k', n_heads=8, n_layers=1, name=None, one_hot=False, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.001, pyg_path='data/pyg/AIDS700nef', seed=3407, split=1.0, top_k=20, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)

Graphs dropped for being too small: 15/560
TransformerModel(
  (att_embeddings_nope): Linear(in_features=32, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=128, bias=True)
)
total params: 153794
Epoch: 0001 | loss_train: 82.5602 loss_val: 78.2794 | f1_val: 0.06 | rmse_val: 8.4632
Epoch: 0002 | loss_train: 79.5640 loss_val: 91.4111 | f1_val: 0.06 | rmse_val: 8.5014
Epoch: 0003 | loss_train: 76.6822 loss_val: 68.6494 | f1_val: 0.07 | rmse_val: 7.9146
Epoch: 0004 | loss_train: 64.9420 loss_val: 59.2096 | f1_val: 0.04 | rmse_val: 7.1220
Epoch: 0005 | loss_train: 49.4232 loss_val: 44.3465 | f1_val: 0.06 | rmse_val: 5.3194
Epoch: 0006 | loss_train: 30.5285 loss_val: 15.7675 | f1_val: 0.08 | rmse_val: 2.8425
Epoch: 0007 | loss_train: 10.7182 loss_val: 4.6404 | f1_val: 0.07 | rmse_val: 1.0837
Epoch: 0008 | loss_train: 7.4666 loss_val: 4.0711 | f1_val: 0.07 | rmse_val: 1.7348
Epoch: 0009 | loss_train: 5.8419 loss_val: 6.2822 | f1_val: 0.09 | rmse_val: 1.1224
Epoch: 0010 | loss_train: 6.8375 loss_val: 9.2256 | f1_val: 0.08 | rmse_val: 0.9360
Epoch: 0011 | loss_train: 7.7370 loss_val: 6.7925 | f1_val: 0.06 | rmse_val: 1.2146
Epoch: 0012 | loss_train: 6.5150 loss_val: 7.7453 | f1_val: 0.08 | rmse_val: 0.7796
Epoch: 0013 | loss_train: 6.5643 loss_val: 7.9135 | f1_val: 0.07 | rmse_val: 1.1091
Epoch: 0014 | loss_train: 7.7738 loss_val: 10.3854 | f1_val: 0.08 | rmse_val: 1.1566
Epoch: 0015 | loss_train: 7.5504 loss_val: 8.0184 | f1_val: 0.07 | rmse_val: 0.8985
Epoch: 0016 | loss_train: 6.5781 loss_val: 4.2889 | f1_val: 0.05 | rmse_val: 0.9489
Epoch: 0017 | loss_train: 7.5220 loss_val: 7.0056 | f1_val: 0.08 | rmse_val: 1.1980
Epoch: 0018 | loss_train: 6.8365 loss_val: 6.5365 | f1_val: 0.08 | rmse_val: 0.9153
Epoch: 0019 | loss_train: 5.7325 loss_val: 8.1964 | f1_val: 0.08 | rmse_val: 1.0078
Epoch: 0020 | loss_train: 6.3377 loss_val: 8.9138 | f1_val: 0.08 | rmse_val: 1.4108
Epoch: 0021 | loss_train: 7.2673 loss_val: 5.8569 | f1_val: 0.07 | rmse_val: 0.9805
Epoch: 0022 | loss_train: 6.2245 loss_val: 8.0077 | f1_val: 0.07 | rmse_val: 1.2324
Epoch: 0023 | loss_train: 6.8141 loss_val: 3.7282 | f1_val: 0.08 | rmse_val: 0.7556
Epoch: 0024 | loss_train: 5.7941 loss_val: 6.9763 | f1_val: 0.05 | rmse_val: 1.3016
Epoch: 0025 | loss_train: 6.3504 loss_val: 6.7577 | f1_val: 0.10 | rmse_val: 1.1891
Epoch: 0026 | loss_train: 7.4181 loss_val: 8.3182 | f1_val: 0.10 | rmse_val: 0.8496
Epoch: 0027 | loss_train: 6.8139 loss_val: 7.7776 | f1_val: 0.08 | rmse_val: 1.1805
Epoch: 0028 | loss_train: 5.7695 loss_val: 8.7625 | f1_val: 0.11 | rmse_val: 0.8677
Epoch: 0029 | loss_train: 5.6495 loss_val: 7.4807 | f1_val: 0.06 | rmse_val: 0.7734
Epoch: 0030 | loss_train: 6.2182 loss_val: 7.8470 | f1_val: 0.08 | rmse_val: 0.9643
Epoch: 0031 | loss_train: 6.0449 loss_val: 7.2340 | f1_val: 0.10 | rmse_val: 1.3704
Epoch: 0032 | loss_train: 7.8496 loss_val: 7.2451 | f1_val: 0.07 | rmse_val: 1.4106
Epoch: 0033 | loss_train: 7.0792 loss_val: 8.7111 | f1_val: 0.10 | rmse_val: 0.8490
Epoch: 0034 | loss_train: 5.9719 loss_val: 4.7027 | f1_val: 0.07 | rmse_val: 1.0468
Epoch: 0035 | loss_train: 6.0842 loss_val: 6.5168 | f1_val: 0.08 | rmse_val: 1.5218
Epoch: 0036 | loss_train: 5.4621 loss_val: 6.1463 | f1_val: 0.09 | rmse_val: 0.8487
Epoch: 0037 | loss_train: 5.4490 loss_val: 7.2943 | f1_val: 0.07 | rmse_val: 0.8715
Epoch: 0038 | loss_train: 6.4242 loss_val: 4.2731 | f1_val: 0.10 | rmse_val: 1.1016
Epoch: 0039 | loss_train: 5.2989 loss_val: 7.3718 | f1_val: 0.09 | rmse_val: 1.7046
Epoch: 0040 | loss_train: 5.1618 loss_val: 6.4533 | f1_val: 0.06 | rmse_val: 0.9522
Epoch: 0041 | loss_train: 5.8337 loss_val: 7.3953 | f1_val: 0.08 | rmse_val: 0.6348
Epoch: 0042 | loss_train: 5.3951 loss_val: 5.4154 | f1_val: 0.10 | rmse_val: 0.9109
Epoch: 0043 | loss_train: 6.0215 loss_val: 5.4543 | f1_val: 0.10 | rmse_val: 0.9525
Epoch: 0044 | loss_train: 6.1742 loss_val: 8.9973 | f1_val: 0.10 | rmse_val: 1.9476
Epoch: 0045 | loss_train: 6.6644 loss_val: 8.6186 | f1_val: 0.09 | rmse_val: 1.9396
Epoch: 0046 | loss_train: 23.1562 loss_val: 13.8797 | f1_val: 0.05 | rmse_val: 2.0021
Epoch: 0047 | loss_train: 9.8246 loss_val: 6.7049 | f1_val: 0.10 | rmse_val: 1.3640
Epoch: 0048 | loss_train: 6.5078 loss_val: 8.9761 | f1_val: 0.07 | rmse_val: 1.0195
Epoch: 0049 | loss_train: 6.7881 loss_val: 6.3384 | f1_val: 0.10 | rmse_val: 1.1104
Epoch: 0050 | loss_train: 5.9803 loss_val: 6.0380 | f1_val: 0.08 | rmse_val: 1.0370
Epoch: 0051 | loss_train: 5.5661 loss_val: 5.5623 | f1_val: 0.08 | rmse_val: 1.2116
Epoch: 0052 | loss_train: 6.5664 loss_val: 8.7654 | f1_val: 0.08 | rmse_val: 0.8589
Epoch: 0053 | loss_train: 7.2310 loss_val: 7.9915 | f1_val: 0.07 | rmse_val: 0.9923
Epoch: 0054 | loss_train: 5.8140 loss_val: 8.4345 | f1_val: 0.06 | rmse_val: 1.0528
Epoch: 0055 | loss_train: 6.6900 loss_val: 9.2906 | f1_val: 0.11 | rmse_val: 0.8016
Epoch: 0056 | loss_train: 7.3949 loss_val: 5.6297 | f1_val: 0.08 | rmse_val: 1.0278
Epoch: 0057 | loss_train: 6.4586 loss_val: 5.4660 | f1_val: 0.12 | rmse_val: 0.9936
Epoch: 0058 | loss_train: 4.8283 loss_val: 6.8235 | f1_val: 0.10 | rmse_val: 0.7745
Epoch: 0059 | loss_train: 6.5769 loss_val: 6.0143 | f1_val: 0.09 | rmse_val: 1.1896
Epoch: 0060 | loss_train: 5.6123 loss_val: 10.2062 | f1_val: 0.06 | rmse_val: 0.8795
Epoch: 0061 | loss_train: 5.1132 loss_val: 5.4029 | f1_val: 0.10 | rmse_val: 1.3869
Epoch: 0062 | loss_train: 5.6445 loss_val: 4.6614 | f1_val: 0.09 | rmse_val: 1.0284
Epoch: 0063 | loss_train: 5.6194 loss_val: 6.3115 | f1_val: 0.09 | rmse_val: 0.9596
Epoch: 0064 | loss_train: 5.6664 loss_val: 3.8491 | f1_val: 0.06 | rmse_val: 0.7745
Epoch: 0065 | loss_train: 5.3443 loss_val: 9.9153 | f1_val: 0.10 | rmse_val: 1.0854
Epoch: 0066 | loss_train: 5.6556 loss_val: 6.5864 | f1_val: 0.12 | rmse_val: 1.0181
Epoch: 0067 | loss_train: 4.8656 loss_val: 4.4539 | f1_val: 0.12 | rmse_val: 0.9669
Epoch: 0068 | loss_train: 5.6325 loss_val: 7.2655 | f1_val: 0.11 | rmse_val: 1.1775
Epoch: 0069 | loss_train: 5.1119 loss_val: 6.2931 | f1_val: 0.08 | rmse_val: 1.1752
Epoch: 0070 | loss_train: 5.6306 loss_val: 4.6644 | f1_val: 0.10 | rmse_val: 0.9984
Epoch: 0071 | loss_train: 5.5429 loss_val: 5.1603 | f1_val: 0.10 | rmse_val: 1.0298
Epoch: 0072 | loss_train: 6.0333 loss_val: 6.2190 | f1_val: 0.08 | rmse_val: 1.1160
Epoch: 0073 | loss_train: 5.2044 loss_val: 4.0643 | f1_val: 0.09 | rmse_val: 1.1255
Epoch: 0074 | loss_train: 5.2564 loss_val: 4.3062 | f1_val: 0.10 | rmse_val: 0.9232
Epoch: 0075 | loss_train: 5.2343 loss_val: 6.6083 | f1_val: 0.09 | rmse_val: 1.0643
Epoch: 0076 | loss_train: 4.8221 loss_val: 4.4840 | f1_val: 0.08 | rmse_val: 0.9174
Epoch: 0077 | loss_train: 6.1510 loss_val: 6.8465 | f1_val: 0.06 | rmse_val: 1.2828
Epoch: 0078 | loss_train: 5.9551 loss_val: 4.1323 | f1_val: 0.07 | rmse_val: 0.6642
Epoch: 0079 | loss_train: 6.4302 loss_val: 5.8552 | f1_val: 0.08 | rmse_val: 0.7488
Epoch: 0080 | loss_train: 5.4841 loss_val: 6.8529 | f1_val: 0.06 | rmse_val: 1.0607
Epoch: 0081 | loss_train: 5.2083 loss_val: 4.8126 | f1_val: 0.09 | rmse_val: 0.9564
Epoch: 0082 | loss_train: 5.3006 loss_val: 4.0387 | f1_val: 0.11 | rmse_val: 0.9217
Epoch: 0083 | loss_train: 5.7320 loss_val: 8.3461 | f1_val: 0.10 | rmse_val: 1.0344
Epoch: 0084 | loss_train: 6.0837 loss_val: 5.3687 | f1_val: 0.09 | rmse_val: 1.0140
Epoch: 0085 | loss_train: 5.0678 loss_val: 5.3541 | f1_val: 0.10 | rmse_val: 0.8512
Epoch: 0086 | loss_train: 5.7829 loss_val: 6.5790 | f1_val: 0.09 | rmse_val: 1.0645
Epoch: 0087 | loss_train: 4.7180 loss_val: 7.3458 | f1_val: 0.09 | rmse_val: 0.9657
Epoch: 0088 | loss_train: 4.3950 loss_val: 5.4440 | f1_val: 0.08 | rmse_val: 0.9961
Epoch: 0089 | loss_train: 5.3899 loss_val: 9.9183 | f1_val: 0.09 | rmse_val: 1.0106
Epoch: 0090 | loss_train: 4.8888 loss_val: 5.5783 | f1_val: 0.11 | rmse_val: 0.8195
Epoch: 0091 | loss_train: 5.5032 loss_val: 6.0579 | f1_val: 0.07 | rmse_val: 1.0866
Epoch: 0092 | loss_train: 4.8921 loss_val: 5.8506 | f1_val: 0.09 | rmse_val: 1.1393
Epoch: 0093 | loss_train: 5.4622 loss_val: 4.6248 | f1_val: 0.08 | rmse_val: 1.0250
Epoch: 0094 | loss_train: 4.7527 loss_val: 7.0229 | f1_val: 0.08 | rmse_val: 0.9017
Epoch: 0095 | loss_train: 5.4412 loss_val: 8.5326 | f1_val: 0.11 | rmse_val: 0.8789
Epoch: 0096 | loss_train: 5.8304 loss_val: 4.5461 | f1_val: 0.09 | rmse_val: 0.9745
Optimization Finished!
Train cost: 364.2457s
Loading 66th epoch
f1_test: 0.09 | rmse_test: 0.9745

>>> run.py: Namespace(dataset='AIDS700nef', device=1, experiment='top_k', log_path='log/nagphormer/AIDS700nef/top_k', path='data/nagphormer', plot_path='plots/nagphormer/AIDS700nef/top_k', pyg_path='data/pyg/AIDS700nef')

##################################################

Experiment: default, Dataset: LINUX

##################################################

Experiment: default, Dataset: AIDS700nef

##################################################

Experiment: one_hot, Dataset: LINUX

##################################################

Experiment: one_hot, Dataset: AIDS700nef

##################################################

Experiment: training-data, Dataset: LINUX

##################################################

Experiment: training-data, Dataset: AIDS700nef

##################################################

Experiment: hops, Dataset: LINUX

##################################################

Experiment: hops, Dataset: AIDS700nef

##################################################

Experiment: pe_dim, Dataset: LINUX

##################################################

Experiment: pe_dim, Dataset: AIDS700nef

##################################################

Experiment: n_layers, Dataset: LINUX

##################################################

Experiment: n_layers, Dataset: AIDS700nef

##################################################

Experiment: hidden_dim, Dataset: LINUX

##################################################

Experiment: hidden_dim, Dataset: AIDS700nef

##################################################

Experiment: top_k, Dataset: LINUX

##################################################

Experiment: top_k, Dataset: AIDS700nef
