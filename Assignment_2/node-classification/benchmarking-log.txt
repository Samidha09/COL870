
>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/default', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9644 loss_val: 1.8685 | acc_train: 0.1500 acc_val: 0.3380 | f1_train: 0.0961 f1_val: 0.1805
Epoch: 0002 | loss_train: 1.9218 loss_val: 1.8201 | acc_train: 0.2429 acc_val: 0.4180 | f1_train: 0.1645 f1_val: 0.2550
Epoch: 0003 | loss_train: 1.8483 loss_val: 1.7494 | acc_train: 0.3143 acc_val: 0.5420 | f1_train: 0.2601 f1_val: 0.4715
Epoch: 0004 | loss_train: 1.7314 loss_val: 1.6593 | acc_train: 0.6143 acc_val: 0.6300 | f1_train: 0.6107 f1_val: 0.5859
Epoch: 0005 | loss_train: 1.5866 loss_val: 1.5494 | acc_train: 0.7643 acc_val: 0.6940 | f1_train: 0.7638 f1_val: 0.6637
Epoch: 0006 | loss_train: 1.4234 loss_val: 1.4177 | acc_train: 0.8429 acc_val: 0.7260 | f1_train: 0.8451 f1_val: 0.7061
Epoch: 0007 | loss_train: 1.2424 loss_val: 1.2689 | acc_train: 0.8929 acc_val: 0.7400 | f1_train: 0.8946 f1_val: 0.7335
Epoch: 0008 | loss_train: 1.0407 loss_val: 1.1175 | acc_train: 0.9214 acc_val: 0.7560 | f1_train: 0.9217 f1_val: 0.7481
Epoch: 0009 | loss_train: 0.8483 loss_val: 0.9828 | acc_train: 0.9500 acc_val: 0.7660 | f1_train: 0.9508 f1_val: 0.7615
Epoch: 0010 | loss_train: 0.6650 loss_val: 0.8742 | acc_train: 0.9429 acc_val: 0.7660 | f1_train: 0.9426 f1_val: 0.7614
Epoch: 0011 | loss_train: 0.5059 loss_val: 0.7956 | acc_train: 0.9714 acc_val: 0.7640 | f1_train: 0.9712 f1_val: 0.7592
Epoch: 0012 | loss_train: 0.3727 loss_val: 0.7420 | acc_train: 0.9786 acc_val: 0.7620 | f1_train: 0.9784 f1_val: 0.7596
Epoch: 0013 | loss_train: 0.2600 loss_val: 0.7074 | acc_train: 0.9857 acc_val: 0.7720 | f1_train: 0.9855 f1_val: 0.7715
Epoch: 0014 | loss_train: 0.1732 loss_val: 0.6932 | acc_train: 0.9929 acc_val: 0.7720 | f1_train: 0.9929 f1_val: 0.7727
Epoch: 0015 | loss_train: 0.1114 loss_val: 0.7023 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7773
Epoch: 0016 | loss_train: 0.0693 loss_val: 0.7350 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7769
Epoch: 0017 | loss_train: 0.0415 loss_val: 0.7858 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0018 | loss_train: 0.0253 loss_val: 0.8479 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7655
Epoch: 0019 | loss_train: 0.0163 loss_val: 0.9169 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0020 | loss_train: 0.0101 loss_val: 0.9886 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7511
Epoch: 0021 | loss_train: 0.0070 loss_val: 1.0602 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0022 | loss_train: 0.0047 loss_val: 1.1301 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7422
Epoch: 0023 | loss_train: 0.0033 loss_val: 1.1974 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7394
Epoch: 0024 | loss_train: 0.0024 loss_val: 1.2620 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7351
Epoch: 0025 | loss_train: 0.0017 loss_val: 1.3238 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7355
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.3828 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7391
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.4388 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7396
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.4920 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7426
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.5427 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7427
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5906 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7439
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6363 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7467
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6800 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7431
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.7215 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7610 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7452
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7988 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7492
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.8345 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7510
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8685 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.9008 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9313 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9601 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9873 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7542
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.0127 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7551
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.0368 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7504
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.0593 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7502
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.0804 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7507
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.0999 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7506
Optimization Finished!
Train cost: 11.5175s
Loading 15th epoch
Test set results: loss= 0.6384 accuracy= 0.7910

>>> run.py: Namespace(dataset='cora', device=1, experiment='default', log_path='log/nagphormer/cora/default', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/cora/default')

>>> Training split = 1.0

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/default', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6315 loss_val: 1.5466 | acc_train: 0.1500 acc_val: 0.3000 | f1_train: 0.1211 f1_val: 0.1308
Epoch: 0002 | loss_train: 1.5652 loss_val: 1.4203 | acc_train: 0.2833 acc_val: 0.5000 | f1_train: 0.1653 f1_val: 0.1644
Epoch: 0003 | loss_train: 1.4443 loss_val: 1.2855 | acc_train: 0.4333 acc_val: 0.5125 | f1_train: 0.1550 f1_val: 0.1526
Epoch: 0004 | loss_train: 1.3147 loss_val: 1.1833 | acc_train: 0.4333 acc_val: 0.5750 | f1_train: 0.1390 f1_val: 0.2358
Epoch: 0005 | loss_train: 1.2048 loss_val: 1.1209 | acc_train: 0.4583 acc_val: 0.6125 | f1_train: 0.1674 f1_val: 0.2715
Epoch: 0006 | loss_train: 1.1178 loss_val: 1.0844 | acc_train: 0.5333 acc_val: 0.5625 | f1_train: 0.2372 f1_val: 0.2529
Epoch: 0007 | loss_train: 1.0281 loss_val: 1.0654 | acc_train: 0.5750 acc_val: 0.5500 | f1_train: 0.2665 f1_val: 0.2474
Epoch: 0008 | loss_train: 0.9321 loss_val: 1.0565 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3003 f1_val: 0.2557
Epoch: 0009 | loss_train: 0.8323 loss_val: 1.0307 | acc_train: 0.6667 acc_val: 0.6125 | f1_train: 0.3606 f1_val: 0.3566
Epoch: 0010 | loss_train: 0.7215 loss_val: 0.9827 | acc_train: 0.7583 acc_val: 0.6000 | f1_train: 0.5366 f1_val: 0.3628
Epoch: 0011 | loss_train: 0.6194 loss_val: 0.9135 | acc_train: 0.8167 acc_val: 0.6500 | f1_train: 0.7483 f1_val: 0.4362
Epoch: 0012 | loss_train: 0.5123 loss_val: 0.8581 | acc_train: 0.8833 acc_val: 0.6500 | f1_train: 0.8614 f1_val: 0.4708
Epoch: 0013 | loss_train: 0.4123 loss_val: 0.8767 | acc_train: 0.9167 acc_val: 0.7000 | f1_train: 0.8844 f1_val: 0.5609
Epoch: 0014 | loss_train: 0.3231 loss_val: 0.9327 | acc_train: 0.9500 acc_val: 0.7000 | f1_train: 0.9142 f1_val: 0.5516
Epoch: 0015 | loss_train: 0.2508 loss_val: 0.9685 | acc_train: 0.9500 acc_val: 0.6875 | f1_train: 0.9209 f1_val: 0.4886
Epoch: 0016 | loss_train: 0.1914 loss_val: 1.0292 | acc_train: 0.9583 acc_val: 0.7000 | f1_train: 0.9259 f1_val: 0.4882
Epoch: 0017 | loss_train: 0.1423 loss_val: 1.1019 | acc_train: 0.9667 acc_val: 0.7000 | f1_train: 0.9297 f1_val: 0.4825
Epoch: 0018 | loss_train: 0.1004 loss_val: 1.1785 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9694 f1_val: 0.4658
Epoch: 0019 | loss_train: 0.0704 loss_val: 1.2618 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4528
Epoch: 0020 | loss_train: 0.0536 loss_val: 1.3591 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0021 | loss_train: 0.0349 loss_val: 1.4517 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0022 | loss_train: 0.0250 loss_val: 1.5643 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0023 | loss_train: 0.0185 loss_val: 1.6873 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0024 | loss_train: 0.0115 loss_val: 1.7941 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0025 | loss_train: 0.0134 loss_val: 1.8016 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0026 | loss_train: 0.0071 loss_val: 1.8700 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0027 | loss_train: 0.0042 loss_val: 1.9669 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4397
Epoch: 0028 | loss_train: 0.0032 loss_val: 2.0669 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4652
Epoch: 0029 | loss_train: 0.0030 loss_val: 2.1614 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0030 | loss_train: 0.0027 loss_val: 2.2514 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4694
Epoch: 0031 | loss_train: 0.0019 loss_val: 2.3389 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0032 | loss_train: 0.0016 loss_val: 2.4217 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0033 | loss_train: 0.0012 loss_val: 2.4973 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0034 | loss_train: 0.0011 loss_val: 2.5645 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0035 | loss_train: 0.0008 loss_val: 2.6212 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.6685 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.7098 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.7466 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0039 | loss_train: 0.0004 loss_val: 2.7690 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.7881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.8042 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.8187 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.8326 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.8465 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.8602 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.8742 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.8881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4756
Optimization Finished!
Train cost: 7.6735s
Loading 13th epoch
Test set results: loss= 1.1536 accuracy= 0.5686

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='default', log_path='log/nagphormer/wisconsin/default', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/wisconsin/default')

>>> Training split = 1.0

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/default', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9404, Val loss: 1.9439 | Train F1: 0.07, Val F1: 0.05
Epoch: 001 | Train loss: 1.9339, Val loss: 1.9413 | Train F1: 0.22, Val F1: 0.11
Epoch: 002 | Train loss: 1.9264, Val loss: 1.9382 | Train F1: 0.45, Val F1: 0.18
Epoch: 003 | Train loss: 1.9172, Val loss: 1.9345 | Train F1: 0.74, Val F1: 0.31
Epoch: 004 | Train loss: 1.9061, Val loss: 1.9299 | Train F1: 0.85, Val F1: 0.43
Epoch: 005 | Train loss: 1.8924, Val loss: 1.9245 | Train F1: 0.88, Val F1: 0.49
Epoch: 006 | Train loss: 1.8756, Val loss: 1.9178 | Train F1: 0.94, Val F1: 0.55
Epoch: 007 | Train loss: 1.8551, Val loss: 1.9099 | Train F1: 0.95, Val F1: 0.58
Epoch: 008 | Train loss: 1.8304, Val loss: 1.9002 | Train F1: 0.94, Val F1: 0.59
Epoch: 009 | Train loss: 1.8012, Val loss: 1.8886 | Train F1: 0.94, Val F1: 0.59
Epoch: 010 | Train loss: 1.7672, Val loss: 1.8749 | Train F1: 0.94, Val F1: 0.60
Epoch: 011 | Train loss: 1.7291, Val loss: 1.8585 | Train F1: 0.94, Val F1: 0.61
Epoch: 012 | Train loss: 1.6877, Val loss: 1.8392 | Train F1: 0.93, Val F1: 0.62
Epoch: 013 | Train loss: 1.6441, Val loss: 1.8168 | Train F1: 0.93, Val F1: 0.63
Epoch: 014 | Train loss: 1.5997, Val loss: 1.7916 | Train F1: 0.93, Val F1: 0.64
Epoch: 015 | Train loss: 1.5557, Val loss: 1.7642 | Train F1: 0.93, Val F1: 0.64
Epoch: 016 | Train loss: 1.5130, Val loss: 1.7354 | Train F1: 0.93, Val F1: 0.65
Epoch: 017 | Train loss: 1.4725, Val loss: 1.7063 | Train F1: 0.93, Val F1: 0.66
Epoch: 018 | Train loss: 1.4349, Val loss: 1.6777 | Train F1: 0.93, Val F1: 0.66
Epoch: 019 | Train loss: 1.4008, Val loss: 1.6507 | Train F1: 0.95, Val F1: 0.69
Epoch: 020 | Train loss: 1.3703, Val loss: 1.6258 | Train F1: 0.95, Val F1: 0.70
Epoch: 021 | Train loss: 1.3431, Val loss: 1.6036 | Train F1: 0.95, Val F1: 0.71
Epoch: 022 | Train loss: 1.3187, Val loss: 1.5837 | Train F1: 0.95, Val F1: 0.72
Epoch: 023 | Train loss: 1.2963, Val loss: 1.5659 | Train F1: 0.97, Val F1: 0.74
Epoch: 024 | Train loss: 1.2757, Val loss: 1.5497 | Train F1: 0.98, Val F1: 0.74
Epoch: 025 | Train loss: 1.2566, Val loss: 1.5347 | Train F1: 0.99, Val F1: 0.75
Epoch: 026 | Train loss: 1.2391, Val loss: 1.5205 | Train F1: 0.99, Val F1: 0.75
Epoch: 027 | Train loss: 1.2237, Val loss: 1.5081 | Train F1: 0.99, Val F1: 0.75
Epoch: 028 | Train loss: 1.2109, Val loss: 1.4972 | Train F1: 0.99, Val F1: 0.76
Epoch: 029 | Train loss: 1.2011, Val loss: 1.4880 | Train F1: 0.99, Val F1: 0.75
Epoch: 030 | Train loss: 1.1941, Val loss: 1.4805 | Train F1: 0.99, Val F1: 0.75
Epoch: 031 | Train loss: 1.1891, Val loss: 1.4742 | Train F1: 0.99, Val F1: 0.75
Epoch: 032 | Train loss: 1.1853, Val loss: 1.4685 | Train F1: 0.99, Val F1: 0.74
Epoch: 033 | Train loss: 1.1821, Val loss: 1.4633 | Train F1: 0.99, Val F1: 0.74
Epoch: 034 | Train loss: 1.1796, Val loss: 1.4582 | Train F1: 0.99, Val F1: 0.75
Epoch: 035 | Train loss: 1.1776, Val loss: 1.4533 | Train F1: 0.99, Val F1: 0.75
Epoch: 036 | Train loss: 1.1761, Val loss: 1.4487 | Train F1: 0.99, Val F1: 0.75
Epoch: 037 | Train loss: 1.1750, Val loss: 1.4442 | Train F1: 0.99, Val F1: 0.75
Epoch: 038 | Train loss: 1.1742, Val loss: 1.4403 | Train F1: 0.99, Val F1: 0.75
Epoch: 039 | Train loss: 1.1735, Val loss: 1.4367 | Train F1: 0.99, Val F1: 0.76
Epoch: 040 | Train loss: 1.1729, Val loss: 1.4337 | Train F1: 0.99, Val F1: 0.75
Epoch: 041 | Train loss: 1.1722, Val loss: 1.4311 | Train F1: 0.99, Val F1: 0.75
Epoch: 042 | Train loss: 1.1713, Val loss: 1.4289 | Train F1: 0.99, Val F1: 0.75
Epoch: 043 | Train loss: 1.1701, Val loss: 1.4273 | Train F1: 0.99, Val F1: 0.75
Epoch: 044 | Train loss: 1.1688, Val loss: 1.4265 | Train F1: 1.00, Val F1: 0.75
Epoch: 045 | Train loss: 1.1676, Val loss: 1.4264 | Train F1: 1.00, Val F1: 0.75
Epoch: 046 | Train loss: 1.1668, Val loss: 1.4269 | Train F1: 1.00, Val F1: 0.75
Epoch: 047 | Train loss: 1.1664, Val loss: 1.4280 | Train F1: 1.00, Val F1: 0.75
Epoch: 048 | Train loss: 1.1663, Val loss: 1.4294 | Train F1: 1.00, Val F1: 0.75
Epoch: 049 | Train loss: 1.1662, Val loss: 1.4309 | Train F1: 1.00, Val F1: 0.75
Epoch: 050 | Train loss: 1.1662, Val loss: 1.4324 | Train F1: 1.00, Val F1: 0.75
Epoch: 051 | Train loss: 1.1662, Val loss: 1.4338 | Train F1: 1.00, Val F1: 0.75
Epoch: 052 | Train loss: 1.1663, Val loss: 1.4349 | Train F1: 1.00, Val F1: 0.75
Epoch: 053 | Train loss: 1.1663, Val loss: 1.4354 | Train F1: 1.00, Val F1: 0.74
Epoch: 054 | Train loss: 1.1662, Val loss: 1.4355 | Train F1: 1.00, Val F1: 0.74
Epoch: 055 | Train loss: 1.1661, Val loss: 1.4347 | Train F1: 1.00, Val F1: 0.75
Epoch: 056 | Train loss: 1.1660, Val loss: 1.4337 | Train F1: 1.00, Val F1: 0.75
Epoch: 057 | Train loss: 1.1659, Val loss: 1.4325 | Train F1: 1.00, Val F1: 0.75
Epoch: 058 | Train loss: 1.1659, Val loss: 1.4312 | Train F1: 1.00, Val F1: 0.75
Epoch: 059 | Train loss: 1.1658, Val loss: 1.4299 | Train F1: 1.00, Val F1: 0.75
Epoch: 060 | Train loss: 1.1657, Val loss: 1.4285 | Train F1: 1.00, Val F1: 0.75
Epoch: 061 | Train loss: 1.1657, Val loss: 1.4272 | Train F1: 1.00, Val F1: 0.76
Epoch: 062 | Train loss: 1.1657, Val loss: 1.4261 | Train F1: 1.00, Val F1: 0.76
Epoch: 063 | Train loss: 1.1656, Val loss: 1.4250 | Train F1: 1.00, Val F1: 0.76
Epoch: 064 | Train loss: 1.1656, Val loss: 1.4240 | Train F1: 1.00, Val F1: 0.76
Epoch: 065 | Train loss: 1.1656, Val loss: 1.4231 | Train F1: 1.00, Val F1: 0.76
Epoch: 066 | Train loss: 1.1656, Val loss: 1.4224 | Train F1: 1.00, Val F1: 0.75
Epoch: 067 | Train loss: 1.1656, Val loss: 1.4218 | Train F1: 1.00, Val F1: 0.76
Epoch: 068 | Train loss: 1.1656, Val loss: 1.4212 | Train F1: 1.00, Val F1: 0.76
Epoch: 069 | Train loss: 1.1655, Val loss: 1.4208 | Train F1: 1.00, Val F1: 0.76
Epoch: 070 | Train loss: 1.1655, Val loss: 1.4205 | Train F1: 1.00, Val F1: 0.76
Epoch: 071 | Train loss: 1.1655, Val loss: 1.4201 | Train F1: 1.00, Val F1: 0.76
Epoch: 072 | Train loss: 1.1655, Val loss: 1.4199 | Train F1: 1.00, Val F1: 0.75
Epoch: 073 | Train loss: 1.1655, Val loss: 1.4197 | Train F1: 1.00, Val F1: 0.75
Epoch: 074 | Train loss: 1.1655, Val loss: 1.4195 | Train F1: 1.00, Val F1: 0.75
Epoch: 075 | Train loss: 1.1655, Val loss: 1.4194 | Train F1: 1.00, Val F1: 0.75
Epoch: 076 | Train loss: 1.1655, Val loss: 1.4193 | Train F1: 1.00, Val F1: 0.75
Epoch: 077 | Train loss: 1.1655, Val loss: 1.4192 | Train F1: 1.00, Val F1: 0.75
Epoch: 078 | Train loss: 1.1655, Val loss: 1.4191 | Train F1: 1.00, Val F1: 0.75
Epoch: 079 | Train loss: 1.1655, Val loss: 1.4191 | Train F1: 1.00, Val F1: 0.75
Epoch: 080 | Train loss: 1.1655, Val loss: 1.4190 | Train F1: 1.00, Val F1: 0.75
Epoch: 081 | Train loss: 1.1655, Val loss: 1.4190 | Train F1: 1.00, Val F1: 0.75
Epoch: 082 | Train loss: 1.1655, Val loss: 1.4190 | Train F1: 1.00, Val F1: 0.75
Epoch: 083 | Train loss: 1.1655, Val loss: 1.4190 | Train F1: 1.00, Val F1: 0.75
Epoch: 084 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 085 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 086 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 087 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 088 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 089 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 090 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 091 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 092 | Train loss: 1.1655, Val loss: 1.4189 | Train F1: 1.00, Val F1: 0.75
Epoch: 093 | Train loss: 1.1655, Val loss: 1.4188 | Train F1: 1.00, Val F1: 0.75
Epoch: 094 | Train loss: 1.1655, Val loss: 1.4188 | Train F1: 1.00, Val F1: 0.75
Epoch: 095 | Train loss: 1.1655, Val loss: 1.4188 | Train F1: 1.00, Val F1: 0.75
Best model:
Train loss: 1.1656, Val loss: 1.4231, Test loss: 1.4111
Train F1: 1.00, Val F1: 0.76, Test F1: 0.77

>>> run.py: Namespace(dataset='cora', device=1, experiment='default', log_path='log/graphsage/cora/default', method='graphsage', path='data/graphsage', plot_path='plots/graphsage/cora/default')

>>> Training split = 1.0

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/default', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5647, Val loss: 1.5715 | Train F1: 0.14, Val F1: 0.16
Epoch: 001 | Train loss: 1.5211, Val loss: 1.5379 | Train F1: 0.19, Val F1: 0.17
Epoch: 002 | Train loss: 1.4744, Val loss: 1.5005 | Train F1: 0.19, Val F1: 0.19
Epoch: 003 | Train loss: 1.4308, Val loss: 1.4628 | Train F1: 0.21, Val F1: 0.19
Epoch: 004 | Train loss: 1.3932, Val loss: 1.4294 | Train F1: 0.24, Val F1: 0.21
Epoch: 005 | Train loss: 1.3576, Val loss: 1.4008 | Train F1: 0.27, Val F1: 0.25
Epoch: 006 | Train loss: 1.3239, Val loss: 1.3760 | Train F1: 0.31, Val F1: 0.28
Epoch: 007 | Train loss: 1.2969, Val loss: 1.3591 | Train F1: 0.32, Val F1: 0.29
Epoch: 008 | Train loss: 1.2751, Val loss: 1.3491 | Train F1: 0.32, Val F1: 0.30
Epoch: 009 | Train loss: 1.2507, Val loss: 1.3371 | Train F1: 0.34, Val F1: 0.31
Epoch: 010 | Train loss: 1.2238, Val loss: 1.3235 | Train F1: 0.47, Val F1: 0.31
Epoch: 011 | Train loss: 1.1999, Val loss: 1.3121 | Train F1: 0.51, Val F1: 0.32
Epoch: 012 | Train loss: 1.1809, Val loss: 1.3038 | Train F1: 0.51, Val F1: 0.31
Epoch: 013 | Train loss: 1.1656, Val loss: 1.2975 | Train F1: 0.56, Val F1: 0.31
Epoch: 014 | Train loss: 1.1531, Val loss: 1.2915 | Train F1: 0.56, Val F1: 0.30
Epoch: 015 | Train loss: 1.1431, Val loss: 1.2856 | Train F1: 0.56, Val F1: 0.30
Epoch: 016 | Train loss: 1.1343, Val loss: 1.2797 | Train F1: 0.56, Val F1: 0.36
Epoch: 017 | Train loss: 1.1258, Val loss: 1.2743 | Train F1: 0.55, Val F1: 0.45
Epoch: 018 | Train loss: 1.1164, Val loss: 1.2701 | Train F1: 0.55, Val F1: 0.44
Epoch: 019 | Train loss: 1.1045, Val loss: 1.2668 | Train F1: 0.55, Val F1: 0.44
Epoch: 020 | Train loss: 1.0902, Val loss: 1.2645 | Train F1: 0.62, Val F1: 0.43
Epoch: 021 | Train loss: 1.0760, Val loss: 1.2633 | Train F1: 0.65, Val F1: 0.44
Epoch: 022 | Train loss: 1.0637, Val loss: 1.2635 | Train F1: 0.66, Val F1: 0.43
Epoch: 023 | Train loss: 1.0542, Val loss: 1.2637 | Train F1: 0.66, Val F1: 0.43
Epoch: 024 | Train loss: 1.0454, Val loss: 1.2633 | Train F1: 0.65, Val F1: 0.45
Epoch: 025 | Train loss: 1.0354, Val loss: 1.2625 | Train F1: 0.67, Val F1: 0.44
Epoch: 026 | Train loss: 1.0255, Val loss: 1.2603 | Train F1: 0.69, Val F1: 0.43
Epoch: 027 | Train loss: 1.0170, Val loss: 1.2580 | Train F1: 0.69, Val F1: 0.44
Epoch: 028 | Train loss: 1.0088, Val loss: 1.2547 | Train F1: 0.68, Val F1: 0.46
Epoch: 029 | Train loss: 1.0005, Val loss: 1.2498 | Train F1: 0.69, Val F1: 0.47
Epoch: 030 | Train loss: 0.9922, Val loss: 1.2422 | Train F1: 0.80, Val F1: 0.47
Epoch: 031 | Train loss: 0.9849, Val loss: 1.2340 | Train F1: 0.80, Val F1: 0.48
Epoch: 032 | Train loss: 0.9792, Val loss: 1.2279 | Train F1: 0.86, Val F1: 0.47
Epoch: 033 | Train loss: 0.9747, Val loss: 1.2239 | Train F1: 0.86, Val F1: 0.47
Epoch: 034 | Train loss: 0.9711, Val loss: 1.2204 | Train F1: 0.86, Val F1: 0.54
Epoch: 035 | Train loss: 0.9680, Val loss: 1.2180 | Train F1: 0.86, Val F1: 0.54
Epoch: 036 | Train loss: 0.9652, Val loss: 1.2161 | Train F1: 0.86, Val F1: 0.49
Epoch: 037 | Train loss: 0.9628, Val loss: 1.2148 | Train F1: 0.86, Val F1: 0.50
Epoch: 038 | Train loss: 0.9607, Val loss: 1.2142 | Train F1: 0.86, Val F1: 0.55
Epoch: 039 | Train loss: 0.9587, Val loss: 1.2134 | Train F1: 0.86, Val F1: 0.55
Epoch: 040 | Train loss: 0.9567, Val loss: 1.2118 | Train F1: 0.88, Val F1: 0.46
Epoch: 041 | Train loss: 0.9546, Val loss: 1.2110 | Train F1: 0.89, Val F1: 0.46
Epoch: 042 | Train loss: 0.9523, Val loss: 1.2103 | Train F1: 0.89, Val F1: 0.48
Epoch: 043 | Train loss: 0.9500, Val loss: 1.2100 | Train F1: 0.89, Val F1: 0.46
Epoch: 044 | Train loss: 0.9479, Val loss: 1.2103 | Train F1: 0.89, Val F1: 0.46
Epoch: 045 | Train loss: 0.9461, Val loss: 1.2116 | Train F1: 0.89, Val F1: 0.46
Epoch: 046 | Train loss: 0.9447, Val loss: 1.2122 | Train F1: 0.89, Val F1: 0.44
Epoch: 047 | Train loss: 0.9436, Val loss: 1.2122 | Train F1: 0.89, Val F1: 0.44
Epoch: 048 | Train loss: 0.9426, Val loss: 1.2116 | Train F1: 0.89, Val F1: 0.44
Epoch: 049 | Train loss: 0.9416, Val loss: 1.2119 | Train F1: 0.89, Val F1: 0.44
Epoch: 050 | Train loss: 0.9405, Val loss: 1.2124 | Train F1: 0.89, Val F1: 0.41
Epoch: 051 | Train loss: 0.9391, Val loss: 1.2129 | Train F1: 0.89, Val F1: 0.41
Epoch: 052 | Train loss: 0.9375, Val loss: 1.2134 | Train F1: 0.94, Val F1: 0.41
Epoch: 053 | Train loss: 0.9355, Val loss: 1.2137 | Train F1: 0.95, Val F1: 0.40
Epoch: 054 | Train loss: 0.9333, Val loss: 1.2142 | Train F1: 0.95, Val F1: 0.42
Epoch: 055 | Train loss: 0.9312, Val loss: 1.2158 | Train F1: 0.95, Val F1: 0.42
Epoch: 056 | Train loss: 0.9293, Val loss: 1.2183 | Train F1: 0.95, Val F1: 0.42
Epoch: 057 | Train loss: 0.9277, Val loss: 1.2203 | Train F1: 0.98, Val F1: 0.43
Epoch: 058 | Train loss: 0.9263, Val loss: 1.2217 | Train F1: 0.98, Val F1: 0.44
Epoch: 059 | Train loss: 0.9250, Val loss: 1.2220 | Train F1: 0.98, Val F1: 0.43
Epoch: 060 | Train loss: 0.9238, Val loss: 1.2207 | Train F1: 0.98, Val F1: 0.47
Epoch: 061 | Train loss: 0.9228, Val loss: 1.2186 | Train F1: 0.98, Val F1: 0.47
Epoch: 062 | Train loss: 0.9219, Val loss: 1.2159 | Train F1: 0.98, Val F1: 0.48
Epoch: 063 | Train loss: 0.9212, Val loss: 1.2132 | Train F1: 0.98, Val F1: 0.48
Epoch: 064 | Train loss: 0.9206, Val loss: 1.2106 | Train F1: 0.98, Val F1: 0.48
Epoch: 065 | Train loss: 0.9201, Val loss: 1.2084 | Train F1: 0.98, Val F1: 0.52
Epoch: 066 | Train loss: 0.9196, Val loss: 1.2069 | Train F1: 0.98, Val F1: 0.52
Epoch: 067 | Train loss: 0.9191, Val loss: 1.2058 | Train F1: 0.98, Val F1: 0.52
Epoch: 068 | Train loss: 0.9187, Val loss: 1.2049 | Train F1: 0.98, Val F1: 0.52
Epoch: 069 | Train loss: 0.9183, Val loss: 1.2041 | Train F1: 0.98, Val F1: 0.52
Best model:
Train loss: 0.9587, Val loss: 1.2134, Test loss: 1.2730
Train F1: 0.86, Val F1: 0.55, Test F1: 0.44

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='default', log_path='log/graphsage/wisconsin/default', method='graphsage', path='data/graphsage', plot_path='plots/graphsage/wisconsin/default')

>>> Training split = 1.0

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=0.2, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9625 loss_val: 1.8768 | acc_train: 0.1429 acc_val: 0.3240 | f1_train: 0.0918 f1_val: 0.1730
Epoch: 0002 | loss_train: 1.9058 loss_val: 1.8441 | acc_train: 0.2857 acc_val: 0.3740 | f1_train: 0.1744 f1_val: 0.2218
Epoch: 0003 | loss_train: 1.7876 loss_val: 1.7955 | acc_train: 0.4286 acc_val: 0.4320 | f1_train: 0.3692 f1_val: 0.3254
Epoch: 0004 | loss_train: 1.6437 loss_val: 1.7320 | acc_train: 0.8571 acc_val: 0.5160 | f1_train: 0.8635 f1_val: 0.4798
Epoch: 0005 | loss_train: 1.4702 loss_val: 1.6537 | acc_train: 1.0000 acc_val: 0.5700 | f1_train: 1.0000 f1_val: 0.5447
Epoch: 0006 | loss_train: 1.2613 loss_val: 1.5588 | acc_train: 1.0000 acc_val: 0.6100 | f1_train: 1.0000 f1_val: 0.5921
Epoch: 0007 | loss_train: 1.0569 loss_val: 1.4513 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6100
Epoch: 0008 | loss_train: 0.8431 loss_val: 1.3427 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6210
Epoch: 0009 | loss_train: 0.6588 loss_val: 1.2409 | acc_train: 1.0000 acc_val: 0.6160 | f1_train: 1.0000 f1_val: 0.6264
Epoch: 0010 | loss_train: 0.4948 loss_val: 1.1581 | acc_train: 1.0000 acc_val: 0.6080 | f1_train: 1.0000 f1_val: 0.6203
Epoch: 0011 | loss_train: 0.3579 loss_val: 1.0976 | acc_train: 1.0000 acc_val: 0.5960 | f1_train: 1.0000 f1_val: 0.6154
Epoch: 0012 | loss_train: 0.2463 loss_val: 1.0632 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.6257
Epoch: 0013 | loss_train: 0.1699 loss_val: 1.0469 | acc_train: 1.0000 acc_val: 0.6040 | f1_train: 1.0000 f1_val: 0.6338
Epoch: 0014 | loss_train: 0.1129 loss_val: 1.0469 | acc_train: 1.0000 acc_val: 0.5980 | f1_train: 1.0000 f1_val: 0.6266
Epoch: 0015 | loss_train: 0.0748 loss_val: 1.0607 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.6289
Epoch: 0016 | loss_train: 0.0490 loss_val: 1.0862 | acc_train: 1.0000 acc_val: 0.6100 | f1_train: 1.0000 f1_val: 0.6364
Epoch: 0017 | loss_train: 0.0328 loss_val: 1.1194 | acc_train: 1.0000 acc_val: 0.6040 | f1_train: 1.0000 f1_val: 0.6227
Epoch: 0018 | loss_train: 0.0229 loss_val: 1.1583 | acc_train: 1.0000 acc_val: 0.6020 | f1_train: 1.0000 f1_val: 0.6160
Epoch: 0019 | loss_train: 0.0156 loss_val: 1.1996 | acc_train: 1.0000 acc_val: 0.6060 | f1_train: 1.0000 f1_val: 0.6228
Epoch: 0020 | loss_train: 0.0109 loss_val: 1.2431 | acc_train: 1.0000 acc_val: 0.6060 | f1_train: 1.0000 f1_val: 0.6217
Epoch: 0021 | loss_train: 0.0077 loss_val: 1.2875 | acc_train: 1.0000 acc_val: 0.6080 | f1_train: 1.0000 f1_val: 0.6240
Epoch: 0022 | loss_train: 0.0055 loss_val: 1.3323 | acc_train: 1.0000 acc_val: 0.6120 | f1_train: 1.0000 f1_val: 0.6240
Epoch: 0023 | loss_train: 0.0041 loss_val: 1.3763 | acc_train: 1.0000 acc_val: 0.6100 | f1_train: 1.0000 f1_val: 0.6212
Epoch: 0024 | loss_train: 0.0031 loss_val: 1.4194 | acc_train: 1.0000 acc_val: 0.6140 | f1_train: 1.0000 f1_val: 0.6243
Epoch: 0025 | loss_train: 0.0022 loss_val: 1.4614 | acc_train: 1.0000 acc_val: 0.6120 | f1_train: 1.0000 f1_val: 0.6188
Epoch: 0026 | loss_train: 0.0017 loss_val: 1.5025 | acc_train: 1.0000 acc_val: 0.6120 | f1_train: 1.0000 f1_val: 0.6186
Epoch: 0027 | loss_train: 0.0013 loss_val: 1.5419 | acc_train: 1.0000 acc_val: 0.6140 | f1_train: 1.0000 f1_val: 0.6214
Epoch: 0028 | loss_train: 0.0011 loss_val: 1.5800 | acc_train: 1.0000 acc_val: 0.6140 | f1_train: 1.0000 f1_val: 0.6215
Epoch: 0029 | loss_train: 0.0008 loss_val: 1.6165 | acc_train: 1.0000 acc_val: 0.6120 | f1_train: 1.0000 f1_val: 0.6203
Epoch: 0030 | loss_train: 0.0006 loss_val: 1.6514 | acc_train: 1.0000 acc_val: 0.6140 | f1_train: 1.0000 f1_val: 0.6219
Epoch: 0031 | loss_train: 0.0005 loss_val: 1.6849 | acc_train: 1.0000 acc_val: 0.6160 | f1_train: 1.0000 f1_val: 0.6262
Epoch: 0032 | loss_train: 0.0004 loss_val: 1.7169 | acc_train: 1.0000 acc_val: 0.6180 | f1_train: 1.0000 f1_val: 0.6280
Epoch: 0033 | loss_train: 0.0003 loss_val: 1.7473 | acc_train: 1.0000 acc_val: 0.6200 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0034 | loss_train: 0.0003 loss_val: 1.7765 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6341
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.8039 | acc_train: 1.0000 acc_val: 0.6260 | f1_train: 1.0000 f1_val: 0.6360
Epoch: 0036 | loss_train: 0.0002 loss_val: 1.8302 | acc_train: 1.0000 acc_val: 0.6280 | f1_train: 1.0000 f1_val: 0.6373
Epoch: 0037 | loss_train: 0.0002 loss_val: 1.8551 | acc_train: 1.0000 acc_val: 0.6280 | f1_train: 1.0000 f1_val: 0.6380
Epoch: 0038 | loss_train: 0.0002 loss_val: 1.8787 | acc_train: 1.0000 acc_val: 0.6280 | f1_train: 1.0000 f1_val: 0.6378
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9009 | acc_train: 1.0000 acc_val: 0.6300 | f1_train: 1.0000 f1_val: 0.6393
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9220 | acc_train: 1.0000 acc_val: 0.6300 | f1_train: 1.0000 f1_val: 0.6393
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9421 | acc_train: 1.0000 acc_val: 0.6300 | f1_train: 1.0000 f1_val: 0.6393
Epoch: 0042 | loss_train: 0.0001 loss_val: 1.9606 | acc_train: 1.0000 acc_val: 0.6300 | f1_train: 1.0000 f1_val: 0.6393
Epoch: 0043 | loss_train: 0.0001 loss_val: 1.9784 | acc_train: 1.0000 acc_val: 0.6260 | f1_train: 1.0000 f1_val: 0.6366
Epoch: 0044 | loss_train: 0.0001 loss_val: 1.9949 | acc_train: 1.0000 acc_val: 0.6260 | f1_train: 1.0000 f1_val: 0.6373
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.0109 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6358
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.0254 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6358
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.0392 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6358
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.0519 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6363
Epoch: 0049 | loss_train: 0.0001 loss_val: 2.0642 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6370
Epoch: 0050 | loss_train: 0.0000 loss_val: 2.0756 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6370
Epoch: 0051 | loss_train: 0.0000 loss_val: 2.0863 | acc_train: 1.0000 acc_val: 0.6220 | f1_train: 1.0000 f1_val: 0.6357
Epoch: 0052 | loss_train: 0.0000 loss_val: 2.0966 | acc_train: 1.0000 acc_val: 0.6220 | f1_train: 1.0000 f1_val: 0.6357
Epoch: 0053 | loss_train: 0.0000 loss_val: 2.1060 | acc_train: 1.0000 acc_val: 0.6220 | f1_train: 1.0000 f1_val: 0.6357
Epoch: 0054 | loss_train: 0.0000 loss_val: 2.1149 | acc_train: 1.0000 acc_val: 0.6220 | f1_train: 1.0000 f1_val: 0.6359
Epoch: 0055 | loss_train: 0.0000 loss_val: 2.1233 | acc_train: 1.0000 acc_val: 0.6220 | f1_train: 1.0000 f1_val: 0.6359
Epoch: 0056 | loss_train: 0.0000 loss_val: 2.1313 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6378
Epoch: 0057 | loss_train: 0.0000 loss_val: 2.1385 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6378
Epoch: 0058 | loss_train: 0.0000 loss_val: 2.1454 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6378
Epoch: 0059 | loss_train: 0.0000 loss_val: 2.1517 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6378
Epoch: 0060 | loss_train: 0.0000 loss_val: 2.1579 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6385
Epoch: 0061 | loss_train: 0.0000 loss_val: 2.1637 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6385
Epoch: 0062 | loss_train: 0.0000 loss_val: 2.1691 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6385
Epoch: 0063 | loss_train: 0.0000 loss_val: 2.1740 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6385
Epoch: 0064 | loss_train: 0.0000 loss_val: 2.1789 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6385
Epoch: 0065 | loss_train: 0.0000 loss_val: 2.1833 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6378
Epoch: 0066 | loss_train: 0.0000 loss_val: 2.1876 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6378
Epoch: 0067 | loss_train: 0.0000 loss_val: 2.1918 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6378
Epoch: 0068 | loss_train: 0.0000 loss_val: 2.1956 | acc_train: 1.0000 acc_val: 0.6260 | f1_train: 1.0000 f1_val: 0.6408
Epoch: 0069 | loss_train: 0.0000 loss_val: 2.1994 | acc_train: 1.0000 acc_val: 0.6260 | f1_train: 1.0000 f1_val: 0.6408
Epoch: 0070 | loss_train: 0.0000 loss_val: 2.2029 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0071 | loss_train: 0.0000 loss_val: 2.2062 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0072 | loss_train: 0.0000 loss_val: 2.2092 | acc_train: 1.0000 acc_val: 0.6240 | f1_train: 1.0000 f1_val: 0.6394
Optimization Finished!
Train cost: 10.7258s
Loading 39th epoch
Test set results: loss= 1.8606 accuracy= 0.6270

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=0.4, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9625 loss_val: 1.8715 | acc_train: 0.1607 acc_val: 0.3320 | f1_train: 0.1065 f1_val: 0.1779
Epoch: 0002 | loss_train: 1.9204 loss_val: 1.8282 | acc_train: 0.2679 acc_val: 0.4080 | f1_train: 0.1886 f1_val: 0.2327
Epoch: 0003 | loss_train: 1.8305 loss_val: 1.7645 | acc_train: 0.3929 acc_val: 0.4960 | f1_train: 0.3410 f1_val: 0.3875
Epoch: 0004 | loss_train: 1.6986 loss_val: 1.6828 | acc_train: 0.6429 acc_val: 0.6080 | f1_train: 0.6356 f1_val: 0.5663
Epoch: 0005 | loss_train: 1.5299 loss_val: 1.5825 | acc_train: 0.9286 acc_val: 0.6880 | f1_train: 0.9288 f1_val: 0.6551
Epoch: 0006 | loss_train: 1.3516 loss_val: 1.4647 | acc_train: 0.9286 acc_val: 0.7100 | f1_train: 0.9296 f1_val: 0.6904
Epoch: 0007 | loss_train: 1.1559 loss_val: 1.3331 | acc_train: 0.9643 acc_val: 0.7240 | f1_train: 0.9637 f1_val: 0.7132
Epoch: 0008 | loss_train: 0.9472 loss_val: 1.1985 | acc_train: 0.9643 acc_val: 0.7360 | f1_train: 0.9637 f1_val: 0.7264
Epoch: 0009 | loss_train: 0.7424 loss_val: 1.0795 | acc_train: 0.9821 acc_val: 0.7380 | f1_train: 0.9821 f1_val: 0.7374
Epoch: 0010 | loss_train: 0.5593 loss_val: 0.9873 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7367
Epoch: 0011 | loss_train: 0.3946 loss_val: 0.9200 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7351
Epoch: 0012 | loss_train: 0.2716 loss_val: 0.8662 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7274
Epoch: 0013 | loss_train: 0.1770 loss_val: 0.8285 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7360
Epoch: 0014 | loss_train: 0.1155 loss_val: 0.8098 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7353
Epoch: 0015 | loss_train: 0.0723 loss_val: 0.8081 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7454
Epoch: 0016 | loss_train: 0.0461 loss_val: 0.8209 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7423
Epoch: 0017 | loss_train: 0.0301 loss_val: 0.8452 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7423
Epoch: 0018 | loss_train: 0.0198 loss_val: 0.8768 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7423
Epoch: 0019 | loss_train: 0.0134 loss_val: 0.9132 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7443
Epoch: 0020 | loss_train: 0.0091 loss_val: 0.9521 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7441
Epoch: 0021 | loss_train: 0.0063 loss_val: 0.9919 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7445
Epoch: 0022 | loss_train: 0.0044 loss_val: 1.0318 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7462
Epoch: 0023 | loss_train: 0.0032 loss_val: 1.0712 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7450
Epoch: 0024 | loss_train: 0.0023 loss_val: 1.1095 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7439
Epoch: 0025 | loss_train: 0.0018 loss_val: 1.1464 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7439
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.1821 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7416
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.2162 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7459
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.2490 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7441
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.2801 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7430
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.3099 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7392
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.3380 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7397
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.3649 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7387
Epoch: 0033 | loss_train: 0.0003 loss_val: 1.3903 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7391
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.4144 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7349
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.4372 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7364
Epoch: 0036 | loss_train: 0.0002 loss_val: 1.4588 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7383
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.4790 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7410
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.4983 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7410
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.5163 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7450
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.5333 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7459
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.5491 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7468
Epoch: 0042 | loss_train: 0.0001 loss_val: 1.5642 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7470
Epoch: 0043 | loss_train: 0.0001 loss_val: 1.5781 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7470
Epoch: 0044 | loss_train: 0.0001 loss_val: 1.5911 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7470
Epoch: 0045 | loss_train: 0.0001 loss_val: 1.6035 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7470
Epoch: 0046 | loss_train: 0.0000 loss_val: 1.6148 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7470
Epoch: 0047 | loss_train: 0.0000 loss_val: 1.6256 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7500
Epoch: 0048 | loss_train: 0.0000 loss_val: 1.6354 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7521
Epoch: 0049 | loss_train: 0.0000 loss_val: 1.6446 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7521
Epoch: 0050 | loss_train: 0.0000 loss_val: 1.6534 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7521
Epoch: 0051 | loss_train: 0.0000 loss_val: 1.6613 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7521
Epoch: 0052 | loss_train: 0.0000 loss_val: 1.6688 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7545
Optimization Finished!
Train cost: 8.8306s
Loading 22th epoch
Test set results: loss= 0.9615 accuracy= 0.7590

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=0.6, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9637 loss_val: 1.8701 | acc_train: 0.1786 acc_val: 0.3320 | f1_train: 0.1240 f1_val: 0.1780
Epoch: 0002 | loss_train: 1.9191 loss_val: 1.8247 | acc_train: 0.2976 acc_val: 0.4000 | f1_train: 0.2173 f1_val: 0.2364
Epoch: 0003 | loss_train: 1.8324 loss_val: 1.7587 | acc_train: 0.3452 acc_val: 0.5180 | f1_train: 0.2851 f1_val: 0.4442
Epoch: 0004 | loss_train: 1.7166 loss_val: 1.6732 | acc_train: 0.6429 acc_val: 0.6380 | f1_train: 0.6441 f1_val: 0.5938
Epoch: 0005 | loss_train: 1.5524 loss_val: 1.5667 | acc_train: 0.8571 acc_val: 0.6860 | f1_train: 0.8609 f1_val: 0.6647
Epoch: 0006 | loss_train: 1.3756 loss_val: 1.4396 | acc_train: 0.9643 acc_val: 0.7380 | f1_train: 0.9641 f1_val: 0.7183
Epoch: 0007 | loss_train: 1.1706 loss_val: 1.2989 | acc_train: 0.9762 acc_val: 0.7420 | f1_train: 0.9760 f1_val: 0.7243
Epoch: 0008 | loss_train: 0.9689 loss_val: 1.1559 | acc_train: 0.9762 acc_val: 0.7460 | f1_train: 0.9760 f1_val: 0.7374
Epoch: 0009 | loss_train: 0.7625 loss_val: 1.0296 | acc_train: 0.9762 acc_val: 0.7560 | f1_train: 0.9760 f1_val: 0.7507
Epoch: 0010 | loss_train: 0.5801 loss_val: 0.9350 | acc_train: 0.9762 acc_val: 0.7420 | f1_train: 0.9760 f1_val: 0.7419
Epoch: 0011 | loss_train: 0.4119 loss_val: 0.8730 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7437
Epoch: 0012 | loss_train: 0.2847 loss_val: 0.8366 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7404
Epoch: 0013 | loss_train: 0.1853 loss_val: 0.8125 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7412
Epoch: 0014 | loss_train: 0.1201 loss_val: 0.8009 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7417
Epoch: 0015 | loss_train: 0.0748 loss_val: 0.8040 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7513
Epoch: 0016 | loss_train: 0.0462 loss_val: 0.8207 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7557
Epoch: 0017 | loss_train: 0.0299 loss_val: 0.8486 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7591
Epoch: 0018 | loss_train: 0.0191 loss_val: 0.8846 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7582
Epoch: 0019 | loss_train: 0.0128 loss_val: 0.9263 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7617
Epoch: 0020 | loss_train: 0.0086 loss_val: 0.9718 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7657
Epoch: 0021 | loss_train: 0.0058 loss_val: 1.0195 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7668
Epoch: 0022 | loss_train: 0.0040 loss_val: 1.0679 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7668
Epoch: 0023 | loss_train: 0.0029 loss_val: 1.1163 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7642
Epoch: 0024 | loss_train: 0.0021 loss_val: 1.1640 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7640
Epoch: 0025 | loss_train: 0.0015 loss_val: 1.2104 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7697
Epoch: 0026 | loss_train: 0.0011 loss_val: 1.2554 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7697
Epoch: 0027 | loss_train: 0.0009 loss_val: 1.2988 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7705
Epoch: 0028 | loss_train: 0.0007 loss_val: 1.3403 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7684
Epoch: 0029 | loss_train: 0.0005 loss_val: 1.3799 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7684
Epoch: 0030 | loss_train: 0.0004 loss_val: 1.4175 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7669
Epoch: 0031 | loss_train: 0.0003 loss_val: 1.4532 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.4869 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.5189 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.5490 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7672
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.5774 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7660
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.6041 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7689
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.6292 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7689
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.6528 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7689
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.6750 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7727
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.6958 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7730
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.7152 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7730
Epoch: 0042 | loss_train: 0.0001 loss_val: 1.7333 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7730
Epoch: 0043 | loss_train: 0.0001 loss_val: 1.7503 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7730
Epoch: 0044 | loss_train: 0.0000 loss_val: 1.7660 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7730
Epoch: 0045 | loss_train: 0.0000 loss_val: 1.7806 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7733
Epoch: 0046 | loss_train: 0.0000 loss_val: 1.7943 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7720
Epoch: 0047 | loss_train: 0.0000 loss_val: 1.8070 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7720
Epoch: 0048 | loss_train: 0.0000 loss_val: 1.8188 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7720
Epoch: 0049 | loss_train: 0.0000 loss_val: 1.8297 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7681
Epoch: 0050 | loss_train: 0.0000 loss_val: 1.8399 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7681
Epoch: 0051 | loss_train: 0.0000 loss_val: 1.8493 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7681
Epoch: 0052 | loss_train: 0.0000 loss_val: 1.8581 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7681
Epoch: 0053 | loss_train: 0.0000 loss_val: 1.8661 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7681
Epoch: 0054 | loss_train: 0.0000 loss_val: 1.8735 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7659
Epoch: 0055 | loss_train: 0.0000 loss_val: 1.8804 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7659
Epoch: 0056 | loss_train: 0.0000 loss_val: 1.8866 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7684
Epoch: 0057 | loss_train: 0.0000 loss_val: 1.8926 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7684
Optimization Finished!
Train cost: 7.5783s
Loading 25th epoch
Test set results: loss= 1.0694 accuracy= 0.7780

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=0.8, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9637 loss_val: 1.8696 | acc_train: 0.1518 acc_val: 0.3360 | f1_train: 0.1065 f1_val: 0.1797
Epoch: 0002 | loss_train: 1.9185 loss_val: 1.8228 | acc_train: 0.2232 acc_val: 0.4080 | f1_train: 0.1580 f1_val: 0.2484
Epoch: 0003 | loss_train: 1.8363 loss_val: 1.7544 | acc_train: 0.3036 acc_val: 0.5420 | f1_train: 0.2232 f1_val: 0.4719
Epoch: 0004 | loss_train: 1.7124 loss_val: 1.6668 | acc_train: 0.6429 acc_val: 0.6420 | f1_train: 0.6500 f1_val: 0.6043
Epoch: 0005 | loss_train: 1.5684 loss_val: 1.5590 | acc_train: 0.8125 acc_val: 0.6840 | f1_train: 0.8156 f1_val: 0.6528
Epoch: 0006 | loss_train: 1.3938 loss_val: 1.4298 | acc_train: 0.9107 acc_val: 0.7160 | f1_train: 0.9116 f1_val: 0.6938
Epoch: 0007 | loss_train: 1.2050 loss_val: 1.2816 | acc_train: 0.9643 acc_val: 0.7220 | f1_train: 0.9642 f1_val: 0.7023
Epoch: 0008 | loss_train: 0.9977 loss_val: 1.1316 | acc_train: 0.9554 acc_val: 0.7440 | f1_train: 0.9558 f1_val: 0.7304
Epoch: 0009 | loss_train: 0.8005 loss_val: 1.0000 | acc_train: 0.9643 acc_val: 0.7620 | f1_train: 0.9643 f1_val: 0.7527
Epoch: 0010 | loss_train: 0.6055 loss_val: 0.9011 | acc_train: 0.9732 acc_val: 0.7600 | f1_train: 0.9732 f1_val: 0.7499
Epoch: 0011 | loss_train: 0.4476 loss_val: 0.8394 | acc_train: 0.9911 acc_val: 0.7460 | f1_train: 0.9911 f1_val: 0.7378
Epoch: 0012 | loss_train: 0.3159 loss_val: 0.8102 | acc_train: 0.9911 acc_val: 0.7420 | f1_train: 0.9911 f1_val: 0.7353
Epoch: 0013 | loss_train: 0.2091 loss_val: 0.8001 | acc_train: 0.9911 acc_val: 0.7440 | f1_train: 0.9911 f1_val: 0.7380
Epoch: 0014 | loss_train: 0.1373 loss_val: 0.8043 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7459
Epoch: 0015 | loss_train: 0.0832 loss_val: 0.8209 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0016 | loss_train: 0.0505 loss_val: 0.8501 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7530
Epoch: 0017 | loss_train: 0.0305 loss_val: 0.8899 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7513
Epoch: 0018 | loss_train: 0.0193 loss_val: 0.9385 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7519
Epoch: 0019 | loss_train: 0.0126 loss_val: 0.9929 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7536
Epoch: 0020 | loss_train: 0.0084 loss_val: 1.0503 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7575
Epoch: 0021 | loss_train: 0.0058 loss_val: 1.1092 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7581
Epoch: 0022 | loss_train: 0.0040 loss_val: 1.1680 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7574
Epoch: 0023 | loss_train: 0.0028 loss_val: 1.2257 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7574
Epoch: 0024 | loss_train: 0.0021 loss_val: 1.2820 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0025 | loss_train: 0.0015 loss_val: 1.3365 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7521
Epoch: 0026 | loss_train: 0.0012 loss_val: 1.3893 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7454
Epoch: 0027 | loss_train: 0.0009 loss_val: 1.4401 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7480
Epoch: 0028 | loss_train: 0.0007 loss_val: 1.4892 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7477
Epoch: 0029 | loss_train: 0.0005 loss_val: 1.5361 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7459
Epoch: 0030 | loss_train: 0.0004 loss_val: 1.5811 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7459
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6241 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7470
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6649 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7455
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.7039 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7455
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7409 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7467
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7760 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7467
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.8092 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7449
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8405 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7449
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.8702 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7474
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.8980 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7474
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9241 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7477
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9486 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7477
Epoch: 0042 | loss_train: 0.0001 loss_val: 1.9716 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7458
Epoch: 0043 | loss_train: 0.0001 loss_val: 1.9930 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7458
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.0130 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7454
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.0317 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7438
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.0490 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7438
Epoch: 0047 | loss_train: 0.0000 loss_val: 2.0653 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7463
Epoch: 0048 | loss_train: 0.0000 loss_val: 2.0803 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7463
Epoch: 0049 | loss_train: 0.0000 loss_val: 2.0943 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7451
Epoch: 0050 | loss_train: 0.0000 loss_val: 2.1073 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7451
Epoch: 0051 | loss_train: 0.0000 loss_val: 2.1193 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7443
Epoch: 0052 | loss_train: 0.0000 loss_val: 2.1305 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7443
Epoch: 0053 | loss_train: 0.0000 loss_val: 2.1409 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7462
Optimization Finished!
Train cost: 9.5232s
Loading 21th epoch
Test set results: loss= 1.0000 accuracy= 0.7780

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9644 loss_val: 1.8685 | acc_train: 0.1500 acc_val: 0.3380 | f1_train: 0.0961 f1_val: 0.1805
Epoch: 0002 | loss_train: 1.9218 loss_val: 1.8201 | acc_train: 0.2429 acc_val: 0.4180 | f1_train: 0.1645 f1_val: 0.2550
Epoch: 0003 | loss_train: 1.8483 loss_val: 1.7494 | acc_train: 0.3143 acc_val: 0.5420 | f1_train: 0.2601 f1_val: 0.4715
Epoch: 0004 | loss_train: 1.7314 loss_val: 1.6593 | acc_train: 0.6143 acc_val: 0.6300 | f1_train: 0.6107 f1_val: 0.5859
Epoch: 0005 | loss_train: 1.5866 loss_val: 1.5494 | acc_train: 0.7643 acc_val: 0.6940 | f1_train: 0.7638 f1_val: 0.6637
Epoch: 0006 | loss_train: 1.4234 loss_val: 1.4177 | acc_train: 0.8429 acc_val: 0.7260 | f1_train: 0.8451 f1_val: 0.7061
Epoch: 0007 | loss_train: 1.2424 loss_val: 1.2689 | acc_train: 0.8929 acc_val: 0.7400 | f1_train: 0.8946 f1_val: 0.7335
Epoch: 0008 | loss_train: 1.0407 loss_val: 1.1175 | acc_train: 0.9214 acc_val: 0.7560 | f1_train: 0.9217 f1_val: 0.7481
Epoch: 0009 | loss_train: 0.8483 loss_val: 0.9828 | acc_train: 0.9500 acc_val: 0.7660 | f1_train: 0.9508 f1_val: 0.7615
Epoch: 0010 | loss_train: 0.6650 loss_val: 0.8742 | acc_train: 0.9429 acc_val: 0.7660 | f1_train: 0.9426 f1_val: 0.7614
Epoch: 0011 | loss_train: 0.5059 loss_val: 0.7956 | acc_train: 0.9714 acc_val: 0.7640 | f1_train: 0.9712 f1_val: 0.7592
Epoch: 0012 | loss_train: 0.3727 loss_val: 0.7420 | acc_train: 0.9786 acc_val: 0.7620 | f1_train: 0.9784 f1_val: 0.7596
Epoch: 0013 | loss_train: 0.2600 loss_val: 0.7074 | acc_train: 0.9857 acc_val: 0.7720 | f1_train: 0.9855 f1_val: 0.7715
Epoch: 0014 | loss_train: 0.1732 loss_val: 0.6932 | acc_train: 0.9929 acc_val: 0.7720 | f1_train: 0.9929 f1_val: 0.7727
Epoch: 0015 | loss_train: 0.1114 loss_val: 0.7023 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7773
Epoch: 0016 | loss_train: 0.0693 loss_val: 0.7350 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7769
Epoch: 0017 | loss_train: 0.0415 loss_val: 0.7858 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0018 | loss_train: 0.0253 loss_val: 0.8479 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7655
Epoch: 0019 | loss_train: 0.0163 loss_val: 0.9169 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0020 | loss_train: 0.0101 loss_val: 0.9886 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7511
Epoch: 0021 | loss_train: 0.0070 loss_val: 1.0602 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0022 | loss_train: 0.0047 loss_val: 1.1301 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7422
Epoch: 0023 | loss_train: 0.0033 loss_val: 1.1974 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7394
Epoch: 0024 | loss_train: 0.0024 loss_val: 1.2620 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7351
Epoch: 0025 | loss_train: 0.0017 loss_val: 1.3238 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7355
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.3828 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7391
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.4388 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7396
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.4920 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7426
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.5427 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7427
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5906 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7439
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6363 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7467
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6800 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7431
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.7215 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7610 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7452
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7988 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7492
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.8345 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7510
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8685 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.9008 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9313 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9601 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9873 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7542
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.0127 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7551
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.0368 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7504
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.0593 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7502
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.0804 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7507
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.0999 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7506
Optimization Finished!
Train cost: 9.7734s
Loading 15th epoch
Test set results: loss= 0.6384 accuracy= 0.7910

>>> run.py: Namespace(dataset='cora', device=1, experiment='training-data', log_path='log/nagphormer/cora/training-data', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/cora/training-data')

>>> Training split = 0.2

>>> Training split = 0.4

>>> Training split = 0.6

>>> Training split = 0.8

>>> Training split = 1.0

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=0.2, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.5832 loss_val: 1.5576 | acc_train: 0.2083 acc_val: 0.3000 | f1_train: 0.1625 f1_val: 0.1466
Epoch: 0002 | loss_train: 1.5163 loss_val: 1.4466 | acc_train: 0.4167 acc_val: 0.5500 | f1_train: 0.3361 f1_val: 0.2336
Epoch: 0003 | loss_train: 1.3525 loss_val: 1.3214 | acc_train: 0.6667 acc_val: 0.5750 | f1_train: 0.5904 f1_val: 0.2414
Epoch: 0004 | loss_train: 1.2126 loss_val: 1.2144 | acc_train: 0.6250 acc_val: 0.6125 | f1_train: 0.4590 f1_val: 0.2743
Epoch: 0005 | loss_train: 1.0671 loss_val: 1.1389 | acc_train: 0.7083 acc_val: 0.6250 | f1_train: 0.5133 f1_val: 0.3259
Epoch: 0006 | loss_train: 0.9348 loss_val: 1.0937 | acc_train: 0.7500 acc_val: 0.6375 | f1_train: 0.6935 f1_val: 0.3371
Epoch: 0007 | loss_train: 0.8063 loss_val: 1.0717 | acc_train: 0.8333 acc_val: 0.5750 | f1_train: 0.7462 f1_val: 0.3010
Epoch: 0008 | loss_train: 0.6750 loss_val: 1.0622 | acc_train: 0.8750 acc_val: 0.5875 | f1_train: 0.8270 f1_val: 0.3793
Epoch: 0009 | loss_train: 0.5522 loss_val: 1.0540 | acc_train: 0.8333 acc_val: 0.6000 | f1_train: 0.7994 f1_val: 0.4081
Epoch: 0010 | loss_train: 0.4311 loss_val: 1.0467 | acc_train: 0.9583 acc_val: 0.6125 | f1_train: 0.9511 f1_val: 0.4083
Epoch: 0011 | loss_train: 0.3317 loss_val: 1.0475 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4195
Epoch: 0012 | loss_train: 0.2453 loss_val: 1.0708 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4195
Epoch: 0013 | loss_train: 0.1796 loss_val: 1.1197 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4251
Epoch: 0014 | loss_train: 0.1300 loss_val: 1.1967 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4257
Epoch: 0015 | loss_train: 0.0933 loss_val: 1.3079 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4290
Epoch: 0016 | loss_train: 0.0572 loss_val: 1.4521 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4299
Epoch: 0017 | loss_train: 0.0380 loss_val: 1.6166 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4023
Epoch: 0018 | loss_train: 0.0245 loss_val: 1.7777 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4007
Epoch: 0019 | loss_train: 0.0162 loss_val: 1.9197 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.3947
Epoch: 0020 | loss_train: 0.0108 loss_val: 2.0321 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4003
Epoch: 0021 | loss_train: 0.0064 loss_val: 2.1285 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4003
Epoch: 0022 | loss_train: 0.0043 loss_val: 2.2133 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4003
Epoch: 0023 | loss_train: 0.0029 loss_val: 2.2873 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4063
Epoch: 0024 | loss_train: 0.0020 loss_val: 2.3549 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4132
Epoch: 0025 | loss_train: 0.0014 loss_val: 2.4199 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4352
Epoch: 0026 | loss_train: 0.0011 loss_val: 2.4823 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4352
Epoch: 0027 | loss_train: 0.0008 loss_val: 2.5430 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4352
Epoch: 0028 | loss_train: 0.0006 loss_val: 2.6022 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4352
Epoch: 0029 | loss_train: 0.0005 loss_val: 2.6585 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4355
Epoch: 0030 | loss_train: 0.0004 loss_val: 2.7136 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4355
Epoch: 0031 | loss_train: 0.0003 loss_val: 2.7666 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4355
Epoch: 0032 | loss_train: 0.0003 loss_val: 2.8178 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4195
Epoch: 0033 | loss_train: 0.0002 loss_val: 2.8671 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4195
Epoch: 0034 | loss_train: 0.0002 loss_val: 2.9148 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4199
Epoch: 0035 | loss_train: 0.0001 loss_val: 2.9606 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4075
Epoch: 0036 | loss_train: 0.0001 loss_val: 3.0047 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4075
Epoch: 0037 | loss_train: 0.0001 loss_val: 3.0475 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4075
Epoch: 0038 | loss_train: 0.0001 loss_val: 3.0889 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4015
Epoch: 0039 | loss_train: 0.0001 loss_val: 3.1290 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4015
Epoch: 0040 | loss_train: 0.0001 loss_val: 3.1672 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4015
Optimization Finished!
Train cost: 9.3526s
Loading 6th epoch
Test set results: loss= 1.1304 accuracy= 0.6078

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=0.4, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6058 loss_val: 1.5449 | acc_train: 0.1667 acc_val: 0.3000 | f1_train: 0.1215 f1_val: 0.1200
Epoch: 0002 | loss_train: 1.5331 loss_val: 1.4151 | acc_train: 0.3542 acc_val: 0.5000 | f1_train: 0.2159 f1_val: 0.1504
Epoch: 0003 | loss_train: 1.4050 loss_val: 1.2735 | acc_train: 0.4792 acc_val: 0.5375 | f1_train: 0.1917 f1_val: 0.1868
Epoch: 0004 | loss_train: 1.2583 loss_val: 1.1616 | acc_train: 0.5000 acc_val: 0.5375 | f1_train: 0.1979 f1_val: 0.1868
Epoch: 0005 | loss_train: 1.1281 loss_val: 1.0849 | acc_train: 0.5000 acc_val: 0.5875 | f1_train: 0.1979 f1_val: 0.2871
Epoch: 0006 | loss_train: 1.0247 loss_val: 1.0292 | acc_train: 0.5417 acc_val: 0.6250 | f1_train: 0.2646 f1_val: 0.3228
Epoch: 0007 | loss_train: 0.9123 loss_val: 0.9861 | acc_train: 0.7083 acc_val: 0.6875 | f1_train: 0.4258 f1_val: 0.3830
Epoch: 0008 | loss_train: 0.7878 loss_val: 0.9574 | acc_train: 0.8125 acc_val: 0.6375 | f1_train: 0.4914 f1_val: 0.3875
Epoch: 0009 | loss_train: 0.6768 loss_val: 0.9168 | acc_train: 0.7708 acc_val: 0.6500 | f1_train: 0.5507 f1_val: 0.3770
Epoch: 0010 | loss_train: 0.5707 loss_val: 0.8625 | acc_train: 0.8125 acc_val: 0.6875 | f1_train: 0.7756 f1_val: 0.4005
Epoch: 0011 | loss_train: 0.4448 loss_val: 0.8315 | acc_train: 0.8958 acc_val: 0.6875 | f1_train: 0.8265 f1_val: 0.3982
Epoch: 0012 | loss_train: 0.3598 loss_val: 0.8148 | acc_train: 0.9167 acc_val: 0.7000 | f1_train: 0.8417 f1_val: 0.5132
Epoch: 0013 | loss_train: 0.2870 loss_val: 0.7955 | acc_train: 0.9167 acc_val: 0.7250 | f1_train: 0.8417 f1_val: 0.5866
Epoch: 0014 | loss_train: 0.2243 loss_val: 0.7987 | acc_train: 0.9583 acc_val: 0.7500 | f1_train: 0.9490 f1_val: 0.5899
Epoch: 0015 | loss_train: 0.1752 loss_val: 0.8261 | acc_train: 0.9583 acc_val: 0.7625 | f1_train: 0.9490 f1_val: 0.6193
Epoch: 0016 | loss_train: 0.1277 loss_val: 0.8600 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.6206
Epoch: 0017 | loss_train: 0.0955 loss_val: 0.9018 | acc_train: 1.0000 acc_val: 0.7375 | f1_train: 1.0000 f1_val: 0.5629
Epoch: 0018 | loss_train: 0.0641 loss_val: 0.9677 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5695
Epoch: 0019 | loss_train: 0.0480 loss_val: 1.0383 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5695
Epoch: 0020 | loss_train: 0.0333 loss_val: 1.1011 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5775
Epoch: 0021 | loss_train: 0.0222 loss_val: 1.1724 | acc_train: 1.0000 acc_val: 0.7375 | f1_train: 1.0000 f1_val: 0.5398
Epoch: 0022 | loss_train: 0.0139 loss_val: 1.2875 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5322
Epoch: 0023 | loss_train: 0.0088 loss_val: 1.4211 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5311
Epoch: 0024 | loss_train: 0.0070 loss_val: 1.5618 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5319
Epoch: 0025 | loss_train: 0.0055 loss_val: 1.6876 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5319
Epoch: 0026 | loss_train: 0.0041 loss_val: 1.7918 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5319
Epoch: 0027 | loss_train: 0.0034 loss_val: 1.8724 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5319
Epoch: 0028 | loss_train: 0.0023 loss_val: 1.9393 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5319
Epoch: 0029 | loss_train: 0.0016 loss_val: 1.9992 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5319
Epoch: 0030 | loss_train: 0.0014 loss_val: 2.0540 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0031 | loss_train: 0.0010 loss_val: 2.1055 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0032 | loss_train: 0.0011 loss_val: 2.1545 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0033 | loss_train: 0.0010 loss_val: 2.2032 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0034 | loss_train: 0.0007 loss_val: 2.2520 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0035 | loss_train: 0.0006 loss_val: 2.3062 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0036 | loss_train: 0.0005 loss_val: 2.3613 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.4206 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.4791 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0039 | loss_train: 0.0002 loss_val: 2.5353 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0040 | loss_train: 0.0002 loss_val: 2.5900 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5241
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.6430 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.6946 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.7433 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.7889 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.8313 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.8703 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.9059 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.9383 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0049 | loss_train: 0.0001 loss_val: 2.9682 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Epoch: 0050 | loss_train: 0.0001 loss_val: 2.9951 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5183
Optimization Finished!
Train cost: 8.4047s
Loading 15th epoch
Test set results: loss= 1.1657 accuracy= 0.6078

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=0.6, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6148 loss_val: 1.5411 | acc_train: 0.1806 acc_val: 0.3250 | f1_train: 0.1415 f1_val: 0.1387
Epoch: 0002 | loss_train: 1.5458 loss_val: 1.4048 | acc_train: 0.3472 acc_val: 0.5375 | f1_train: 0.2004 f1_val: 0.1997
Epoch: 0003 | loss_train: 1.4135 loss_val: 1.2585 | acc_train: 0.5000 acc_val: 0.5500 | f1_train: 0.1993 f1_val: 0.2019
Epoch: 0004 | loss_train: 1.2637 loss_val: 1.1452 | acc_train: 0.5417 acc_val: 0.5875 | f1_train: 0.2338 f1_val: 0.2416
Epoch: 0005 | loss_train: 1.1453 loss_val: 1.0704 | acc_train: 0.5417 acc_val: 0.6125 | f1_train: 0.2338 f1_val: 0.2641
Epoch: 0006 | loss_train: 1.0380 loss_val: 1.0177 | acc_train: 0.5833 acc_val: 0.6375 | f1_train: 0.2642 f1_val: 0.3266
Epoch: 0007 | loss_train: 0.9317 loss_val: 0.9741 | acc_train: 0.5972 acc_val: 0.6750 | f1_train: 0.2732 f1_val: 0.3824
Epoch: 0008 | loss_train: 0.8228 loss_val: 0.9372 | acc_train: 0.7361 acc_val: 0.6250 | f1_train: 0.4301 f1_val: 0.3636
Epoch: 0009 | loss_train: 0.7079 loss_val: 0.8816 | acc_train: 0.7778 acc_val: 0.6625 | f1_train: 0.4886 f1_val: 0.3900
Epoch: 0010 | loss_train: 0.5996 loss_val: 0.8188 | acc_train: 0.7917 acc_val: 0.7125 | f1_train: 0.6170 f1_val: 0.4236
Epoch: 0011 | loss_train: 0.4904 loss_val: 0.7771 | acc_train: 0.8611 acc_val: 0.7250 | f1_train: 0.7175 f1_val: 0.4713
Epoch: 0012 | loss_train: 0.3976 loss_val: 0.7458 | acc_train: 0.9167 acc_val: 0.7375 | f1_train: 0.8208 f1_val: 0.5323
Epoch: 0013 | loss_train: 0.3154 loss_val: 0.7313 | acc_train: 0.9167 acc_val: 0.7500 | f1_train: 0.8625 f1_val: 0.5222
Epoch: 0014 | loss_train: 0.2536 loss_val: 0.7432 | acc_train: 0.9306 acc_val: 0.7750 | f1_train: 0.9011 f1_val: 0.6096
Epoch: 0015 | loss_train: 0.2065 loss_val: 0.7714 | acc_train: 0.9722 acc_val: 0.7250 | f1_train: 0.9620 f1_val: 0.5092
Epoch: 0016 | loss_train: 0.1633 loss_val: 0.8182 | acc_train: 0.9722 acc_val: 0.7500 | f1_train: 0.9620 f1_val: 0.5229
Epoch: 0017 | loss_train: 0.1232 loss_val: 0.8678 | acc_train: 0.9722 acc_val: 0.7500 | f1_train: 0.9608 f1_val: 0.5215
Epoch: 0018 | loss_train: 0.0863 loss_val: 0.9204 | acc_train: 0.9861 acc_val: 0.7375 | f1_train: 0.9746 f1_val: 0.5139
Epoch: 0019 | loss_train: 0.0669 loss_val: 0.9820 | acc_train: 0.9861 acc_val: 0.7500 | f1_train: 0.9746 f1_val: 0.5487
Epoch: 0020 | loss_train: 0.0490 loss_val: 1.0519 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5481
Epoch: 0021 | loss_train: 0.0329 loss_val: 1.1320 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5487
Epoch: 0022 | loss_train: 0.0210 loss_val: 1.2124 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5487
Epoch: 0023 | loss_train: 0.0142 loss_val: 1.2930 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5488
Epoch: 0024 | loss_train: 0.0094 loss_val: 1.3949 | acc_train: 1.0000 acc_val: 0.7375 | f1_train: 1.0000 f1_val: 0.5428
Epoch: 0025 | loss_train: 0.0072 loss_val: 1.4918 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5926
Epoch: 0026 | loss_train: 0.0063 loss_val: 1.5582 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5986
Epoch: 0027 | loss_train: 0.0050 loss_val: 1.6143 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5986
Epoch: 0028 | loss_train: 0.0032 loss_val: 1.6625 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5986
Epoch: 0029 | loss_train: 0.0022 loss_val: 1.7054 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5488
Epoch: 0030 | loss_train: 0.0016 loss_val: 1.7447 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5488
Epoch: 0031 | loss_train: 0.0015 loss_val: 1.7805 | acc_train: 1.0000 acc_val: 0.7375 | f1_train: 1.0000 f1_val: 0.5410
Epoch: 0032 | loss_train: 0.0009 loss_val: 1.8139 | acc_train: 1.0000 acc_val: 0.7375 | f1_train: 1.0000 f1_val: 0.5410
Epoch: 0033 | loss_train: 0.0007 loss_val: 1.8441 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5779
Epoch: 0034 | loss_train: 0.0006 loss_val: 1.8726 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5779
Epoch: 0035 | loss_train: 0.0005 loss_val: 1.8994 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5779
Epoch: 0036 | loss_train: 0.0004 loss_val: 1.9258 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0037 | loss_train: 0.0003 loss_val: 1.9525 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0038 | loss_train: 0.0003 loss_val: 1.9786 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0039 | loss_train: 0.0002 loss_val: 2.0037 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0040 | loss_train: 0.0002 loss_val: 2.0285 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.0526 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.0763 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.0989 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.1208 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0045 | loss_train: 0.0002 loss_val: 2.1415 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.1610 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.1794 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.1966 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0049 | loss_train: 0.0001 loss_val: 2.2126 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0050 | loss_train: 0.0001 loss_val: 2.2277 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0051 | loss_train: 0.0001 loss_val: 2.2420 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0052 | loss_train: 0.0001 loss_val: 2.2555 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0053 | loss_train: 0.0000 loss_val: 2.2688 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0054 | loss_train: 0.0000 loss_val: 2.2815 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0055 | loss_train: 0.0000 loss_val: 2.2934 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0056 | loss_train: 0.0000 loss_val: 2.3049 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0057 | loss_train: 0.0000 loss_val: 2.3157 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0058 | loss_train: 0.0000 loss_val: 2.3260 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0059 | loss_train: 0.0000 loss_val: 2.3358 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0060 | loss_train: 0.0000 loss_val: 2.3448 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0061 | loss_train: 0.0000 loss_val: 2.3536 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0062 | loss_train: 0.0000 loss_val: 2.3618 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0063 | loss_train: 0.0000 loss_val: 2.3696 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0064 | loss_train: 0.0000 loss_val: 2.3768 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0065 | loss_train: 0.0000 loss_val: 2.3837 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0066 | loss_train: 0.0000 loss_val: 2.3901 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0067 | loss_train: 0.0000 loss_val: 2.3963 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.5910
Epoch: 0068 | loss_train: 0.0000 loss_val: 2.4018 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0069 | loss_train: 0.0000 loss_val: 2.4069 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0070 | loss_train: 0.0000 loss_val: 2.4120 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0071 | loss_train: 0.0000 loss_val: 2.4167 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0072 | loss_train: 0.0000 loss_val: 2.4210 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5831
Epoch: 0073 | loss_train: 0.0000 loss_val: 2.4248 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0074 | loss_train: 0.0000 loss_val: 2.4286 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0075 | loss_train: 0.0000 loss_val: 2.4321 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0076 | loss_train: 0.0000 loss_val: 2.4355 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0077 | loss_train: 0.0000 loss_val: 2.4385 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0078 | loss_train: 0.0000 loss_val: 2.4415 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0079 | loss_train: 0.0000 loss_val: 2.4442 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0080 | loss_train: 0.0000 loss_val: 2.4467 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0081 | loss_train: 0.0000 loss_val: 2.4491 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0082 | loss_train: 0.0000 loss_val: 2.4512 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0083 | loss_train: 0.0000 loss_val: 2.4532 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0084 | loss_train: 0.0000 loss_val: 2.4551 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0085 | loss_train: 0.0000 loss_val: 2.4570 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0086 | loss_train: 0.0000 loss_val: 2.4587 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0087 | loss_train: 0.0000 loss_val: 2.4603 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0088 | loss_train: 0.0000 loss_val: 2.4619 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0089 | loss_train: 0.0000 loss_val: 2.4634 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0090 | loss_train: 0.0000 loss_val: 2.4647 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0091 | loss_train: 0.0000 loss_val: 2.4659 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0092 | loss_train: 0.0000 loss_val: 2.4670 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0093 | loss_train: 0.0000 loss_val: 2.4682 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0094 | loss_train: 0.0000 loss_val: 2.4693 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0095 | loss_train: 0.0000 loss_val: 2.4703 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0096 | loss_train: 0.0000 loss_val: 2.4713 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Epoch: 0097 | loss_train: 0.0000 loss_val: 2.4723 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5842
Optimization Finished!
Train cost: 11.2358s
Loading 57th epoch
Test set results: loss= 3.2183 accuracy= 0.6471

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=0.8, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6143 loss_val: 1.5378 | acc_train: 0.1562 acc_val: 0.3250 | f1_train: 0.1203 f1_val: 0.1387
Epoch: 0002 | loss_train: 1.5382 loss_val: 1.3967 | acc_train: 0.3750 acc_val: 0.5125 | f1_train: 0.1919 f1_val: 0.1681
Epoch: 0003 | loss_train: 1.4032 loss_val: 1.2494 | acc_train: 0.5000 acc_val: 0.5500 | f1_train: 0.1789 f1_val: 0.2019
Epoch: 0004 | loss_train: 1.2641 loss_val: 1.1383 | acc_train: 0.4896 acc_val: 0.5625 | f1_train: 0.1660 f1_val: 0.2161
Epoch: 0005 | loss_train: 1.1398 loss_val: 1.0659 | acc_train: 0.5417 acc_val: 0.6000 | f1_train: 0.2186 f1_val: 0.2532
Epoch: 0006 | loss_train: 1.0470 loss_val: 1.0126 | acc_train: 0.5625 acc_val: 0.6375 | f1_train: 0.2364 f1_val: 0.2843
Epoch: 0007 | loss_train: 0.9573 loss_val: 0.9691 | acc_train: 0.6042 acc_val: 0.6625 | f1_train: 0.2671 f1_val: 0.3030
Epoch: 0008 | loss_train: 0.8596 loss_val: 0.9346 | acc_train: 0.6771 acc_val: 0.6625 | f1_train: 0.3499 f1_val: 0.3642
Epoch: 0009 | loss_train: 0.7723 loss_val: 0.8869 | acc_train: 0.7396 acc_val: 0.6750 | f1_train: 0.4134 f1_val: 0.3961
Epoch: 0010 | loss_train: 0.6714 loss_val: 0.8157 | acc_train: 0.7812 acc_val: 0.7125 | f1_train: 0.6050 f1_val: 0.4158
Epoch: 0011 | loss_train: 0.5709 loss_val: 0.7498 | acc_train: 0.8125 acc_val: 0.7750 | f1_train: 0.6103 f1_val: 0.5075
Epoch: 0012 | loss_train: 0.4736 loss_val: 0.6994 | acc_train: 0.8542 acc_val: 0.7750 | f1_train: 0.7862 f1_val: 0.5268
Epoch: 0013 | loss_train: 0.3898 loss_val: 0.6775 | acc_train: 0.9167 acc_val: 0.8000 | f1_train: 0.8969 f1_val: 0.6376
Epoch: 0014 | loss_train: 0.3267 loss_val: 0.6931 | acc_train: 0.9375 acc_val: 0.7750 | f1_train: 0.9318 f1_val: 0.6075
Epoch: 0015 | loss_train: 0.2707 loss_val: 0.7237 | acc_train: 0.9271 acc_val: 0.7375 | f1_train: 0.9042 f1_val: 0.5440
Epoch: 0016 | loss_train: 0.2140 loss_val: 0.7725 | acc_train: 0.9375 acc_val: 0.7375 | f1_train: 0.9191 f1_val: 0.5440
Epoch: 0017 | loss_train: 0.1595 loss_val: 0.8389 | acc_train: 0.9583 acc_val: 0.7500 | f1_train: 0.9396 f1_val: 0.5487
Epoch: 0018 | loss_train: 0.1262 loss_val: 0.9170 | acc_train: 0.9792 acc_val: 0.7625 | f1_train: 0.9728 f1_val: 0.5775
Epoch: 0019 | loss_train: 0.0991 loss_val: 0.9821 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5892
Epoch: 0020 | loss_train: 0.0740 loss_val: 1.0466 | acc_train: 0.9896 acc_val: 0.7500 | f1_train: 0.9896 f1_val: 0.5851
Epoch: 0021 | loss_train: 0.0479 loss_val: 1.1573 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.6063
Epoch: 0022 | loss_train: 0.0324 loss_val: 1.2891 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5968
Epoch: 0023 | loss_train: 0.0259 loss_val: 1.3822 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.5270
Epoch: 0024 | loss_train: 0.0180 loss_val: 1.4320 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5968
Epoch: 0025 | loss_train: 0.0118 loss_val: 1.4680 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.5891
Epoch: 0026 | loss_train: 0.0082 loss_val: 1.5108 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5988
Epoch: 0027 | loss_train: 0.0062 loss_val: 1.5618 | acc_train: 1.0000 acc_val: 0.7750 | f1_train: 1.0000 f1_val: 0.5988
Epoch: 0028 | loss_train: 0.0047 loss_val: 1.6154 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.6265
Epoch: 0029 | loss_train: 0.0034 loss_val: 1.6692 | acc_train: 1.0000 acc_val: 0.7875 | f1_train: 1.0000 f1_val: 0.6265
Epoch: 0030 | loss_train: 0.0024 loss_val: 1.7218 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6329
Epoch: 0031 | loss_train: 0.0016 loss_val: 1.7720 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0032 | loss_train: 0.0011 loss_val: 1.8199 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0033 | loss_train: 0.0009 loss_val: 1.8649 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0034 | loss_train: 0.0010 loss_val: 1.9085 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0035 | loss_train: 0.0008 loss_val: 1.9512 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0036 | loss_train: 0.0007 loss_val: 1.9932 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.0325 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6394
Epoch: 0038 | loss_train: 0.0004 loss_val: 2.0690 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6239
Epoch: 0039 | loss_train: 0.0003 loss_val: 2.1022 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6239
Epoch: 0040 | loss_train: 0.0005 loss_val: 2.1329 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6239
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.1605 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6239
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.1856 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6239
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.2086 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6239
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.2297 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6239
Epoch: 0045 | loss_train: 0.0002 loss_val: 2.2489 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0046 | loss_train: 0.0002 loss_val: 2.2666 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.2826 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.2974 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0049 | loss_train: 0.0001 loss_val: 2.3112 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0050 | loss_train: 0.0001 loss_val: 2.3239 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0051 | loss_train: 0.0001 loss_val: 2.3360 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0052 | loss_train: 0.0001 loss_val: 2.3471 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0053 | loss_train: 0.0001 loss_val: 2.3578 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0054 | loss_train: 0.0001 loss_val: 2.3680 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0055 | loss_train: 0.0001 loss_val: 2.3775 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0056 | loss_train: 0.0001 loss_val: 2.3866 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0057 | loss_train: 0.0001 loss_val: 2.3952 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0058 | loss_train: 0.0001 loss_val: 2.4034 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0059 | loss_train: 0.0001 loss_val: 2.4111 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0060 | loss_train: 0.0000 loss_val: 2.4185 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0061 | loss_train: 0.0000 loss_val: 2.4255 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0062 | loss_train: 0.0000 loss_val: 2.4322 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0063 | loss_train: 0.0000 loss_val: 2.4386 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0064 | loss_train: 0.0000 loss_val: 2.4447 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0065 | loss_train: 0.0000 loss_val: 2.4504 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0066 | loss_train: 0.0000 loss_val: 2.4560 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0067 | loss_train: 0.0000 loss_val: 2.4612 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0068 | loss_train: 0.0000 loss_val: 2.4661 | acc_train: 1.0000 acc_val: 0.8125 | f1_train: 1.0000 f1_val: 0.6304
Epoch: 0069 | loss_train: 0.0000 loss_val: 2.4710 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0070 | loss_train: 0.0000 loss_val: 2.4755 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0071 | loss_train: 0.0000 loss_val: 2.4799 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0072 | loss_train: 0.0000 loss_val: 2.4841 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0073 | loss_train: 0.0000 loss_val: 2.4883 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0074 | loss_train: 0.0000 loss_val: 2.4920 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0075 | loss_train: 0.0000 loss_val: 2.4959 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0076 | loss_train: 0.0000 loss_val: 2.4995 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0077 | loss_train: 0.0000 loss_val: 2.5034 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0078 | loss_train: 0.0000 loss_val: 2.5069 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0079 | loss_train: 0.0000 loss_val: 2.5103 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0080 | loss_train: 0.0000 loss_val: 2.5137 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0081 | loss_train: 0.0000 loss_val: 2.5170 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0082 | loss_train: 0.0000 loss_val: 2.5203 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0083 | loss_train: 0.0000 loss_val: 2.5235 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0084 | loss_train: 0.0000 loss_val: 2.5268 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0085 | loss_train: 0.0000 loss_val: 2.5301 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0086 | loss_train: 0.0000 loss_val: 2.5333 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0087 | loss_train: 0.0000 loss_val: 2.5365 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0088 | loss_train: 0.0000 loss_val: 2.5396 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0089 | loss_train: 0.0000 loss_val: 2.5429 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0090 | loss_train: 0.0000 loss_val: 2.5459 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0091 | loss_train: 0.0000 loss_val: 2.5490 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0092 | loss_train: 0.0000 loss_val: 2.5521 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0093 | loss_train: 0.0000 loss_val: 2.5551 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0094 | loss_train: 0.0000 loss_val: 2.5580 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0095 | loss_train: 0.0000 loss_val: 2.5610 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0096 | loss_train: 0.0000 loss_val: 2.5640 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0097 | loss_train: 0.0000 loss_val: 2.5668 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Epoch: 0098 | loss_train: 0.0000 loss_val: 2.5697 | acc_train: 1.0000 acc_val: 0.8000 | f1_train: 1.0000 f1_val: 0.6052
Optimization Finished!
Train cost: 11.0778s
Loading 31th epoch
Test set results: loss= 1.6146 accuracy= 0.7843

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/training-data', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6315 loss_val: 1.5466 | acc_train: 0.1500 acc_val: 0.3000 | f1_train: 0.1211 f1_val: 0.1308
Epoch: 0002 | loss_train: 1.5652 loss_val: 1.4203 | acc_train: 0.2833 acc_val: 0.5000 | f1_train: 0.1653 f1_val: 0.1644
Epoch: 0003 | loss_train: 1.4443 loss_val: 1.2855 | acc_train: 0.4333 acc_val: 0.5125 | f1_train: 0.1550 f1_val: 0.1526
Epoch: 0004 | loss_train: 1.3147 loss_val: 1.1833 | acc_train: 0.4333 acc_val: 0.5750 | f1_train: 0.1390 f1_val: 0.2358
Epoch: 0005 | loss_train: 1.2048 loss_val: 1.1209 | acc_train: 0.4583 acc_val: 0.6125 | f1_train: 0.1674 f1_val: 0.2715
Epoch: 0006 | loss_train: 1.1178 loss_val: 1.0844 | acc_train: 0.5333 acc_val: 0.5625 | f1_train: 0.2372 f1_val: 0.2529
Epoch: 0007 | loss_train: 1.0281 loss_val: 1.0654 | acc_train: 0.5750 acc_val: 0.5500 | f1_train: 0.2665 f1_val: 0.2474
Epoch: 0008 | loss_train: 0.9321 loss_val: 1.0565 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3003 f1_val: 0.2557
Epoch: 0009 | loss_train: 0.8323 loss_val: 1.0307 | acc_train: 0.6667 acc_val: 0.6125 | f1_train: 0.3606 f1_val: 0.3566
Epoch: 0010 | loss_train: 0.7215 loss_val: 0.9827 | acc_train: 0.7583 acc_val: 0.6000 | f1_train: 0.5366 f1_val: 0.3628
Epoch: 0011 | loss_train: 0.6194 loss_val: 0.9135 | acc_train: 0.8167 acc_val: 0.6500 | f1_train: 0.7483 f1_val: 0.4362
Epoch: 0012 | loss_train: 0.5123 loss_val: 0.8581 | acc_train: 0.8833 acc_val: 0.6500 | f1_train: 0.8614 f1_val: 0.4708
Epoch: 0013 | loss_train: 0.4123 loss_val: 0.8767 | acc_train: 0.9167 acc_val: 0.7000 | f1_train: 0.8844 f1_val: 0.5609
Epoch: 0014 | loss_train: 0.3231 loss_val: 0.9327 | acc_train: 0.9500 acc_val: 0.7000 | f1_train: 0.9142 f1_val: 0.5516
Epoch: 0015 | loss_train: 0.2508 loss_val: 0.9685 | acc_train: 0.9500 acc_val: 0.6875 | f1_train: 0.9209 f1_val: 0.4886
Epoch: 0016 | loss_train: 0.1914 loss_val: 1.0292 | acc_train: 0.9583 acc_val: 0.7000 | f1_train: 0.9259 f1_val: 0.4882
Epoch: 0017 | loss_train: 0.1423 loss_val: 1.1019 | acc_train: 0.9667 acc_val: 0.7000 | f1_train: 0.9297 f1_val: 0.4825
Epoch: 0018 | loss_train: 0.1004 loss_val: 1.1785 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9694 f1_val: 0.4658
Epoch: 0019 | loss_train: 0.0704 loss_val: 1.2618 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4528
Epoch: 0020 | loss_train: 0.0536 loss_val: 1.3591 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0021 | loss_train: 0.0349 loss_val: 1.4517 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0022 | loss_train: 0.0250 loss_val: 1.5643 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0023 | loss_train: 0.0185 loss_val: 1.6873 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0024 | loss_train: 0.0115 loss_val: 1.7941 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0025 | loss_train: 0.0134 loss_val: 1.8016 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0026 | loss_train: 0.0071 loss_val: 1.8700 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0027 | loss_train: 0.0042 loss_val: 1.9669 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4397
Epoch: 0028 | loss_train: 0.0032 loss_val: 2.0669 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4652
Epoch: 0029 | loss_train: 0.0030 loss_val: 2.1614 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0030 | loss_train: 0.0027 loss_val: 2.2514 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4694
Epoch: 0031 | loss_train: 0.0019 loss_val: 2.3389 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0032 | loss_train: 0.0016 loss_val: 2.4217 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0033 | loss_train: 0.0012 loss_val: 2.4973 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0034 | loss_train: 0.0011 loss_val: 2.5645 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0035 | loss_train: 0.0008 loss_val: 2.6212 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.6685 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.7098 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.7466 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0039 | loss_train: 0.0004 loss_val: 2.7690 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.7881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.8042 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.8187 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.8326 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.8465 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.8602 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.8742 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.8881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4756
Optimization Finished!
Train cost: 9.6256s
Loading 13th epoch
Test set results: loss= 1.1536 accuracy= 0.5686

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='training-data', log_path='log/nagphormer/wisconsin/training-data', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/wisconsin/training-data')

>>> Training split = 0.2

>>> Training split = 0.4

>>> Training split = 0.6

>>> Training split = 0.8

>>> Training split = 1.0

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=0.2, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9383, Val loss: 1.9455 | Train F1: 0.09, Val F1: 0.04
Epoch: 001 | Train loss: 1.9303, Val loss: 1.9442 | Train F1: 0.52, Val F1: 0.10
Epoch: 002 | Train loss: 1.9211, Val loss: 1.9427 | Train F1: 0.75, Val F1: 0.19
Epoch: 003 | Train loss: 1.9101, Val loss: 1.9409 | Train F1: 0.93, Val F1: 0.28
Epoch: 004 | Train loss: 1.8969, Val loss: 1.9387 | Train F1: 1.00, Val F1: 0.34
Epoch: 005 | Train loss: 1.8809, Val loss: 1.9359 | Train F1: 1.00, Val F1: 0.39
Epoch: 006 | Train loss: 1.8615, Val loss: 1.9327 | Train F1: 1.00, Val F1: 0.45
Epoch: 007 | Train loss: 1.8379, Val loss: 1.9287 | Train F1: 1.00, Val F1: 0.48
Epoch: 008 | Train loss: 1.8094, Val loss: 1.9241 | Train F1: 1.00, Val F1: 0.50
Epoch: 009 | Train loss: 1.7755, Val loss: 1.9185 | Train F1: 1.00, Val F1: 0.53
Epoch: 010 | Train loss: 1.7358, Val loss: 1.9118 | Train F1: 1.00, Val F1: 0.55
Epoch: 011 | Train loss: 1.6904, Val loss: 1.9038 | Train F1: 1.00, Val F1: 0.56
Epoch: 012 | Train loss: 1.6405, Val loss: 1.8943 | Train F1: 1.00, Val F1: 0.57
Epoch: 013 | Train loss: 1.5875, Val loss: 1.8831 | Train F1: 1.00, Val F1: 0.58
Epoch: 014 | Train loss: 1.5335, Val loss: 1.8699 | Train F1: 1.00, Val F1: 0.59
Epoch: 015 | Train loss: 1.4803, Val loss: 1.8548 | Train F1: 1.00, Val F1: 0.59
Epoch: 016 | Train loss: 1.4293, Val loss: 1.8379 | Train F1: 1.00, Val F1: 0.60
Epoch: 017 | Train loss: 1.3820, Val loss: 1.8196 | Train F1: 1.00, Val F1: 0.61
Epoch: 018 | Train loss: 1.3390, Val loss: 1.8002 | Train F1: 1.00, Val F1: 0.61
Epoch: 019 | Train loss: 1.3007, Val loss: 1.7804 | Train F1: 1.00, Val F1: 0.61
Epoch: 020 | Train loss: 1.2673, Val loss: 1.7608 | Train F1: 1.00, Val F1: 0.62
Epoch: 021 | Train loss: 1.2393, Val loss: 1.7416 | Train F1: 1.00, Val F1: 0.63
Epoch: 022 | Train loss: 1.2169, Val loss: 1.7233 | Train F1: 1.00, Val F1: 0.63
Epoch: 023 | Train loss: 1.2001, Val loss: 1.7060 | Train F1: 1.00, Val F1: 0.63
Epoch: 024 | Train loss: 1.1882, Val loss: 1.6900 | Train F1: 1.00, Val F1: 0.63
Epoch: 025 | Train loss: 1.1802, Val loss: 1.6758 | Train F1: 1.00, Val F1: 0.63
Epoch: 026 | Train loss: 1.1751, Val loss: 1.6632 | Train F1: 1.00, Val F1: 0.63
Epoch: 027 | Train loss: 1.1717, Val loss: 1.6524 | Train F1: 1.00, Val F1: 0.62
Epoch: 028 | Train loss: 1.1696, Val loss: 1.6433 | Train F1: 1.00, Val F1: 0.62
Epoch: 029 | Train loss: 1.1683, Val loss: 1.6357 | Train F1: 1.00, Val F1: 0.62
Epoch: 030 | Train loss: 1.1674, Val loss: 1.6292 | Train F1: 1.00, Val F1: 0.62
Epoch: 031 | Train loss: 1.1669, Val loss: 1.6239 | Train F1: 1.00, Val F1: 0.62
Epoch: 032 | Train loss: 1.1665, Val loss: 1.6194 | Train F1: 1.00, Val F1: 0.62
Epoch: 033 | Train loss: 1.1662, Val loss: 1.6157 | Train F1: 1.00, Val F1: 0.61
Epoch: 034 | Train loss: 1.1660, Val loss: 1.6126 | Train F1: 1.00, Val F1: 0.61
Epoch: 035 | Train loss: 1.1659, Val loss: 1.6101 | Train F1: 1.00, Val F1: 0.60
Epoch: 036 | Train loss: 1.1658, Val loss: 1.6080 | Train F1: 1.00, Val F1: 0.60
Epoch: 037 | Train loss: 1.1657, Val loss: 1.6062 | Train F1: 1.00, Val F1: 0.60
Epoch: 038 | Train loss: 1.1657, Val loss: 1.6047 | Train F1: 1.00, Val F1: 0.59
Epoch: 039 | Train loss: 1.1656, Val loss: 1.6035 | Train F1: 1.00, Val F1: 0.59
Epoch: 040 | Train loss: 1.1656, Val loss: 1.6024 | Train F1: 1.00, Val F1: 0.59
Epoch: 041 | Train loss: 1.1656, Val loss: 1.6015 | Train F1: 1.00, Val F1: 0.59
Epoch: 042 | Train loss: 1.1656, Val loss: 1.6007 | Train F1: 1.00, Val F1: 0.59
Epoch: 043 | Train loss: 1.1656, Val loss: 1.6001 | Train F1: 1.00, Val F1: 0.59
Epoch: 044 | Train loss: 1.1655, Val loss: 1.5995 | Train F1: 1.00, Val F1: 0.58
Epoch: 045 | Train loss: 1.1655, Val loss: 1.5990 | Train F1: 1.00, Val F1: 0.58
Epoch: 046 | Train loss: 1.1655, Val loss: 1.5986 | Train F1: 1.00, Val F1: 0.59
Epoch: 047 | Train loss: 1.1655, Val loss: 1.5983 | Train F1: 1.00, Val F1: 0.58
Epoch: 048 | Train loss: 1.1655, Val loss: 1.5980 | Train F1: 1.00, Val F1: 0.59
Epoch: 049 | Train loss: 1.1655, Val loss: 1.5977 | Train F1: 1.00, Val F1: 0.58
Epoch: 050 | Train loss: 1.1655, Val loss: 1.5975 | Train F1: 1.00, Val F1: 0.58
Epoch: 051 | Train loss: 1.1655, Val loss: 1.5972 | Train F1: 1.00, Val F1: 0.58
Epoch: 052 | Train loss: 1.1655, Val loss: 1.5970 | Train F1: 1.00, Val F1: 0.59
Epoch: 053 | Train loss: 1.1655, Val loss: 1.5969 | Train F1: 1.00, Val F1: 0.58
Epoch: 054 | Train loss: 1.1655, Val loss: 1.5967 | Train F1: 1.00, Val F1: 0.58
Epoch: 055 | Train loss: 1.1655, Val loss: 1.5966 | Train F1: 1.00, Val F1: 0.58
Epoch: 056 | Train loss: 1.1655, Val loss: 1.5964 | Train F1: 1.00, Val F1: 0.58
Epoch: 057 | Train loss: 1.1655, Val loss: 1.5963 | Train F1: 1.00, Val F1: 0.58
Epoch: 058 | Train loss: 1.1655, Val loss: 1.5962 | Train F1: 1.00, Val F1: 0.58
Epoch: 059 | Train loss: 1.1655, Val loss: 1.5961 | Train F1: 1.00, Val F1: 0.58
Epoch: 060 | Train loss: 1.1655, Val loss: 1.5960 | Train F1: 1.00, Val F1: 0.58
Epoch: 061 | Train loss: 1.1655, Val loss: 1.5959 | Train F1: 1.00, Val F1: 0.58
Epoch: 062 | Train loss: 1.1655, Val loss: 1.5958 | Train F1: 1.00, Val F1: 0.58
Epoch: 063 | Train loss: 1.1655, Val loss: 1.5958 | Train F1: 1.00, Val F1: 0.58
Epoch: 064 | Train loss: 1.1655, Val loss: 1.5957 | Train F1: 1.00, Val F1: 0.58
Epoch: 065 | Train loss: 1.1655, Val loss: 1.5956 | Train F1: 1.00, Val F1: 0.58
Epoch: 066 | Train loss: 1.1655, Val loss: 1.5956 | Train F1: 1.00, Val F1: 0.58
Epoch: 067 | Train loss: 1.1655, Val loss: 1.5955 | Train F1: 1.00, Val F1: 0.58
Epoch: 068 | Train loss: 1.1655, Val loss: 1.5955 | Train F1: 1.00, Val F1: 0.58
Epoch: 069 | Train loss: 1.1655, Val loss: 1.5954 | Train F1: 1.00, Val F1: 0.58
Epoch: 070 | Train loss: 1.1655, Val loss: 1.5954 | Train F1: 1.00, Val F1: 0.58
Best model:
Train loss: 1.1656, Val loss: 1.6024, Test loss: 1.6024
Train F1: 1.00, Val F1: 0.59, Test F1: 0.59

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=0.4, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9399, Val loss: 1.9439 | Train F1: 0.31, Val F1: 0.13
Epoch: 001 | Train loss: 1.9320, Val loss: 1.9412 | Train F1: 0.57, Val F1: 0.27
Epoch: 002 | Train loss: 1.9230, Val loss: 1.9382 | Train F1: 0.84, Val F1: 0.39
Epoch: 003 | Train loss: 1.9123, Val loss: 1.9346 | Train F1: 0.98, Val F1: 0.46
Epoch: 004 | Train loss: 1.8994, Val loss: 1.9304 | Train F1: 1.00, Val F1: 0.52
Epoch: 005 | Train loss: 1.8836, Val loss: 1.9254 | Train F1: 1.00, Val F1: 0.58
Epoch: 006 | Train loss: 1.8644, Val loss: 1.9192 | Train F1: 1.00, Val F1: 0.62
Epoch: 007 | Train loss: 1.8410, Val loss: 1.9118 | Train F1: 1.00, Val F1: 0.67
Epoch: 008 | Train loss: 1.8127, Val loss: 1.9028 | Train F1: 1.00, Val F1: 0.68
Epoch: 009 | Train loss: 1.7789, Val loss: 1.8921 | Train F1: 1.00, Val F1: 0.69
Epoch: 010 | Train loss: 1.7390, Val loss: 1.8795 | Train F1: 1.00, Val F1: 0.70
Epoch: 011 | Train loss: 1.6932, Val loss: 1.8645 | Train F1: 1.00, Val F1: 0.71
Epoch: 012 | Train loss: 1.6420, Val loss: 1.8470 | Train F1: 1.00, Val F1: 0.71
Epoch: 013 | Train loss: 1.5871, Val loss: 1.8271 | Train F1: 1.00, Val F1: 0.72
Epoch: 014 | Train loss: 1.5304, Val loss: 1.8047 | Train F1: 1.00, Val F1: 0.73
Epoch: 015 | Train loss: 1.4741, Val loss: 1.7803 | Train F1: 1.00, Val F1: 0.73
Epoch: 016 | Train loss: 1.4203, Val loss: 1.7544 | Train F1: 1.00, Val F1: 0.74
Epoch: 017 | Train loss: 1.3708, Val loss: 1.7277 | Train F1: 1.00, Val F1: 0.74
Epoch: 018 | Train loss: 1.3266, Val loss: 1.7009 | Train F1: 1.00, Val F1: 0.74
Epoch: 019 | Train loss: 1.2886, Val loss: 1.6746 | Train F1: 1.00, Val F1: 0.74
Epoch: 020 | Train loss: 1.2572, Val loss: 1.6494 | Train F1: 1.00, Val F1: 0.75
Epoch: 021 | Train loss: 1.2324, Val loss: 1.6259 | Train F1: 1.00, Val F1: 0.75
Epoch: 022 | Train loss: 1.2134, Val loss: 1.6040 | Train F1: 1.00, Val F1: 0.75
Epoch: 023 | Train loss: 1.1994, Val loss: 1.5843 | Train F1: 1.00, Val F1: 0.76
Epoch: 024 | Train loss: 1.1893, Val loss: 1.5669 | Train F1: 1.00, Val F1: 0.75
Epoch: 025 | Train loss: 1.1821, Val loss: 1.5517 | Train F1: 1.00, Val F1: 0.76
Epoch: 026 | Train loss: 1.1771, Val loss: 1.5387 | Train F1: 1.00, Val F1: 0.76
Epoch: 027 | Train loss: 1.1736, Val loss: 1.5277 | Train F1: 1.00, Val F1: 0.76
Epoch: 028 | Train loss: 1.1711, Val loss: 1.5186 | Train F1: 1.00, Val F1: 0.76
Epoch: 029 | Train loss: 1.1695, Val loss: 1.5111 | Train F1: 1.00, Val F1: 0.75
Epoch: 030 | Train loss: 1.1683, Val loss: 1.5046 | Train F1: 1.00, Val F1: 0.75
Epoch: 031 | Train loss: 1.1675, Val loss: 1.4992 | Train F1: 1.00, Val F1: 0.74
Epoch: 032 | Train loss: 1.1670, Val loss: 1.4947 | Train F1: 1.00, Val F1: 0.74
Epoch: 033 | Train loss: 1.1666, Val loss: 1.4907 | Train F1: 1.00, Val F1: 0.74
Epoch: 034 | Train loss: 1.1663, Val loss: 1.4873 | Train F1: 1.00, Val F1: 0.74
Epoch: 035 | Train loss: 1.1661, Val loss: 1.4843 | Train F1: 1.00, Val F1: 0.73
Epoch: 036 | Train loss: 1.1660, Val loss: 1.4818 | Train F1: 1.00, Val F1: 0.73
Epoch: 037 | Train loss: 1.1659, Val loss: 1.4795 | Train F1: 1.00, Val F1: 0.73
Epoch: 038 | Train loss: 1.1658, Val loss: 1.4775 | Train F1: 1.00, Val F1: 0.73
Epoch: 039 | Train loss: 1.1657, Val loss: 1.4758 | Train F1: 1.00, Val F1: 0.73
Epoch: 040 | Train loss: 1.1657, Val loss: 1.4742 | Train F1: 1.00, Val F1: 0.73
Epoch: 041 | Train loss: 1.1657, Val loss: 1.4728 | Train F1: 1.00, Val F1: 0.73
Epoch: 042 | Train loss: 1.1656, Val loss: 1.4714 | Train F1: 1.00, Val F1: 0.73
Epoch: 043 | Train loss: 1.1656, Val loss: 1.4702 | Train F1: 1.00, Val F1: 0.73
Epoch: 044 | Train loss: 1.1656, Val loss: 1.4690 | Train F1: 1.00, Val F1: 0.73
Epoch: 045 | Train loss: 1.1656, Val loss: 1.4679 | Train F1: 1.00, Val F1: 0.73
Epoch: 046 | Train loss: 1.1655, Val loss: 1.4670 | Train F1: 1.00, Val F1: 0.73
Epoch: 047 | Train loss: 1.1655, Val loss: 1.4661 | Train F1: 1.00, Val F1: 0.73
Epoch: 048 | Train loss: 1.1655, Val loss: 1.4653 | Train F1: 1.00, Val F1: 0.73
Epoch: 049 | Train loss: 1.1655, Val loss: 1.4646 | Train F1: 1.00, Val F1: 0.73
Epoch: 050 | Train loss: 1.1655, Val loss: 1.4639 | Train F1: 1.00, Val F1: 0.73
Epoch: 051 | Train loss: 1.1655, Val loss: 1.4633 | Train F1: 1.00, Val F1: 0.73
Epoch: 052 | Train loss: 1.1655, Val loss: 1.4628 | Train F1: 1.00, Val F1: 0.73
Epoch: 053 | Train loss: 1.1655, Val loss: 1.4622 | Train F1: 1.00, Val F1: 0.73
Epoch: 054 | Train loss: 1.1655, Val loss: 1.4618 | Train F1: 1.00, Val F1: 0.73
Epoch: 055 | Train loss: 1.1655, Val loss: 1.4613 | Train F1: 1.00, Val F1: 0.73
Epoch: 056 | Train loss: 1.1655, Val loss: 1.4610 | Train F1: 1.00, Val F1: 0.73
Epoch: 057 | Train loss: 1.1655, Val loss: 1.4606 | Train F1: 1.00, Val F1: 0.73
Epoch: 058 | Train loss: 1.1655, Val loss: 1.4603 | Train F1: 1.00, Val F1: 0.73
Epoch: 059 | Train loss: 1.1655, Val loss: 1.4600 | Train F1: 1.00, Val F1: 0.73
Epoch: 060 | Train loss: 1.1655, Val loss: 1.4597 | Train F1: 1.00, Val F1: 0.73
Epoch: 061 | Train loss: 1.1655, Val loss: 1.4594 | Train F1: 1.00, Val F1: 0.73
Epoch: 062 | Train loss: 1.1655, Val loss: 1.4592 | Train F1: 1.00, Val F1: 0.73
Epoch: 063 | Train loss: 1.1655, Val loss: 1.4590 | Train F1: 1.00, Val F1: 0.73
Epoch: 064 | Train loss: 1.1655, Val loss: 1.4588 | Train F1: 1.00, Val F1: 0.73
Epoch: 065 | Train loss: 1.1655, Val loss: 1.4586 | Train F1: 1.00, Val F1: 0.73
Epoch: 066 | Train loss: 1.1655, Val loss: 1.4585 | Train F1: 1.00, Val F1: 0.73
Epoch: 067 | Train loss: 1.1655, Val loss: 1.4583 | Train F1: 1.00, Val F1: 0.73
Epoch: 068 | Train loss: 1.1655, Val loss: 1.4582 | Train F1: 1.00, Val F1: 0.73
Epoch: 069 | Train loss: 1.1655, Val loss: 1.4581 | Train F1: 1.00, Val F1: 0.73
Epoch: 070 | Train loss: 1.1655, Val loss: 1.4580 | Train F1: 1.00, Val F1: 0.73
Epoch: 071 | Train loss: 1.1655, Val loss: 1.4579 | Train F1: 1.00, Val F1: 0.73
Epoch: 072 | Train loss: 1.1655, Val loss: 1.4578 | Train F1: 1.00, Val F1: 0.73
Epoch: 073 | Train loss: 1.1655, Val loss: 1.4577 | Train F1: 1.00, Val F1: 0.73
Epoch: 074 | Train loss: 1.1655, Val loss: 1.4577 | Train F1: 1.00, Val F1: 0.73
Epoch: 075 | Train loss: 1.1655, Val loss: 1.4576 | Train F1: 1.00, Val F1: 0.73
Epoch: 076 | Train loss: 1.1655, Val loss: 1.4575 | Train F1: 1.00, Val F1: 0.73
Epoch: 077 | Train loss: 1.1655, Val loss: 1.4575 | Train F1: 1.00, Val F1: 0.73
Epoch: 078 | Train loss: 1.1655, Val loss: 1.4575 | Train F1: 1.00, Val F1: 0.73
Best model:
Train loss: 1.1655, Val loss: 1.4653, Test loss: 1.4601
Train F1: 1.00, Val F1: 0.73, Test F1: 0.72

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=0.6, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9398, Val loss: 1.9399 | Train F1: 0.30, Val F1: 0.13
Epoch: 001 | Train loss: 1.9328, Val loss: 1.9376 | Train F1: 0.42, Val F1: 0.22
Epoch: 002 | Train loss: 1.9247, Val loss: 1.9350 | Train F1: 0.58, Val F1: 0.28
Epoch: 003 | Train loss: 1.9150, Val loss: 1.9318 | Train F1: 0.81, Val F1: 0.32
Epoch: 004 | Train loss: 1.9032, Val loss: 1.9279 | Train F1: 0.93, Val F1: 0.49
Epoch: 005 | Train loss: 1.8888, Val loss: 1.9231 | Train F1: 0.96, Val F1: 0.56
Epoch: 006 | Train loss: 1.8712, Val loss: 1.9171 | Train F1: 0.99, Val F1: 0.63
Epoch: 007 | Train loss: 1.8498, Val loss: 1.9099 | Train F1: 0.99, Val F1: 0.68
Epoch: 008 | Train loss: 1.8238, Val loss: 1.9011 | Train F1: 0.99, Val F1: 0.71
Epoch: 009 | Train loss: 1.7927, Val loss: 1.8908 | Train F1: 1.00, Val F1: 0.73
Epoch: 010 | Train loss: 1.7562, Val loss: 1.8785 | Train F1: 1.00, Val F1: 0.74
Epoch: 011 | Train loss: 1.7143, Val loss: 1.8639 | Train F1: 1.00, Val F1: 0.74
Epoch: 012 | Train loss: 1.6678, Val loss: 1.8469 | Train F1: 0.99, Val F1: 0.74
Epoch: 013 | Train loss: 1.6180, Val loss: 1.8274 | Train F1: 0.99, Val F1: 0.74
Epoch: 014 | Train loss: 1.5669, Val loss: 1.8052 | Train F1: 0.99, Val F1: 0.74
Epoch: 015 | Train loss: 1.5165, Val loss: 1.7812 | Train F1: 0.99, Val F1: 0.74
Epoch: 016 | Train loss: 1.4684, Val loss: 1.7555 | Train F1: 0.99, Val F1: 0.74
Epoch: 017 | Train loss: 1.4237, Val loss: 1.7287 | Train F1: 0.99, Val F1: 0.74
Epoch: 018 | Train loss: 1.3828, Val loss: 1.7018 | Train F1: 0.99, Val F1: 0.74
Epoch: 019 | Train loss: 1.3457, Val loss: 1.6747 | Train F1: 0.99, Val F1: 0.75
Epoch: 020 | Train loss: 1.3121, Val loss: 1.6483 | Train F1: 0.99, Val F1: 0.76
Epoch: 021 | Train loss: 1.2818, Val loss: 1.6224 | Train F1: 1.00, Val F1: 0.76
Epoch: 022 | Train loss: 1.2552, Val loss: 1.5979 | Train F1: 1.00, Val F1: 0.76
Epoch: 023 | Train loss: 1.2328, Val loss: 1.5754 | Train F1: 1.00, Val F1: 0.76
Epoch: 024 | Train loss: 1.2148, Val loss: 1.5551 | Train F1: 1.00, Val F1: 0.77
Epoch: 025 | Train loss: 1.2012, Val loss: 1.5375 | Train F1: 1.00, Val F1: 0.77
Epoch: 026 | Train loss: 1.1914, Val loss: 1.5227 | Train F1: 1.00, Val F1: 0.76
Epoch: 027 | Train loss: 1.1845, Val loss: 1.5106 | Train F1: 1.00, Val F1: 0.76
Epoch: 028 | Train loss: 1.1796, Val loss: 1.5008 | Train F1: 1.00, Val F1: 0.76
Epoch: 029 | Train loss: 1.1758, Val loss: 1.4930 | Train F1: 1.00, Val F1: 0.76
Epoch: 030 | Train loss: 1.1730, Val loss: 1.4868 | Train F1: 1.00, Val F1: 0.76
Epoch: 031 | Train loss: 1.1709, Val loss: 1.4818 | Train F1: 1.00, Val F1: 0.76
Epoch: 032 | Train loss: 1.1693, Val loss: 1.4778 | Train F1: 1.00, Val F1: 0.75
Epoch: 033 | Train loss: 1.1683, Val loss: 1.4745 | Train F1: 1.00, Val F1: 0.76
Epoch: 034 | Train loss: 1.1675, Val loss: 1.4719 | Train F1: 1.00, Val F1: 0.75
Epoch: 035 | Train loss: 1.1670, Val loss: 1.4694 | Train F1: 1.00, Val F1: 0.75
Epoch: 036 | Train loss: 1.1666, Val loss: 1.4671 | Train F1: 1.00, Val F1: 0.75
Epoch: 037 | Train loss: 1.1664, Val loss: 1.4651 | Train F1: 1.00, Val F1: 0.75
Epoch: 038 | Train loss: 1.1662, Val loss: 1.4633 | Train F1: 1.00, Val F1: 0.75
Epoch: 039 | Train loss: 1.1661, Val loss: 1.4615 | Train F1: 1.00, Val F1: 0.75
Epoch: 040 | Train loss: 1.1660, Val loss: 1.4600 | Train F1: 1.00, Val F1: 0.75
Epoch: 041 | Train loss: 1.1659, Val loss: 1.4585 | Train F1: 1.00, Val F1: 0.75
Epoch: 042 | Train loss: 1.1658, Val loss: 1.4572 | Train F1: 1.00, Val F1: 0.75
Epoch: 043 | Train loss: 1.1658, Val loss: 1.4560 | Train F1: 1.00, Val F1: 0.75
Epoch: 044 | Train loss: 1.1657, Val loss: 1.4548 | Train F1: 1.00, Val F1: 0.75
Epoch: 045 | Train loss: 1.1657, Val loss: 1.4537 | Train F1: 1.00, Val F1: 0.75
Epoch: 046 | Train loss: 1.1656, Val loss: 1.4527 | Train F1: 1.00, Val F1: 0.75
Epoch: 047 | Train loss: 1.1656, Val loss: 1.4518 | Train F1: 1.00, Val F1: 0.74
Epoch: 048 | Train loss: 1.1656, Val loss: 1.4509 | Train F1: 1.00, Val F1: 0.74
Epoch: 049 | Train loss: 1.1656, Val loss: 1.4501 | Train F1: 1.00, Val F1: 0.74
Epoch: 050 | Train loss: 1.1656, Val loss: 1.4494 | Train F1: 1.00, Val F1: 0.74
Epoch: 051 | Train loss: 1.1655, Val loss: 1.4487 | Train F1: 1.00, Val F1: 0.74
Epoch: 052 | Train loss: 1.1655, Val loss: 1.4481 | Train F1: 1.00, Val F1: 0.74
Epoch: 053 | Train loss: 1.1655, Val loss: 1.4475 | Train F1: 1.00, Val F1: 0.74
Epoch: 054 | Train loss: 1.1655, Val loss: 1.4470 | Train F1: 1.00, Val F1: 0.74
Epoch: 055 | Train loss: 1.1655, Val loss: 1.4465 | Train F1: 1.00, Val F1: 0.74
Epoch: 056 | Train loss: 1.1655, Val loss: 1.4461 | Train F1: 1.00, Val F1: 0.74
Epoch: 057 | Train loss: 1.1655, Val loss: 1.4456 | Train F1: 1.00, Val F1: 0.74
Epoch: 058 | Train loss: 1.1655, Val loss: 1.4452 | Train F1: 1.00, Val F1: 0.74
Epoch: 059 | Train loss: 1.1655, Val loss: 1.4448 | Train F1: 1.00, Val F1: 0.74
Epoch: 060 | Train loss: 1.1655, Val loss: 1.4445 | Train F1: 1.00, Val F1: 0.74
Epoch: 061 | Train loss: 1.1655, Val loss: 1.4441 | Train F1: 1.00, Val F1: 0.74
Epoch: 062 | Train loss: 1.1655, Val loss: 1.4439 | Train F1: 1.00, Val F1: 0.74
Epoch: 063 | Train loss: 1.1655, Val loss: 1.4436 | Train F1: 1.00, Val F1: 0.73
Epoch: 064 | Train loss: 1.1655, Val loss: 1.4434 | Train F1: 1.00, Val F1: 0.73
Epoch: 065 | Train loss: 1.1655, Val loss: 1.4432 | Train F1: 1.00, Val F1: 0.73
Epoch: 066 | Train loss: 1.1655, Val loss: 1.4430 | Train F1: 1.00, Val F1: 0.73
Epoch: 067 | Train loss: 1.1655, Val loss: 1.4428 | Train F1: 1.00, Val F1: 0.73
Epoch: 068 | Train loss: 1.1655, Val loss: 1.4426 | Train F1: 1.00, Val F1: 0.73
Epoch: 069 | Train loss: 1.1655, Val loss: 1.4425 | Train F1: 1.00, Val F1: 0.73
Epoch: 070 | Train loss: 1.1655, Val loss: 1.4424 | Train F1: 1.00, Val F1: 0.73
Epoch: 071 | Train loss: 1.1655, Val loss: 1.4422 | Train F1: 1.00, Val F1: 0.74
Epoch: 072 | Train loss: 1.1655, Val loss: 1.4421 | Train F1: 1.00, Val F1: 0.74
Epoch: 073 | Train loss: 1.1655, Val loss: 1.4420 | Train F1: 1.00, Val F1: 0.74
Epoch: 074 | Train loss: 1.1655, Val loss: 1.4419 | Train F1: 1.00, Val F1: 0.74
Epoch: 075 | Train loss: 1.1655, Val loss: 1.4418 | Train F1: 1.00, Val F1: 0.74
Epoch: 076 | Train loss: 1.1655, Val loss: 1.4417 | Train F1: 1.00, Val F1: 0.74
Epoch: 077 | Train loss: 1.1655, Val loss: 1.4416 | Train F1: 1.00, Val F1: 0.74
Epoch: 078 | Train loss: 1.1655, Val loss: 1.4415 | Train F1: 1.00, Val F1: 0.74
Epoch: 079 | Train loss: 1.1655, Val loss: 1.4415 | Train F1: 1.00, Val F1: 0.74
Epoch: 080 | Train loss: 1.1655, Val loss: 1.4414 | Train F1: 1.00, Val F1: 0.74
Epoch: 081 | Train loss: 1.1655, Val loss: 1.4413 | Train F1: 1.00, Val F1: 0.74
Epoch: 082 | Train loss: 1.1655, Val loss: 1.4412 | Train F1: 1.00, Val F1: 0.74
Best model:
Train loss: 1.1655, Val loss: 1.4481, Test loss: 1.4377
Train F1: 1.00, Val F1: 0.74, Test F1: 0.75

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=0.8, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9403, Val loss: 1.9439 | Train F1: 0.12, Val F1: 0.04
Epoch: 001 | Train loss: 1.9343, Val loss: 1.9417 | Train F1: 0.28, Val F1: 0.07
Epoch: 002 | Train loss: 1.9274, Val loss: 1.9391 | Train F1: 0.58, Val F1: 0.13
Epoch: 003 | Train loss: 1.9191, Val loss: 1.9360 | Train F1: 0.80, Val F1: 0.19
Epoch: 004 | Train loss: 1.9090, Val loss: 1.9323 | Train F1: 0.90, Val F1: 0.37
Epoch: 005 | Train loss: 1.8968, Val loss: 1.9278 | Train F1: 0.94, Val F1: 0.46
Epoch: 006 | Train loss: 1.8820, Val loss: 1.9224 | Train F1: 0.96, Val F1: 0.52
Epoch: 007 | Train loss: 1.8640, Val loss: 1.9158 | Train F1: 0.98, Val F1: 0.56
Epoch: 008 | Train loss: 1.8423, Val loss: 1.9079 | Train F1: 0.98, Val F1: 0.61
Epoch: 009 | Train loss: 1.8163, Val loss: 1.8985 | Train F1: 0.98, Val F1: 0.63
Epoch: 010 | Train loss: 1.7854, Val loss: 1.8872 | Train F1: 0.98, Val F1: 0.66
Epoch: 011 | Train loss: 1.7495, Val loss: 1.8737 | Train F1: 0.98, Val F1: 0.67
Epoch: 012 | Train loss: 1.7087, Val loss: 1.8577 | Train F1: 0.98, Val F1: 0.67
Epoch: 013 | Train loss: 1.6637, Val loss: 1.8390 | Train F1: 0.99, Val F1: 0.67
Epoch: 014 | Train loss: 1.6155, Val loss: 1.8175 | Train F1: 0.99, Val F1: 0.67
Epoch: 015 | Train loss: 1.5660, Val loss: 1.7934 | Train F1: 0.99, Val F1: 0.68
Epoch: 016 | Train loss: 1.5167, Val loss: 1.7672 | Train F1: 0.98, Val F1: 0.69
Epoch: 017 | Train loss: 1.4692, Val loss: 1.7397 | Train F1: 0.99, Val F1: 0.69
Epoch: 018 | Train loss: 1.4246, Val loss: 1.7117 | Train F1: 1.00, Val F1: 0.69
Epoch: 019 | Train loss: 1.3833, Val loss: 1.6839 | Train F1: 1.00, Val F1: 0.70
Epoch: 020 | Train loss: 1.3456, Val loss: 1.6566 | Train F1: 1.00, Val F1: 0.72
Epoch: 021 | Train loss: 1.3117, Val loss: 1.6297 | Train F1: 1.00, Val F1: 0.73
Epoch: 022 | Train loss: 1.2817, Val loss: 1.6033 | Train F1: 1.00, Val F1: 0.73
Epoch: 023 | Train loss: 1.2559, Val loss: 1.5775 | Train F1: 1.00, Val F1: 0.74
Epoch: 024 | Train loss: 1.2346, Val loss: 1.5530 | Train F1: 1.00, Val F1: 0.76
Epoch: 025 | Train loss: 1.2177, Val loss: 1.5306 | Train F1: 1.00, Val F1: 0.76
Epoch: 026 | Train loss: 1.2047, Val loss: 1.5106 | Train F1: 1.00, Val F1: 0.77
Epoch: 027 | Train loss: 1.1949, Val loss: 1.4935 | Train F1: 1.00, Val F1: 0.77
Epoch: 028 | Train loss: 1.1873, Val loss: 1.4791 | Train F1: 1.00, Val F1: 0.77
Epoch: 029 | Train loss: 1.1814, Val loss: 1.4672 | Train F1: 1.00, Val F1: 0.77
Epoch: 030 | Train loss: 1.1769, Val loss: 1.4571 | Train F1: 1.00, Val F1: 0.76
Epoch: 031 | Train loss: 1.1736, Val loss: 1.4487 | Train F1: 1.00, Val F1: 0.76
Epoch: 032 | Train loss: 1.1712, Val loss: 1.4417 | Train F1: 1.00, Val F1: 0.77
Epoch: 033 | Train loss: 1.1695, Val loss: 1.4360 | Train F1: 1.00, Val F1: 0.77
Epoch: 034 | Train loss: 1.1684, Val loss: 1.4313 | Train F1: 1.00, Val F1: 0.77
Epoch: 035 | Train loss: 1.1676, Val loss: 1.4274 | Train F1: 1.00, Val F1: 0.77
Epoch: 036 | Train loss: 1.1671, Val loss: 1.4241 | Train F1: 1.00, Val F1: 0.77
Epoch: 037 | Train loss: 1.1667, Val loss: 1.4215 | Train F1: 1.00, Val F1: 0.77
Epoch: 038 | Train loss: 1.1664, Val loss: 1.4193 | Train F1: 1.00, Val F1: 0.76
Epoch: 039 | Train loss: 1.1662, Val loss: 1.4176 | Train F1: 1.00, Val F1: 0.76
Epoch: 040 | Train loss: 1.1660, Val loss: 1.4162 | Train F1: 1.00, Val F1: 0.76
Epoch: 041 | Train loss: 1.1659, Val loss: 1.4150 | Train F1: 1.00, Val F1: 0.75
Epoch: 042 | Train loss: 1.1658, Val loss: 1.4141 | Train F1: 1.00, Val F1: 0.75
Epoch: 043 | Train loss: 1.1658, Val loss: 1.4133 | Train F1: 1.00, Val F1: 0.75
Epoch: 044 | Train loss: 1.1657, Val loss: 1.4127 | Train F1: 1.00, Val F1: 0.75
Epoch: 045 | Train loss: 1.1657, Val loss: 1.4121 | Train F1: 1.00, Val F1: 0.75
Epoch: 046 | Train loss: 1.1657, Val loss: 1.4117 | Train F1: 1.00, Val F1: 0.75
Epoch: 047 | Train loss: 1.1656, Val loss: 1.4113 | Train F1: 1.00, Val F1: 0.75
Epoch: 048 | Train loss: 1.1656, Val loss: 1.4109 | Train F1: 1.00, Val F1: 0.75
Epoch: 049 | Train loss: 1.1656, Val loss: 1.4107 | Train F1: 1.00, Val F1: 0.75
Epoch: 050 | Train loss: 1.1656, Val loss: 1.4104 | Train F1: 1.00, Val F1: 0.75
Epoch: 051 | Train loss: 1.1656, Val loss: 1.4102 | Train F1: 1.00, Val F1: 0.75
Epoch: 052 | Train loss: 1.1655, Val loss: 1.4100 | Train F1: 1.00, Val F1: 0.75
Epoch: 053 | Train loss: 1.1655, Val loss: 1.4098 | Train F1: 1.00, Val F1: 0.75
Epoch: 054 | Train loss: 1.1655, Val loss: 1.4097 | Train F1: 1.00, Val F1: 0.75
Epoch: 055 | Train loss: 1.1655, Val loss: 1.4096 | Train F1: 1.00, Val F1: 0.75
Epoch: 056 | Train loss: 1.1655, Val loss: 1.4094 | Train F1: 1.00, Val F1: 0.75
Epoch: 057 | Train loss: 1.1655, Val loss: 1.4093 | Train F1: 1.00, Val F1: 0.75
Epoch: 058 | Train loss: 1.1655, Val loss: 1.4092 | Train F1: 1.00, Val F1: 0.75
Epoch: 059 | Train loss: 1.1655, Val loss: 1.4091 | Train F1: 1.00, Val F1: 0.75
Epoch: 060 | Train loss: 1.1655, Val loss: 1.4090 | Train F1: 1.00, Val F1: 0.75
Epoch: 061 | Train loss: 1.1655, Val loss: 1.4089 | Train F1: 1.00, Val F1: 0.75
Epoch: 062 | Train loss: 1.1655, Val loss: 1.4088 | Train F1: 1.00, Val F1: 0.75
Epoch: 063 | Train loss: 1.1655, Val loss: 1.4087 | Train F1: 1.00, Val F1: 0.75
Epoch: 064 | Train loss: 1.1655, Val loss: 1.4086 | Train F1: 1.00, Val F1: 0.75
Epoch: 065 | Train loss: 1.1655, Val loss: 1.4085 | Train F1: 1.00, Val F1: 0.76
Epoch: 066 | Train loss: 1.1655, Val loss: 1.4085 | Train F1: 1.00, Val F1: 0.76
Epoch: 067 | Train loss: 1.1655, Val loss: 1.4084 | Train F1: 1.00, Val F1: 0.76
Epoch: 068 | Train loss: 1.1655, Val loss: 1.4083 | Train F1: 1.00, Val F1: 0.76
Epoch: 069 | Train loss: 1.1655, Val loss: 1.4082 | Train F1: 1.00, Val F1: 0.76
Epoch: 070 | Train loss: 1.1655, Val loss: 1.4082 | Train F1: 1.00, Val F1: 0.75
Epoch: 071 | Train loss: 1.1655, Val loss: 1.4081 | Train F1: 1.00, Val F1: 0.75
Epoch: 072 | Train loss: 1.1655, Val loss: 1.4081 | Train F1: 1.00, Val F1: 0.75
Epoch: 073 | Train loss: 1.1655, Val loss: 1.4080 | Train F1: 1.00, Val F1: 0.75
Epoch: 074 | Train loss: 1.1655, Val loss: 1.4080 | Train F1: 1.00, Val F1: 0.75
Epoch: 075 | Train loss: 1.1655, Val loss: 1.4079 | Train F1: 1.00, Val F1: 0.75
Epoch: 076 | Train loss: 1.1655, Val loss: 1.4079 | Train F1: 1.00, Val F1: 0.75
Epoch: 077 | Train loss: 1.1655, Val loss: 1.4079 | Train F1: 1.00, Val F1: 0.75
Epoch: 078 | Train loss: 1.1655, Val loss: 1.4078 | Train F1: 1.00, Val F1: 0.75
Epoch: 079 | Train loss: 1.1655, Val loss: 1.4078 | Train F1: 1.00, Val F1: 0.75
Epoch: 080 | Train loss: 1.1655, Val loss: 1.4078 | Train F1: 1.00, Val F1: 0.75
Epoch: 081 | Train loss: 1.1655, Val loss: 1.4077 | Train F1: 1.00, Val F1: 0.75
Epoch: 082 | Train loss: 1.1655, Val loss: 1.4077 | Train F1: 1.00, Val F1: 0.75
Epoch: 083 | Train loss: 1.1655, Val loss: 1.4077 | Train F1: 1.00, Val F1: 0.75
Epoch: 084 | Train loss: 1.1655, Val loss: 1.4077 | Train F1: 1.00, Val F1: 0.75
Epoch: 085 | Train loss: 1.1655, Val loss: 1.4076 | Train F1: 1.00, Val F1: 0.75
Epoch: 086 | Train loss: 1.1655, Val loss: 1.4076 | Train F1: 1.00, Val F1: 0.75
Epoch: 087 | Train loss: 1.1655, Val loss: 1.4076 | Train F1: 1.00, Val F1: 0.75
Best model:
Train loss: 1.1655, Val loss: 1.4093, Test loss: 1.3977
Train F1: 1.00, Val F1: 0.75, Test F1: 0.77

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9397, Val loss: 1.9395 | Train F1: 0.28, Val F1: 0.16
Epoch: 001 | Train loss: 1.9333, Val loss: 1.9371 | Train F1: 0.45, Val F1: 0.29
Epoch: 002 | Train loss: 1.9256, Val loss: 1.9342 | Train F1: 0.52, Val F1: 0.33
Epoch: 003 | Train loss: 1.9163, Val loss: 1.9307 | Train F1: 0.63, Val F1: 0.34
Epoch: 004 | Train loss: 1.9050, Val loss: 1.9263 | Train F1: 0.73, Val F1: 0.41
Epoch: 005 | Train loss: 1.8909, Val loss: 1.9209 | Train F1: 0.83, Val F1: 0.50
Epoch: 006 | Train loss: 1.8737, Val loss: 1.9143 | Train F1: 0.87, Val F1: 0.55
Epoch: 007 | Train loss: 1.8525, Val loss: 1.9061 | Train F1: 0.87, Val F1: 0.56
Epoch: 008 | Train loss: 1.8270, Val loss: 1.8960 | Train F1: 0.88, Val F1: 0.59
Epoch: 009 | Train loss: 1.7970, Val loss: 1.8838 | Train F1: 0.86, Val F1: 0.58
Epoch: 010 | Train loss: 1.7631, Val loss: 1.8694 | Train F1: 0.87, Val F1: 0.56
Epoch: 011 | Train loss: 1.7263, Val loss: 1.8527 | Train F1: 0.89, Val F1: 0.55
Epoch: 012 | Train loss: 1.6883, Val loss: 1.8345 | Train F1: 0.89, Val F1: 0.55
Epoch: 013 | Train loss: 1.6503, Val loss: 1.8155 | Train F1: 0.89, Val F1: 0.53
Epoch: 014 | Train loss: 1.6129, Val loss: 1.7961 | Train F1: 0.88, Val F1: 0.52
Epoch: 015 | Train loss: 1.5765, Val loss: 1.7770 | Train F1: 0.89, Val F1: 0.52
Epoch: 016 | Train loss: 1.5415, Val loss: 1.7581 | Train F1: 0.90, Val F1: 0.53
Epoch: 017 | Train loss: 1.5084, Val loss: 1.7394 | Train F1: 0.90, Val F1: 0.53
Epoch: 018 | Train loss: 1.4768, Val loss: 1.7210 | Train F1: 0.90, Val F1: 0.54
Epoch: 019 | Train loss: 1.4460, Val loss: 1.7020 | Train F1: 0.90, Val F1: 0.56
Epoch: 020 | Train loss: 1.4147, Val loss: 1.6813 | Train F1: 0.94, Val F1: 0.60
Epoch: 021 | Train loss: 1.3822, Val loss: 1.6581 | Train F1: 0.94, Val F1: 0.63
Epoch: 022 | Train loss: 1.3489, Val loss: 1.6323 | Train F1: 0.96, Val F1: 0.67
Epoch: 023 | Train loss: 1.3163, Val loss: 1.6044 | Train F1: 0.96, Val F1: 0.70
Epoch: 024 | Train loss: 1.2862, Val loss: 1.5760 | Train F1: 0.98, Val F1: 0.73
Epoch: 025 | Train loss: 1.2603, Val loss: 1.5491 | Train F1: 0.98, Val F1: 0.73
Epoch: 026 | Train loss: 1.2393, Val loss: 1.5253 | Train F1: 0.99, Val F1: 0.75
Epoch: 027 | Train loss: 1.2231, Val loss: 1.5056 | Train F1: 0.99, Val F1: 0.76
Epoch: 028 | Train loss: 1.2108, Val loss: 1.4897 | Train F1: 0.99, Val F1: 0.76
Epoch: 029 | Train loss: 1.2016, Val loss: 1.4770 | Train F1: 0.99, Val F1: 0.76
Epoch: 030 | Train loss: 1.1947, Val loss: 1.4670 | Train F1: 0.99, Val F1: 0.76
Epoch: 031 | Train loss: 1.1895, Val loss: 1.4590 | Train F1: 0.99, Val F1: 0.75
Epoch: 032 | Train loss: 1.1855, Val loss: 1.4523 | Train F1: 0.99, Val F1: 0.76
Epoch: 033 | Train loss: 1.1824, Val loss: 1.4468 | Train F1: 0.99, Val F1: 0.76
Epoch: 034 | Train loss: 1.1799, Val loss: 1.4422 | Train F1: 0.99, Val F1: 0.76
Epoch: 035 | Train loss: 1.1780, Val loss: 1.4384 | Train F1: 0.99, Val F1: 0.76
Epoch: 036 | Train loss: 1.1764, Val loss: 1.4353 | Train F1: 0.99, Val F1: 0.77
Epoch: 037 | Train loss: 1.1752, Val loss: 1.4327 | Train F1: 0.99, Val F1: 0.76
Epoch: 038 | Train loss: 1.1742, Val loss: 1.4306 | Train F1: 0.99, Val F1: 0.76
Epoch: 039 | Train loss: 1.1733, Val loss: 1.4290 | Train F1: 0.99, Val F1: 0.76
Epoch: 040 | Train loss: 1.1723, Val loss: 1.4278 | Train F1: 0.99, Val F1: 0.76
Epoch: 041 | Train loss: 1.1710, Val loss: 1.4271 | Train F1: 0.99, Val F1: 0.76
Epoch: 042 | Train loss: 1.1696, Val loss: 1.4267 | Train F1: 1.00, Val F1: 0.76
Epoch: 043 | Train loss: 1.1682, Val loss: 1.4269 | Train F1: 1.00, Val F1: 0.77
Epoch: 044 | Train loss: 1.1672, Val loss: 1.4277 | Train F1: 1.00, Val F1: 0.76
Epoch: 045 | Train loss: 1.1667, Val loss: 1.4285 | Train F1: 1.00, Val F1: 0.76
Epoch: 046 | Train loss: 1.1664, Val loss: 1.4295 | Train F1: 1.00, Val F1: 0.76
Epoch: 047 | Train loss: 1.1663, Val loss: 1.4303 | Train F1: 1.00, Val F1: 0.76
Epoch: 048 | Train loss: 1.1662, Val loss: 1.4310 | Train F1: 1.00, Val F1: 0.76
Epoch: 049 | Train loss: 1.1661, Val loss: 1.4316 | Train F1: 1.00, Val F1: 0.75
Epoch: 050 | Train loss: 1.1661, Val loss: 1.4319 | Train F1: 1.00, Val F1: 0.75
Epoch: 051 | Train loss: 1.1660, Val loss: 1.4318 | Train F1: 1.00, Val F1: 0.75
Epoch: 052 | Train loss: 1.1660, Val loss: 1.4314 | Train F1: 1.00, Val F1: 0.75
Epoch: 053 | Train loss: 1.1659, Val loss: 1.4307 | Train F1: 1.00, Val F1: 0.75
Epoch: 054 | Train loss: 1.1659, Val loss: 1.4298 | Train F1: 1.00, Val F1: 0.75
Epoch: 055 | Train loss: 1.1658, Val loss: 1.4288 | Train F1: 1.00, Val F1: 0.75
Epoch: 056 | Train loss: 1.1658, Val loss: 1.4276 | Train F1: 1.00, Val F1: 0.75
Epoch: 057 | Train loss: 1.1658, Val loss: 1.4265 | Train F1: 1.00, Val F1: 0.76
Epoch: 058 | Train loss: 1.1657, Val loss: 1.4253 | Train F1: 1.00, Val F1: 0.76
Epoch: 059 | Train loss: 1.1657, Val loss: 1.4242 | Train F1: 1.00, Val F1: 0.75
Epoch: 060 | Train loss: 1.1657, Val loss: 1.4228 | Train F1: 1.00, Val F1: 0.75
Epoch: 061 | Train loss: 1.1656, Val loss: 1.4217 | Train F1: 1.00, Val F1: 0.76
Epoch: 062 | Train loss: 1.1656, Val loss: 1.4205 | Train F1: 1.00, Val F1: 0.75
Epoch: 063 | Train loss: 1.1656, Val loss: 1.4194 | Train F1: 1.00, Val F1: 0.75
Epoch: 064 | Train loss: 1.1656, Val loss: 1.4184 | Train F1: 1.00, Val F1: 0.75
Epoch: 065 | Train loss: 1.1656, Val loss: 1.4175 | Train F1: 1.00, Val F1: 0.75
Epoch: 066 | Train loss: 1.1656, Val loss: 1.4167 | Train F1: 1.00, Val F1: 0.75
Epoch: 067 | Train loss: 1.1656, Val loss: 1.4159 | Train F1: 1.00, Val F1: 0.75
Epoch: 068 | Train loss: 1.1655, Val loss: 1.4153 | Train F1: 1.00, Val F1: 0.75
Epoch: 069 | Train loss: 1.1655, Val loss: 1.4146 | Train F1: 1.00, Val F1: 0.75
Epoch: 070 | Train loss: 1.1655, Val loss: 1.4141 | Train F1: 1.00, Val F1: 0.75
Epoch: 071 | Train loss: 1.1655, Val loss: 1.4136 | Train F1: 1.00, Val F1: 0.75
Epoch: 072 | Train loss: 1.1655, Val loss: 1.4131 | Train F1: 1.00, Val F1: 0.75
Epoch: 073 | Train loss: 1.1655, Val loss: 1.4127 | Train F1: 1.00, Val F1: 0.75
Epoch: 074 | Train loss: 1.1655, Val loss: 1.4123 | Train F1: 1.00, Val F1: 0.75
Epoch: 075 | Train loss: 1.1655, Val loss: 1.4119 | Train F1: 1.00, Val F1: 0.75
Epoch: 076 | Train loss: 1.1655, Val loss: 1.4115 | Train F1: 1.00, Val F1: 0.75
Epoch: 077 | Train loss: 1.1655, Val loss: 1.4112 | Train F1: 1.00, Val F1: 0.75
Epoch: 078 | Train loss: 1.1655, Val loss: 1.4109 | Train F1: 1.00, Val F1: 0.75
Epoch: 079 | Train loss: 1.1655, Val loss: 1.4107 | Train F1: 1.00, Val F1: 0.75
Epoch: 080 | Train loss: 1.1655, Val loss: 1.4104 | Train F1: 1.00, Val F1: 0.75
Epoch: 081 | Train loss: 1.1655, Val loss: 1.4102 | Train F1: 1.00, Val F1: 0.75
Epoch: 082 | Train loss: 1.1655, Val loss: 1.4100 | Train F1: 1.00, Val F1: 0.75
Epoch: 083 | Train loss: 1.1655, Val loss: 1.4098 | Train F1: 1.00, Val F1: 0.75
Epoch: 084 | Train loss: 1.1655, Val loss: 1.4096 | Train F1: 1.00, Val F1: 0.75
Epoch: 085 | Train loss: 1.1655, Val loss: 1.4095 | Train F1: 1.00, Val F1: 0.75
Epoch: 086 | Train loss: 1.1655, Val loss: 1.4094 | Train F1: 1.00, Val F1: 0.75
Epoch: 087 | Train loss: 1.1655, Val loss: 1.4092 | Train F1: 1.00, Val F1: 0.75
Epoch: 088 | Train loss: 1.1655, Val loss: 1.4091 | Train F1: 1.00, Val F1: 0.75
Epoch: 089 | Train loss: 1.1655, Val loss: 1.4090 | Train F1: 1.00, Val F1: 0.75
Epoch: 090 | Train loss: 1.1655, Val loss: 1.4089 | Train F1: 1.00, Val F1: 0.75
Epoch: 091 | Train loss: 1.1655, Val loss: 1.4088 | Train F1: 1.00, Val F1: 0.75
Epoch: 092 | Train loss: 1.1655, Val loss: 1.4087 | Train F1: 1.00, Val F1: 0.75
Epoch: 093 | Train loss: 1.1655, Val loss: 1.4086 | Train F1: 1.00, Val F1: 0.75
Epoch: 094 | Train loss: 1.1655, Val loss: 1.4085 | Train F1: 1.00, Val F1: 0.75
Epoch: 095 | Train loss: 1.1655, Val loss: 1.4084 | Train F1: 1.00, Val F1: 0.75
Epoch: 096 | Train loss: 1.1655, Val loss: 1.4083 | Train F1: 1.00, Val F1: 0.75
Epoch: 097 | Train loss: 1.1655, Val loss: 1.4083 | Train F1: 1.00, Val F1: 0.75
Epoch: 098 | Train loss: 1.1655, Val loss: 1.4082 | Train F1: 1.00, Val F1: 0.75
Epoch: 099 | Train loss: 1.1655, Val loss: 1.4081 | Train F1: 1.00, Val F1: 0.75
Epoch: 100 | Train loss: 1.1655, Val loss: 1.4081 | Train F1: 1.00, Val F1: 0.75
Best model:
Train loss: 1.1655, Val loss: 1.4141, Test loss: 1.3944
Train F1: 1.00, Val F1: 0.75, Test F1: 0.78

>>> run.py: Namespace(dataset='cora', device=1, experiment='training-data', log_path='log/graphsage/cora/training-data', method='graphsage', path='data/graphsage', plot_path='plots/graphsage/cora/training-data')

>>> Training split = 0.2

>>> Training split = 0.4

>>> Training split = 0.6

>>> Training split = 0.8

>>> Training split = 1.0

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=0.2, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5632, Val loss: 1.5844 | Train F1: 0.41, Val F1: 0.22
Epoch: 001 | Train loss: 1.5119, Val loss: 1.5555 | Train F1: 0.42, Val F1: 0.24
Epoch: 002 | Train loss: 1.4491, Val loss: 1.5202 | Train F1: 0.42, Val F1: 0.26
Epoch: 003 | Train loss: 1.3803, Val loss: 1.4809 | Train F1: 0.42, Val F1: 0.27
Epoch: 004 | Train loss: 1.3160, Val loss: 1.4426 | Train F1: 0.42, Val F1: 0.28
Epoch: 005 | Train loss: 1.2640, Val loss: 1.4106 | Train F1: 0.42, Val F1: 0.28
Epoch: 006 | Train loss: 1.2247, Val loss: 1.3880 | Train F1: 0.42, Val F1: 0.28
Epoch: 007 | Train loss: 1.1956, Val loss: 1.3727 | Train F1: 0.42, Val F1: 0.29
Epoch: 008 | Train loss: 1.1725, Val loss: 1.3621 | Train F1: 0.42, Val F1: 0.29
Epoch: 009 | Train loss: 1.1529, Val loss: 1.3554 | Train F1: 0.53, Val F1: 0.28
Epoch: 010 | Train loss: 1.1351, Val loss: 1.3508 | Train F1: 0.53, Val F1: 0.28
Epoch: 011 | Train loss: 1.1200, Val loss: 1.3462 | Train F1: 0.53, Val F1: 0.28
Epoch: 012 | Train loss: 1.1049, Val loss: 1.3415 | Train F1: 0.53, Val F1: 0.28
Epoch: 013 | Train loss: 1.0860, Val loss: 1.3377 | Train F1: 0.60, Val F1: 0.28
Epoch: 014 | Train loss: 1.0633, Val loss: 1.3350 | Train F1: 0.80, Val F1: 0.32
Epoch: 015 | Train loss: 1.0413, Val loss: 1.3352 | Train F1: 0.80, Val F1: 0.32
Epoch: 016 | Train loss: 1.0234, Val loss: 1.3407 | Train F1: 0.85, Val F1: 0.39
Epoch: 017 | Train loss: 1.0105, Val loss: 1.3519 | Train F1: 0.85, Val F1: 0.36
Epoch: 018 | Train loss: 1.0020, Val loss: 1.3672 | Train F1: 0.85, Val F1: 0.34
Epoch: 019 | Train loss: 0.9968, Val loss: 1.3836 | Train F1: 0.83, Val F1: 0.33
Epoch: 020 | Train loss: 0.9928, Val loss: 1.3978 | Train F1: 0.83, Val F1: 0.32
Epoch: 021 | Train loss: 0.9864, Val loss: 1.4025 | Train F1: 0.83, Val F1: 0.31
Epoch: 022 | Train loss: 0.9778, Val loss: 1.4023 | Train F1: 0.83, Val F1: 0.33
Epoch: 023 | Train loss: 0.9673, Val loss: 1.3999 | Train F1: 0.94, Val F1: 0.33
Epoch: 024 | Train loss: 0.9547, Val loss: 1.3982 | Train F1: 1.00, Val F1: 0.34
Epoch: 025 | Train loss: 0.9437, Val loss: 1.3978 | Train F1: 1.00, Val F1: 0.35
Epoch: 026 | Train loss: 0.9374, Val loss: 1.4006 | Train F1: 1.00, Val F1: 0.36
Epoch: 027 | Train loss: 0.9345, Val loss: 1.4054 | Train F1: 1.00, Val F1: 0.34
Epoch: 028 | Train loss: 0.9328, Val loss: 1.4111 | Train F1: 1.00, Val F1: 0.35
Epoch: 029 | Train loss: 0.9310, Val loss: 1.4156 | Train F1: 1.00, Val F1: 0.36
Epoch: 030 | Train loss: 0.9290, Val loss: 1.4187 | Train F1: 1.00, Val F1: 0.36
Epoch: 031 | Train loss: 0.9269, Val loss: 1.4207 | Train F1: 1.00, Val F1: 0.36
Epoch: 032 | Train loss: 0.9248, Val loss: 1.4192 | Train F1: 1.00, Val F1: 0.36
Epoch: 033 | Train loss: 0.9231, Val loss: 1.4175 | Train F1: 1.00, Val F1: 0.36
Epoch: 034 | Train loss: 0.9215, Val loss: 1.4157 | Train F1: 1.00, Val F1: 0.36
Epoch: 035 | Train loss: 0.9199, Val loss: 1.4139 | Train F1: 1.00, Val F1: 0.36
Epoch: 036 | Train loss: 0.9183, Val loss: 1.4126 | Train F1: 1.00, Val F1: 0.36
Epoch: 037 | Train loss: 0.9167, Val loss: 1.4114 | Train F1: 1.00, Val F1: 0.36
Epoch: 038 | Train loss: 0.9151, Val loss: 1.4091 | Train F1: 1.00, Val F1: 0.37
Epoch: 039 | Train loss: 0.9135, Val loss: 1.4077 | Train F1: 1.00, Val F1: 0.37
Epoch: 040 | Train loss: 0.9121, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.37
Epoch: 041 | Train loss: 0.9108, Val loss: 1.4062 | Train F1: 1.00, Val F1: 0.37
Epoch: 042 | Train loss: 0.9097, Val loss: 1.4061 | Train F1: 1.00, Val F1: 0.37
Epoch: 043 | Train loss: 0.9087, Val loss: 1.4062 | Train F1: 1.00, Val F1: 0.35
Epoch: 044 | Train loss: 0.9079, Val loss: 1.4062 | Train F1: 1.00, Val F1: 0.36
Epoch: 045 | Train loss: 0.9073, Val loss: 1.4063 | Train F1: 1.00, Val F1: 0.36
Epoch: 046 | Train loss: 0.9068, Val loss: 1.4067 | Train F1: 1.00, Val F1: 0.35
Epoch: 047 | Train loss: 0.9064, Val loss: 1.4071 | Train F1: 1.00, Val F1: 0.36
Epoch: 048 | Train loss: 0.9061, Val loss: 1.4077 | Train F1: 1.00, Val F1: 0.36
Epoch: 049 | Train loss: 0.9058, Val loss: 1.4082 | Train F1: 1.00, Val F1: 0.36
Epoch: 050 | Train loss: 0.9056, Val loss: 1.4079 | Train F1: 1.00, Val F1: 0.36
Epoch: 051 | Train loss: 0.9055, Val loss: 1.4076 | Train F1: 1.00, Val F1: 0.32
Epoch: 052 | Train loss: 0.9054, Val loss: 1.4070 | Train F1: 1.00, Val F1: 0.32
Epoch: 053 | Train loss: 0.9053, Val loss: 1.4050 | Train F1: 1.00, Val F1: 0.32
Epoch: 054 | Train loss: 0.9052, Val loss: 1.4000 | Train F1: 1.00, Val F1: 0.32
Epoch: 055 | Train loss: 0.9052, Val loss: 1.3950 | Train F1: 1.00, Val F1: 0.33
Epoch: 056 | Train loss: 0.9051, Val loss: 1.3902 | Train F1: 1.00, Val F1: 0.34
Epoch: 057 | Train loss: 0.9051, Val loss: 1.3852 | Train F1: 1.00, Val F1: 0.34
Epoch: 058 | Train loss: 0.9051, Val loss: 1.3807 | Train F1: 1.00, Val F1: 0.34
Epoch: 059 | Train loss: 0.9050, Val loss: 1.3769 | Train F1: 1.00, Val F1: 0.34
Epoch: 060 | Train loss: 0.9050, Val loss: 1.3737 | Train F1: 1.00, Val F1: 0.35
Epoch: 061 | Train loss: 0.9050, Val loss: 1.3710 | Train F1: 1.00, Val F1: 0.35
Epoch: 062 | Train loss: 0.9050, Val loss: 1.3690 | Train F1: 1.00, Val F1: 0.35
Epoch: 063 | Train loss: 0.9050, Val loss: 1.3673 | Train F1: 1.00, Val F1: 0.35
Epoch: 064 | Train loss: 0.9050, Val loss: 1.3659 | Train F1: 1.00, Val F1: 0.35
Epoch: 065 | Train loss: 0.9050, Val loss: 1.3648 | Train F1: 1.00, Val F1: 0.35
Epoch: 066 | Train loss: 0.9049, Val loss: 1.3638 | Train F1: 1.00, Val F1: 0.35
Epoch: 067 | Train loss: 0.9049, Val loss: 1.3631 | Train F1: 1.00, Val F1: 0.35
Epoch: 068 | Train loss: 0.9049, Val loss: 1.3624 | Train F1: 1.00, Val F1: 0.35
Epoch: 069 | Train loss: 0.9049, Val loss: 1.3618 | Train F1: 1.00, Val F1: 0.35
Epoch: 070 | Train loss: 0.9049, Val loss: 1.3613 | Train F1: 1.00, Val F1: 0.35
Epoch: 071 | Train loss: 0.9049, Val loss: 1.3609 | Train F1: 1.00, Val F1: 0.35
Epoch: 072 | Train loss: 0.9049, Val loss: 1.3606 | Train F1: 1.00, Val F1: 0.36
Epoch: 073 | Train loss: 0.9049, Val loss: 1.3603 | Train F1: 1.00, Val F1: 0.36
Epoch: 074 | Train loss: 0.9049, Val loss: 1.3601 | Train F1: 1.00, Val F1: 0.36
Epoch: 075 | Train loss: 0.9049, Val loss: 1.3599 | Train F1: 1.00, Val F1: 0.36
Epoch: 076 | Train loss: 0.9049, Val loss: 1.3597 | Train F1: 1.00, Val F1: 0.36
Epoch: 077 | Train loss: 0.9049, Val loss: 1.3596 | Train F1: 1.00, Val F1: 0.36
Epoch: 078 | Train loss: 0.9049, Val loss: 1.3594 | Train F1: 1.00, Val F1: 0.36
Epoch: 079 | Train loss: 0.9049, Val loss: 1.3593 | Train F1: 1.00, Val F1: 0.36
Epoch: 080 | Train loss: 0.9049, Val loss: 1.3592 | Train F1: 1.00, Val F1: 0.36
Epoch: 081 | Train loss: 0.9049, Val loss: 1.3591 | Train F1: 1.00, Val F1: 0.36
Epoch: 082 | Train loss: 0.9049, Val loss: 1.3590 | Train F1: 1.00, Val F1: 0.36
Epoch: 083 | Train loss: 0.9049, Val loss: 1.3589 | Train F1: 1.00, Val F1: 0.36
Epoch: 084 | Train loss: 0.9049, Val loss: 1.3588 | Train F1: 1.00, Val F1: 0.36
Epoch: 085 | Train loss: 0.9049, Val loss: 1.3588 | Train F1: 1.00, Val F1: 0.36
Epoch: 086 | Train loss: 0.9049, Val loss: 1.3587 | Train F1: 1.00, Val F1: 0.36
Epoch: 087 | Train loss: 0.9049, Val loss: 1.3587 | Train F1: 1.00, Val F1: 0.36
Epoch: 088 | Train loss: 0.9049, Val loss: 1.3587 | Train F1: 1.00, Val F1: 0.36
Epoch: 089 | Train loss: 0.9049, Val loss: 1.3587 | Train F1: 1.00, Val F1: 0.36
Epoch: 090 | Train loss: 0.9049, Val loss: 1.3587 | Train F1: 1.00, Val F1: 0.36
Epoch: 091 | Train loss: 0.9049, Val loss: 1.3587 | Train F1: 1.00, Val F1: 0.36
Epoch: 092 | Train loss: 0.9049, Val loss: 1.3587 | Train F1: 1.00, Val F1: 0.36
Epoch: 093 | Train loss: 0.9049, Val loss: 1.3587 | Train F1: 1.00, Val F1: 0.36
Epoch: 094 | Train loss: 0.9049, Val loss: 1.3588 | Train F1: 1.00, Val F1: 0.36
Epoch: 095 | Train loss: 0.9049, Val loss: 1.3588 | Train F1: 1.00, Val F1: 0.36
Epoch: 096 | Train loss: 0.9049, Val loss: 1.3588 | Train F1: 1.00, Val F1: 0.36
Epoch: 097 | Train loss: 0.9049, Val loss: 1.3588 | Train F1: 1.00, Val F1: 0.36
Best model:
Train loss: 0.9049, Val loss: 1.3631, Test loss: 1.3975
Train F1: 1.00, Val F1: 0.35, Test F1: 0.31

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=0.4, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5690, Val loss: 1.5846 | Train F1: 0.70, Val F1: 0.26
Epoch: 001 | Train loss: 1.5179, Val loss: 1.5475 | Train F1: 0.39, Val F1: 0.26
Epoch: 002 | Train loss: 1.4571, Val loss: 1.5031 | Train F1: 0.38, Val F1: 0.28
Epoch: 003 | Train loss: 1.3927, Val loss: 1.4558 | Train F1: 0.35, Val F1: 0.34
Epoch: 004 | Train loss: 1.3350, Val loss: 1.4134 | Train F1: 0.37, Val F1: 0.36
Epoch: 005 | Train loss: 1.2859, Val loss: 1.3790 | Train F1: 0.39, Val F1: 0.38
Epoch: 006 | Train loss: 1.2438, Val loss: 1.3523 | Train F1: 0.42, Val F1: 0.40
Epoch: 007 | Train loss: 1.2087, Val loss: 1.3331 | Train F1: 0.45, Val F1: 0.41
Epoch: 008 | Train loss: 1.1791, Val loss: 1.3205 | Train F1: 0.45, Val F1: 0.42
Epoch: 009 | Train loss: 1.1494, Val loss: 1.3085 | Train F1: 0.65, Val F1: 0.40
Epoch: 010 | Train loss: 1.1189, Val loss: 1.2959 | Train F1: 0.71, Val F1: 0.42
Epoch: 011 | Train loss: 1.0907, Val loss: 1.2855 | Train F1: 0.79, Val F1: 0.42
Epoch: 012 | Train loss: 1.0699, Val loss: 1.2784 | Train F1: 0.82, Val F1: 0.42
Epoch: 013 | Train loss: 1.0572, Val loss: 1.2767 | Train F1: 0.81, Val F1: 0.39
Epoch: 014 | Train loss: 1.0476, Val loss: 1.2808 | Train F1: 0.81, Val F1: 0.38
Epoch: 015 | Train loss: 1.0382, Val loss: 1.2881 | Train F1: 0.82, Val F1: 0.37
Epoch: 016 | Train loss: 1.0295, Val loss: 1.2966 | Train F1: 0.82, Val F1: 0.38
Epoch: 017 | Train loss: 1.0224, Val loss: 1.3034 | Train F1: 0.84, Val F1: 0.39
Epoch: 018 | Train loss: 1.0138, Val loss: 1.3014 | Train F1: 0.84, Val F1: 0.39
Epoch: 019 | Train loss: 1.0041, Val loss: 1.2935 | Train F1: 0.84, Val F1: 0.38
Epoch: 020 | Train loss: 0.9951, Val loss: 1.2858 | Train F1: 0.84, Val F1: 0.38
Epoch: 021 | Train loss: 0.9871, Val loss: 1.2790 | Train F1: 0.91, Val F1: 0.39
Epoch: 022 | Train loss: 0.9804, Val loss: 1.2733 | Train F1: 0.91, Val F1: 0.38
Epoch: 023 | Train loss: 0.9748, Val loss: 1.2689 | Train F1: 0.91, Val F1: 0.38
Epoch: 024 | Train loss: 0.9697, Val loss: 1.2655 | Train F1: 0.95, Val F1: 0.44
Epoch: 025 | Train loss: 0.9649, Val loss: 1.2633 | Train F1: 0.95, Val F1: 0.44
Epoch: 026 | Train loss: 0.9605, Val loss: 1.2620 | Train F1: 0.95, Val F1: 0.44
Epoch: 027 | Train loss: 0.9567, Val loss: 1.2616 | Train F1: 0.95, Val F1: 0.46
Epoch: 028 | Train loss: 0.9536, Val loss: 1.2621 | Train F1: 0.99, Val F1: 0.46
Epoch: 029 | Train loss: 0.9509, Val loss: 1.2633 | Train F1: 0.99, Val F1: 0.46
Epoch: 030 | Train loss: 0.9484, Val loss: 1.2649 | Train F1: 0.99, Val F1: 0.47
Epoch: 031 | Train loss: 0.9461, Val loss: 1.2668 | Train F1: 0.99, Val F1: 0.46
Epoch: 032 | Train loss: 0.9439, Val loss: 1.2690 | Train F1: 0.99, Val F1: 0.48
Epoch: 033 | Train loss: 0.9419, Val loss: 1.2713 | Train F1: 0.99, Val F1: 0.48
Epoch: 034 | Train loss: 0.9399, Val loss: 1.2735 | Train F1: 0.99, Val F1: 0.47
Epoch: 035 | Train loss: 0.9381, Val loss: 1.2756 | Train F1: 0.99, Val F1: 0.46
Epoch: 036 | Train loss: 0.9364, Val loss: 1.2779 | Train F1: 0.99, Val F1: 0.46
Epoch: 037 | Train loss: 0.9348, Val loss: 1.2803 | Train F1: 0.99, Val F1: 0.46
Epoch: 038 | Train loss: 0.9334, Val loss: 1.2822 | Train F1: 0.99, Val F1: 0.46
Epoch: 039 | Train loss: 0.9321, Val loss: 1.2833 | Train F1: 0.99, Val F1: 0.43
Epoch: 040 | Train loss: 0.9310, Val loss: 1.2837 | Train F1: 0.99, Val F1: 0.43
Epoch: 041 | Train loss: 0.9300, Val loss: 1.2840 | Train F1: 0.99, Val F1: 0.44
Epoch: 042 | Train loss: 0.9291, Val loss: 1.2840 | Train F1: 0.99, Val F1: 0.44
Epoch: 043 | Train loss: 0.9283, Val loss: 1.2841 | Train F1: 0.99, Val F1: 0.44
Epoch: 044 | Train loss: 0.9277, Val loss: 1.2843 | Train F1: 0.99, Val F1: 0.44
Epoch: 045 | Train loss: 0.9271, Val loss: 1.2837 | Train F1: 0.99, Val F1: 0.44
Epoch: 046 | Train loss: 0.9267, Val loss: 1.2830 | Train F1: 0.99, Val F1: 0.44
Epoch: 047 | Train loss: 0.9263, Val loss: 1.2820 | Train F1: 0.99, Val F1: 0.44
Epoch: 048 | Train loss: 0.9260, Val loss: 1.2806 | Train F1: 0.99, Val F1: 0.44
Epoch: 049 | Train loss: 0.9258, Val loss: 1.2796 | Train F1: 0.98, Val F1: 0.42
Epoch: 050 | Train loss: 0.9257, Val loss: 1.2792 | Train F1: 0.98, Val F1: 0.42
Epoch: 051 | Train loss: 0.9256, Val loss: 1.2778 | Train F1: 0.98, Val F1: 0.42
Epoch: 052 | Train loss: 0.9254, Val loss: 1.2755 | Train F1: 0.98, Val F1: 0.42
Epoch: 053 | Train loss: 0.9253, Val loss: 1.2725 | Train F1: 0.98, Val F1: 0.42
Epoch: 054 | Train loss: 0.9251, Val loss: 1.2696 | Train F1: 0.98, Val F1: 0.40
Epoch: 055 | Train loss: 0.9250, Val loss: 1.2660 | Train F1: 0.99, Val F1: 0.42
Epoch: 056 | Train loss: 0.9250, Val loss: 1.2621 | Train F1: 0.99, Val F1: 0.42
Epoch: 057 | Train loss: 0.9250, Val loss: 1.2591 | Train F1: 0.99, Val F1: 0.42
Epoch: 058 | Train loss: 0.9250, Val loss: 1.2564 | Train F1: 0.99, Val F1: 0.42
Epoch: 059 | Train loss: 0.9250, Val loss: 1.2544 | Train F1: 0.99, Val F1: 0.43
Epoch: 060 | Train loss: 0.9250, Val loss: 1.2518 | Train F1: 0.99, Val F1: 0.43
Epoch: 061 | Train loss: 0.9251, Val loss: 1.2487 | Train F1: 0.99, Val F1: 0.43
Epoch: 062 | Train loss: 0.9251, Val loss: 1.2470 | Train F1: 0.99, Val F1: 0.43
Epoch: 063 | Train loss: 0.9250, Val loss: 1.2462 | Train F1: 0.99, Val F1: 0.43
Epoch: 064 | Train loss: 0.9249, Val loss: 1.2465 | Train F1: 0.99, Val F1: 0.43
Epoch: 065 | Train loss: 0.9248, Val loss: 1.2471 | Train F1: 0.99, Val F1: 0.43
Epoch: 066 | Train loss: 0.9247, Val loss: 1.2471 | Train F1: 0.99, Val F1: 0.40
Epoch: 067 | Train loss: 0.9246, Val loss: 1.2478 | Train F1: 0.99, Val F1: 0.40
Epoch: 068 | Train loss: 0.9246, Val loss: 1.2489 | Train F1: 0.98, Val F1: 0.40
Epoch: 069 | Train loss: 0.9246, Val loss: 1.2489 | Train F1: 0.98, Val F1: 0.40
Epoch: 070 | Train loss: 0.9245, Val loss: 1.2480 | Train F1: 0.98, Val F1: 0.40
Epoch: 071 | Train loss: 0.9245, Val loss: 1.2471 | Train F1: 0.98, Val F1: 0.40
Epoch: 072 | Train loss: 0.9245, Val loss: 1.2470 | Train F1: 0.98, Val F1: 0.40
Epoch: 073 | Train loss: 0.9245, Val loss: 1.2470 | Train F1: 0.98, Val F1: 0.40
Epoch: 074 | Train loss: 0.9245, Val loss: 1.2461 | Train F1: 0.98, Val F1: 0.40
Epoch: 075 | Train loss: 0.9245, Val loss: 1.2442 | Train F1: 0.98, Val F1: 0.40
Epoch: 076 | Train loss: 0.9245, Val loss: 1.2421 | Train F1: 0.99, Val F1: 0.40
Epoch: 077 | Train loss: 0.9245, Val loss: 1.2412 | Train F1: 0.99, Val F1: 0.40
Epoch: 078 | Train loss: 0.9244, Val loss: 1.2405 | Train F1: 0.99, Val F1: 0.40
Epoch: 079 | Train loss: 0.9244, Val loss: 1.2405 | Train F1: 0.99, Val F1: 0.40
Epoch: 080 | Train loss: 0.9244, Val loss: 1.2402 | Train F1: 0.99, Val F1: 0.40
Epoch: 081 | Train loss: 0.9244, Val loss: 1.2391 | Train F1: 0.99, Val F1: 0.40
Epoch: 082 | Train loss: 0.9244, Val loss: 1.2387 | Train F1: 0.99, Val F1: 0.40
Epoch: 083 | Train loss: 0.9244, Val loss: 1.2374 | Train F1: 0.99, Val F1: 0.40
Epoch: 084 | Train loss: 0.9245, Val loss: 1.2349 | Train F1: 0.99, Val F1: 0.43
Epoch: 085 | Train loss: 0.9247, Val loss: 1.2327 | Train F1: 0.99, Val F1: 0.43
Epoch: 086 | Train loss: 0.9248, Val loss: 1.2311 | Train F1: 0.99, Val F1: 0.44
Epoch: 087 | Train loss: 0.9248, Val loss: 1.2303 | Train F1: 0.99, Val F1: 0.44
Epoch: 088 | Train loss: 0.9247, Val loss: 1.2305 | Train F1: 0.99, Val F1: 0.44
Epoch: 089 | Train loss: 0.9245, Val loss: 1.2320 | Train F1: 0.99, Val F1: 0.44
Epoch: 090 | Train loss: 0.9241, Val loss: 1.2346 | Train F1: 0.99, Val F1: 0.44
Epoch: 091 | Train loss: 0.9239, Val loss: 1.2379 | Train F1: 0.98, Val F1: 0.44
Epoch: 092 | Train loss: 0.9236, Val loss: 1.2419 | Train F1: 0.98, Val F1: 0.44
Epoch: 093 | Train loss: 0.9232, Val loss: 1.2445 | Train F1: 0.98, Val F1: 0.44
Epoch: 094 | Train loss: 0.9207, Val loss: 1.2450 | Train F1: 0.98, Val F1: 0.44
Epoch: 095 | Train loss: 0.9146, Val loss: 1.2427 | Train F1: 1.00, Val F1: 0.44
Epoch: 096 | Train loss: 0.9073, Val loss: 1.2366 | Train F1: 1.00, Val F1: 0.44
Epoch: 097 | Train loss: 0.9053, Val loss: 1.2311 | Train F1: 1.00, Val F1: 0.45
Epoch: 098 | Train loss: 0.9051, Val loss: 1.2293 | Train F1: 1.00, Val F1: 0.45
Epoch: 099 | Train loss: 0.9051, Val loss: 1.2303 | Train F1: 1.00, Val F1: 0.45
Epoch: 100 | Train loss: 0.9051, Val loss: 1.2334 | Train F1: 1.00, Val F1: 0.45
Epoch: 101 | Train loss: 0.9053, Val loss: 1.2382 | Train F1: 1.00, Val F1: 0.45
Epoch: 102 | Train loss: 0.9054, Val loss: 1.2378 | Train F1: 1.00, Val F1: 0.45
Epoch: 103 | Train loss: 0.9054, Val loss: 1.2368 | Train F1: 1.00, Val F1: 0.46
Epoch: 104 | Train loss: 0.9054, Val loss: 1.2316 | Train F1: 1.00, Val F1: 0.49
Epoch: 105 | Train loss: 0.9053, Val loss: 1.2233 | Train F1: 1.00, Val F1: 0.49
Epoch: 106 | Train loss: 0.9052, Val loss: 1.2148 | Train F1: 1.00, Val F1: 0.49
Epoch: 107 | Train loss: 0.9052, Val loss: 1.2079 | Train F1: 1.00, Val F1: 0.49
Epoch: 108 | Train loss: 0.9051, Val loss: 1.2019 | Train F1: 1.00, Val F1: 0.49
Epoch: 109 | Train loss: 0.9051, Val loss: 1.1965 | Train F1: 1.00, Val F1: 0.49
Epoch: 110 | Train loss: 0.9051, Val loss: 1.1917 | Train F1: 1.00, Val F1: 0.49
Epoch: 111 | Train loss: 0.9050, Val loss: 1.1873 | Train F1: 1.00, Val F1: 0.46
Epoch: 112 | Train loss: 0.9050, Val loss: 1.1832 | Train F1: 1.00, Val F1: 0.46
Epoch: 113 | Train loss: 0.9050, Val loss: 1.1795 | Train F1: 1.00, Val F1: 0.46
Epoch: 114 | Train loss: 0.9050, Val loss: 1.1764 | Train F1: 1.00, Val F1: 0.47
Epoch: 115 | Train loss: 0.9050, Val loss: 1.1744 | Train F1: 1.00, Val F1: 0.47
Epoch: 116 | Train loss: 0.9050, Val loss: 1.1736 | Train F1: 1.00, Val F1: 0.47
Epoch: 117 | Train loss: 0.9049, Val loss: 1.1734 | Train F1: 1.00, Val F1: 0.47
Epoch: 118 | Train loss: 0.9049, Val loss: 1.1738 | Train F1: 1.00, Val F1: 0.47
Epoch: 119 | Train loss: 0.9049, Val loss: 1.1745 | Train F1: 1.00, Val F1: 0.47
Epoch: 120 | Train loss: 0.9049, Val loss: 1.1753 | Train F1: 1.00, Val F1: 0.47
Epoch: 121 | Train loss: 0.9049, Val loss: 1.1763 | Train F1: 1.00, Val F1: 0.47
Epoch: 122 | Train loss: 0.9049, Val loss: 1.1772 | Train F1: 1.00, Val F1: 0.47
Epoch: 123 | Train loss: 0.9049, Val loss: 1.1780 | Train F1: 1.00, Val F1: 0.47
Epoch: 124 | Train loss: 0.9049, Val loss: 1.1788 | Train F1: 1.00, Val F1: 0.47
Epoch: 125 | Train loss: 0.9049, Val loss: 1.1795 | Train F1: 1.00, Val F1: 0.47
Epoch: 126 | Train loss: 0.9049, Val loss: 1.1802 | Train F1: 1.00, Val F1: 0.47
Epoch: 127 | Train loss: 0.9049, Val loss: 1.1808 | Train F1: 1.00, Val F1: 0.47
Epoch: 128 | Train loss: 0.9049, Val loss: 1.1814 | Train F1: 1.00, Val F1: 0.47
Epoch: 129 | Train loss: 0.9049, Val loss: 1.1818 | Train F1: 1.00, Val F1: 0.47
Epoch: 130 | Train loss: 0.9049, Val loss: 1.1823 | Train F1: 1.00, Val F1: 0.47
Epoch: 131 | Train loss: 0.9049, Val loss: 1.1827 | Train F1: 1.00, Val F1: 0.47
Epoch: 132 | Train loss: 0.9049, Val loss: 1.1830 | Train F1: 1.00, Val F1: 0.47
Epoch: 133 | Train loss: 0.9049, Val loss: 1.1833 | Train F1: 1.00, Val F1: 0.47
Epoch: 134 | Train loss: 0.9049, Val loss: 1.1836 | Train F1: 1.00, Val F1: 0.47
Epoch: 135 | Train loss: 0.9049, Val loss: 1.1838 | Train F1: 1.00, Val F1: 0.47
Epoch: 136 | Train loss: 0.9049, Val loss: 1.1840 | Train F1: 1.00, Val F1: 0.47
Epoch: 137 | Train loss: 0.9049, Val loss: 1.1842 | Train F1: 1.00, Val F1: 0.47
Epoch: 138 | Train loss: 0.9049, Val loss: 1.1844 | Train F1: 1.00, Val F1: 0.47
Epoch: 139 | Train loss: 0.9049, Val loss: 1.1845 | Train F1: 1.00, Val F1: 0.47
Epoch: 140 | Train loss: 0.9049, Val loss: 1.1846 | Train F1: 1.00, Val F1: 0.47
Epoch: 141 | Train loss: 0.9049, Val loss: 1.1847 | Train F1: 1.00, Val F1: 0.47
Epoch: 142 | Train loss: 0.9049, Val loss: 1.1849 | Train F1: 1.00, Val F1: 0.47
Epoch: 143 | Train loss: 0.9049, Val loss: 1.1850 | Train F1: 1.00, Val F1: 0.47
Best model:
Train loss: 0.9050, Val loss: 1.1795, Test loss: 1.1865
Train F1: 1.00, Val F1: 0.46, Test F1: 0.53

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=0.6, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5726, Val loss: 1.5868 | Train F1: 0.34, Val F1: 0.33
Epoch: 001 | Train loss: 1.5318, Val loss: 1.5593 | Train F1: 0.34, Val F1: 0.26
Epoch: 002 | Train loss: 1.4832, Val loss: 1.5270 | Train F1: 0.35, Val F1: 0.26
Epoch: 003 | Train loss: 1.4311, Val loss: 1.4918 | Train F1: 0.35, Val F1: 0.27
Epoch: 004 | Train loss: 1.3813, Val loss: 1.4541 | Train F1: 0.37, Val F1: 0.28
Epoch: 005 | Train loss: 1.3355, Val loss: 1.4139 | Train F1: 0.39, Val F1: 0.28
Epoch: 006 | Train loss: 1.2964, Val loss: 1.3764 | Train F1: 0.38, Val F1: 0.30
Epoch: 007 | Train loss: 1.2680, Val loss: 1.3484 | Train F1: 0.41, Val F1: 0.30
Epoch: 008 | Train loss: 1.2473, Val loss: 1.3307 | Train F1: 0.42, Val F1: 0.35
Epoch: 009 | Train loss: 1.2279, Val loss: 1.3192 | Train F1: 0.42, Val F1: 0.35
Epoch: 010 | Train loss: 1.2090, Val loss: 1.3097 | Train F1: 0.43, Val F1: 0.35
Epoch: 011 | Train loss: 1.1902, Val loss: 1.3007 | Train F1: 0.42, Val F1: 0.39
Epoch: 012 | Train loss: 1.1717, Val loss: 1.2915 | Train F1: 0.46, Val F1: 0.41
Epoch: 013 | Train loss: 1.1546, Val loss: 1.2826 | Train F1: 0.46, Val F1: 0.41
Epoch: 014 | Train loss: 1.1404, Val loss: 1.2743 | Train F1: 0.48, Val F1: 0.40
Epoch: 015 | Train loss: 1.1289, Val loss: 1.2677 | Train F1: 0.49, Val F1: 0.43
Epoch: 016 | Train loss: 1.1217, Val loss: 1.2650 | Train F1: 0.50, Val F1: 0.43
Epoch: 017 | Train loss: 1.1172, Val loss: 1.2641 | Train F1: 0.50, Val F1: 0.42
Epoch: 018 | Train loss: 1.1125, Val loss: 1.2625 | Train F1: 0.50, Val F1: 0.41
Epoch: 019 | Train loss: 1.1050, Val loss: 1.2569 | Train F1: 0.50, Val F1: 0.43
Epoch: 020 | Train loss: 1.0941, Val loss: 1.2486 | Train F1: 0.50, Val F1: 0.43
Epoch: 021 | Train loss: 1.0800, Val loss: 1.2402 | Train F1: 0.72, Val F1: 0.44
Epoch: 022 | Train loss: 1.0651, Val loss: 1.2321 | Train F1: 0.72, Val F1: 0.42
Epoch: 023 | Train loss: 1.0507, Val loss: 1.2253 | Train F1: 0.72, Val F1: 0.42
Epoch: 024 | Train loss: 1.0363, Val loss: 1.2190 | Train F1: 0.75, Val F1: 0.42
Epoch: 025 | Train loss: 1.0232, Val loss: 1.2131 | Train F1: 0.80, Val F1: 0.50
Epoch: 026 | Train loss: 1.0143, Val loss: 1.2087 | Train F1: 0.84, Val F1: 0.50
Epoch: 027 | Train loss: 1.0078, Val loss: 1.2052 | Train F1: 0.84, Val F1: 0.49
Epoch: 028 | Train loss: 1.0021, Val loss: 1.2006 | Train F1: 0.84, Val F1: 0.49
Epoch: 029 | Train loss: 0.9973, Val loss: 1.1963 | Train F1: 0.84, Val F1: 0.57
Epoch: 030 | Train loss: 0.9926, Val loss: 1.1919 | Train F1: 0.85, Val F1: 0.56
Epoch: 031 | Train loss: 0.9863, Val loss: 1.1849 | Train F1: 0.88, Val F1: 0.59
Epoch: 032 | Train loss: 0.9788, Val loss: 1.1759 | Train F1: 0.89, Val F1: 0.60
Epoch: 033 | Train loss: 0.9704, Val loss: 1.1663 | Train F1: 0.89, Val F1: 0.55
Epoch: 034 | Train loss: 0.9589, Val loss: 1.1555 | Train F1: 0.99, Val F1: 0.64
Epoch: 035 | Train loss: 0.9459, Val loss: 1.1460 | Train F1: 0.99, Val F1: 0.64
Epoch: 036 | Train loss: 0.9362, Val loss: 1.1416 | Train F1: 0.99, Val F1: 0.65
Epoch: 037 | Train loss: 0.9282, Val loss: 1.1421 | Train F1: 0.99, Val F1: 0.65
Epoch: 038 | Train loss: 0.9216, Val loss: 1.1461 | Train F1: 1.00, Val F1: 0.65
Epoch: 039 | Train loss: 0.9194, Val loss: 1.1511 | Train F1: 1.00, Val F1: 0.60
Epoch: 040 | Train loss: 0.9195, Val loss: 1.1568 | Train F1: 1.00, Val F1: 0.60
Epoch: 041 | Train loss: 0.9197, Val loss: 1.1607 | Train F1: 1.00, Val F1: 0.59
Epoch: 042 | Train loss: 0.9167, Val loss: 1.1562 | Train F1: 1.00, Val F1: 0.59
Epoch: 043 | Train loss: 0.9138, Val loss: 1.1497 | Train F1: 1.00, Val F1: 0.59
Epoch: 044 | Train loss: 0.9118, Val loss: 1.1442 | Train F1: 1.00, Val F1: 0.58
Epoch: 045 | Train loss: 0.9103, Val loss: 1.1401 | Train F1: 1.00, Val F1: 0.61
Epoch: 046 | Train loss: 0.9093, Val loss: 1.1371 | Train F1: 1.00, Val F1: 0.61
Epoch: 047 | Train loss: 0.9084, Val loss: 1.1349 | Train F1: 1.00, Val F1: 0.61
Epoch: 048 | Train loss: 0.9078, Val loss: 1.1332 | Train F1: 1.00, Val F1: 0.61
Epoch: 049 | Train loss: 0.9073, Val loss: 1.1319 | Train F1: 1.00, Val F1: 0.61
Epoch: 050 | Train loss: 0.9069, Val loss: 1.1310 | Train F1: 1.00, Val F1: 0.61
Epoch: 051 | Train loss: 0.9066, Val loss: 1.1304 | Train F1: 1.00, Val F1: 0.59
Epoch: 052 | Train loss: 0.9063, Val loss: 1.1301 | Train F1: 1.00, Val F1: 0.60
Epoch: 053 | Train loss: 0.9061, Val loss: 1.1300 | Train F1: 1.00, Val F1: 0.60
Epoch: 054 | Train loss: 0.9059, Val loss: 1.1300 | Train F1: 1.00, Val F1: 0.60
Epoch: 055 | Train loss: 0.9057, Val loss: 1.1301 | Train F1: 1.00, Val F1: 0.60
Epoch: 056 | Train loss: 0.9056, Val loss: 1.1303 | Train F1: 1.00, Val F1: 0.60
Epoch: 057 | Train loss: 0.9055, Val loss: 1.1304 | Train F1: 1.00, Val F1: 0.58
Epoch: 058 | Train loss: 0.9054, Val loss: 1.1306 | Train F1: 1.00, Val F1: 0.58
Epoch: 059 | Train loss: 0.9054, Val loss: 1.1307 | Train F1: 1.00, Val F1: 0.58
Epoch: 060 | Train loss: 0.9053, Val loss: 1.1306 | Train F1: 1.00, Val F1: 0.58
Epoch: 061 | Train loss: 0.9053, Val loss: 1.1302 | Train F1: 1.00, Val F1: 0.58
Epoch: 062 | Train loss: 0.9052, Val loss: 1.1300 | Train F1: 1.00, Val F1: 0.58
Epoch: 063 | Train loss: 0.9052, Val loss: 1.1297 | Train F1: 1.00, Val F1: 0.58
Epoch: 064 | Train loss: 0.9051, Val loss: 1.1296 | Train F1: 1.00, Val F1: 0.58
Epoch: 065 | Train loss: 0.9051, Val loss: 1.1294 | Train F1: 1.00, Val F1: 0.58
Epoch: 066 | Train loss: 0.9051, Val loss: 1.1293 | Train F1: 1.00, Val F1: 0.58
Epoch: 067 | Train loss: 0.9051, Val loss: 1.1292 | Train F1: 1.00, Val F1: 0.58
Epoch: 068 | Train loss: 0.9051, Val loss: 1.1291 | Train F1: 1.00, Val F1: 0.58
Epoch: 069 | Train loss: 0.9051, Val loss: 1.1290 | Train F1: 1.00, Val F1: 0.58
Epoch: 070 | Train loss: 0.9050, Val loss: 1.1290 | Train F1: 1.00, Val F1: 0.58
Epoch: 071 | Train loss: 0.9050, Val loss: 1.1289 | Train F1: 1.00, Val F1: 0.58
Epoch: 072 | Train loss: 0.9050, Val loss: 1.1289 | Train F1: 1.00, Val F1: 0.58
Epoch: 073 | Train loss: 0.9050, Val loss: 1.1289 | Train F1: 1.00, Val F1: 0.58
Epoch: 074 | Train loss: 0.9050, Val loss: 1.1288 | Train F1: 1.00, Val F1: 0.58
Epoch: 075 | Train loss: 0.9050, Val loss: 1.1288 | Train F1: 1.00, Val F1: 0.58
Epoch: 076 | Train loss: 0.9050, Val loss: 1.1288 | Train F1: 1.00, Val F1: 0.58
Epoch: 077 | Train loss: 0.9050, Val loss: 1.1288 | Train F1: 1.00, Val F1: 0.58
Epoch: 078 | Train loss: 0.9050, Val loss: 1.1288 | Train F1: 1.00, Val F1: 0.58
Best model:
Train loss: 0.9078, Val loss: 1.1332, Test loss: 1.1861
Train F1: 1.00, Val F1: 0.61, Test F1: 0.52

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=0.8, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5734, Val loss: 1.5761 | Train F1: 0.16, Val F1: 0.13
Epoch: 001 | Train loss: 1.5311, Val loss: 1.5409 | Train F1: 0.25, Val F1: 0.23
Epoch: 002 | Train loss: 1.4828, Val loss: 1.5014 | Train F1: 0.27, Val F1: 0.25
Epoch: 003 | Train loss: 1.4321, Val loss: 1.4599 | Train F1: 0.30, Val F1: 0.27
Epoch: 004 | Train loss: 1.3842, Val loss: 1.4206 | Train F1: 0.31, Val F1: 0.28
Epoch: 005 | Train loss: 1.3428, Val loss: 1.3888 | Train F1: 0.33, Val F1: 0.30
Epoch: 006 | Train loss: 1.3107, Val loss: 1.3656 | Train F1: 0.33, Val F1: 0.30
Epoch: 007 | Train loss: 1.2867, Val loss: 1.3486 | Train F1: 0.32, Val F1: 0.30
Epoch: 008 | Train loss: 1.2659, Val loss: 1.3331 | Train F1: 0.33, Val F1: 0.30
Epoch: 009 | Train loss: 1.2484, Val loss: 1.3205 | Train F1: 0.33, Val F1: 0.30
Epoch: 010 | Train loss: 1.2336, Val loss: 1.3092 | Train F1: 0.33, Val F1: 0.31
Epoch: 011 | Train loss: 1.2211, Val loss: 1.3000 | Train F1: 0.33, Val F1: 0.31
Epoch: 012 | Train loss: 1.2091, Val loss: 1.2921 | Train F1: 0.39, Val F1: 0.31
Epoch: 013 | Train loss: 1.1974, Val loss: 1.2852 | Train F1: 0.41, Val F1: 0.31
Epoch: 014 | Train loss: 1.1863, Val loss: 1.2791 | Train F1: 0.41, Val F1: 0.32
Epoch: 015 | Train loss: 1.1761, Val loss: 1.2730 | Train F1: 0.41, Val F1: 0.32
Epoch: 016 | Train loss: 1.1663, Val loss: 1.2671 | Train F1: 0.50, Val F1: 0.32
Epoch: 017 | Train loss: 1.1565, Val loss: 1.2617 | Train F1: 0.52, Val F1: 0.32
Epoch: 018 | Train loss: 1.1482, Val loss: 1.2566 | Train F1: 0.52, Val F1: 0.32
Epoch: 019 | Train loss: 1.1411, Val loss: 1.2515 | Train F1: 0.52, Val F1: 0.32
Epoch: 020 | Train loss: 1.1332, Val loss: 1.2459 | Train F1: 0.54, Val F1: 0.37
Epoch: 021 | Train loss: 1.1232, Val loss: 1.2387 | Train F1: 0.54, Val F1: 0.37
Epoch: 022 | Train loss: 1.1104, Val loss: 1.2305 | Train F1: 0.56, Val F1: 0.43
Epoch: 023 | Train loss: 1.0965, Val loss: 1.2232 | Train F1: 0.62, Val F1: 0.44
Epoch: 024 | Train loss: 1.0846, Val loss: 1.2169 | Train F1: 0.62, Val F1: 0.44
Epoch: 025 | Train loss: 1.0781, Val loss: 1.2136 | Train F1: 0.61, Val F1: 0.43
Epoch: 026 | Train loss: 1.0752, Val loss: 1.2131 | Train F1: 0.61, Val F1: 0.45
Epoch: 027 | Train loss: 1.0701, Val loss: 1.2105 | Train F1: 0.66, Val F1: 0.53
Epoch: 028 | Train loss: 1.0625, Val loss: 1.2049 | Train F1: 0.66, Val F1: 0.56
Epoch: 029 | Train loss: 1.0545, Val loss: 1.1974 | Train F1: 0.66, Val F1: 0.56
Epoch: 030 | Train loss: 1.0475, Val loss: 1.1921 | Train F1: 0.81, Val F1: 0.54
Epoch: 031 | Train loss: 1.0412, Val loss: 1.1877 | Train F1: 0.81, Val F1: 0.53
Epoch: 032 | Train loss: 1.0363, Val loss: 1.1835 | Train F1: 0.81, Val F1: 0.54
Epoch: 033 | Train loss: 1.0329, Val loss: 1.1794 | Train F1: 0.81, Val F1: 0.55
Epoch: 034 | Train loss: 1.0304, Val loss: 1.1756 | Train F1: 0.81, Val F1: 0.55
Epoch: 035 | Train loss: 1.0281, Val loss: 1.1721 | Train F1: 0.81, Val F1: 0.55
Epoch: 036 | Train loss: 1.0252, Val loss: 1.1691 | Train F1: 0.81, Val F1: 0.55
Epoch: 037 | Train loss: 1.0208, Val loss: 1.1662 | Train F1: 0.81, Val F1: 0.55
Epoch: 038 | Train loss: 1.0151, Val loss: 1.1638 | Train F1: 0.89, Val F1: 0.62
Epoch: 039 | Train loss: 1.0106, Val loss: 1.1620 | Train F1: 0.90, Val F1: 0.62
Epoch: 040 | Train loss: 1.0072, Val loss: 1.1606 | Train F1: 0.90, Val F1: 0.62
Epoch: 041 | Train loss: 1.0030, Val loss: 1.1594 | Train F1: 0.90, Val F1: 0.63
Epoch: 042 | Train loss: 0.9940, Val loss: 1.1579 | Train F1: 0.90, Val F1: 0.62
Epoch: 043 | Train loss: 0.9788, Val loss: 1.1565 | Train F1: 0.93, Val F1: 0.63
Epoch: 044 | Train loss: 0.9742, Val loss: 1.1648 | Train F1: 0.93, Val F1: 0.63
Epoch: 045 | Train loss: 0.9750, Val loss: 1.1759 | Train F1: 0.93, Val F1: 0.59
Epoch: 046 | Train loss: 0.9761, Val loss: 1.1822 | Train F1: 0.93, Val F1: 0.58
Epoch: 047 | Train loss: 0.9751, Val loss: 1.1829 | Train F1: 0.92, Val F1: 0.56
Epoch: 048 | Train loss: 0.9698, Val loss: 1.1721 | Train F1: 0.92, Val F1: 0.56
Epoch: 049 | Train loss: 0.9670, Val loss: 1.1605 | Train F1: 0.93, Val F1: 0.58
Epoch: 050 | Train loss: 0.9660, Val loss: 1.1539 | Train F1: 0.93, Val F1: 0.58
Epoch: 051 | Train loss: 0.9655, Val loss: 1.1498 | Train F1: 0.93, Val F1: 0.58
Epoch: 052 | Train loss: 0.9651, Val loss: 1.1470 | Train F1: 0.93, Val F1: 0.58
Epoch: 053 | Train loss: 0.9647, Val loss: 1.1447 | Train F1: 0.93, Val F1: 0.61
Epoch: 054 | Train loss: 0.9642, Val loss: 1.1428 | Train F1: 0.93, Val F1: 0.61
Epoch: 055 | Train loss: 0.9635, Val loss: 1.1412 | Train F1: 0.96, Val F1: 0.60
Epoch: 056 | Train loss: 0.9627, Val loss: 1.1398 | Train F1: 0.96, Val F1: 0.60
Epoch: 057 | Train loss: 0.9618, Val loss: 1.1385 | Train F1: 0.96, Val F1: 0.60
Epoch: 058 | Train loss: 0.9608, Val loss: 1.1375 | Train F1: 0.96, Val F1: 0.60
Epoch: 059 | Train loss: 0.9598, Val loss: 1.1365 | Train F1: 0.96, Val F1: 0.60
Epoch: 060 | Train loss: 0.9590, Val loss: 1.1358 | Train F1: 0.94, Val F1: 0.59
Epoch: 061 | Train loss: 0.9584, Val loss: 1.1351 | Train F1: 0.94, Val F1: 0.59
Epoch: 062 | Train loss: 0.9579, Val loss: 1.1344 | Train F1: 0.94, Val F1: 0.59
Epoch: 063 | Train loss: 0.9575, Val loss: 1.1338 | Train F1: 0.94, Val F1: 0.59
Epoch: 064 | Train loss: 0.9570, Val loss: 1.1333 | Train F1: 0.94, Val F1: 0.59
Epoch: 065 | Train loss: 0.9566, Val loss: 1.1327 | Train F1: 0.94, Val F1: 0.59
Epoch: 066 | Train loss: 0.9561, Val loss: 1.1320 | Train F1: 0.94, Val F1: 0.59
Epoch: 067 | Train loss: 0.9552, Val loss: 1.1311 | Train F1: 0.94, Val F1: 0.59
Epoch: 068 | Train loss: 0.9539, Val loss: 1.1297 | Train F1: 0.94, Val F1: 0.59
Epoch: 069 | Train loss: 0.9507, Val loss: 1.1270 | Train F1: 0.94, Val F1: 0.59
Epoch: 070 | Train loss: 0.9420, Val loss: 1.1206 | Train F1: 0.96, Val F1: 0.60
Epoch: 071 | Train loss: 0.9296, Val loss: 1.1119 | Train F1: 0.96, Val F1: 0.62
Epoch: 072 | Train loss: 0.9222, Val loss: 1.1067 | Train F1: 0.99, Val F1: 0.66
Epoch: 073 | Train loss: 0.9161, Val loss: 1.1052 | Train F1: 0.99, Val F1: 0.66
Epoch: 074 | Train loss: 0.9090, Val loss: 1.1086 | Train F1: 1.00, Val F1: 0.64
Epoch: 075 | Train loss: 0.9089, Val loss: 1.1147 | Train F1: 1.00, Val F1: 0.64
Epoch: 076 | Train loss: 0.9103, Val loss: 1.1172 | Train F1: 1.00, Val F1: 0.64
Epoch: 077 | Train loss: 0.9100, Val loss: 1.1162 | Train F1: 1.00, Val F1: 0.64
Epoch: 078 | Train loss: 0.9072, Val loss: 1.1093 | Train F1: 1.00, Val F1: 0.65
Epoch: 079 | Train loss: 0.9061, Val loss: 1.1045 | Train F1: 1.00, Val F1: 0.65
Epoch: 080 | Train loss: 0.9058, Val loss: 1.1012 | Train F1: 1.00, Val F1: 0.65
Epoch: 081 | Train loss: 0.9057, Val loss: 1.0991 | Train F1: 1.00, Val F1: 0.65
Epoch: 082 | Train loss: 0.9057, Val loss: 1.0979 | Train F1: 1.00, Val F1: 0.71
Epoch: 083 | Train loss: 0.9059, Val loss: 1.0967 | Train F1: 1.00, Val F1: 0.71
Epoch: 084 | Train loss: 0.9060, Val loss: 1.0955 | Train F1: 1.00, Val F1: 0.71
Epoch: 085 | Train loss: 0.9060, Val loss: 1.0942 | Train F1: 1.00, Val F1: 0.65
Epoch: 086 | Train loss: 0.9060, Val loss: 1.0927 | Train F1: 1.00, Val F1: 0.64
Epoch: 087 | Train loss: 0.9059, Val loss: 1.0910 | Train F1: 1.00, Val F1: 0.64
Epoch: 088 | Train loss: 0.9058, Val loss: 1.0893 | Train F1: 1.00, Val F1: 0.64
Epoch: 089 | Train loss: 0.9057, Val loss: 1.0877 | Train F1: 1.00, Val F1: 0.64
Epoch: 090 | Train loss: 0.9055, Val loss: 1.0862 | Train F1: 1.00, Val F1: 0.71
Epoch: 091 | Train loss: 0.9054, Val loss: 1.0848 | Train F1: 1.00, Val F1: 0.71
Epoch: 092 | Train loss: 0.9053, Val loss: 1.0838 | Train F1: 1.00, Val F1: 0.71
Epoch: 093 | Train loss: 0.9052, Val loss: 1.0831 | Train F1: 1.00, Val F1: 0.71
Epoch: 094 | Train loss: 0.9052, Val loss: 1.0825 | Train F1: 1.00, Val F1: 0.68
Epoch: 095 | Train loss: 0.9051, Val loss: 1.0819 | Train F1: 1.00, Val F1: 0.68
Epoch: 096 | Train loss: 0.9051, Val loss: 1.0813 | Train F1: 1.00, Val F1: 0.68
Epoch: 097 | Train loss: 0.9051, Val loss: 1.0809 | Train F1: 1.00, Val F1: 0.68
Epoch: 098 | Train loss: 0.9051, Val loss: 1.0805 | Train F1: 1.00, Val F1: 0.68
Epoch: 099 | Train loss: 0.9051, Val loss: 1.0803 | Train F1: 1.00, Val F1: 0.68
Epoch: 100 | Train loss: 0.9051, Val loss: 1.0801 | Train F1: 1.00, Val F1: 0.68
Epoch: 101 | Train loss: 0.9050, Val loss: 1.0799 | Train F1: 1.00, Val F1: 0.68
Epoch: 102 | Train loss: 0.9050, Val loss: 1.0797 | Train F1: 1.00, Val F1: 0.68
Epoch: 103 | Train loss: 0.9050, Val loss: 1.0797 | Train F1: 1.00, Val F1: 0.68
Epoch: 104 | Train loss: 0.9050, Val loss: 1.0797 | Train F1: 1.00, Val F1: 0.68
Epoch: 105 | Train loss: 0.9050, Val loss: 1.0798 | Train F1: 1.00, Val F1: 0.68
Epoch: 106 | Train loss: 0.9050, Val loss: 1.0799 | Train F1: 1.00, Val F1: 0.68
Epoch: 107 | Train loss: 0.9050, Val loss: 1.0800 | Train F1: 1.00, Val F1: 0.68
Epoch: 108 | Train loss: 0.9050, Val loss: 1.0801 | Train F1: 1.00, Val F1: 0.68
Epoch: 109 | Train loss: 0.9050, Val loss: 1.0802 | Train F1: 1.00, Val F1: 0.68
Epoch: 110 | Train loss: 0.9050, Val loss: 1.0803 | Train F1: 1.00, Val F1: 0.68
Epoch: 111 | Train loss: 0.9050, Val loss: 1.0804 | Train F1: 1.00, Val F1: 0.68
Epoch: 112 | Train loss: 0.9050, Val loss: 1.0804 | Train F1: 1.00, Val F1: 0.67
Epoch: 113 | Train loss: 0.9050, Val loss: 1.0805 | Train F1: 1.00, Val F1: 0.67
Epoch: 114 | Train loss: 0.9050, Val loss: 1.0805 | Train F1: 1.00, Val F1: 0.67
Epoch: 115 | Train loss: 0.9050, Val loss: 1.0806 | Train F1: 1.00, Val F1: 0.67
Epoch: 116 | Train loss: 0.9050, Val loss: 1.0806 | Train F1: 1.00, Val F1: 0.67
Epoch: 117 | Train loss: 0.9050, Val loss: 1.0806 | Train F1: 1.00, Val F1: 0.68
Epoch: 118 | Train loss: 0.9050, Val loss: 1.0806 | Train F1: 1.00, Val F1: 0.68
Epoch: 119 | Train loss: 0.9050, Val loss: 1.0806 | Train F1: 1.00, Val F1: 0.68
Epoch: 120 | Train loss: 0.9050, Val loss: 1.0806 | Train F1: 1.00, Val F1: 0.68
Epoch: 121 | Train loss: 0.9049, Val loss: 1.0806 | Train F1: 1.00, Val F1: 0.68
Epoch: 122 | Train loss: 0.9049, Val loss: 1.0805 | Train F1: 1.00, Val F1: 0.68
Epoch: 123 | Train loss: 0.9049, Val loss: 1.0805 | Train F1: 1.00, Val F1: 0.68
Epoch: 124 | Train loss: 0.9049, Val loss: 1.0805 | Train F1: 1.00, Val F1: 0.68
Epoch: 125 | Train loss: 0.9049, Val loss: 1.0804 | Train F1: 1.00, Val F1: 0.68
Epoch: 126 | Train loss: 0.9049, Val loss: 1.0804 | Train F1: 1.00, Val F1: 0.68
Epoch: 127 | Train loss: 0.9049, Val loss: 1.0804 | Train F1: 1.00, Val F1: 0.68
Best model:
Train loss: 0.9051, Val loss: 1.0809, Test loss: 1.0868
Train F1: 1.00, Val F1: 0.68, Test F1: 0.65

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/training-data', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5727, Val loss: 1.5834 | Train F1: 0.37, Val F1: 0.31
Epoch: 001 | Train loss: 1.5291, Val loss: 1.5523 | Train F1: 0.39, Val F1: 0.30
Epoch: 002 | Train loss: 1.4773, Val loss: 1.5144 | Train F1: 0.38, Val F1: 0.31
Epoch: 003 | Train loss: 1.4227, Val loss: 1.4724 | Train F1: 0.36, Val F1: 0.25
Epoch: 004 | Train loss: 1.3730, Val loss: 1.4315 | Train F1: 0.33, Val F1: 0.26
Epoch: 005 | Train loss: 1.3312, Val loss: 1.3963 | Train F1: 0.31, Val F1: 0.27
Epoch: 006 | Train loss: 1.2964, Val loss: 1.3684 | Train F1: 0.31, Val F1: 0.28
Epoch: 007 | Train loss: 1.2682, Val loss: 1.3498 | Train F1: 0.32, Val F1: 0.28
Epoch: 008 | Train loss: 1.2467, Val loss: 1.3386 | Train F1: 0.37, Val F1: 0.29
Epoch: 009 | Train loss: 1.2260, Val loss: 1.3223 | Train F1: 0.39, Val F1: 0.29
Epoch: 010 | Train loss: 1.2107, Val loss: 1.3102 | Train F1: 0.41, Val F1: 0.29
Epoch: 011 | Train loss: 1.1982, Val loss: 1.3026 | Train F1: 0.41, Val F1: 0.28
Epoch: 012 | Train loss: 1.1869, Val loss: 1.2969 | Train F1: 0.43, Val F1: 0.28
Epoch: 013 | Train loss: 1.1760, Val loss: 1.2927 | Train F1: 0.44, Val F1: 0.34
Epoch: 014 | Train loss: 1.1677, Val loss: 1.2900 | Train F1: 0.44, Val F1: 0.38
Epoch: 015 | Train loss: 1.1600, Val loss: 1.2872 | Train F1: 0.44, Val F1: 0.38
Epoch: 016 | Train loss: 1.1515, Val loss: 1.2835 | Train F1: 0.45, Val F1: 0.38
Epoch: 017 | Train loss: 1.1424, Val loss: 1.2801 | Train F1: 0.45, Val F1: 0.38
Epoch: 018 | Train loss: 1.1323, Val loss: 1.2769 | Train F1: 0.45, Val F1: 0.38
Epoch: 019 | Train loss: 1.1204, Val loss: 1.2744 | Train F1: 0.51, Val F1: 0.36
Epoch: 020 | Train loss: 1.1075, Val loss: 1.2727 | Train F1: 0.57, Val F1: 0.37
Epoch: 021 | Train loss: 1.0952, Val loss: 1.2706 | Train F1: 0.57, Val F1: 0.37
Epoch: 022 | Train loss: 1.0827, Val loss: 1.2662 | Train F1: 0.57, Val F1: 0.36
Epoch: 023 | Train loss: 1.0712, Val loss: 1.2597 | Train F1: 0.57, Val F1: 0.39
Epoch: 024 | Train loss: 1.0593, Val loss: 1.2544 | Train F1: 0.59, Val F1: 0.38
Epoch: 025 | Train loss: 1.0475, Val loss: 1.2493 | Train F1: 0.62, Val F1: 0.38
Epoch: 026 | Train loss: 1.0399, Val loss: 1.2467 | Train F1: 0.64, Val F1: 0.38
Epoch: 027 | Train loss: 1.0373, Val loss: 1.2506 | Train F1: 0.63, Val F1: 0.40
Epoch: 028 | Train loss: 1.0321, Val loss: 1.2519 | Train F1: 0.63, Val F1: 0.39
Epoch: 029 | Train loss: 1.0237, Val loss: 1.2489 | Train F1: 0.65, Val F1: 0.40
Epoch: 030 | Train loss: 1.0147, Val loss: 1.2421 | Train F1: 0.67, Val F1: 0.47
Epoch: 031 | Train loss: 1.0081, Val loss: 1.2359 | Train F1: 0.67, Val F1: 0.51
Epoch: 032 | Train loss: 1.0027, Val loss: 1.2305 | Train F1: 0.67, Val F1: 0.51
Epoch: 033 | Train loss: 0.9981, Val loss: 1.2265 | Train F1: 0.69, Val F1: 0.49
Epoch: 034 | Train loss: 0.9941, Val loss: 1.2233 | Train F1: 0.71, Val F1: 0.49
Epoch: 035 | Train loss: 0.9902, Val loss: 1.2211 | Train F1: 0.71, Val F1: 0.53
Epoch: 036 | Train loss: 0.9864, Val loss: 1.2199 | Train F1: 0.71, Val F1: 0.53
Epoch: 037 | Train loss: 0.9829, Val loss: 1.2191 | Train F1: 0.71, Val F1: 0.51
Epoch: 038 | Train loss: 0.9799, Val loss: 1.2191 | Train F1: 0.72, Val F1: 0.48
Epoch: 039 | Train loss: 0.9770, Val loss: 1.2194 | Train F1: 0.72, Val F1: 0.48
Epoch: 040 | Train loss: 0.9741, Val loss: 1.2196 | Train F1: 0.80, Val F1: 0.48
Epoch: 041 | Train loss: 0.9712, Val loss: 1.2196 | Train F1: 0.80, Val F1: 0.46
Epoch: 042 | Train loss: 0.9685, Val loss: 1.2185 | Train F1: 0.87, Val F1: 0.43
Epoch: 043 | Train loss: 0.9661, Val loss: 1.2170 | Train F1: 0.87, Val F1: 0.43
Epoch: 044 | Train loss: 0.9640, Val loss: 1.2157 | Train F1: 0.87, Val F1: 0.48
Epoch: 045 | Train loss: 0.9622, Val loss: 1.2149 | Train F1: 0.87, Val F1: 0.48
Epoch: 046 | Train loss: 0.9601, Val loss: 1.2151 | Train F1: 0.86, Val F1: 0.48
Epoch: 047 | Train loss: 0.9583, Val loss: 1.2159 | Train F1: 0.88, Val F1: 0.47
Epoch: 048 | Train loss: 0.9563, Val loss: 1.2179 | Train F1: 0.88, Val F1: 0.47
Epoch: 049 | Train loss: 0.9544, Val loss: 1.2206 | Train F1: 0.88, Val F1: 0.41
Epoch: 050 | Train loss: 0.9524, Val loss: 1.2234 | Train F1: 0.88, Val F1: 0.41
Epoch: 051 | Train loss: 0.9508, Val loss: 1.2263 | Train F1: 0.89, Val F1: 0.41
Epoch: 052 | Train loss: 0.9495, Val loss: 1.2283 | Train F1: 0.89, Val F1: 0.40
Epoch: 053 | Train loss: 0.9484, Val loss: 1.2297 | Train F1: 0.89, Val F1: 0.40
Epoch: 054 | Train loss: 0.9473, Val loss: 1.2301 | Train F1: 0.89, Val F1: 0.40
Epoch: 055 | Train loss: 0.9462, Val loss: 1.2301 | Train F1: 0.89, Val F1: 0.40
Epoch: 056 | Train loss: 0.9451, Val loss: 1.2283 | Train F1: 0.89, Val F1: 0.40
Epoch: 057 | Train loss: 0.9440, Val loss: 1.2257 | Train F1: 0.89, Val F1: 0.45
Epoch: 058 | Train loss: 0.9430, Val loss: 1.2236 | Train F1: 0.89, Val F1: 0.48
Epoch: 059 | Train loss: 0.9420, Val loss: 1.2225 | Train F1: 0.89, Val F1: 0.51
Epoch: 060 | Train loss: 0.9409, Val loss: 1.2212 | Train F1: 0.94, Val F1: 0.52
Epoch: 061 | Train loss: 0.9398, Val loss: 1.2202 | Train F1: 0.94, Val F1: 0.52
Epoch: 062 | Train loss: 0.9386, Val loss: 1.2204 | Train F1: 0.94, Val F1: 0.52
Epoch: 063 | Train loss: 0.9372, Val loss: 1.2216 | Train F1: 0.93, Val F1: 0.54
Epoch: 064 | Train loss: 0.9357, Val loss: 1.2224 | Train F1: 0.93, Val F1: 0.50
Best model:
Train loss: 0.9941, Val loss: 1.2233, Test loss: 1.2501
Train F1: 0.71, Val F1: 0.49, Test F1: 0.47

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='training-data', log_path='log/graphsage/wisconsin/training-data', method='graphsage', path='data/graphsage', plot_path='plots/graphsage/wisconsin/training-data')

>>> Training split = 0.2

>>> Training split = 0.4

>>> Training split = 0.6

>>> Training split = 0.8

>>> Training split = 1.0

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=1, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9623 loss_val: 1.8781 | acc_train: 0.1214 acc_val: 0.2800 | f1_train: 0.0837 f1_val: 0.1730
Epoch: 0002 | loss_train: 1.9227 loss_val: 1.8447 | acc_train: 0.1786 acc_val: 0.3440 | f1_train: 0.1455 f1_val: 0.2369
Epoch: 0003 | loss_train: 1.8502 loss_val: 1.7957 | acc_train: 0.2714 acc_val: 0.4480 | f1_train: 0.2484 f1_val: 0.3843
Epoch: 0004 | loss_train: 1.7293 loss_val: 1.7309 | acc_train: 0.6000 acc_val: 0.5460 | f1_train: 0.5993 f1_val: 0.5234
Epoch: 0005 | loss_train: 1.5867 loss_val: 1.6479 | acc_train: 0.8214 acc_val: 0.5960 | f1_train: 0.8201 f1_val: 0.5743
Epoch: 0006 | loss_train: 1.4107 loss_val: 1.5440 | acc_train: 0.9429 acc_val: 0.6380 | f1_train: 0.9426 f1_val: 0.6210
Epoch: 0007 | loss_train: 1.2247 loss_val: 1.4199 | acc_train: 0.9571 acc_val: 0.6800 | f1_train: 0.9574 f1_val: 0.6646
Epoch: 0008 | loss_train: 1.0307 loss_val: 1.2826 | acc_train: 0.9714 acc_val: 0.7040 | f1_train: 0.9712 f1_val: 0.6968
Epoch: 0009 | loss_train: 0.8282 loss_val: 1.1445 | acc_train: 0.9857 acc_val: 0.7120 | f1_train: 0.9857 f1_val: 0.7087
Epoch: 0010 | loss_train: 0.6396 loss_val: 1.0202 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7156
Epoch: 0011 | loss_train: 0.4623 loss_val: 0.9205 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7288
Epoch: 0012 | loss_train: 0.3217 loss_val: 0.8479 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7461
Epoch: 0013 | loss_train: 0.2166 loss_val: 0.8016 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7398
Epoch: 0014 | loss_train: 0.1348 loss_val: 0.7809 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7371
Epoch: 0015 | loss_train: 0.0811 loss_val: 0.7833 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7340
Epoch: 0016 | loss_train: 0.0479 loss_val: 0.8043 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7394
Epoch: 0017 | loss_train: 0.0294 loss_val: 0.8387 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7303
Epoch: 0018 | loss_train: 0.0181 loss_val: 0.8823 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7319
Epoch: 0019 | loss_train: 0.0116 loss_val: 0.9315 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7319
Epoch: 0020 | loss_train: 0.0075 loss_val: 0.9841 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0021 | loss_train: 0.0050 loss_val: 1.0382 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7336
Epoch: 0022 | loss_train: 0.0034 loss_val: 1.0923 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7347
Epoch: 0023 | loss_train: 0.0025 loss_val: 1.1456 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7347
Epoch: 0024 | loss_train: 0.0018 loss_val: 1.1975 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7377
Epoch: 0025 | loss_train: 0.0013 loss_val: 1.2478 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0026 | loss_train: 0.0009 loss_val: 1.2963 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7324
Epoch: 0027 | loss_train: 0.0007 loss_val: 1.3429 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7301
Epoch: 0028 | loss_train: 0.0006 loss_val: 1.3874 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7297
Epoch: 0029 | loss_train: 0.0004 loss_val: 1.4299 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7297
Epoch: 0030 | loss_train: 0.0003 loss_val: 1.4705 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7282
Epoch: 0031 | loss_train: 0.0003 loss_val: 1.5091 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0032 | loss_train: 0.0002 loss_val: 1.5457 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.5803 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7268
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.6131 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7287
Epoch: 0035 | loss_train: 0.0001 loss_val: 1.6441 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7286
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.6732 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7288
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.7006 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7288
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.7264 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7288
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.7506 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.7733 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7282
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.7946 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0042 | loss_train: 0.0000 loss_val: 1.8144 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0043 | loss_train: 0.0000 loss_val: 1.8330 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0044 | loss_train: 0.0000 loss_val: 1.8502 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0045 | loss_train: 0.0000 loss_val: 1.8664 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0046 | loss_train: 0.0000 loss_val: 1.8815 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0047 | loss_train: 0.0000 loss_val: 1.8955 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7288
Epoch: 0048 | loss_train: 0.0000 loss_val: 1.9085 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7288
Epoch: 0049 | loss_train: 0.0000 loss_val: 1.9205 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7288
Epoch: 0050 | loss_train: 0.0000 loss_val: 1.9318 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7270
Epoch: 0051 | loss_train: 0.0000 loss_val: 1.9423 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7270
Epoch: 0052 | loss_train: 0.0000 loss_val: 1.9519 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7270
Epoch: 0053 | loss_train: 0.0000 loss_val: 1.9609 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7270
Epoch: 0054 | loss_train: 0.0000 loss_val: 1.9691 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7253
Optimization Finished!
Train cost: 12.3036s
Loading 24th epoch
Test set results: loss= 1.0758 accuracy= 0.7650

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=2, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9742 loss_val: 1.8708 | acc_train: 0.1071 acc_val: 0.3160 | f1_train: 0.0674 f1_val: 0.1795
Epoch: 0002 | loss_train: 1.9219 loss_val: 1.8273 | acc_train: 0.2071 acc_val: 0.3760 | f1_train: 0.1355 f1_val: 0.2455
Epoch: 0003 | loss_train: 1.8468 loss_val: 1.7649 | acc_train: 0.3214 acc_val: 0.5060 | f1_train: 0.2605 f1_val: 0.4403
Epoch: 0004 | loss_train: 1.7297 loss_val: 1.6834 | acc_train: 0.6071 acc_val: 0.6280 | f1_train: 0.6025 f1_val: 0.5788
Epoch: 0005 | loss_train: 1.5899 loss_val: 1.5825 | acc_train: 0.7714 acc_val: 0.6800 | f1_train: 0.7709 f1_val: 0.6426
Epoch: 0006 | loss_train: 1.4226 loss_val: 1.4597 | acc_train: 0.9071 acc_val: 0.7120 | f1_train: 0.9081 f1_val: 0.6860
Epoch: 0007 | loss_train: 1.2342 loss_val: 1.3168 | acc_train: 0.9500 acc_val: 0.7280 | f1_train: 0.9507 f1_val: 0.7190
Epoch: 0008 | loss_train: 1.0348 loss_val: 1.1658 | acc_train: 0.9643 acc_val: 0.7340 | f1_train: 0.9641 f1_val: 0.7272
Epoch: 0009 | loss_train: 0.8400 loss_val: 1.0251 | acc_train: 0.9714 acc_val: 0.7380 | f1_train: 0.9714 f1_val: 0.7338
Epoch: 0010 | loss_train: 0.6511 loss_val: 0.9094 | acc_train: 0.9714 acc_val: 0.7380 | f1_train: 0.9712 f1_val: 0.7357
Epoch: 0011 | loss_train: 0.4850 loss_val: 0.8218 | acc_train: 0.9857 acc_val: 0.7440 | f1_train: 0.9855 f1_val: 0.7460
Epoch: 0012 | loss_train: 0.3475 loss_val: 0.7592 | acc_train: 0.9857 acc_val: 0.7520 | f1_train: 0.9855 f1_val: 0.7529
Epoch: 0013 | loss_train: 0.2385 loss_val: 0.7201 | acc_train: 0.9857 acc_val: 0.7640 | f1_train: 0.9855 f1_val: 0.7670
Epoch: 0014 | loss_train: 0.1563 loss_val: 0.7019 | acc_train: 0.9929 acc_val: 0.7640 | f1_train: 0.9929 f1_val: 0.7651
Epoch: 0015 | loss_train: 0.0975 loss_val: 0.7059 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7633
Epoch: 0016 | loss_train: 0.0584 loss_val: 0.7285 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7587
Epoch: 0017 | loss_train: 0.0356 loss_val: 0.7664 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7604
Epoch: 0018 | loss_train: 0.0216 loss_val: 0.8145 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7632
Epoch: 0019 | loss_train: 0.0134 loss_val: 0.8689 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7569
Epoch: 0020 | loss_train: 0.0086 loss_val: 0.9264 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7547
Epoch: 0021 | loss_train: 0.0058 loss_val: 0.9852 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7527
Epoch: 0022 | loss_train: 0.0039 loss_val: 1.0440 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7555
Epoch: 0023 | loss_train: 0.0028 loss_val: 1.1017 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7614
Epoch: 0024 | loss_train: 0.0020 loss_val: 1.1576 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7622
Epoch: 0025 | loss_train: 0.0015 loss_val: 1.2115 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7631
Epoch: 0026 | loss_train: 0.0011 loss_val: 1.2636 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7630
Epoch: 0027 | loss_train: 0.0008 loss_val: 1.3135 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7642
Epoch: 0028 | loss_train: 0.0006 loss_val: 1.3612 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7606
Epoch: 0029 | loss_train: 0.0005 loss_val: 1.4069 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7655
Epoch: 0030 | loss_train: 0.0004 loss_val: 1.4503 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7623
Epoch: 0031 | loss_train: 0.0003 loss_val: 1.4916 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7639
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.5308 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7616
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.5680 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7573
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.6032 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7549
Epoch: 0035 | loss_train: 0.0001 loss_val: 1.6365 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7576
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.6678 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7584
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.6973 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7584
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.7251 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7584
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.7512 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.7759 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.7990 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7612
Epoch: 0042 | loss_train: 0.0001 loss_val: 1.8206 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7612
Epoch: 0043 | loss_train: 0.0000 loss_val: 1.8409 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7612
Epoch: 0044 | loss_train: 0.0000 loss_val: 1.8599 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7539
Epoch: 0045 | loss_train: 0.0000 loss_val: 1.8777 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7523
Epoch: 0046 | loss_train: 0.0000 loss_val: 1.8943 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7523
Epoch: 0047 | loss_train: 0.0000 loss_val: 1.9097 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7523
Epoch: 0048 | loss_train: 0.0000 loss_val: 1.9242 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0049 | loss_train: 0.0000 loss_val: 1.9377 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0050 | loss_train: 0.0000 loss_val: 1.9503 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0051 | loss_train: 0.0000 loss_val: 1.9620 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0052 | loss_train: 0.0000 loss_val: 1.9729 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0053 | loss_train: 0.0000 loss_val: 1.9829 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0054 | loss_train: 0.0000 loss_val: 1.9923 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0055 | loss_train: 0.0000 loss_val: 2.0010 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7556
Epoch: 0056 | loss_train: 0.0000 loss_val: 2.0090 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7556
Epoch: 0057 | loss_train: 0.0000 loss_val: 2.0165 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7556
Epoch: 0058 | loss_train: 0.0000 loss_val: 2.0233 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7556
Epoch: 0059 | loss_train: 0.0000 loss_val: 2.0297 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7556
Epoch: 0060 | loss_train: 0.0000 loss_val: 2.0356 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7553
Epoch: 0061 | loss_train: 0.0000 loss_val: 2.0411 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7553
Optimization Finished!
Train cost: 10.4074s
Loading 29th epoch
Test set results: loss= 1.2799 accuracy= 0.7730

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9644 loss_val: 1.8685 | acc_train: 0.1500 acc_val: 0.3380 | f1_train: 0.0961 f1_val: 0.1805
Epoch: 0002 | loss_train: 1.9218 loss_val: 1.8201 | acc_train: 0.2429 acc_val: 0.4180 | f1_train: 0.1645 f1_val: 0.2550
Epoch: 0003 | loss_train: 1.8483 loss_val: 1.7494 | acc_train: 0.3143 acc_val: 0.5420 | f1_train: 0.2601 f1_val: 0.4715
Epoch: 0004 | loss_train: 1.7314 loss_val: 1.6593 | acc_train: 0.6143 acc_val: 0.6300 | f1_train: 0.6107 f1_val: 0.5859
Epoch: 0005 | loss_train: 1.5866 loss_val: 1.5494 | acc_train: 0.7643 acc_val: 0.6940 | f1_train: 0.7638 f1_val: 0.6637
Epoch: 0006 | loss_train: 1.4234 loss_val: 1.4177 | acc_train: 0.8429 acc_val: 0.7260 | f1_train: 0.8451 f1_val: 0.7061
Epoch: 0007 | loss_train: 1.2424 loss_val: 1.2689 | acc_train: 0.8929 acc_val: 0.7400 | f1_train: 0.8946 f1_val: 0.7335
Epoch: 0008 | loss_train: 1.0407 loss_val: 1.1175 | acc_train: 0.9214 acc_val: 0.7560 | f1_train: 0.9217 f1_val: 0.7481
Epoch: 0009 | loss_train: 0.8483 loss_val: 0.9828 | acc_train: 0.9500 acc_val: 0.7660 | f1_train: 0.9508 f1_val: 0.7615
Epoch: 0010 | loss_train: 0.6650 loss_val: 0.8742 | acc_train: 0.9429 acc_val: 0.7660 | f1_train: 0.9426 f1_val: 0.7614
Epoch: 0011 | loss_train: 0.5059 loss_val: 0.7956 | acc_train: 0.9714 acc_val: 0.7640 | f1_train: 0.9712 f1_val: 0.7592
Epoch: 0012 | loss_train: 0.3727 loss_val: 0.7420 | acc_train: 0.9786 acc_val: 0.7620 | f1_train: 0.9784 f1_val: 0.7596
Epoch: 0013 | loss_train: 0.2600 loss_val: 0.7074 | acc_train: 0.9857 acc_val: 0.7720 | f1_train: 0.9855 f1_val: 0.7715
Epoch: 0014 | loss_train: 0.1732 loss_val: 0.6932 | acc_train: 0.9929 acc_val: 0.7720 | f1_train: 0.9929 f1_val: 0.7727
Epoch: 0015 | loss_train: 0.1114 loss_val: 0.7023 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7773
Epoch: 0016 | loss_train: 0.0693 loss_val: 0.7350 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7769
Epoch: 0017 | loss_train: 0.0415 loss_val: 0.7858 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0018 | loss_train: 0.0253 loss_val: 0.8479 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7655
Epoch: 0019 | loss_train: 0.0163 loss_val: 0.9169 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0020 | loss_train: 0.0101 loss_val: 0.9886 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7511
Epoch: 0021 | loss_train: 0.0070 loss_val: 1.0602 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0022 | loss_train: 0.0047 loss_val: 1.1301 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7422
Epoch: 0023 | loss_train: 0.0033 loss_val: 1.1974 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7394
Epoch: 0024 | loss_train: 0.0024 loss_val: 1.2620 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7351
Epoch: 0025 | loss_train: 0.0017 loss_val: 1.3238 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7355
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.3828 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7391
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.4388 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7396
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.4920 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7426
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.5427 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7427
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5906 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7439
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6363 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7467
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6800 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7431
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.7215 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7610 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7452
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7988 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7492
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.8345 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7510
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8685 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.9008 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9313 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9601 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9873 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7542
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.0127 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7551
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.0368 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7504
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.0593 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7502
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.0804 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7507
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.0999 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7506
Optimization Finished!
Train cost: 10.8004s
Loading 15th epoch
Test set results: loss= 0.6384 accuracy= 0.7910

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=4, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9629 loss_val: 1.8653 | acc_train: 0.1714 acc_val: 0.3600 | f1_train: 0.1176 f1_val: 0.1856
Epoch: 0002 | loss_train: 1.9257 loss_val: 1.8126 | acc_train: 0.1929 acc_val: 0.4320 | f1_train: 0.1218 f1_val: 0.2607
Epoch: 0003 | loss_train: 1.8427 loss_val: 1.7363 | acc_train: 0.3214 acc_val: 0.5700 | f1_train: 0.2436 f1_val: 0.4867
Epoch: 0004 | loss_train: 1.7269 loss_val: 1.6414 | acc_train: 0.5571 acc_val: 0.6600 | f1_train: 0.5449 f1_val: 0.5937
Epoch: 0005 | loss_train: 1.5867 loss_val: 1.5277 | acc_train: 0.6929 acc_val: 0.7000 | f1_train: 0.6951 f1_val: 0.6694
Epoch: 0006 | loss_train: 1.4226 loss_val: 1.3930 | acc_train: 0.8143 acc_val: 0.7220 | f1_train: 0.8173 f1_val: 0.7026
Epoch: 0007 | loss_train: 1.2469 loss_val: 1.2421 | acc_train: 0.8714 acc_val: 0.7500 | f1_train: 0.8736 f1_val: 0.7413
Epoch: 0008 | loss_train: 1.0566 loss_val: 1.0929 | acc_train: 0.9000 acc_val: 0.7620 | f1_train: 0.9003 f1_val: 0.7558
Epoch: 0009 | loss_train: 0.8713 loss_val: 0.9652 | acc_train: 0.9000 acc_val: 0.7600 | f1_train: 0.9003 f1_val: 0.7541
Epoch: 0010 | loss_train: 0.6969 loss_val: 0.8661 | acc_train: 0.9143 acc_val: 0.7580 | f1_train: 0.9152 f1_val: 0.7552
Epoch: 0011 | loss_train: 0.5266 loss_val: 0.7958 | acc_train: 0.9429 acc_val: 0.7600 | f1_train: 0.9432 f1_val: 0.7588
Epoch: 0012 | loss_train: 0.3934 loss_val: 0.7466 | acc_train: 0.9714 acc_val: 0.7580 | f1_train: 0.9712 f1_val: 0.7576
Epoch: 0013 | loss_train: 0.2801 loss_val: 0.7142 | acc_train: 0.9714 acc_val: 0.7720 | f1_train: 0.9712 f1_val: 0.7725
Epoch: 0014 | loss_train: 0.1963 loss_val: 0.6994 | acc_train: 0.9929 acc_val: 0.7760 | f1_train: 0.9929 f1_val: 0.7828
Epoch: 0015 | loss_train: 0.1290 loss_val: 0.7075 | acc_train: 0.9929 acc_val: 0.7720 | f1_train: 0.9929 f1_val: 0.7775
Epoch: 0016 | loss_train: 0.0793 loss_val: 0.7346 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7743
Epoch: 0017 | loss_train: 0.0481 loss_val: 0.7799 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7670
Epoch: 0018 | loss_train: 0.0291 loss_val: 0.8401 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7648
Epoch: 0019 | loss_train: 0.0188 loss_val: 0.9103 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7561
Epoch: 0020 | loss_train: 0.0123 loss_val: 0.9854 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7536
Epoch: 0021 | loss_train: 0.0080 loss_val: 1.0622 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7408
Epoch: 0022 | loss_train: 0.0055 loss_val: 1.1383 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7388
Epoch: 0023 | loss_train: 0.0039 loss_val: 1.2127 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7476
Epoch: 0024 | loss_train: 0.0028 loss_val: 1.2849 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7419
Epoch: 0025 | loss_train: 0.0020 loss_val: 1.3551 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7384
Epoch: 0026 | loss_train: 0.0015 loss_val: 1.4231 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7354
Epoch: 0027 | loss_train: 0.0011 loss_val: 1.4884 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7360
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.5512 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7420
Epoch: 0029 | loss_train: 0.0007 loss_val: 1.6114 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7458
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.6691 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7465
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.7239 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7432
Epoch: 0032 | loss_train: 0.0004 loss_val: 1.7759 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7499
Epoch: 0033 | loss_train: 0.0003 loss_val: 1.8252 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7475
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.8716 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7486
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.9154 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7486
Epoch: 0036 | loss_train: 0.0002 loss_val: 1.9565 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7487
Epoch: 0037 | loss_train: 0.0002 loss_val: 1.9952 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7446
Epoch: 0038 | loss_train: 0.0001 loss_val: 2.0312 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7460
Epoch: 0039 | loss_train: 0.0001 loss_val: 2.0647 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7438
Epoch: 0040 | loss_train: 0.0001 loss_val: 2.0959 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7462
Epoch: 0041 | loss_train: 0.0001 loss_val: 2.1248 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7486
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.1515 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7486
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.1757 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7470
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.1981 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7437
Optimization Finished!
Train cost: 12.6094s
Loading 14th epoch
Test set results: loss= 0.6434 accuracy= 0.7900

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=5, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9736 loss_val: 1.8638 | acc_train: 0.1500 acc_val: 0.3640 | f1_train: 0.0953 f1_val: 0.1860
Epoch: 0002 | loss_train: 1.9268 loss_val: 1.8080 | acc_train: 0.2429 acc_val: 0.4260 | f1_train: 0.1402 f1_val: 0.2587
Epoch: 0003 | loss_train: 1.8454 loss_val: 1.7295 | acc_train: 0.3357 acc_val: 0.5700 | f1_train: 0.2545 f1_val: 0.4882
Epoch: 0004 | loss_train: 1.7293 loss_val: 1.6333 | acc_train: 0.5643 acc_val: 0.6580 | f1_train: 0.5519 f1_val: 0.5976
Epoch: 0005 | loss_train: 1.5927 loss_val: 1.5181 | acc_train: 0.6929 acc_val: 0.7040 | f1_train: 0.6889 f1_val: 0.6678
Epoch: 0006 | loss_train: 1.4346 loss_val: 1.3816 | acc_train: 0.7786 acc_val: 0.7360 | f1_train: 0.7797 f1_val: 0.7248
Epoch: 0007 | loss_train: 1.2607 loss_val: 1.2300 | acc_train: 0.8429 acc_val: 0.7420 | f1_train: 0.8448 f1_val: 0.7369
Epoch: 0008 | loss_train: 1.0657 loss_val: 1.0842 | acc_train: 0.8857 acc_val: 0.7680 | f1_train: 0.8875 f1_val: 0.7617
Epoch: 0009 | loss_train: 0.8873 loss_val: 0.9617 | acc_train: 0.9000 acc_val: 0.7560 | f1_train: 0.9003 f1_val: 0.7497
Epoch: 0010 | loss_train: 0.7131 loss_val: 0.8707 | acc_train: 0.9143 acc_val: 0.7540 | f1_train: 0.9139 f1_val: 0.7449
Epoch: 0011 | loss_train: 0.5584 loss_val: 0.8044 | acc_train: 0.9143 acc_val: 0.7480 | f1_train: 0.9141 f1_val: 0.7427
Epoch: 0012 | loss_train: 0.4208 loss_val: 0.7553 | acc_train: 0.9429 acc_val: 0.7560 | f1_train: 0.9414 f1_val: 0.7560
Epoch: 0013 | loss_train: 0.3064 loss_val: 0.7216 | acc_train: 0.9643 acc_val: 0.7700 | f1_train: 0.9636 f1_val: 0.7713
Epoch: 0014 | loss_train: 0.2128 loss_val: 0.7038 | acc_train: 0.9857 acc_val: 0.7720 | f1_train: 0.9859 f1_val: 0.7750
Epoch: 0015 | loss_train: 0.1430 loss_val: 0.7062 | acc_train: 0.9929 acc_val: 0.7700 | f1_train: 0.9929 f1_val: 0.7729
Epoch: 0016 | loss_train: 0.0895 loss_val: 0.7325 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7760
Epoch: 0017 | loss_train: 0.0540 loss_val: 0.7805 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7726
Epoch: 0018 | loss_train: 0.0340 loss_val: 0.8485 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7607
Epoch: 0019 | loss_train: 0.0214 loss_val: 0.9282 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7575
Epoch: 0020 | loss_train: 0.0139 loss_val: 1.0117 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7570
Epoch: 0021 | loss_train: 0.0090 loss_val: 1.0955 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7450
Epoch: 0022 | loss_train: 0.0061 loss_val: 1.1774 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7433
Epoch: 0023 | loss_train: 0.0042 loss_val: 1.2573 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7398
Epoch: 0024 | loss_train: 0.0030 loss_val: 1.3349 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7374
Epoch: 0025 | loss_train: 0.0022 loss_val: 1.4097 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7374
Epoch: 0026 | loss_train: 0.0016 loss_val: 1.4816 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7395
Epoch: 0027 | loss_train: 0.0012 loss_val: 1.5506 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7379
Epoch: 0028 | loss_train: 0.0009 loss_val: 1.6164 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7391
Epoch: 0029 | loss_train: 0.0007 loss_val: 1.6796 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7411
Epoch: 0030 | loss_train: 0.0006 loss_val: 1.7397 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7418
Epoch: 0031 | loss_train: 0.0005 loss_val: 1.7969 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7421
Epoch: 0032 | loss_train: 0.0004 loss_val: 1.8512 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7401
Epoch: 0033 | loss_train: 0.0003 loss_val: 1.9024 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7379
Epoch: 0034 | loss_train: 0.0003 loss_val: 1.9505 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7432
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.9958 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7436
Epoch: 0036 | loss_train: 0.0002 loss_val: 2.0379 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7441
Epoch: 0037 | loss_train: 0.0002 loss_val: 2.0775 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7444
Epoch: 0038 | loss_train: 0.0001 loss_val: 2.1144 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7396
Epoch: 0039 | loss_train: 0.0001 loss_val: 2.1489 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7384
Epoch: 0040 | loss_train: 0.0001 loss_val: 2.1808 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7340
Epoch: 0041 | loss_train: 0.0001 loss_val: 2.2102 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7314
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.2374 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7314
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.2623 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7315
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.2851 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7315
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.3061 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7289
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.3254 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7289
Optimization Finished!
Train cost: 10.9337s
Loading 16th epoch
Test set results: loss= 0.6668 accuracy= 0.7870

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=6, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9689 loss_val: 1.8622 | acc_train: 0.1500 acc_val: 0.3740 | f1_train: 0.0930 f1_val: 0.1901
Epoch: 0002 | loss_train: 1.9269 loss_val: 1.8049 | acc_train: 0.2214 acc_val: 0.4400 | f1_train: 0.1222 f1_val: 0.2530
Epoch: 0003 | loss_train: 1.8431 loss_val: 1.7251 | acc_train: 0.3000 acc_val: 0.5760 | f1_train: 0.1971 f1_val: 0.4777
Epoch: 0004 | loss_train: 1.7335 loss_val: 1.6295 | acc_train: 0.5714 acc_val: 0.6600 | f1_train: 0.5568 f1_val: 0.5825
Epoch: 0005 | loss_train: 1.6013 loss_val: 1.5169 | acc_train: 0.6714 acc_val: 0.7120 | f1_train: 0.6727 f1_val: 0.6771
Epoch: 0006 | loss_train: 1.4544 loss_val: 1.3863 | acc_train: 0.7786 acc_val: 0.7220 | f1_train: 0.7828 f1_val: 0.7074
Epoch: 0007 | loss_train: 1.2788 loss_val: 1.2411 | acc_train: 0.8286 acc_val: 0.7600 | f1_train: 0.8306 f1_val: 0.7569
Epoch: 0008 | loss_train: 1.0972 loss_val: 1.0986 | acc_train: 0.8643 acc_val: 0.7560 | f1_train: 0.8647 f1_val: 0.7502
Epoch: 0009 | loss_train: 0.9116 loss_val: 0.9774 | acc_train: 0.8643 acc_val: 0.7500 | f1_train: 0.8639 f1_val: 0.7435
Epoch: 0010 | loss_train: 0.7403 loss_val: 0.8820 | acc_train: 0.8929 acc_val: 0.7520 | f1_train: 0.8925 f1_val: 0.7502
Epoch: 0011 | loss_train: 0.5908 loss_val: 0.8136 | acc_train: 0.9000 acc_val: 0.7540 | f1_train: 0.8993 f1_val: 0.7514
Epoch: 0012 | loss_train: 0.4528 loss_val: 0.7655 | acc_train: 0.9143 acc_val: 0.7540 | f1_train: 0.9133 f1_val: 0.7521
Epoch: 0013 | loss_train: 0.3341 loss_val: 0.7335 | acc_train: 0.9500 acc_val: 0.7720 | f1_train: 0.9489 f1_val: 0.7750
Epoch: 0014 | loss_train: 0.2409 loss_val: 0.7186 | acc_train: 0.9714 acc_val: 0.7740 | f1_train: 0.9712 f1_val: 0.7777
Epoch: 0015 | loss_train: 0.1604 loss_val: 0.7235 | acc_train: 0.9929 acc_val: 0.7660 | f1_train: 0.9929 f1_val: 0.7702
Epoch: 0016 | loss_train: 0.1012 loss_val: 0.7481 | acc_train: 0.9929 acc_val: 0.7640 | f1_train: 0.9929 f1_val: 0.7673
Epoch: 0017 | loss_train: 0.0618 loss_val: 0.7910 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7612
Epoch: 0018 | loss_train: 0.0370 loss_val: 0.8525 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7555
Epoch: 0019 | loss_train: 0.0223 loss_val: 0.9252 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7523
Epoch: 0020 | loss_train: 0.0150 loss_val: 1.0034 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7517
Epoch: 0021 | loss_train: 0.0098 loss_val: 1.0835 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7509
Epoch: 0022 | loss_train: 0.0064 loss_val: 1.1629 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7364
Epoch: 0023 | loss_train: 0.0044 loss_val: 1.2412 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0024 | loss_train: 0.0032 loss_val: 1.3171 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7296
Epoch: 0025 | loss_train: 0.0023 loss_val: 1.3905 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7269
Epoch: 0026 | loss_train: 0.0017 loss_val: 1.4612 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7258
Epoch: 0027 | loss_train: 0.0013 loss_val: 1.5289 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7339
Epoch: 0028 | loss_train: 0.0011 loss_val: 1.5936 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7383
Epoch: 0029 | loss_train: 0.0008 loss_val: 1.6552 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7312
Epoch: 0030 | loss_train: 0.0007 loss_val: 1.7136 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7313
Epoch: 0031 | loss_train: 0.0005 loss_val: 1.7688 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7282
Epoch: 0032 | loss_train: 0.0004 loss_val: 1.8208 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7312
Epoch: 0033 | loss_train: 0.0004 loss_val: 1.8698 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7290
Epoch: 0034 | loss_train: 0.0003 loss_val: 1.9161 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7271
Epoch: 0035 | loss_train: 0.0003 loss_val: 1.9597 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7283
Epoch: 0036 | loss_train: 0.0002 loss_val: 2.0008 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7283
Epoch: 0037 | loss_train: 0.0002 loss_val: 2.0395 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7320
Epoch: 0038 | loss_train: 0.0002 loss_val: 2.0756 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7301
Epoch: 0039 | loss_train: 0.0001 loss_val: 2.1093 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7276
Epoch: 0040 | loss_train: 0.0001 loss_val: 2.1408 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7259
Epoch: 0041 | loss_train: 0.0001 loss_val: 2.1703 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7274
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.1977 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7274
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.2234 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7274
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.2475 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7293
Optimization Finished!
Train cost: 10.3465s
Loading 14th epoch
Test set results: loss= 0.6591 accuracy= 0.7850

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=7, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9674 loss_val: 1.8629 | acc_train: 0.1571 acc_val: 0.3720 | f1_train: 0.0983 f1_val: 0.1834
Epoch: 0002 | loss_train: 1.9265 loss_val: 1.8055 | acc_train: 0.2143 acc_val: 0.4320 | f1_train: 0.1280 f1_val: 0.2395
Epoch: 0003 | loss_train: 1.8511 loss_val: 1.7268 | acc_train: 0.2857 acc_val: 0.5900 | f1_train: 0.1825 f1_val: 0.4926
Epoch: 0004 | loss_train: 1.7382 loss_val: 1.6324 | acc_train: 0.6000 acc_val: 0.6660 | f1_train: 0.6039 f1_val: 0.5896
Epoch: 0005 | loss_train: 1.6111 loss_val: 1.5194 | acc_train: 0.7071 acc_val: 0.7140 | f1_train: 0.7153 f1_val: 0.6716
Epoch: 0006 | loss_train: 1.4609 loss_val: 1.3880 | acc_train: 0.7500 acc_val: 0.7260 | f1_train: 0.7521 f1_val: 0.7123
Epoch: 0007 | loss_train: 1.2982 loss_val: 1.2418 | acc_train: 0.8071 acc_val: 0.7500 | f1_train: 0.8102 f1_val: 0.7467
Epoch: 0008 | loss_train: 1.1190 loss_val: 1.1008 | acc_train: 0.8429 acc_val: 0.7500 | f1_train: 0.8448 f1_val: 0.7432
Epoch: 0009 | loss_train: 0.9422 loss_val: 0.9826 | acc_train: 0.8714 acc_val: 0.7460 | f1_train: 0.8712 f1_val: 0.7395
Epoch: 0010 | loss_train: 0.7748 loss_val: 0.8909 | acc_train: 0.8786 acc_val: 0.7420 | f1_train: 0.8785 f1_val: 0.7357
Epoch: 0011 | loss_train: 0.6182 loss_val: 0.8282 | acc_train: 0.8786 acc_val: 0.7420 | f1_train: 0.8778 f1_val: 0.7381
Epoch: 0012 | loss_train: 0.4777 loss_val: 0.7815 | acc_train: 0.9000 acc_val: 0.7460 | f1_train: 0.8987 f1_val: 0.7453
Epoch: 0013 | loss_train: 0.3593 loss_val: 0.7423 | acc_train: 0.9357 acc_val: 0.7680 | f1_train: 0.9345 f1_val: 0.7716
Epoch: 0014 | loss_train: 0.2573 loss_val: 0.7226 | acc_train: 0.9571 acc_val: 0.7700 | f1_train: 0.9560 f1_val: 0.7714
Epoch: 0015 | loss_train: 0.1777 loss_val: 0.7283 | acc_train: 0.9929 acc_val: 0.7700 | f1_train: 0.9929 f1_val: 0.7744
Epoch: 0016 | loss_train: 0.1156 loss_val: 0.7484 | acc_train: 0.9929 acc_val: 0.7700 | f1_train: 0.9929 f1_val: 0.7721
Epoch: 0017 | loss_train: 0.0695 loss_val: 0.7806 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7626
Epoch: 0018 | loss_train: 0.0416 loss_val: 0.8364 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7598
Epoch: 0019 | loss_train: 0.0259 loss_val: 0.9071 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7535
Epoch: 0020 | loss_train: 0.0167 loss_val: 0.9848 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7595
Epoch: 0021 | loss_train: 0.0108 loss_val: 1.0656 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7484
Epoch: 0022 | loss_train: 0.0071 loss_val: 1.1467 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7460
Epoch: 0023 | loss_train: 0.0048 loss_val: 1.2260 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7366
Epoch: 0024 | loss_train: 0.0035 loss_val: 1.3027 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7318
Epoch: 0025 | loss_train: 0.0026 loss_val: 1.3772 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0026 | loss_train: 0.0019 loss_val: 1.4483 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7298
Epoch: 0027 | loss_train: 0.0014 loss_val: 1.5159 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7337
Epoch: 0028 | loss_train: 0.0011 loss_val: 1.5797 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7335
Epoch: 0029 | loss_train: 0.0009 loss_val: 1.6397 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7326
Epoch: 0030 | loss_train: 0.0007 loss_val: 1.6968 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7338
Epoch: 0031 | loss_train: 0.0006 loss_val: 1.7511 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7338
Epoch: 0032 | loss_train: 0.0005 loss_val: 1.8027 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7316
Epoch: 0033 | loss_train: 0.0004 loss_val: 1.8519 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7316
Epoch: 0034 | loss_train: 0.0003 loss_val: 1.8984 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7293
Epoch: 0035 | loss_train: 0.0003 loss_val: 1.9428 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7335
Epoch: 0036 | loss_train: 0.0002 loss_val: 1.9845 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7335
Epoch: 0037 | loss_train: 0.0002 loss_val: 2.0241 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7340
Epoch: 0038 | loss_train: 0.0002 loss_val: 2.0612 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7359
Epoch: 0039 | loss_train: 0.0001 loss_val: 2.0963 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7355
Epoch: 0040 | loss_train: 0.0001 loss_val: 2.1293 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7367
Epoch: 0041 | loss_train: 0.0001 loss_val: 2.1604 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7342
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.1899 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7298
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.2177 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7298
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.2437 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7286
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.2681 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7286
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.2908 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7253
Optimization Finished!
Train cost: 10.6198s
Loading 14th epoch
Test set results: loss= 0.6628 accuracy= 0.7830

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=8, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9664 loss_val: 1.8627 | acc_train: 0.1714 acc_val: 0.3800 | f1_train: 0.0990 f1_val: 0.1894
Epoch: 0002 | loss_train: 1.9231 loss_val: 1.8041 | acc_train: 0.2214 acc_val: 0.4520 | f1_train: 0.1230 f1_val: 0.2451
Epoch: 0003 | loss_train: 1.8499 loss_val: 1.7249 | acc_train: 0.3000 acc_val: 0.5960 | f1_train: 0.1920 f1_val: 0.4929
Epoch: 0004 | loss_train: 1.7384 loss_val: 1.6310 | acc_train: 0.5571 acc_val: 0.6480 | f1_train: 0.5523 f1_val: 0.5530
Epoch: 0005 | loss_train: 1.6184 loss_val: 1.5209 | acc_train: 0.6786 acc_val: 0.7040 | f1_train: 0.6821 f1_val: 0.6567
Epoch: 0006 | loss_train: 1.4665 loss_val: 1.3921 | acc_train: 0.7500 acc_val: 0.7300 | f1_train: 0.7543 f1_val: 0.7195
Epoch: 0007 | loss_train: 1.3057 loss_val: 1.2508 | acc_train: 0.7786 acc_val: 0.7420 | f1_train: 0.7825 f1_val: 0.7360
Epoch: 0008 | loss_train: 1.1349 loss_val: 1.1129 | acc_train: 0.8143 acc_val: 0.7380 | f1_train: 0.8168 f1_val: 0.7292
Epoch: 0009 | loss_train: 0.9613 loss_val: 0.9937 | acc_train: 0.8429 acc_val: 0.7420 | f1_train: 0.8441 f1_val: 0.7328
Epoch: 0010 | loss_train: 0.7989 loss_val: 0.9005 | acc_train: 0.8643 acc_val: 0.7440 | f1_train: 0.8635 f1_val: 0.7390
Epoch: 0011 | loss_train: 0.6393 loss_val: 0.8363 | acc_train: 0.8786 acc_val: 0.7420 | f1_train: 0.8778 f1_val: 0.7390
Epoch: 0012 | loss_train: 0.4997 loss_val: 0.7948 | acc_train: 0.9000 acc_val: 0.7420 | f1_train: 0.8987 f1_val: 0.7429
Epoch: 0013 | loss_train: 0.3821 loss_val: 0.7588 | acc_train: 0.9214 acc_val: 0.7500 | f1_train: 0.9191 f1_val: 0.7534
Epoch: 0014 | loss_train: 0.2739 loss_val: 0.7302 | acc_train: 0.9500 acc_val: 0.7580 | f1_train: 0.9489 f1_val: 0.7603
Epoch: 0015 | loss_train: 0.1882 loss_val: 0.7254 | acc_train: 0.9929 acc_val: 0.7700 | f1_train: 0.9929 f1_val: 0.7734
Epoch: 0016 | loss_train: 0.1240 loss_val: 0.7410 | acc_train: 0.9929 acc_val: 0.7720 | f1_train: 0.9929 f1_val: 0.7738
Epoch: 0017 | loss_train: 0.0742 loss_val: 0.7813 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7669
Epoch: 0018 | loss_train: 0.0451 loss_val: 0.8447 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7568
Epoch: 0019 | loss_train: 0.0284 loss_val: 0.9213 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7503
Epoch: 0020 | loss_train: 0.0178 loss_val: 1.0041 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7524
Epoch: 0021 | loss_train: 0.0117 loss_val: 1.0859 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7442
Epoch: 0022 | loss_train: 0.0077 loss_val: 1.1643 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7476
Epoch: 0023 | loss_train: 0.0054 loss_val: 1.2414 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7448
Epoch: 0024 | loss_train: 0.0038 loss_val: 1.3179 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7423
Epoch: 0025 | loss_train: 0.0027 loss_val: 1.3925 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7348
Epoch: 0026 | loss_train: 0.0020 loss_val: 1.4645 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7329
Epoch: 0027 | loss_train: 0.0015 loss_val: 1.5333 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7277
Epoch: 0028 | loss_train: 0.0012 loss_val: 1.5983 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7280
Epoch: 0029 | loss_train: 0.0010 loss_val: 1.6598 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0030 | loss_train: 0.0008 loss_val: 1.7180 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7281
Epoch: 0031 | loss_train: 0.0006 loss_val: 1.7727 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7291
Epoch: 0032 | loss_train: 0.0005 loss_val: 1.8245 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7325
Epoch: 0033 | loss_train: 0.0004 loss_val: 1.8735 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7325
Epoch: 0034 | loss_train: 0.0003 loss_val: 1.9195 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7301
Epoch: 0035 | loss_train: 0.0003 loss_val: 1.9629 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7320
Epoch: 0036 | loss_train: 0.0002 loss_val: 2.0041 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7303
Epoch: 0037 | loss_train: 0.0002 loss_val: 2.0430 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7298
Epoch: 0038 | loss_train: 0.0002 loss_val: 2.0799 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0039 | loss_train: 0.0001 loss_val: 2.1148 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7314
Epoch: 0040 | loss_train: 0.0001 loss_val: 2.1478 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7297
Epoch: 0041 | loss_train: 0.0001 loss_val: 2.1789 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7297
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.2086 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7316
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.2365 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7335
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.2631 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7320
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.2882 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7315
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.3119 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7315
Optimization Finished!
Train cost: 11.1206s
Loading 16th epoch
Test set results: loss= 0.6714 accuracy= 0.7840

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=9, log_path='log/nagphormer/cora/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9656 loss_val: 1.8653 | acc_train: 0.1857 acc_val: 0.3780 | f1_train: 0.1068 f1_val: 0.1886
Epoch: 0002 | loss_train: 1.9259 loss_val: 1.8101 | acc_train: 0.2357 acc_val: 0.4380 | f1_train: 0.1310 f1_val: 0.2395
Epoch: 0003 | loss_train: 1.8518 loss_val: 1.7336 | acc_train: 0.2929 acc_val: 0.5740 | f1_train: 0.1976 f1_val: 0.4622
Epoch: 0004 | loss_train: 1.7528 loss_val: 1.6429 | acc_train: 0.5286 acc_val: 0.6400 | f1_train: 0.5116 f1_val: 0.5527
Epoch: 0005 | loss_train: 1.6323 loss_val: 1.5352 | acc_train: 0.6786 acc_val: 0.7060 | f1_train: 0.6794 f1_val: 0.6541
Epoch: 0006 | loss_train: 1.4836 loss_val: 1.4079 | acc_train: 0.7500 acc_val: 0.7200 | f1_train: 0.7530 f1_val: 0.6934
Epoch: 0007 | loss_train: 1.3301 loss_val: 1.2671 | acc_train: 0.7571 acc_val: 0.7400 | f1_train: 0.7615 f1_val: 0.7336
Epoch: 0008 | loss_train: 1.1585 loss_val: 1.1301 | acc_train: 0.8000 acc_val: 0.7360 | f1_train: 0.8037 f1_val: 0.7278
Epoch: 0009 | loss_train: 0.9893 loss_val: 1.0117 | acc_train: 0.8429 acc_val: 0.7340 | f1_train: 0.8441 f1_val: 0.7229
Epoch: 0010 | loss_train: 0.8209 loss_val: 0.9146 | acc_train: 0.8643 acc_val: 0.7260 | f1_train: 0.8645 f1_val: 0.7198
Epoch: 0011 | loss_train: 0.6672 loss_val: 0.8470 | acc_train: 0.8643 acc_val: 0.7360 | f1_train: 0.8639 f1_val: 0.7317
Epoch: 0012 | loss_train: 0.5244 loss_val: 0.8017 | acc_train: 0.9000 acc_val: 0.7420 | f1_train: 0.8987 f1_val: 0.7446
Epoch: 0013 | loss_train: 0.3935 loss_val: 0.7626 | acc_train: 0.9286 acc_val: 0.7500 | f1_train: 0.9271 f1_val: 0.7516
Epoch: 0014 | loss_train: 0.2887 loss_val: 0.7383 | acc_train: 0.9500 acc_val: 0.7540 | f1_train: 0.9489 f1_val: 0.7557
Epoch: 0015 | loss_train: 0.2004 loss_val: 0.7357 | acc_train: 0.9786 acc_val: 0.7620 | f1_train: 0.9785 f1_val: 0.7666
Epoch: 0016 | loss_train: 0.1325 loss_val: 0.7426 | acc_train: 0.9929 acc_val: 0.7640 | f1_train: 0.9929 f1_val: 0.7665
Epoch: 0017 | loss_train: 0.0817 loss_val: 0.7674 | acc_train: 0.9929 acc_val: 0.7660 | f1_train: 0.9929 f1_val: 0.7696
Epoch: 0018 | loss_train: 0.0472 loss_val: 0.8252 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7676
Epoch: 0019 | loss_train: 0.0296 loss_val: 0.9048 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7515
Epoch: 0020 | loss_train: 0.0187 loss_val: 0.9918 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7549
Epoch: 0021 | loss_train: 0.0118 loss_val: 1.0788 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7477
Epoch: 0022 | loss_train: 0.0077 loss_val: 1.1631 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7414
Epoch: 0023 | loss_train: 0.0052 loss_val: 1.2442 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7384
Epoch: 0024 | loss_train: 0.0037 loss_val: 1.3211 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7338
Epoch: 0025 | loss_train: 0.0028 loss_val: 1.3947 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7355
Epoch: 0026 | loss_train: 0.0021 loss_val: 1.4648 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7329
Epoch: 0027 | loss_train: 0.0016 loss_val: 1.5318 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7326
Epoch: 0028 | loss_train: 0.0013 loss_val: 1.5953 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7315
Epoch: 0029 | loss_train: 0.0009 loss_val: 1.6554 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7365
Epoch: 0030 | loss_train: 0.0007 loss_val: 1.7122 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7340
Epoch: 0031 | loss_train: 0.0006 loss_val: 1.7656 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0032 | loss_train: 0.0005 loss_val: 1.8164 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7314
Epoch: 0033 | loss_train: 0.0004 loss_val: 1.8646 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7343
Epoch: 0034 | loss_train: 0.0003 loss_val: 1.9102 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7335
Epoch: 0035 | loss_train: 0.0003 loss_val: 1.9531 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7335
Epoch: 0036 | loss_train: 0.0002 loss_val: 1.9938 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7367
Epoch: 0037 | loss_train: 0.0002 loss_val: 2.0323 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7367
Epoch: 0038 | loss_train: 0.0002 loss_val: 2.0687 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7326
Epoch: 0039 | loss_train: 0.0001 loss_val: 2.1035 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7326
Epoch: 0040 | loss_train: 0.0001 loss_val: 2.1365 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7280
Epoch: 0041 | loss_train: 0.0001 loss_val: 2.1677 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7230
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.1972 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7196
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.2252 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7210
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.2517 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7222
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.2767 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7210
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.3003 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7199
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.3226 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7210
Epoch: 0048 | loss_train: 0.0000 loss_val: 2.3436 | acc_train: 1.0000 acc_val: 0.7180 | f1_train: 1.0000 f1_val: 0.7186
Optimization Finished!
Train cost: 10.4596s
Loading 18th epoch
Test set results: loss= 0.7422 accuracy= 0.7880

>>> run.py: Namespace(dataset='cora', device=1, experiment='hops', log_path='log/nagphormer/cora/hops', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/cora/hops')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=1, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6359 loss_val: 1.5509 | acc_train: 0.1583 acc_val: 0.2500 | f1_train: 0.1137 f1_val: 0.1165
Epoch: 0002 | loss_train: 1.5722 loss_val: 1.4314 | acc_train: 0.2417 acc_val: 0.5375 | f1_train: 0.1297 f1_val: 0.1753
Epoch: 0003 | loss_train: 1.4516 loss_val: 1.3023 | acc_train: 0.4500 acc_val: 0.5250 | f1_train: 0.1758 f1_val: 0.1557
Epoch: 0004 | loss_train: 1.3200 loss_val: 1.2023 | acc_train: 0.4417 acc_val: 0.5250 | f1_train: 0.1560 f1_val: 0.1557
Epoch: 0005 | loss_train: 1.2110 loss_val: 1.1401 | acc_train: 0.4583 acc_val: 0.5750 | f1_train: 0.1736 f1_val: 0.2414
Epoch: 0006 | loss_train: 1.1141 loss_val: 1.0972 | acc_train: 0.5250 acc_val: 0.6000 | f1_train: 0.2282 f1_val: 0.2686
Epoch: 0007 | loss_train: 1.0197 loss_val: 1.0609 | acc_train: 0.6167 acc_val: 0.6000 | f1_train: 0.2911 f1_val: 0.2713
Epoch: 0008 | loss_train: 0.9189 loss_val: 1.0254 | acc_train: 0.6750 acc_val: 0.6125 | f1_train: 0.3135 f1_val: 0.2771
Epoch: 0009 | loss_train: 0.7986 loss_val: 0.9732 | acc_train: 0.7000 acc_val: 0.6375 | f1_train: 0.4214 f1_val: 0.3539
Epoch: 0010 | loss_train: 0.6693 loss_val: 0.9184 | acc_train: 0.8000 acc_val: 0.6500 | f1_train: 0.6179 f1_val: 0.4310
Epoch: 0011 | loss_train: 0.5573 loss_val: 0.8771 | acc_train: 0.8500 acc_val: 0.6750 | f1_train: 0.7510 f1_val: 0.4517
Epoch: 0012 | loss_train: 0.4568 loss_val: 0.8480 | acc_train: 0.9083 acc_val: 0.6875 | f1_train: 0.8644 f1_val: 0.4881
Epoch: 0013 | loss_train: 0.3627 loss_val: 0.8267 | acc_train: 0.9417 acc_val: 0.7000 | f1_train: 0.9212 f1_val: 0.5048
Epoch: 0014 | loss_train: 0.2772 loss_val: 0.8438 | acc_train: 0.9500 acc_val: 0.7000 | f1_train: 0.9257 f1_val: 0.4892
Epoch: 0015 | loss_train: 0.2155 loss_val: 0.8990 | acc_train: 0.9583 acc_val: 0.6875 | f1_train: 0.9470 f1_val: 0.4850
Epoch: 0016 | loss_train: 0.1693 loss_val: 0.9282 | acc_train: 0.9667 acc_val: 0.6875 | f1_train: 0.9517 f1_val: 0.4764
Epoch: 0017 | loss_train: 0.1408 loss_val: 1.0650 | acc_train: 0.9500 acc_val: 0.6750 | f1_train: 0.9355 f1_val: 0.4687
Epoch: 0018 | loss_train: 0.1018 loss_val: 1.2396 | acc_train: 0.9667 acc_val: 0.6250 | f1_train: 0.9516 f1_val: 0.4363
Epoch: 0019 | loss_train: 0.0911 loss_val: 1.2762 | acc_train: 0.9750 acc_val: 0.6375 | f1_train: 0.9824 f1_val: 0.4425
Epoch: 0020 | loss_train: 0.0730 loss_val: 1.3713 | acc_train: 0.9750 acc_val: 0.6500 | f1_train: 0.9821 f1_val: 0.4517
Epoch: 0021 | loss_train: 0.0740 loss_val: 1.3963 | acc_train: 0.9667 acc_val: 0.6750 | f1_train: 0.9777 f1_val: 0.4642
Epoch: 0022 | loss_train: 0.0696 loss_val: 1.6251 | acc_train: 0.9667 acc_val: 0.6125 | f1_train: 0.9773 f1_val: 0.4168
Epoch: 0023 | loss_train: 0.0739 loss_val: 1.5706 | acc_train: 0.9667 acc_val: 0.6625 | f1_train: 0.9778 f1_val: 0.4393
Epoch: 0024 | loss_train: 0.0777 loss_val: 1.6043 | acc_train: 0.9667 acc_val: 0.6750 | f1_train: 0.9775 f1_val: 0.4455
Epoch: 0025 | loss_train: 0.0516 loss_val: 1.7606 | acc_train: 0.9667 acc_val: 0.6375 | f1_train: 0.9775 f1_val: 0.4255
Epoch: 0026 | loss_train: 0.0510 loss_val: 1.7878 | acc_train: 0.9750 acc_val: 0.6500 | f1_train: 0.9824 f1_val: 0.4332
Epoch: 0027 | loss_train: 0.0508 loss_val: 1.7798 | acc_train: 0.9750 acc_val: 0.6625 | f1_train: 0.9823 f1_val: 0.4382
Epoch: 0028 | loss_train: 0.0463 loss_val: 1.8877 | acc_train: 0.9750 acc_val: 0.6500 | f1_train: 0.9821 f1_val: 0.4332
Epoch: 0029 | loss_train: 0.0495 loss_val: 2.0905 | acc_train: 0.9750 acc_val: 0.5750 | f1_train: 0.9820 f1_val: 0.3777
Epoch: 0030 | loss_train: 0.0501 loss_val: 2.0696 | acc_train: 0.9750 acc_val: 0.6000 | f1_train: 0.9823 f1_val: 0.3899
Epoch: 0031 | loss_train: 0.0483 loss_val: 2.0091 | acc_train: 0.9667 acc_val: 0.6500 | f1_train: 0.9776 f1_val: 0.4317
Epoch: 0032 | loss_train: 0.0471 loss_val: 2.0931 | acc_train: 0.9667 acc_val: 0.6250 | f1_train: 0.9775 f1_val: 0.4182
Epoch: 0033 | loss_train: 0.0421 loss_val: 2.2818 | acc_train: 0.9750 acc_val: 0.5750 | f1_train: 0.9823 f1_val: 0.3771
Epoch: 0034 | loss_train: 0.0409 loss_val: 2.3033 | acc_train: 0.9750 acc_val: 0.5750 | f1_train: 0.9823 f1_val: 0.3771
Epoch: 0035 | loss_train: 0.0459 loss_val: 2.2115 | acc_train: 0.9750 acc_val: 0.6125 | f1_train: 0.9821 f1_val: 0.3957
Epoch: 0036 | loss_train: 0.0381 loss_val: 2.2570 | acc_train: 0.9833 acc_val: 0.6125 | f1_train: 0.9907 f1_val: 0.4118
Epoch: 0037 | loss_train: 0.0383 loss_val: 2.4899 | acc_train: 0.9833 acc_val: 0.5750 | f1_train: 0.9907 f1_val: 0.3771
Epoch: 0038 | loss_train: 0.0461 loss_val: 2.4553 | acc_train: 0.9667 acc_val: 0.5875 | f1_train: 0.9778 f1_val: 0.3833
Epoch: 0039 | loss_train: 0.0282 loss_val: 2.3412 | acc_train: 0.9917 acc_val: 0.6125 | f1_train: 0.9954 f1_val: 0.3957
Epoch: 0040 | loss_train: 0.0296 loss_val: 2.2548 | acc_train: 0.9833 acc_val: 0.6250 | f1_train: 0.9907 f1_val: 0.4019
Epoch: 0041 | loss_train: 0.0366 loss_val: 2.3136 | acc_train: 0.9833 acc_val: 0.6125 | f1_train: 0.9907 f1_val: 0.3957
Epoch: 0042 | loss_train: 0.0441 loss_val: 2.4541 | acc_train: 0.9667 acc_val: 0.6125 | f1_train: 0.9815 f1_val: 0.4157
Epoch: 0043 | loss_train: 0.0349 loss_val: 2.3919 | acc_train: 0.9833 acc_val: 0.6250 | f1_train: 0.9908 f1_val: 0.4219
Epoch: 0044 | loss_train: 0.0319 loss_val: 2.2112 | acc_train: 0.9833 acc_val: 0.6375 | f1_train: 0.9908 f1_val: 0.4281
Optimization Finished!
Train cost: 9.5334s
Loading 13th epoch
Test set results: loss= 1.2674 accuracy= 0.5294

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=2, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6343 loss_val: 1.5472 | acc_train: 0.1167 acc_val: 0.3500 | f1_train: 0.0830 f1_val: 0.1427
Epoch: 0002 | loss_train: 1.5668 loss_val: 1.4178 | acc_train: 0.2667 acc_val: 0.5125 | f1_train: 0.1370 f1_val: 0.1813
Epoch: 0003 | loss_train: 1.4367 loss_val: 1.2789 | acc_train: 0.4333 acc_val: 0.5250 | f1_train: 0.1550 f1_val: 0.1704
Epoch: 0004 | loss_train: 1.3101 loss_val: 1.1718 | acc_train: 0.4417 acc_val: 0.5625 | f1_train: 0.1490 f1_val: 0.2068
Epoch: 0005 | loss_train: 1.1917 loss_val: 1.1021 | acc_train: 0.4667 acc_val: 0.6250 | f1_train: 0.1760 f1_val: 0.2805
Epoch: 0006 | loss_train: 1.0959 loss_val: 1.0556 | acc_train: 0.5583 acc_val: 0.6375 | f1_train: 0.2548 f1_val: 0.2898
Epoch: 0007 | loss_train: 0.9959 loss_val: 1.0222 | acc_train: 0.6250 acc_val: 0.5625 | f1_train: 0.2962 f1_val: 0.2547
Epoch: 0008 | loss_train: 0.8890 loss_val: 1.0012 | acc_train: 0.6583 acc_val: 0.5750 | f1_train: 0.3042 f1_val: 0.2621
Epoch: 0009 | loss_train: 0.7821 loss_val: 0.9587 | acc_train: 0.6833 acc_val: 0.6250 | f1_train: 0.3944 f1_val: 0.3778
Epoch: 0010 | loss_train: 0.6614 loss_val: 0.8873 | acc_train: 0.7667 acc_val: 0.6125 | f1_train: 0.5597 f1_val: 0.3654
Epoch: 0011 | loss_train: 0.5509 loss_val: 0.8105 | acc_train: 0.9000 acc_val: 0.7125 | f1_train: 0.8443 f1_val: 0.4643
Epoch: 0012 | loss_train: 0.4529 loss_val: 0.7616 | acc_train: 0.9167 acc_val: 0.7500 | f1_train: 0.8884 f1_val: 0.5355
Epoch: 0013 | loss_train: 0.3509 loss_val: 0.7691 | acc_train: 0.9417 acc_val: 0.7375 | f1_train: 0.9083 f1_val: 0.5780
Epoch: 0014 | loss_train: 0.2647 loss_val: 0.8103 | acc_train: 0.9667 acc_val: 0.7375 | f1_train: 0.9454 f1_val: 0.5737
Epoch: 0015 | loss_train: 0.2065 loss_val: 0.8616 | acc_train: 0.9667 acc_val: 0.7250 | f1_train: 0.9372 f1_val: 0.5455
Epoch: 0016 | loss_train: 0.1486 loss_val: 0.9331 | acc_train: 0.9833 acc_val: 0.7000 | f1_train: 0.9608 f1_val: 0.4642
Epoch: 0017 | loss_train: 0.1077 loss_val: 1.0120 | acc_train: 0.9917 acc_val: 0.7000 | f1_train: 0.9694 f1_val: 0.4634
Epoch: 0018 | loss_train: 0.0798 loss_val: 1.0773 | acc_train: 0.9917 acc_val: 0.7125 | f1_train: 0.9694 f1_val: 0.4952
Epoch: 0019 | loss_train: 0.0505 loss_val: 1.1534 | acc_train: 0.9917 acc_val: 0.7125 | f1_train: 0.9694 f1_val: 0.4834
Epoch: 0020 | loss_train: 0.0330 loss_val: 1.2492 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4769
Epoch: 0021 | loss_train: 0.0252 loss_val: 1.3069 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4699
Epoch: 0022 | loss_train: 0.0167 loss_val: 1.3469 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4744
Epoch: 0023 | loss_train: 0.0101 loss_val: 1.4065 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4801
Epoch: 0024 | loss_train: 0.0081 loss_val: 1.5022 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4715
Epoch: 0025 | loss_train: 0.0060 loss_val: 1.6372 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4502
Epoch: 0026 | loss_train: 0.0040 loss_val: 1.7972 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4445
Epoch: 0027 | loss_train: 0.0027 loss_val: 1.9627 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4364
Epoch: 0028 | loss_train: 0.0023 loss_val: 2.1155 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4364
Epoch: 0029 | loss_train: 0.0018 loss_val: 2.2470 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4102
Epoch: 0030 | loss_train: 0.0028 loss_val: 2.3228 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4286
Epoch: 0031 | loss_train: 0.0012 loss_val: 2.3886 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4491
Epoch: 0032 | loss_train: 0.0011 loss_val: 2.4756 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4491
Epoch: 0033 | loss_train: 0.0009 loss_val: 2.5575 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4683
Epoch: 0034 | loss_train: 0.0006 loss_val: 2.6320 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4683
Epoch: 0035 | loss_train: 0.0006 loss_val: 2.7001 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4719
Epoch: 0036 | loss_train: 0.0005 loss_val: 2.7626 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4719
Epoch: 0037 | loss_train: 0.0004 loss_val: 2.8232 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.8803 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0039 | loss_train: 0.0003 loss_val: 2.9330 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0040 | loss_train: 0.0008 loss_val: 2.9648 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4694
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.9953 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4601
Epoch: 0042 | loss_train: 0.0002 loss_val: 3.0245 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4601
Optimization Finished!
Train cost: 10.0408s
Loading 12th epoch
Test set results: loss= 1.0619 accuracy= 0.5686

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6315 loss_val: 1.5466 | acc_train: 0.1500 acc_val: 0.3000 | f1_train: 0.1211 f1_val: 0.1308
Epoch: 0002 | loss_train: 1.5652 loss_val: 1.4203 | acc_train: 0.2833 acc_val: 0.5000 | f1_train: 0.1653 f1_val: 0.1644
Epoch: 0003 | loss_train: 1.4443 loss_val: 1.2855 | acc_train: 0.4333 acc_val: 0.5125 | f1_train: 0.1550 f1_val: 0.1526
Epoch: 0004 | loss_train: 1.3147 loss_val: 1.1833 | acc_train: 0.4333 acc_val: 0.5750 | f1_train: 0.1390 f1_val: 0.2358
Epoch: 0005 | loss_train: 1.2048 loss_val: 1.1209 | acc_train: 0.4583 acc_val: 0.6125 | f1_train: 0.1674 f1_val: 0.2715
Epoch: 0006 | loss_train: 1.1178 loss_val: 1.0844 | acc_train: 0.5333 acc_val: 0.5625 | f1_train: 0.2372 f1_val: 0.2529
Epoch: 0007 | loss_train: 1.0281 loss_val: 1.0654 | acc_train: 0.5750 acc_val: 0.5500 | f1_train: 0.2665 f1_val: 0.2474
Epoch: 0008 | loss_train: 0.9321 loss_val: 1.0565 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3003 f1_val: 0.2557
Epoch: 0009 | loss_train: 0.8323 loss_val: 1.0307 | acc_train: 0.6667 acc_val: 0.6125 | f1_train: 0.3606 f1_val: 0.3566
Epoch: 0010 | loss_train: 0.7215 loss_val: 0.9827 | acc_train: 0.7583 acc_val: 0.6000 | f1_train: 0.5366 f1_val: 0.3628
Epoch: 0011 | loss_train: 0.6194 loss_val: 0.9135 | acc_train: 0.8167 acc_val: 0.6500 | f1_train: 0.7483 f1_val: 0.4362
Epoch: 0012 | loss_train: 0.5123 loss_val: 0.8581 | acc_train: 0.8833 acc_val: 0.6500 | f1_train: 0.8614 f1_val: 0.4708
Epoch: 0013 | loss_train: 0.4123 loss_val: 0.8767 | acc_train: 0.9167 acc_val: 0.7000 | f1_train: 0.8844 f1_val: 0.5609
Epoch: 0014 | loss_train: 0.3231 loss_val: 0.9327 | acc_train: 0.9500 acc_val: 0.7000 | f1_train: 0.9142 f1_val: 0.5516
Epoch: 0015 | loss_train: 0.2508 loss_val: 0.9685 | acc_train: 0.9500 acc_val: 0.6875 | f1_train: 0.9209 f1_val: 0.4886
Epoch: 0016 | loss_train: 0.1914 loss_val: 1.0292 | acc_train: 0.9583 acc_val: 0.7000 | f1_train: 0.9259 f1_val: 0.4882
Epoch: 0017 | loss_train: 0.1423 loss_val: 1.1019 | acc_train: 0.9667 acc_val: 0.7000 | f1_train: 0.9297 f1_val: 0.4825
Epoch: 0018 | loss_train: 0.1004 loss_val: 1.1785 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9694 f1_val: 0.4658
Epoch: 0019 | loss_train: 0.0704 loss_val: 1.2618 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4528
Epoch: 0020 | loss_train: 0.0536 loss_val: 1.3591 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0021 | loss_train: 0.0349 loss_val: 1.4517 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0022 | loss_train: 0.0250 loss_val: 1.5643 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0023 | loss_train: 0.0185 loss_val: 1.6873 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0024 | loss_train: 0.0115 loss_val: 1.7941 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0025 | loss_train: 0.0134 loss_val: 1.8016 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0026 | loss_train: 0.0071 loss_val: 1.8700 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0027 | loss_train: 0.0042 loss_val: 1.9669 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4397
Epoch: 0028 | loss_train: 0.0032 loss_val: 2.0669 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4652
Epoch: 0029 | loss_train: 0.0030 loss_val: 2.1614 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0030 | loss_train: 0.0027 loss_val: 2.2514 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4694
Epoch: 0031 | loss_train: 0.0019 loss_val: 2.3389 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0032 | loss_train: 0.0016 loss_val: 2.4217 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0033 | loss_train: 0.0012 loss_val: 2.4973 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0034 | loss_train: 0.0011 loss_val: 2.5645 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0035 | loss_train: 0.0008 loss_val: 2.6212 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.6685 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.7098 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.7466 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0039 | loss_train: 0.0004 loss_val: 2.7690 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.7881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.8042 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.8187 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.8326 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.8465 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.8602 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.8742 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.8881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4756
Optimization Finished!
Train cost: 9.5945s
Loading 13th epoch
Test set results: loss= 1.1536 accuracy= 0.5686

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=4, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6274 loss_val: 1.5432 | acc_train: 0.1333 acc_val: 0.3375 | f1_train: 0.0920 f1_val: 0.1383
Epoch: 0002 | loss_train: 1.5588 loss_val: 1.4142 | acc_train: 0.3333 acc_val: 0.5000 | f1_train: 0.2064 f1_val: 0.1644
Epoch: 0003 | loss_train: 1.4431 loss_val: 1.2794 | acc_train: 0.4167 acc_val: 0.5125 | f1_train: 0.1376 f1_val: 0.1526
Epoch: 0004 | loss_train: 1.3129 loss_val: 1.1758 | acc_train: 0.4417 acc_val: 0.6000 | f1_train: 0.1490 f1_val: 0.2532
Epoch: 0005 | loss_train: 1.2009 loss_val: 1.1134 | acc_train: 0.4667 acc_val: 0.6250 | f1_train: 0.1760 f1_val: 0.2828
Epoch: 0006 | loss_train: 1.1150 loss_val: 1.0778 | acc_train: 0.5250 acc_val: 0.5625 | f1_train: 0.2310 f1_val: 0.2519
Epoch: 0007 | loss_train: 1.0285 loss_val: 1.0620 | acc_train: 0.5833 acc_val: 0.5625 | f1_train: 0.2679 f1_val: 0.2537
Epoch: 0008 | loss_train: 0.9275 loss_val: 1.0569 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3000 f1_val: 0.2557
Epoch: 0009 | loss_train: 0.8280 loss_val: 1.0278 | acc_train: 0.6750 acc_val: 0.6125 | f1_train: 0.3762 f1_val: 0.3720
Epoch: 0010 | loss_train: 0.7239 loss_val: 0.9685 | acc_train: 0.7667 acc_val: 0.5875 | f1_train: 0.6376 f1_val: 0.3457
Epoch: 0011 | loss_train: 0.6179 loss_val: 0.8923 | acc_train: 0.8333 acc_val: 0.6500 | f1_train: 0.7614 f1_val: 0.4246
Epoch: 0012 | loss_train: 0.5106 loss_val: 0.8429 | acc_train: 0.9000 acc_val: 0.6625 | f1_train: 0.8639 f1_val: 0.4575
Epoch: 0013 | loss_train: 0.4096 loss_val: 0.8613 | acc_train: 0.9250 acc_val: 0.7250 | f1_train: 0.8952 f1_val: 0.5424
Epoch: 0014 | loss_train: 0.3286 loss_val: 0.9000 | acc_train: 0.9333 acc_val: 0.7250 | f1_train: 0.9036 f1_val: 0.5050
Epoch: 0015 | loss_train: 0.2494 loss_val: 0.9723 | acc_train: 0.9417 acc_val: 0.7125 | f1_train: 0.9087 f1_val: 0.5369
Epoch: 0016 | loss_train: 0.1888 loss_val: 1.0572 | acc_train: 0.9750 acc_val: 0.7000 | f1_train: 0.9456 f1_val: 0.4934
Epoch: 0017 | loss_train: 0.1426 loss_val: 1.1201 | acc_train: 0.9750 acc_val: 0.7125 | f1_train: 0.9456 f1_val: 0.5296
Epoch: 0018 | loss_train: 0.1028 loss_val: 1.1599 | acc_train: 0.9917 acc_val: 0.7125 | f1_train: 0.9694 f1_val: 0.4879
Epoch: 0019 | loss_train: 0.0720 loss_val: 1.2047 | acc_train: 0.9917 acc_val: 0.7000 | f1_train: 0.9694 f1_val: 0.4638
Epoch: 0020 | loss_train: 0.0515 loss_val: 1.2529 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4638
Epoch: 0021 | loss_train: 0.0338 loss_val: 1.3385 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4638
Epoch: 0022 | loss_train: 0.0202 loss_val: 1.4536 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4638
Epoch: 0023 | loss_train: 0.0138 loss_val: 1.5587 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4638
Epoch: 0024 | loss_train: 0.0101 loss_val: 1.6511 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4522
Epoch: 0025 | loss_train: 0.0074 loss_val: 1.7631 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4400
Epoch: 0026 | loss_train: 0.0060 loss_val: 1.9347 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4364
Epoch: 0027 | loss_train: 0.0041 loss_val: 2.0945 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4299
Epoch: 0028 | loss_train: 0.0035 loss_val: 2.2049 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4299
Epoch: 0029 | loss_train: 0.0030 loss_val: 2.2735 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4222
Epoch: 0030 | loss_train: 0.0025 loss_val: 2.3197 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4286
Epoch: 0031 | loss_train: 0.0020 loss_val: 2.3642 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4387
Epoch: 0032 | loss_train: 0.0015 loss_val: 2.4104 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4372
Epoch: 0033 | loss_train: 0.0011 loss_val: 2.4573 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4372
Epoch: 0034 | loss_train: 0.0009 loss_val: 2.5026 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4372
Epoch: 0035 | loss_train: 0.0007 loss_val: 2.5461 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4438
Epoch: 0036 | loss_train: 0.0007 loss_val: 2.5870 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4438
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.6263 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4438
Epoch: 0038 | loss_train: 0.0004 loss_val: 2.6638 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4438
Epoch: 0039 | loss_train: 0.0003 loss_val: 2.6998 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4438
Epoch: 0040 | loss_train: 0.0002 loss_val: 2.7334 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4438
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.7652 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4438
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.7953 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4352
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.8237 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4352
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.8501 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4352
Optimization Finished!
Train cost: 12.4747s
Loading 13th epoch
Test set results: loss= 1.1220 accuracy= 0.6078

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=5, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6288 loss_val: 1.5427 | acc_train: 0.1500 acc_val: 0.3375 | f1_train: 0.1127 f1_val: 0.1372
Epoch: 0002 | loss_train: 1.5588 loss_val: 1.4143 | acc_train: 0.3333 acc_val: 0.5000 | f1_train: 0.1745 f1_val: 0.1644
Epoch: 0003 | loss_train: 1.4356 loss_val: 1.2820 | acc_train: 0.4417 acc_val: 0.5250 | f1_train: 0.1512 f1_val: 0.1704
Epoch: 0004 | loss_train: 1.3076 loss_val: 1.1813 | acc_train: 0.4500 acc_val: 0.5875 | f1_train: 0.1586 f1_val: 0.2519
Epoch: 0005 | loss_train: 1.2077 loss_val: 1.1214 | acc_train: 0.4833 acc_val: 0.6250 | f1_train: 0.1923 f1_val: 0.2805
Epoch: 0006 | loss_train: 1.1227 loss_val: 1.0898 | acc_train: 0.5333 acc_val: 0.5500 | f1_train: 0.2372 f1_val: 0.2450
Epoch: 0007 | loss_train: 1.0338 loss_val: 1.0777 | acc_train: 0.5750 acc_val: 0.5375 | f1_train: 0.2636 f1_val: 0.2417
Epoch: 0008 | loss_train: 0.9417 loss_val: 1.0763 | acc_train: 0.6333 acc_val: 0.5625 | f1_train: 0.2922 f1_val: 0.2557
Epoch: 0009 | loss_train: 0.8403 loss_val: 1.0525 | acc_train: 0.6750 acc_val: 0.6125 | f1_train: 0.3781 f1_val: 0.3764
Epoch: 0010 | loss_train: 0.7484 loss_val: 1.0014 | acc_train: 0.7833 acc_val: 0.5750 | f1_train: 0.6969 f1_val: 0.3399
Epoch: 0011 | loss_train: 0.6411 loss_val: 0.9279 | acc_train: 0.8333 acc_val: 0.6125 | f1_train: 0.8102 f1_val: 0.4058
Epoch: 0012 | loss_train: 0.5318 loss_val: 0.8817 | acc_train: 0.8667 acc_val: 0.6375 | f1_train: 0.8399 f1_val: 0.4835
Epoch: 0013 | loss_train: 0.4348 loss_val: 0.9017 | acc_train: 0.9083 acc_val: 0.7000 | f1_train: 0.8860 f1_val: 0.5565
Epoch: 0014 | loss_train: 0.3491 loss_val: 0.9360 | acc_train: 0.9417 acc_val: 0.6750 | f1_train: 0.9082 f1_val: 0.4991
Epoch: 0015 | loss_train: 0.2752 loss_val: 0.9953 | acc_train: 0.9500 acc_val: 0.7000 | f1_train: 0.9203 f1_val: 0.5059
Epoch: 0016 | loss_train: 0.2052 loss_val: 1.0659 | acc_train: 0.9667 acc_val: 0.6750 | f1_train: 0.9349 f1_val: 0.4652
Epoch: 0017 | loss_train: 0.1590 loss_val: 1.1054 | acc_train: 0.9750 acc_val: 0.7125 | f1_train: 0.9435 f1_val: 0.5348
Epoch: 0018 | loss_train: 0.1097 loss_val: 1.1373 | acc_train: 0.9917 acc_val: 0.7125 | f1_train: 0.9694 f1_val: 0.5382
Epoch: 0019 | loss_train: 0.0804 loss_val: 1.1782 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5320
Epoch: 0020 | loss_train: 0.0574 loss_val: 1.2800 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5444
Epoch: 0021 | loss_train: 0.0389 loss_val: 1.4306 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4577
Epoch: 0022 | loss_train: 0.0221 loss_val: 1.5807 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4468
Epoch: 0023 | loss_train: 0.0180 loss_val: 1.6749 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4468
Epoch: 0024 | loss_train: 0.0130 loss_val: 1.7151 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4468
Epoch: 0025 | loss_train: 0.0092 loss_val: 1.7421 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4534
Epoch: 0026 | loss_train: 0.0089 loss_val: 2.0185 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4208
Epoch: 0027 | loss_train: 0.0052 loss_val: 2.3275 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4083
Epoch: 0028 | loss_train: 0.0064 loss_val: 2.5295 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4155
Epoch: 0029 | loss_train: 0.0052 loss_val: 2.6304 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4155
Epoch: 0030 | loss_train: 0.0047 loss_val: 2.6429 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4155
Epoch: 0031 | loss_train: 0.0022 loss_val: 2.6609 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4282
Epoch: 0032 | loss_train: 0.0018 loss_val: 2.7009 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4410
Epoch: 0033 | loss_train: 0.0014 loss_val: 2.7602 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4410
Epoch: 0034 | loss_train: 0.0013 loss_val: 2.7228 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4410
Epoch: 0035 | loss_train: 0.0016 loss_val: 2.7221 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4410
Epoch: 0036 | loss_train: 0.0018 loss_val: 2.7318 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4410
Epoch: 0037 | loss_train: 0.0013 loss_val: 2.7372 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4410
Epoch: 0038 | loss_train: 0.0005 loss_val: 2.7436 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4321
Epoch: 0039 | loss_train: 0.0003 loss_val: 2.7522 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4321
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.7652 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4321
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.7826 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4321
Epoch: 0042 | loss_train: 0.0003 loss_val: 2.8038 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Epoch: 0043 | loss_train: 0.0005 loss_val: 2.8260 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Epoch: 0044 | loss_train: 0.0004 loss_val: 2.8501 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Epoch: 0045 | loss_train: 0.0006 loss_val: 2.8708 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Epoch: 0046 | loss_train: 0.0004 loss_val: 2.8903 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Epoch: 0047 | loss_train: 0.0003 loss_val: 2.9095 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Epoch: 0048 | loss_train: 0.0002 loss_val: 2.9278 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Epoch: 0049 | loss_train: 0.0002 loss_val: 2.9457 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Epoch: 0050 | loss_train: 0.0001 loss_val: 2.9633 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4292
Optimization Finished!
Train cost: 12.9216s
Loading 20th epoch
Test set results: loss= 1.5633 accuracy= 0.5882

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=6, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6295 loss_val: 1.5402 | acc_train: 0.1333 acc_val: 0.3750 | f1_train: 0.0926 f1_val: 0.1462
Epoch: 0002 | loss_train: 1.5535 loss_val: 1.4099 | acc_train: 0.3500 acc_val: 0.4875 | f1_train: 0.1791 f1_val: 0.1473
Epoch: 0003 | loss_train: 1.4336 loss_val: 1.2779 | acc_train: 0.4250 acc_val: 0.5125 | f1_train: 0.1471 f1_val: 0.1526
Epoch: 0004 | loss_train: 1.3108 loss_val: 1.1777 | acc_train: 0.4333 acc_val: 0.5875 | f1_train: 0.1390 f1_val: 0.2472
Epoch: 0005 | loss_train: 1.2073 loss_val: 1.1189 | acc_train: 0.4750 acc_val: 0.6375 | f1_train: 0.1843 f1_val: 0.2911
Epoch: 0006 | loss_train: 1.1252 loss_val: 1.0887 | acc_train: 0.5417 acc_val: 0.5500 | f1_train: 0.2432 f1_val: 0.2457
Epoch: 0007 | loss_train: 1.0410 loss_val: 1.0794 | acc_train: 0.5750 acc_val: 0.5375 | f1_train: 0.2636 f1_val: 0.2417
Epoch: 0008 | loss_train: 0.9473 loss_val: 1.0783 | acc_train: 0.6417 acc_val: 0.5625 | f1_train: 0.2962 f1_val: 0.2538
Epoch: 0009 | loss_train: 0.8528 loss_val: 1.0470 | acc_train: 0.6583 acc_val: 0.6125 | f1_train: 0.3566 f1_val: 0.3764
Epoch: 0010 | loss_train: 0.7502 loss_val: 0.9922 | acc_train: 0.7833 acc_val: 0.5750 | f1_train: 0.6327 f1_val: 0.3418
Epoch: 0011 | loss_train: 0.6435 loss_val: 0.9231 | acc_train: 0.8250 acc_val: 0.6250 | f1_train: 0.7726 f1_val: 0.4145
Epoch: 0012 | loss_train: 0.5309 loss_val: 0.8818 | acc_train: 0.8917 acc_val: 0.6625 | f1_train: 0.8577 f1_val: 0.4288
Epoch: 0013 | loss_train: 0.4382 loss_val: 0.8917 | acc_train: 0.9167 acc_val: 0.7125 | f1_train: 0.8945 f1_val: 0.5202
Epoch: 0014 | loss_train: 0.3533 loss_val: 0.9405 | acc_train: 0.9250 acc_val: 0.7250 | f1_train: 0.8899 f1_val: 0.5251
Epoch: 0015 | loss_train: 0.2727 loss_val: 1.0147 | acc_train: 0.9417 acc_val: 0.7125 | f1_train: 0.9082 f1_val: 0.5148
Epoch: 0016 | loss_train: 0.2049 loss_val: 1.0598 | acc_train: 0.9750 acc_val: 0.6875 | f1_train: 0.9435 f1_val: 0.4726
Epoch: 0017 | loss_train: 0.1503 loss_val: 1.0985 | acc_train: 0.9833 acc_val: 0.7000 | f1_train: 0.9542 f1_val: 0.5141
Epoch: 0018 | loss_train: 0.1073 loss_val: 1.1431 | acc_train: 0.9917 acc_val: 0.7125 | f1_train: 0.9694 f1_val: 0.5320
Epoch: 0019 | loss_train: 0.0765 loss_val: 1.1724 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5444
Epoch: 0020 | loss_train: 0.0569 loss_val: 1.2431 | acc_train: 0.9917 acc_val: 0.7250 | f1_train: 0.9920 f1_val: 0.5444
Epoch: 0021 | loss_train: 0.0357 loss_val: 1.3790 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5388
Epoch: 0022 | loss_train: 0.0211 loss_val: 1.5235 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5281
Epoch: 0023 | loss_train: 0.0159 loss_val: 1.6410 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4558
Epoch: 0024 | loss_train: 0.0132 loss_val: 1.6716 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4474
Epoch: 0025 | loss_train: 0.0082 loss_val: 1.6919 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4454
Epoch: 0026 | loss_train: 0.0055 loss_val: 1.7505 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4530
Epoch: 0027 | loss_train: 0.0050 loss_val: 1.8386 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4452
Epoch: 0028 | loss_train: 0.0041 loss_val: 1.9189 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4452
Epoch: 0029 | loss_train: 0.0030 loss_val: 1.9964 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4372
Epoch: 0030 | loss_train: 0.0024 loss_val: 2.0634 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4372
Epoch: 0031 | loss_train: 0.0017 loss_val: 2.1257 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4372
Epoch: 0032 | loss_train: 0.0013 loss_val: 2.1869 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4285
Epoch: 0033 | loss_train: 0.0010 loss_val: 2.2473 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4285
Epoch: 0034 | loss_train: 0.0008 loss_val: 2.3069 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4257
Epoch: 0035 | loss_train: 0.0006 loss_val: 2.3654 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4257
Epoch: 0036 | loss_train: 0.0005 loss_val: 2.4232 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4257
Epoch: 0037 | loss_train: 0.0004 loss_val: 2.4800 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4257
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.5352 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4182
Epoch: 0039 | loss_train: 0.0003 loss_val: 2.5880 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4182
Epoch: 0040 | loss_train: 0.0002 loss_val: 2.6385 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4182
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.6864 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4465
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.7310 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4465
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.7724 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4465
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.8100 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4465
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.8450 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4545
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.8770 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4545
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.9063 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4545
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.9334 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4545
Epoch: 0049 | loss_train: 0.0001 loss_val: 2.9580 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4545
Epoch: 0050 | loss_train: 0.0001 loss_val: 2.9804 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4545
Epoch: 0051 | loss_train: 0.0001 loss_val: 3.0011 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4545
Optimization Finished!
Train cost: 11.2350s
Loading 14th epoch
Test set results: loss= 1.2004 accuracy= 0.6078

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=7, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6176 loss_val: 1.5378 | acc_train: 0.2000 acc_val: 0.3875 | f1_train: 0.1538 f1_val: 0.1479
Epoch: 0002 | loss_train: 1.5469 loss_val: 1.4068 | acc_train: 0.3750 acc_val: 0.4875 | f1_train: 0.2037 f1_val: 0.1473
Epoch: 0003 | loss_train: 1.4335 loss_val: 1.2763 | acc_train: 0.4167 acc_val: 0.5250 | f1_train: 0.1366 f1_val: 0.1825
Epoch: 0004 | loss_train: 1.3076 loss_val: 1.1805 | acc_train: 0.4583 acc_val: 0.5750 | f1_train: 0.1674 f1_val: 0.2461
Epoch: 0005 | loss_train: 1.2107 loss_val: 1.1250 | acc_train: 0.4833 acc_val: 0.6250 | f1_train: 0.1930 f1_val: 0.2806
Epoch: 0006 | loss_train: 1.1291 loss_val: 1.0975 | acc_train: 0.5333 acc_val: 0.5500 | f1_train: 0.2373 f1_val: 0.2450
Epoch: 0007 | loss_train: 1.0441 loss_val: 1.0922 | acc_train: 0.5583 acc_val: 0.5375 | f1_train: 0.2538 f1_val: 0.2415
Epoch: 0008 | loss_train: 0.9551 loss_val: 1.0965 | acc_train: 0.6250 acc_val: 0.5750 | f1_train: 0.2882 f1_val: 0.2599
Epoch: 0009 | loss_train: 0.8625 loss_val: 1.0666 | acc_train: 0.6667 acc_val: 0.6000 | f1_train: 0.3743 f1_val: 0.3584
Epoch: 0010 | loss_train: 0.7652 loss_val: 1.0066 | acc_train: 0.7833 acc_val: 0.5750 | f1_train: 0.6327 f1_val: 0.3482
Epoch: 0011 | loss_train: 0.6610 loss_val: 0.9435 | acc_train: 0.8250 acc_val: 0.5875 | f1_train: 0.7785 f1_val: 0.3855
Epoch: 0012 | loss_train: 0.5483 loss_val: 0.9236 | acc_train: 0.8917 acc_val: 0.6375 | f1_train: 0.8705 f1_val: 0.4848
Epoch: 0013 | loss_train: 0.4596 loss_val: 0.9032 | acc_train: 0.9083 acc_val: 0.7000 | f1_train: 0.8808 f1_val: 0.4524
Epoch: 0014 | loss_train: 0.3704 loss_val: 0.9587 | acc_train: 0.9417 acc_val: 0.7000 | f1_train: 0.9082 f1_val: 0.5106
Epoch: 0015 | loss_train: 0.2855 loss_val: 1.0502 | acc_train: 0.9500 acc_val: 0.6625 | f1_train: 0.9203 f1_val: 0.4227
Epoch: 0016 | loss_train: 0.2238 loss_val: 1.0397 | acc_train: 0.9583 acc_val: 0.7000 | f1_train: 0.9274 f1_val: 0.5080
Epoch: 0017 | loss_train: 0.1656 loss_val: 1.0356 | acc_train: 0.9833 acc_val: 0.7250 | f1_train: 0.9542 f1_val: 0.5701
Epoch: 0018 | loss_train: 0.1195 loss_val: 1.1030 | acc_train: 0.9833 acc_val: 0.7375 | f1_train: 0.9580 f1_val: 0.6194
Epoch: 0019 | loss_train: 0.0828 loss_val: 1.1799 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5382
Epoch: 0020 | loss_train: 0.0599 loss_val: 1.2358 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5326
Epoch: 0021 | loss_train: 0.0388 loss_val: 1.3322 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5219
Epoch: 0022 | loss_train: 0.0241 loss_val: 1.4443 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5281
Epoch: 0023 | loss_train: 0.0171 loss_val: 1.5340 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.5218
Epoch: 0024 | loss_train: 0.0124 loss_val: 1.5957 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4593
Epoch: 0025 | loss_train: 0.0088 loss_val: 1.6501 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4454
Epoch: 0026 | loss_train: 0.0066 loss_val: 1.7143 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4465
Epoch: 0027 | loss_train: 0.0049 loss_val: 1.7819 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4530
Epoch: 0028 | loss_train: 0.0038 loss_val: 1.8450 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4530
Epoch: 0029 | loss_train: 0.0029 loss_val: 1.9014 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4530
Epoch: 0030 | loss_train: 0.0023 loss_val: 1.9533 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4530
Epoch: 0031 | loss_train: 0.0018 loss_val: 2.0019 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4597
Epoch: 0032 | loss_train: 0.0015 loss_val: 2.0493 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4597
Epoch: 0033 | loss_train: 0.0012 loss_val: 2.0979 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4511
Epoch: 0034 | loss_train: 0.0010 loss_val: 2.1479 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4511
Epoch: 0035 | loss_train: 0.0007 loss_val: 2.1986 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4511
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.2490 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.2989 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0038 | loss_train: 0.0004 loss_val: 2.3476 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0039 | loss_train: 0.0003 loss_val: 2.3947 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.4407 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4697
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.4848 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.5268 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.5670 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.6049 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.6403 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.6736 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.7045 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.7328 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Optimization Finished!
Train cost: 12.4557s
Loading 18th epoch
Test set results: loss= 1.2939 accuracy= 0.6078

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=8, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6225 loss_val: 1.5369 | acc_train: 0.1667 acc_val: 0.3750 | f1_train: 0.1370 f1_val: 0.1320
Epoch: 0002 | loss_train: 1.5474 loss_val: 1.4068 | acc_train: 0.3500 acc_val: 0.4750 | f1_train: 0.1938 f1_val: 0.1450
Epoch: 0003 | loss_train: 1.4293 loss_val: 1.2783 | acc_train: 0.4250 acc_val: 0.5250 | f1_train: 0.1395 f1_val: 0.1704
Epoch: 0004 | loss_train: 1.3096 loss_val: 1.1820 | acc_train: 0.4583 acc_val: 0.5750 | f1_train: 0.1677 f1_val: 0.2461
Epoch: 0005 | loss_train: 1.2088 loss_val: 1.1259 | acc_train: 0.4750 acc_val: 0.6250 | f1_train: 0.1892 f1_val: 0.2806
Epoch: 0006 | loss_train: 1.1270 loss_val: 1.0986 | acc_train: 0.5500 acc_val: 0.5500 | f1_train: 0.2468 f1_val: 0.2450
Epoch: 0007 | loss_train: 1.0524 loss_val: 1.0953 | acc_train: 0.5667 acc_val: 0.5375 | f1_train: 0.2585 f1_val: 0.2415
Epoch: 0008 | loss_train: 0.9588 loss_val: 1.0998 | acc_train: 0.6417 acc_val: 0.5750 | f1_train: 0.2959 f1_val: 0.2599
Epoch: 0009 | loss_train: 0.8677 loss_val: 1.0654 | acc_train: 0.6583 acc_val: 0.5875 | f1_train: 0.3566 f1_val: 0.3342
Epoch: 0010 | loss_train: 0.7731 loss_val: 1.0067 | acc_train: 0.7750 acc_val: 0.5750 | f1_train: 0.6073 f1_val: 0.3482
Epoch: 0011 | loss_train: 0.6676 loss_val: 0.9455 | acc_train: 0.8333 acc_val: 0.6125 | f1_train: 0.7830 f1_val: 0.3944
Epoch: 0012 | loss_train: 0.5479 loss_val: 0.9068 | acc_train: 0.8750 acc_val: 0.6750 | f1_train: 0.8328 f1_val: 0.4417
Epoch: 0013 | loss_train: 0.4617 loss_val: 0.9042 | acc_train: 0.9083 acc_val: 0.7000 | f1_train: 0.8802 f1_val: 0.4524
Epoch: 0014 | loss_train: 0.3682 loss_val: 0.9641 | acc_train: 0.9417 acc_val: 0.7000 | f1_train: 0.9082 f1_val: 0.5088
Epoch: 0015 | loss_train: 0.2912 loss_val: 1.0206 | acc_train: 0.9417 acc_val: 0.7125 | f1_train: 0.9157 f1_val: 0.5149
Epoch: 0016 | loss_train: 0.2260 loss_val: 1.0239 | acc_train: 0.9667 acc_val: 0.7000 | f1_train: 0.9336 f1_val: 0.5080
Epoch: 0017 | loss_train: 0.1633 loss_val: 1.0655 | acc_train: 0.9750 acc_val: 0.7125 | f1_train: 0.9456 f1_val: 0.5638
Epoch: 0018 | loss_train: 0.1165 loss_val: 1.1085 | acc_train: 0.9833 acc_val: 0.7250 | f1_train: 0.9580 f1_val: 0.5839
Epoch: 0019 | loss_train: 0.0815 loss_val: 1.1449 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5444
Epoch: 0020 | loss_train: 0.0570 loss_val: 1.2396 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5344
Epoch: 0021 | loss_train: 0.0391 loss_val: 1.3741 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5344
Epoch: 0022 | loss_train: 0.0250 loss_val: 1.5036 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4558
Epoch: 0023 | loss_train: 0.0184 loss_val: 1.5899 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4558
Epoch: 0024 | loss_train: 0.0120 loss_val: 1.6429 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4474
Epoch: 0025 | loss_train: 0.0081 loss_val: 1.6801 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4400
Epoch: 0026 | loss_train: 0.0058 loss_val: 1.7334 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4530
Epoch: 0027 | loss_train: 0.0045 loss_val: 1.7945 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4299
Epoch: 0028 | loss_train: 0.0038 loss_val: 1.8531 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4299
Epoch: 0029 | loss_train: 0.0030 loss_val: 1.9126 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4576
Epoch: 0030 | loss_train: 0.0022 loss_val: 1.9745 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4576
Epoch: 0031 | loss_train: 0.0017 loss_val: 2.0394 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4597
Epoch: 0032 | loss_train: 0.0014 loss_val: 2.1049 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4830
Epoch: 0033 | loss_train: 0.0009 loss_val: 2.1730 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0034 | loss_train: 0.0007 loss_val: 2.2398 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0035 | loss_train: 0.0006 loss_val: 2.3023 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0036 | loss_train: 0.0005 loss_val: 2.3605 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0037 | loss_train: 0.0004 loss_val: 2.4144 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4697
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.4643 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4697
Epoch: 0039 | loss_train: 0.0003 loss_val: 2.5106 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4616
Epoch: 0040 | loss_train: 0.0002 loss_val: 2.5532 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4616
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.5926 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4616
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.6291 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4616
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.6627 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4616
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.6938 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4616
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.7223 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4533
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.7485 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4533
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.7722 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4533
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.7936 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4533
Epoch: 0049 | loss_train: 0.0001 loss_val: 2.8132 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4533
Epoch: 0050 | loss_train: 0.0001 loss_val: 2.8308 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4451
Epoch: 0051 | loss_train: 0.0001 loss_val: 2.8469 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4451
Optimization Finished!
Train cost: 11.0888s
Loading 18th epoch
Test set results: loss= 1.2954 accuracy= 0.6275

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=9, log_path='log/nagphormer/wisconsin/hops', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6219 loss_val: 1.5332 | acc_train: 0.1500 acc_val: 0.3875 | f1_train: 0.1257 f1_val: 0.1348
Epoch: 0002 | loss_train: 1.5503 loss_val: 1.4018 | acc_train: 0.3417 acc_val: 0.4750 | f1_train: 0.2039 f1_val: 0.1450
Epoch: 0003 | loss_train: 1.4285 loss_val: 1.2755 | acc_train: 0.4250 acc_val: 0.5250 | f1_train: 0.1390 f1_val: 0.1704
Epoch: 0004 | loss_train: 1.3065 loss_val: 1.1832 | acc_train: 0.4417 acc_val: 0.5750 | f1_train: 0.1491 f1_val: 0.2461
Epoch: 0005 | loss_train: 1.2162 loss_val: 1.1307 | acc_train: 0.4667 acc_val: 0.6250 | f1_train: 0.1813 f1_val: 0.2788
Epoch: 0006 | loss_train: 1.1368 loss_val: 1.1062 | acc_train: 0.5417 acc_val: 0.5500 | f1_train: 0.2421 f1_val: 0.2450
Epoch: 0007 | loss_train: 1.0575 loss_val: 1.1056 | acc_train: 0.5500 acc_val: 0.5375 | f1_train: 0.2486 f1_val: 0.2413
Epoch: 0008 | loss_train: 0.9721 loss_val: 1.1096 | acc_train: 0.6250 acc_val: 0.5750 | f1_train: 0.2884 f1_val: 0.2599
Epoch: 0009 | loss_train: 0.8837 loss_val: 1.0793 | acc_train: 0.6500 acc_val: 0.5875 | f1_train: 0.3373 f1_val: 0.3339
Epoch: 0010 | loss_train: 0.7872 loss_val: 1.0229 | acc_train: 0.7500 acc_val: 0.5625 | f1_train: 0.5575 f1_val: 0.3302
Epoch: 0011 | loss_train: 0.6870 loss_val: 0.9647 | acc_train: 0.8083 acc_val: 0.5875 | f1_train: 0.7183 f1_val: 0.3855
Epoch: 0012 | loss_train: 0.5715 loss_val: 0.9380 | acc_train: 0.8833 acc_val: 0.6375 | f1_train: 0.8559 f1_val: 0.4223
Epoch: 0013 | loss_train: 0.4888 loss_val: 0.9265 | acc_train: 0.9000 acc_val: 0.6750 | f1_train: 0.8755 f1_val: 0.4350
Epoch: 0014 | loss_train: 0.3980 loss_val: 0.9793 | acc_train: 0.9417 acc_val: 0.6750 | f1_train: 0.9136 f1_val: 0.4928
Epoch: 0015 | loss_train: 0.3109 loss_val: 1.0279 | acc_train: 0.9417 acc_val: 0.6875 | f1_train: 0.9157 f1_val: 0.5026
Epoch: 0016 | loss_train: 0.2413 loss_val: 0.9856 | acc_train: 0.9583 acc_val: 0.6875 | f1_train: 0.9274 f1_val: 0.5058
Epoch: 0017 | loss_train: 0.1844 loss_val: 1.0195 | acc_train: 0.9667 acc_val: 0.7125 | f1_train: 0.9411 f1_val: 0.5638
Epoch: 0018 | loss_train: 0.1242 loss_val: 1.1299 | acc_train: 0.9833 acc_val: 0.7000 | f1_train: 0.9580 f1_val: 0.5746
Epoch: 0019 | loss_train: 0.0938 loss_val: 1.0912 | acc_train: 1.0000 acc_val: 0.7375 | f1_train: 1.0000 f1_val: 0.6011
Epoch: 0020 | loss_train: 0.0598 loss_val: 1.1193 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5444
Epoch: 0021 | loss_train: 0.0425 loss_val: 1.2164 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5344
Epoch: 0022 | loss_train: 0.0265 loss_val: 1.3400 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4620
Epoch: 0023 | loss_train: 0.0180 loss_val: 1.4592 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4558
Epoch: 0024 | loss_train: 0.0132 loss_val: 1.5391 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4558
Epoch: 0025 | loss_train: 0.0091 loss_val: 1.5941 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4498
Epoch: 0026 | loss_train: 0.0061 loss_val: 1.6509 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4664
Epoch: 0027 | loss_train: 0.0045 loss_val: 1.7118 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4609
Epoch: 0028 | loss_train: 0.0032 loss_val: 1.7682 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4530
Epoch: 0029 | loss_train: 0.0031 loss_val: 1.8188 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4530
Epoch: 0030 | loss_train: 0.0025 loss_val: 1.8668 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4597
Epoch: 0031 | loss_train: 0.0020 loss_val: 1.9115 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4597
Epoch: 0032 | loss_train: 0.0015 loss_val: 1.9552 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4597
Epoch: 0033 | loss_train: 0.0012 loss_val: 1.9993 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4830
Epoch: 0034 | loss_train: 0.0009 loss_val: 2.0441 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0035 | loss_train: 0.0007 loss_val: 2.0899 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.1370 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.1833 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0038 | loss_train: 0.0004 loss_val: 2.2286 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0039 | loss_train: 0.0003 loss_val: 2.2721 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0040 | loss_train: 0.0002 loss_val: 2.3138 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.3533 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.3909 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.4266 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.4606 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.4924 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.5221 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4734
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.5502 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4697
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.5764 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4697
Epoch: 0049 | loss_train: 0.0001 loss_val: 2.6009 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4615
Optimization Finished!
Train cost: 12.4222s
Loading 19th epoch
Test set results: loss= 1.3665 accuracy= 0.5882

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='hops', log_path='log/nagphormer/wisconsin/hops', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/wisconsin/hops')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9644 loss_val: 1.8685 | acc_train: 0.1500 acc_val: 0.3380 | f1_train: 0.0961 f1_val: 0.1805
Epoch: 0002 | loss_train: 1.9218 loss_val: 1.8201 | acc_train: 0.2429 acc_val: 0.4180 | f1_train: 0.1645 f1_val: 0.2550
Epoch: 0003 | loss_train: 1.8483 loss_val: 1.7494 | acc_train: 0.3143 acc_val: 0.5420 | f1_train: 0.2601 f1_val: 0.4715
Epoch: 0004 | loss_train: 1.7314 loss_val: 1.6593 | acc_train: 0.6143 acc_val: 0.6300 | f1_train: 0.6107 f1_val: 0.5859
Epoch: 0005 | loss_train: 1.5866 loss_val: 1.5494 | acc_train: 0.7643 acc_val: 0.6940 | f1_train: 0.7638 f1_val: 0.6637
Epoch: 0006 | loss_train: 1.4234 loss_val: 1.4177 | acc_train: 0.8429 acc_val: 0.7260 | f1_train: 0.8451 f1_val: 0.7061
Epoch: 0007 | loss_train: 1.2424 loss_val: 1.2689 | acc_train: 0.8929 acc_val: 0.7400 | f1_train: 0.8946 f1_val: 0.7335
Epoch: 0008 | loss_train: 1.0407 loss_val: 1.1175 | acc_train: 0.9214 acc_val: 0.7560 | f1_train: 0.9217 f1_val: 0.7481
Epoch: 0009 | loss_train: 0.8483 loss_val: 0.9828 | acc_train: 0.9500 acc_val: 0.7660 | f1_train: 0.9508 f1_val: 0.7615
Epoch: 0010 | loss_train: 0.6650 loss_val: 0.8742 | acc_train: 0.9429 acc_val: 0.7660 | f1_train: 0.9426 f1_val: 0.7614
Epoch: 0011 | loss_train: 0.5059 loss_val: 0.7956 | acc_train: 0.9714 acc_val: 0.7640 | f1_train: 0.9712 f1_val: 0.7592
Epoch: 0012 | loss_train: 0.3727 loss_val: 0.7420 | acc_train: 0.9786 acc_val: 0.7620 | f1_train: 0.9784 f1_val: 0.7596
Epoch: 0013 | loss_train: 0.2600 loss_val: 0.7074 | acc_train: 0.9857 acc_val: 0.7720 | f1_train: 0.9855 f1_val: 0.7715
Epoch: 0014 | loss_train: 0.1732 loss_val: 0.6932 | acc_train: 0.9929 acc_val: 0.7720 | f1_train: 0.9929 f1_val: 0.7727
Epoch: 0015 | loss_train: 0.1114 loss_val: 0.7023 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7773
Epoch: 0016 | loss_train: 0.0693 loss_val: 0.7350 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7769
Epoch: 0017 | loss_train: 0.0415 loss_val: 0.7858 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0018 | loss_train: 0.0253 loss_val: 0.8479 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7655
Epoch: 0019 | loss_train: 0.0163 loss_val: 0.9169 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0020 | loss_train: 0.0101 loss_val: 0.9886 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7511
Epoch: 0021 | loss_train: 0.0070 loss_val: 1.0602 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0022 | loss_train: 0.0047 loss_val: 1.1301 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7422
Epoch: 0023 | loss_train: 0.0033 loss_val: 1.1974 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7394
Epoch: 0024 | loss_train: 0.0024 loss_val: 1.2620 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7351
Epoch: 0025 | loss_train: 0.0017 loss_val: 1.3238 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7355
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.3828 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7391
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.4388 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7396
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.4920 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7426
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.5427 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7427
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5906 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7439
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6363 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7467
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6800 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7431
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.7215 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7610 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7452
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7988 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7492
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.8345 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7510
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8685 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.9008 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9313 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9601 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9873 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7542
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.0127 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7551
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.0368 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7504
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.0593 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7502
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.0804 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7507
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.0999 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7506
Optimization Finished!
Train cost: 10.2192s
Loading 15th epoch
Test set results: loss= 0.6384 accuracy= 0.7910

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=15, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1448, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2979849
Epoch: 0001 | loss_train: 1.9645 loss_val: 1.9399 | acc_train: 0.0857 acc_val: 0.1520 | f1_train: 0.0627 f1_val: 0.1272
Epoch: 0002 | loss_train: 1.9317 loss_val: 1.8907 | acc_train: 0.1500 acc_val: 0.2940 | f1_train: 0.1218 f1_val: 0.2941
Epoch: 0003 | loss_train: 1.8462 loss_val: 1.8203 | acc_train: 0.4500 acc_val: 0.4520 | f1_train: 0.4214 f1_val: 0.4514
Epoch: 0004 | loss_train: 1.7404 loss_val: 1.7303 | acc_train: 0.6643 acc_val: 0.5580 | f1_train: 0.6514 f1_val: 0.5536
Epoch: 0005 | loss_train: 1.5980 loss_val: 1.6190 | acc_train: 0.8143 acc_val: 0.6440 | f1_train: 0.8048 f1_val: 0.6366
Epoch: 0006 | loss_train: 1.4386 loss_val: 1.4871 | acc_train: 0.8643 acc_val: 0.7040 | f1_train: 0.8609 f1_val: 0.6934
Epoch: 0007 | loss_train: 1.2569 loss_val: 1.3380 | acc_train: 0.8786 acc_val: 0.7360 | f1_train: 0.8767 f1_val: 0.7269
Epoch: 0008 | loss_train: 1.0604 loss_val: 1.1813 | acc_train: 0.9143 acc_val: 0.7520 | f1_train: 0.9136 f1_val: 0.7466
Epoch: 0009 | loss_train: 0.8605 loss_val: 1.0334 | acc_train: 0.9429 acc_val: 0.7660 | f1_train: 0.9424 f1_val: 0.7595
Epoch: 0010 | loss_train: 0.6804 loss_val: 0.9084 | acc_train: 0.9429 acc_val: 0.7740 | f1_train: 0.9424 f1_val: 0.7676
Epoch: 0011 | loss_train: 0.5166 loss_val: 0.8132 | acc_train: 0.9500 acc_val: 0.7780 | f1_train: 0.9492 f1_val: 0.7727
Epoch: 0012 | loss_train: 0.3770 loss_val: 0.7484 | acc_train: 0.9643 acc_val: 0.7660 | f1_train: 0.9639 f1_val: 0.7612
Epoch: 0013 | loss_train: 0.2664 loss_val: 0.7100 | acc_train: 0.9714 acc_val: 0.7740 | f1_train: 0.9709 f1_val: 0.7704
Epoch: 0014 | loss_train: 0.1829 loss_val: 0.6962 | acc_train: 0.9714 acc_val: 0.7780 | f1_train: 0.9709 f1_val: 0.7754
Epoch: 0015 | loss_train: 0.1207 loss_val: 0.7085 | acc_train: 0.9929 acc_val: 0.7760 | f1_train: 0.9929 f1_val: 0.7757
Epoch: 0016 | loss_train: 0.0740 loss_val: 0.7403 | acc_train: 0.9929 acc_val: 0.7680 | f1_train: 0.9929 f1_val: 0.7623
Epoch: 0017 | loss_train: 0.0493 loss_val: 0.7829 | acc_train: 0.9929 acc_val: 0.7660 | f1_train: 0.9929 f1_val: 0.7607
Epoch: 0018 | loss_train: 0.0311 loss_val: 0.8333 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7670
Epoch: 0019 | loss_train: 0.0186 loss_val: 0.8904 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7660
Epoch: 0020 | loss_train: 0.0116 loss_val: 0.9529 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7597
Epoch: 0021 | loss_train: 0.0075 loss_val: 1.0187 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7608
Epoch: 0022 | loss_train: 0.0049 loss_val: 1.0856 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7627
Epoch: 0023 | loss_train: 0.0036 loss_val: 1.1520 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7535
Epoch: 0024 | loss_train: 0.0023 loss_val: 1.2162 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0025 | loss_train: 0.0016 loss_val: 1.2779 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7527
Epoch: 0026 | loss_train: 0.0012 loss_val: 1.3366 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7553
Epoch: 0027 | loss_train: 0.0009 loss_val: 1.3922 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7567
Epoch: 0028 | loss_train: 0.0007 loss_val: 1.4451 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7548
Epoch: 0029 | loss_train: 0.0005 loss_val: 1.4955 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7509
Epoch: 0030 | loss_train: 0.0004 loss_val: 1.5434 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7477
Epoch: 0031 | loss_train: 0.0003 loss_val: 1.5891 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7477
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6324 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7508
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.6737 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7490
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7127 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7514
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7496 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7514
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.7845 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7514
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8171 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7514
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.8480 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7504
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.8770 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7471
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9041 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7471
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9295 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7471
Epoch: 0042 | loss_train: 0.0001 loss_val: 1.9534 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7471
Epoch: 0043 | loss_train: 0.0001 loss_val: 1.9757 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7471
Epoch: 0044 | loss_train: 0.0000 loss_val: 1.9965 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7471
Optimization Finished!
Train cost: 10.8504s
Loading 14th epoch
Test set results: loss= 0.6400 accuracy= 0.7880

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=30, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1463, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2987529
Epoch: 0001 | loss_train: 1.9635 loss_val: 1.9444 | acc_train: 0.0786 acc_val: 0.1460 | f1_train: 0.0558 f1_val: 0.1149
Epoch: 0002 | loss_train: 1.9284 loss_val: 1.8978 | acc_train: 0.1714 acc_val: 0.2720 | f1_train: 0.1549 f1_val: 0.2344
Epoch: 0003 | loss_train: 1.8545 loss_val: 1.8312 | acc_train: 0.3857 acc_val: 0.4700 | f1_train: 0.3439 f1_val: 0.4455
Epoch: 0004 | loss_train: 1.7499 loss_val: 1.7474 | acc_train: 0.6857 acc_val: 0.6160 | f1_train: 0.6774 f1_val: 0.5945
Epoch: 0005 | loss_train: 1.6221 loss_val: 1.6444 | acc_train: 0.8643 acc_val: 0.6920 | f1_train: 0.8625 f1_val: 0.6751
Epoch: 0006 | loss_train: 1.4718 loss_val: 1.5188 | acc_train: 0.9000 acc_val: 0.7240 | f1_train: 0.9000 f1_val: 0.7114
Epoch: 0007 | loss_train: 1.2947 loss_val: 1.3718 | acc_train: 0.9357 acc_val: 0.7520 | f1_train: 0.9356 f1_val: 0.7413
Epoch: 0008 | loss_train: 1.1026 loss_val: 1.2168 | acc_train: 0.9500 acc_val: 0.7660 | f1_train: 0.9499 f1_val: 0.7575
Epoch: 0009 | loss_train: 0.9051 loss_val: 1.0684 | acc_train: 0.9357 acc_val: 0.7720 | f1_train: 0.9360 f1_val: 0.7638
Epoch: 0010 | loss_train: 0.7158 loss_val: 0.9378 | acc_train: 0.9500 acc_val: 0.7740 | f1_train: 0.9503 f1_val: 0.7689
Epoch: 0011 | loss_train: 0.5435 loss_val: 0.8297 | acc_train: 0.9714 acc_val: 0.7760 | f1_train: 0.9710 f1_val: 0.7727
Epoch: 0012 | loss_train: 0.4021 loss_val: 0.7488 | acc_train: 0.9857 acc_val: 0.7880 | f1_train: 0.9857 f1_val: 0.7861
Epoch: 0013 | loss_train: 0.2826 loss_val: 0.6987 | acc_train: 0.9786 acc_val: 0.7920 | f1_train: 0.9784 f1_val: 0.7883
Epoch: 0014 | loss_train: 0.1951 loss_val: 0.6823 | acc_train: 0.9929 acc_val: 0.7880 | f1_train: 0.9929 f1_val: 0.7843
Epoch: 0015 | loss_train: 0.1262 loss_val: 0.6928 | acc_train: 0.9929 acc_val: 0.7820 | f1_train: 0.9929 f1_val: 0.7756
Epoch: 0016 | loss_train: 0.0792 loss_val: 0.7235 | acc_train: 1.0000 acc_val: 0.7860 | f1_train: 1.0000 f1_val: 0.7810
Epoch: 0017 | loss_train: 0.0498 loss_val: 0.7640 | acc_train: 1.0000 acc_val: 0.7860 | f1_train: 1.0000 f1_val: 0.7807
Epoch: 0018 | loss_train: 0.0310 loss_val: 0.8104 | acc_train: 1.0000 acc_val: 0.7780 | f1_train: 1.0000 f1_val: 0.7669
Epoch: 0019 | loss_train: 0.0192 loss_val: 0.8593 | acc_train: 1.0000 acc_val: 0.7780 | f1_train: 1.0000 f1_val: 0.7680
Epoch: 0020 | loss_train: 0.0125 loss_val: 0.9097 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7651
Epoch: 0021 | loss_train: 0.0082 loss_val: 0.9598 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7629
Epoch: 0022 | loss_train: 0.0055 loss_val: 1.0097 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7571
Epoch: 0023 | loss_train: 0.0040 loss_val: 1.0597 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7585
Epoch: 0024 | loss_train: 0.0027 loss_val: 1.1093 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7602
Epoch: 0025 | loss_train: 0.0019 loss_val: 1.1588 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7577
Epoch: 0026 | loss_train: 0.0014 loss_val: 1.2074 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7560
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.2556 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7561
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.3035 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.3505 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7506
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.3967 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7490
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.4417 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7523
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.4857 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7520
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.5281 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7498
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.5690 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7498
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.6083 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7498
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.6460 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7481
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.6820 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7463
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.7163 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7457
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.7490 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7385
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.7800 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7413
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.8094 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7413
Epoch: 0042 | loss_train: 0.0001 loss_val: 1.8372 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7379
Epoch: 0043 | loss_train: 0.0000 loss_val: 1.8634 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7379
Epoch: 0044 | loss_train: 0.0000 loss_val: 1.8880 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7410
Optimization Finished!
Train cost: 10.0371s
Loading 13th epoch
Test set results: loss= 0.6560 accuracy= 0.7960

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=60, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1493, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 3002889
Epoch: 0001 | loss_train: 1.9636 loss_val: 1.9060 | acc_train: 0.1357 acc_val: 0.2360 | f1_train: 0.1181 f1_val: 0.1923
Epoch: 0002 | loss_train: 1.9185 loss_val: 1.8497 | acc_train: 0.2000 acc_val: 0.3880 | f1_train: 0.1890 f1_val: 0.3308
Epoch: 0003 | loss_train: 1.8350 loss_val: 1.7691 | acc_train: 0.4071 acc_val: 0.5680 | f1_train: 0.3805 f1_val: 0.5325
Epoch: 0004 | loss_train: 1.7100 loss_val: 1.6694 | acc_train: 0.6929 acc_val: 0.6980 | f1_train: 0.6731 f1_val: 0.6820
Epoch: 0005 | loss_train: 1.5570 loss_val: 1.5524 | acc_train: 0.8429 acc_val: 0.7440 | f1_train: 0.8428 f1_val: 0.7308
Epoch: 0006 | loss_train: 1.3921 loss_val: 1.4150 | acc_train: 0.8857 acc_val: 0.7500 | f1_train: 0.8847 f1_val: 0.7415
Epoch: 0007 | loss_train: 1.2078 loss_val: 1.2599 | acc_train: 0.9071 acc_val: 0.7540 | f1_train: 0.9070 f1_val: 0.7480
Epoch: 0008 | loss_train: 1.0049 loss_val: 1.1012 | acc_train: 0.8857 acc_val: 0.7620 | f1_train: 0.8858 f1_val: 0.7613
Epoch: 0009 | loss_train: 0.8136 loss_val: 0.9584 | acc_train: 0.9143 acc_val: 0.7700 | f1_train: 0.9153 f1_val: 0.7629
Epoch: 0010 | loss_train: 0.6325 loss_val: 0.8414 | acc_train: 0.9357 acc_val: 0.7740 | f1_train: 0.9350 f1_val: 0.7644
Epoch: 0011 | loss_train: 0.4883 loss_val: 0.7536 | acc_train: 0.9357 acc_val: 0.7860 | f1_train: 0.9346 f1_val: 0.7774
Epoch: 0012 | loss_train: 0.3568 loss_val: 0.6948 | acc_train: 0.9714 acc_val: 0.7860 | f1_train: 0.9714 f1_val: 0.7773
Epoch: 0013 | loss_train: 0.2523 loss_val: 0.6637 | acc_train: 0.9857 acc_val: 0.7900 | f1_train: 0.9857 f1_val: 0.7813
Epoch: 0014 | loss_train: 0.1690 loss_val: 0.6606 | acc_train: 1.0000 acc_val: 0.7780 | f1_train: 1.0000 f1_val: 0.7724
Epoch: 0015 | loss_train: 0.1091 loss_val: 0.6830 | acc_train: 1.0000 acc_val: 0.7800 | f1_train: 1.0000 f1_val: 0.7768
Epoch: 0016 | loss_train: 0.0685 loss_val: 0.7266 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7715
Epoch: 0017 | loss_train: 0.0428 loss_val: 0.7858 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7582
Epoch: 0018 | loss_train: 0.0273 loss_val: 0.8558 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7509
Epoch: 0019 | loss_train: 0.0172 loss_val: 0.9320 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7473
Epoch: 0020 | loss_train: 0.0110 loss_val: 1.0110 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7425
Epoch: 0021 | loss_train: 0.0070 loss_val: 1.0903 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7428
Epoch: 0022 | loss_train: 0.0048 loss_val: 1.1683 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7364
Epoch: 0023 | loss_train: 0.0034 loss_val: 1.2436 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7346
Epoch: 0024 | loss_train: 0.0024 loss_val: 1.3151 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0025 | loss_train: 0.0018 loss_val: 1.3830 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.4471 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7294
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.5073 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7294
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.5637 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7294
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.6168 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7297
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.6666 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7321
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.7133 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7294
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.7572 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7277
Epoch: 0033 | loss_train: 0.0003 loss_val: 1.7985 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7256
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.8373 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7298
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.8737 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7256
Epoch: 0036 | loss_train: 0.0002 loss_val: 1.9079 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7273
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.9397 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7275
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.9697 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7254
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9977 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7254
Epoch: 0040 | loss_train: 0.0001 loss_val: 2.0239 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7254
Epoch: 0041 | loss_train: 0.0001 loss_val: 2.0484 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7237
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.0712 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7237
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.0924 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7264
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.1122 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7264
Optimization Finished!
Train cost: 9.6759s
Loading 13th epoch
Test set results: loss= 0.6454 accuracy= 0.7990

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=120, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1553, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 3033609
Epoch: 0001 | loss_train: 1.9539 loss_val: 1.8915 | acc_train: 0.1786 acc_val: 0.2660 | f1_train: 0.1072 f1_val: 0.1659
Epoch: 0002 | loss_train: 1.9115 loss_val: 1.8372 | acc_train: 0.2357 acc_val: 0.3580 | f1_train: 0.1413 f1_val: 0.2907
Epoch: 0003 | loss_train: 1.8165 loss_val: 1.7588 | acc_train: 0.3429 acc_val: 0.4980 | f1_train: 0.2991 f1_val: 0.4694
Epoch: 0004 | loss_train: 1.6922 loss_val: 1.6605 | acc_train: 0.6143 acc_val: 0.6240 | f1_train: 0.6113 f1_val: 0.6215
Epoch: 0005 | loss_train: 1.5388 loss_val: 1.5432 | acc_train: 0.8143 acc_val: 0.6780 | f1_train: 0.8088 f1_val: 0.6754
Epoch: 0006 | loss_train: 1.3700 loss_val: 1.4068 | acc_train: 0.8857 acc_val: 0.7160 | f1_train: 0.8838 f1_val: 0.7115
Epoch: 0007 | loss_train: 1.1742 loss_val: 1.2540 | acc_train: 0.9143 acc_val: 0.7420 | f1_train: 0.9126 f1_val: 0.7365
Epoch: 0008 | loss_train: 0.9764 loss_val: 1.0998 | acc_train: 0.9286 acc_val: 0.7620 | f1_train: 0.9280 f1_val: 0.7531
Epoch: 0009 | loss_train: 0.7924 loss_val: 0.9592 | acc_train: 0.9214 acc_val: 0.7720 | f1_train: 0.9204 f1_val: 0.7577
Epoch: 0010 | loss_train: 0.6153 loss_val: 0.8417 | acc_train: 0.9571 acc_val: 0.7760 | f1_train: 0.9563 f1_val: 0.7658
Epoch: 0011 | loss_train: 0.4620 loss_val: 0.7533 | acc_train: 0.9714 acc_val: 0.7900 | f1_train: 0.9712 f1_val: 0.7816
Epoch: 0012 | loss_train: 0.3292 loss_val: 0.6980 | acc_train: 0.9786 acc_val: 0.7820 | f1_train: 0.9784 f1_val: 0.7708
Epoch: 0013 | loss_train: 0.2270 loss_val: 0.6740 | acc_train: 0.9786 acc_val: 0.7880 | f1_train: 0.9784 f1_val: 0.7776
Epoch: 0014 | loss_train: 0.1490 loss_val: 0.6796 | acc_train: 0.9857 acc_val: 0.7800 | f1_train: 0.9857 f1_val: 0.7697
Epoch: 0015 | loss_train: 0.0940 loss_val: 0.7068 | acc_train: 1.0000 acc_val: 0.7840 | f1_train: 1.0000 f1_val: 0.7770
Epoch: 0016 | loss_train: 0.0576 loss_val: 0.7451 | acc_train: 1.0000 acc_val: 0.7820 | f1_train: 1.0000 f1_val: 0.7759
Epoch: 0017 | loss_train: 0.0355 loss_val: 0.7897 | acc_train: 1.0000 acc_val: 0.7800 | f1_train: 1.0000 f1_val: 0.7730
Epoch: 0018 | loss_train: 0.0231 loss_val: 0.8394 | acc_train: 1.0000 acc_val: 0.7800 | f1_train: 1.0000 f1_val: 0.7717
Epoch: 0019 | loss_train: 0.0160 loss_val: 0.8940 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7649
Epoch: 0020 | loss_train: 0.0103 loss_val: 0.9507 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7607
Epoch: 0021 | loss_train: 0.0070 loss_val: 1.0105 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7581
Epoch: 0022 | loss_train: 0.0047 loss_val: 1.0731 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7570
Epoch: 0023 | loss_train: 0.0032 loss_val: 1.1374 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7514
Epoch: 0024 | loss_train: 0.0023 loss_val: 1.2023 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7479
Epoch: 0025 | loss_train: 0.0017 loss_val: 1.2668 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7441
Epoch: 0026 | loss_train: 0.0012 loss_val: 1.3299 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7435
Epoch: 0027 | loss_train: 0.0009 loss_val: 1.3912 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7414
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.4503 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7414
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.5070 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7384
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5609 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7380
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6122 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7363
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6607 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0033 | loss_train: 0.0003 loss_val: 1.7062 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7313
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7490 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7313
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7891 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7299
Epoch: 0036 | loss_train: 0.0002 loss_val: 1.8264 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7286
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8613 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7235
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.8937 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7258
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9239 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7251
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9519 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7251
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9778 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7251
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.0017 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7251
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.0239 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7251
Optimization Finished!
Train cost: 10.5961s
Loading 11th epoch
Test set results: loss= 0.7396 accuracy= 0.7780

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=240, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1673, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 3095049
Epoch: 0001 | loss_train: 1.9502 loss_val: 1.9570 | acc_train: 0.1929 acc_val: 0.1740 | f1_train: 0.1667 f1_val: 0.1604
Epoch: 0002 | loss_train: 1.9121 loss_val: 1.9027 | acc_train: 0.2357 acc_val: 0.2760 | f1_train: 0.1938 f1_val: 0.2698
Epoch: 0003 | loss_train: 1.8253 loss_val: 1.8244 | acc_train: 0.4143 acc_val: 0.4240 | f1_train: 0.3892 f1_val: 0.4253
Epoch: 0004 | loss_train: 1.7060 loss_val: 1.7246 | acc_train: 0.7286 acc_val: 0.5620 | f1_train: 0.7114 f1_val: 0.5645
Epoch: 0005 | loss_train: 1.5522 loss_val: 1.6058 | acc_train: 0.8214 acc_val: 0.6160 | f1_train: 0.8116 f1_val: 0.6278
Epoch: 0006 | loss_train: 1.3797 loss_val: 1.4683 | acc_train: 0.8643 acc_val: 0.6660 | f1_train: 0.8602 f1_val: 0.6803
Epoch: 0007 | loss_train: 1.1787 loss_val: 1.3146 | acc_train: 0.9143 acc_val: 0.7100 | f1_train: 0.9128 f1_val: 0.7279
Epoch: 0008 | loss_train: 0.9744 loss_val: 1.1574 | acc_train: 0.9357 acc_val: 0.7300 | f1_train: 0.9339 f1_val: 0.7438
Epoch: 0009 | loss_train: 0.7748 loss_val: 1.0140 | acc_train: 0.9286 acc_val: 0.7440 | f1_train: 0.9272 f1_val: 0.7517
Epoch: 0010 | loss_train: 0.6009 loss_val: 0.8930 | acc_train: 0.9357 acc_val: 0.7500 | f1_train: 0.9339 f1_val: 0.7535
Epoch: 0011 | loss_train: 0.4463 loss_val: 0.8001 | acc_train: 0.9643 acc_val: 0.7600 | f1_train: 0.9641 f1_val: 0.7634
Epoch: 0012 | loss_train: 0.3264 loss_val: 0.7386 | acc_train: 0.9714 acc_val: 0.7740 | f1_train: 0.9714 f1_val: 0.7792
Epoch: 0013 | loss_train: 0.2241 loss_val: 0.7084 | acc_train: 0.9857 acc_val: 0.7840 | f1_train: 0.9857 f1_val: 0.7870
Epoch: 0014 | loss_train: 0.1461 loss_val: 0.7057 | acc_train: 0.9929 acc_val: 0.7820 | f1_train: 0.9929 f1_val: 0.7864
Epoch: 0015 | loss_train: 0.0946 loss_val: 0.7279 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7835
Epoch: 0016 | loss_train: 0.0586 loss_val: 0.7687 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7704
Epoch: 0017 | loss_train: 0.0366 loss_val: 0.8230 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7680
Epoch: 0018 | loss_train: 0.0238 loss_val: 0.8867 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7647
Epoch: 0019 | loss_train: 0.0145 loss_val: 0.9546 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7594
Epoch: 0020 | loss_train: 0.0099 loss_val: 1.0246 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7580
Epoch: 0021 | loss_train: 0.0067 loss_val: 1.0945 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7573
Epoch: 0022 | loss_train: 0.0046 loss_val: 1.1622 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7495
Epoch: 0023 | loss_train: 0.0033 loss_val: 1.2268 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7539
Epoch: 0024 | loss_train: 0.0024 loss_val: 1.2880 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7516
Epoch: 0025 | loss_train: 0.0018 loss_val: 1.3459 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7516
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.4006 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7542
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.4523 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7523
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.5008 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7518
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.5465 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7516
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5896 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7487
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6302 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7512
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6685 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7512
Epoch: 0033 | loss_train: 0.0003 loss_val: 1.7045 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7479
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7382 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7500
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7702 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7500
Epoch: 0036 | loss_train: 0.0002 loss_val: 1.8003 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7513
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8286 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7555
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.8552 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7530
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.8802 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7506
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9036 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9256 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7546
Epoch: 0042 | loss_train: 0.0001 loss_val: 1.9464 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7571
Epoch: 0043 | loss_train: 0.0001 loss_val: 1.9658 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7540
Epoch: 0044 | loss_train: 0.0001 loss_val: 1.9840 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7540
Optimization Finished!
Train cost: 10.8093s
Loading 13th epoch
Test set results: loss= 0.6868 accuracy= 0.7800

>>> run.py: Namespace(dataset='cora', device=1, experiment='pe_dim', log_path='log/nagphormer/cora/pe_dim', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/cora/pe_dim')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6315 loss_val: 1.5466 | acc_train: 0.1500 acc_val: 0.3000 | f1_train: 0.1211 f1_val: 0.1308
Epoch: 0002 | loss_train: 1.5652 loss_val: 1.4203 | acc_train: 0.2833 acc_val: 0.5000 | f1_train: 0.1653 f1_val: 0.1644
Epoch: 0003 | loss_train: 1.4443 loss_val: 1.2855 | acc_train: 0.4333 acc_val: 0.5125 | f1_train: 0.1550 f1_val: 0.1526
Epoch: 0004 | loss_train: 1.3147 loss_val: 1.1833 | acc_train: 0.4333 acc_val: 0.5750 | f1_train: 0.1390 f1_val: 0.2358
Epoch: 0005 | loss_train: 1.2048 loss_val: 1.1209 | acc_train: 0.4583 acc_val: 0.6125 | f1_train: 0.1674 f1_val: 0.2715
Epoch: 0006 | loss_train: 1.1178 loss_val: 1.0844 | acc_train: 0.5333 acc_val: 0.5625 | f1_train: 0.2372 f1_val: 0.2529
Epoch: 0007 | loss_train: 1.0281 loss_val: 1.0654 | acc_train: 0.5750 acc_val: 0.5500 | f1_train: 0.2665 f1_val: 0.2474
Epoch: 0008 | loss_train: 0.9321 loss_val: 1.0565 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3003 f1_val: 0.2557
Epoch: 0009 | loss_train: 0.8323 loss_val: 1.0307 | acc_train: 0.6667 acc_val: 0.6125 | f1_train: 0.3606 f1_val: 0.3566
Epoch: 0010 | loss_train: 0.7215 loss_val: 0.9827 | acc_train: 0.7583 acc_val: 0.6000 | f1_train: 0.5366 f1_val: 0.3628
Epoch: 0011 | loss_train: 0.6194 loss_val: 0.9135 | acc_train: 0.8167 acc_val: 0.6500 | f1_train: 0.7483 f1_val: 0.4362
Epoch: 0012 | loss_train: 0.5123 loss_val: 0.8581 | acc_train: 0.8833 acc_val: 0.6500 | f1_train: 0.8614 f1_val: 0.4708
Epoch: 0013 | loss_train: 0.4123 loss_val: 0.8767 | acc_train: 0.9167 acc_val: 0.7000 | f1_train: 0.8844 f1_val: 0.5609
Epoch: 0014 | loss_train: 0.3231 loss_val: 0.9327 | acc_train: 0.9500 acc_val: 0.7000 | f1_train: 0.9142 f1_val: 0.5516
Epoch: 0015 | loss_train: 0.2508 loss_val: 0.9685 | acc_train: 0.9500 acc_val: 0.6875 | f1_train: 0.9209 f1_val: 0.4886
Epoch: 0016 | loss_train: 0.1914 loss_val: 1.0292 | acc_train: 0.9583 acc_val: 0.7000 | f1_train: 0.9259 f1_val: 0.4882
Epoch: 0017 | loss_train: 0.1423 loss_val: 1.1019 | acc_train: 0.9667 acc_val: 0.7000 | f1_train: 0.9297 f1_val: 0.4825
Epoch: 0018 | loss_train: 0.1004 loss_val: 1.1785 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9694 f1_val: 0.4658
Epoch: 0019 | loss_train: 0.0704 loss_val: 1.2618 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4528
Epoch: 0020 | loss_train: 0.0536 loss_val: 1.3591 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0021 | loss_train: 0.0349 loss_val: 1.4517 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0022 | loss_train: 0.0250 loss_val: 1.5643 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0023 | loss_train: 0.0185 loss_val: 1.6873 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0024 | loss_train: 0.0115 loss_val: 1.7941 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0025 | loss_train: 0.0134 loss_val: 1.8016 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0026 | loss_train: 0.0071 loss_val: 1.8700 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0027 | loss_train: 0.0042 loss_val: 1.9669 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4397
Epoch: 0028 | loss_train: 0.0032 loss_val: 2.0669 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4652
Epoch: 0029 | loss_train: 0.0030 loss_val: 2.1614 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0030 | loss_train: 0.0027 loss_val: 2.2514 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4694
Epoch: 0031 | loss_train: 0.0019 loss_val: 2.3389 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0032 | loss_train: 0.0016 loss_val: 2.4217 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0033 | loss_train: 0.0012 loss_val: 2.4973 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0034 | loss_train: 0.0011 loss_val: 2.5645 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0035 | loss_train: 0.0008 loss_val: 2.6212 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.6685 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.7098 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.7466 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0039 | loss_train: 0.0004 loss_val: 2.7690 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.7881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.8042 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.8187 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.8326 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.8465 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.8602 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.8742 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.8881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4756
Optimization Finished!
Train cost: 10.8493s
Loading 13th epoch
Test set results: loss= 1.1536 accuracy= 0.5686

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=15, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1718, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3117575
Epoch: 0001 | loss_train: 1.6175 loss_val: 1.5410 | acc_train: 0.2917 acc_val: 0.3750 | f1_train: 0.1442 f1_val: 0.1309
Epoch: 0002 | loss_train: 1.5567 loss_val: 1.4315 | acc_train: 0.3500 acc_val: 0.5500 | f1_train: 0.1282 f1_val: 0.2107
Epoch: 0003 | loss_train: 1.4393 loss_val: 1.3032 | acc_train: 0.5083 acc_val: 0.5875 | f1_train: 0.2198 f1_val: 0.2539
Epoch: 0004 | loss_train: 1.3081 loss_val: 1.1871 | acc_train: 0.5333 acc_val: 0.6125 | f1_train: 0.2372 f1_val: 0.2720
Epoch: 0005 | loss_train: 1.1908 loss_val: 1.1100 | acc_train: 0.5500 acc_val: 0.6125 | f1_train: 0.2491 f1_val: 0.2745
Epoch: 0006 | loss_train: 1.0982 loss_val: 1.0662 | acc_train: 0.5500 acc_val: 0.6000 | f1_train: 0.2471 f1_val: 0.2724
Epoch: 0007 | loss_train: 1.0107 loss_val: 1.0425 | acc_train: 0.5917 acc_val: 0.5375 | f1_train: 0.2766 f1_val: 0.2435
Epoch: 0008 | loss_train: 0.9088 loss_val: 1.0415 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3390 f1_val: 0.2554
Epoch: 0009 | loss_train: 0.8021 loss_val: 1.0438 | acc_train: 0.6750 acc_val: 0.6250 | f1_train: 0.3905 f1_val: 0.3600
Epoch: 0010 | loss_train: 0.6970 loss_val: 1.0104 | acc_train: 0.7750 acc_val: 0.6250 | f1_train: 0.5743 f1_val: 0.3681
Epoch: 0011 | loss_train: 0.5985 loss_val: 0.9374 | acc_train: 0.8083 acc_val: 0.6250 | f1_train: 0.7323 f1_val: 0.3517
Epoch: 0012 | loss_train: 0.4884 loss_val: 0.9076 | acc_train: 0.8750 acc_val: 0.6750 | f1_train: 0.8044 f1_val: 0.4903
Epoch: 0013 | loss_train: 0.3969 loss_val: 0.9850 | acc_train: 0.9083 acc_val: 0.6500 | f1_train: 0.8719 f1_val: 0.4612
Epoch: 0014 | loss_train: 0.3232 loss_val: 1.0009 | acc_train: 0.9250 acc_val: 0.6750 | f1_train: 0.9006 f1_val: 0.5051
Epoch: 0015 | loss_train: 0.2409 loss_val: 1.0548 | acc_train: 0.9333 acc_val: 0.6750 | f1_train: 0.9004 f1_val: 0.5332
Epoch: 0016 | loss_train: 0.1874 loss_val: 1.1803 | acc_train: 0.9583 acc_val: 0.6625 | f1_train: 0.9286 f1_val: 0.5377
Epoch: 0017 | loss_train: 0.1474 loss_val: 1.2663 | acc_train: 0.9667 acc_val: 0.6625 | f1_train: 0.9337 f1_val: 0.4652
Epoch: 0018 | loss_train: 0.1140 loss_val: 1.3337 | acc_train: 0.9750 acc_val: 0.6750 | f1_train: 0.9383 f1_val: 0.4728
Epoch: 0019 | loss_train: 0.0740 loss_val: 1.4667 | acc_train: 0.9917 acc_val: 0.6375 | f1_train: 0.9694 f1_val: 0.4483
Epoch: 0020 | loss_train: 0.0492 loss_val: 1.6736 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4830
Epoch: 0021 | loss_train: 0.0351 loss_val: 1.8190 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.5074
Epoch: 0022 | loss_train: 0.0267 loss_val: 1.8245 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5379
Epoch: 0023 | loss_train: 0.0158 loss_val: 1.8240 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4715
Epoch: 0024 | loss_train: 0.0108 loss_val: 1.8844 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4561
Epoch: 0025 | loss_train: 0.0085 loss_val: 1.9888 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4416
Epoch: 0026 | loss_train: 0.0055 loss_val: 2.1160 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4365
Epoch: 0027 | loss_train: 0.0041 loss_val: 2.2518 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4306
Epoch: 0028 | loss_train: 0.0027 loss_val: 2.3838 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4202
Epoch: 0029 | loss_train: 0.0022 loss_val: 2.5058 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4175
Epoch: 0030 | loss_train: 0.0020 loss_val: 2.5877 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4177
Epoch: 0031 | loss_train: 0.0014 loss_val: 2.6555 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4177
Epoch: 0032 | loss_train: 0.0019 loss_val: 2.8297 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4108
Epoch: 0033 | loss_train: 0.0012 loss_val: 2.9868 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4131
Epoch: 0034 | loss_train: 0.0014 loss_val: 3.1241 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4212
Epoch: 0035 | loss_train: 0.0016 loss_val: 3.2314 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4212
Epoch: 0036 | loss_train: 0.0012 loss_val: 3.3148 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4212
Epoch: 0037 | loss_train: 0.0009 loss_val: 3.3827 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4212
Epoch: 0038 | loss_train: 0.0007 loss_val: 3.4371 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4287
Epoch: 0039 | loss_train: 0.0004 loss_val: 3.4838 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4364
Epoch: 0040 | loss_train: 0.0005 loss_val: 3.5158 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4364
Epoch: 0041 | loss_train: 0.0003 loss_val: 3.5448 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4364
Epoch: 0042 | loss_train: 0.0006 loss_val: 3.5698 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4373
Epoch: 0043 | loss_train: 0.0005 loss_val: 3.5951 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4353
Epoch: 0044 | loss_train: 0.0004 loss_val: 3.6201 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4353
Epoch: 0045 | loss_train: 0.0002 loss_val: 3.6436 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4353
Epoch: 0046 | loss_train: 0.0008 loss_val: 3.2447 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4439
Epoch: 0047 | loss_train: 0.0004 loss_val: 2.8466 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4386
Epoch: 0048 | loss_train: 0.0008 loss_val: 2.7633 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4492
Optimization Finished!
Train cost: 10.2468s
Loading 12th epoch
Test set results: loss= 1.1111 accuracy= 0.5686

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=30, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1733, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3125255
Epoch: 0001 | loss_train: 1.6405 loss_val: 1.5908 | acc_train: 0.1000 acc_val: 0.1875 | f1_train: 0.0579 f1_val: 0.1157
Epoch: 0002 | loss_train: 1.5643 loss_val: 1.4546 | acc_train: 0.1667 acc_val: 0.4375 | f1_train: 0.1257 f1_val: 0.1856
Epoch: 0003 | loss_train: 1.4421 loss_val: 1.3168 | acc_train: 0.4500 acc_val: 0.5750 | f1_train: 0.2349 f1_val: 0.2544
Epoch: 0004 | loss_train: 1.3044 loss_val: 1.2125 | acc_train: 0.5083 acc_val: 0.5750 | f1_train: 0.2257 f1_val: 0.2543
Epoch: 0005 | loss_train: 1.1879 loss_val: 1.1473 | acc_train: 0.5667 acc_val: 0.5750 | f1_train: 0.2607 f1_val: 0.2583
Epoch: 0006 | loss_train: 1.0984 loss_val: 1.1127 | acc_train: 0.5833 acc_val: 0.5500 | f1_train: 0.2712 f1_val: 0.2501
Epoch: 0007 | loss_train: 1.0177 loss_val: 1.0940 | acc_train: 0.6000 acc_val: 0.5500 | f1_train: 0.2776 f1_val: 0.2467
Epoch: 0008 | loss_train: 0.9199 loss_val: 1.0766 | acc_train: 0.6417 acc_val: 0.5500 | f1_train: 0.2965 f1_val: 0.2497
Epoch: 0009 | loss_train: 0.8152 loss_val: 1.0488 | acc_train: 0.6583 acc_val: 0.6000 | f1_train: 0.3413 f1_val: 0.3541
Epoch: 0010 | loss_train: 0.6938 loss_val: 1.0127 | acc_train: 0.7333 acc_val: 0.6000 | f1_train: 0.4877 f1_val: 0.3388
Epoch: 0011 | loss_train: 0.5949 loss_val: 0.9574 | acc_train: 0.8333 acc_val: 0.6250 | f1_train: 0.8131 f1_val: 0.3922
Epoch: 0012 | loss_train: 0.4982 loss_val: 0.9199 | acc_train: 0.9000 acc_val: 0.6375 | f1_train: 0.8781 f1_val: 0.4459
Epoch: 0013 | loss_train: 0.4072 loss_val: 0.9899 | acc_train: 0.9000 acc_val: 0.6000 | f1_train: 0.8694 f1_val: 0.4292
Epoch: 0014 | loss_train: 0.3178 loss_val: 1.0453 | acc_train: 0.9250 acc_val: 0.6500 | f1_train: 0.8866 f1_val: 0.5047
Epoch: 0015 | loss_train: 0.2508 loss_val: 1.1083 | acc_train: 0.9500 acc_val: 0.6625 | f1_train: 0.9114 f1_val: 0.5088
Epoch: 0016 | loss_train: 0.1965 loss_val: 1.1865 | acc_train: 0.9583 acc_val: 0.6625 | f1_train: 0.9226 f1_val: 0.5036
Epoch: 0017 | loss_train: 0.1543 loss_val: 1.2326 | acc_train: 0.9667 acc_val: 0.6625 | f1_train: 0.9349 f1_val: 0.5031
Epoch: 0018 | loss_train: 0.1128 loss_val: 1.2863 | acc_train: 0.9833 acc_val: 0.6625 | f1_train: 0.9542 f1_val: 0.4595
Epoch: 0019 | loss_train: 0.0803 loss_val: 1.4205 | acc_train: 0.9917 acc_val: 0.6250 | f1_train: 0.9694 f1_val: 0.4977
Epoch: 0020 | loss_train: 0.0603 loss_val: 1.5837 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4920
Epoch: 0021 | loss_train: 0.0466 loss_val: 1.6514 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.5011
Epoch: 0022 | loss_train: 0.0313 loss_val: 1.6771 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.5106
Epoch: 0023 | loss_train: 0.0229 loss_val: 1.7369 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5235
Epoch: 0024 | loss_train: 0.0164 loss_val: 1.8384 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4463
Epoch: 0025 | loss_train: 0.0093 loss_val: 1.9650 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4482
Epoch: 0026 | loss_train: 0.0063 loss_val: 2.1048 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4482
Epoch: 0027 | loss_train: 0.0055 loss_val: 2.2453 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4482
Epoch: 0028 | loss_train: 0.0042 loss_val: 2.3750 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4381
Epoch: 0029 | loss_train: 0.0036 loss_val: 2.4763 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4381
Epoch: 0030 | loss_train: 0.0025 loss_val: 2.5616 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4417
Epoch: 0031 | loss_train: 0.0020 loss_val: 2.6238 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4417
Epoch: 0032 | loss_train: 0.0015 loss_val: 2.6526 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4482
Epoch: 0033 | loss_train: 0.0011 loss_val: 2.6766 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4482
Epoch: 0034 | loss_train: 0.0010 loss_val: 2.6956 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4482
Epoch: 0035 | loss_train: 0.0009 loss_val: 2.7160 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4482
Epoch: 0036 | loss_train: 0.0007 loss_val: 2.7429 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4482
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.7757 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0038 | loss_train: 0.0005 loss_val: 2.8174 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0039 | loss_train: 0.0005 loss_val: 2.8663 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0040 | loss_train: 0.0004 loss_val: 2.9196 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0041 | loss_train: 0.0003 loss_val: 2.9745 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0042 | loss_train: 0.0002 loss_val: 3.0273 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0043 | loss_train: 0.0002 loss_val: 3.0768 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0044 | loss_train: 0.0002 loss_val: 3.1244 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0045 | loss_train: 0.0001 loss_val: 3.1694 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0046 | loss_train: 0.0001 loss_val: 3.2116 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0047 | loss_train: 0.0001 loss_val: 3.2509 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0048 | loss_train: 0.0001 loss_val: 3.2881 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0049 | loss_train: 0.0001 loss_val: 3.3226 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0050 | loss_train: 0.0001 loss_val: 3.3506 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0051 | loss_train: 0.0001 loss_val: 3.3728 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0052 | loss_train: 0.0001 loss_val: 3.3922 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0053 | loss_train: 0.0001 loss_val: 3.4090 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0054 | loss_train: 0.0001 loss_val: 3.4114 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0055 | loss_train: 0.0001 loss_val: 3.4124 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0056 | loss_train: 0.0000 loss_val: 3.4131 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0057 | loss_train: 0.0000 loss_val: 3.4143 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0058 | loss_train: 0.0001 loss_val: 3.4155 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0059 | loss_train: 0.0000 loss_val: 3.4168 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0060 | loss_train: 0.0000 loss_val: 3.4184 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0061 | loss_train: 0.0000 loss_val: 3.4202 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0062 | loss_train: 0.0000 loss_val: 3.4224 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0063 | loss_train: 0.0000 loss_val: 3.4247 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0064 | loss_train: 0.0000 loss_val: 3.4271 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0065 | loss_train: 0.0000 loss_val: 3.4303 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0066 | loss_train: 0.0000 loss_val: 3.4337 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0067 | loss_train: 0.0000 loss_val: 3.4382 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0068 | loss_train: 0.0000 loss_val: 3.4427 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0069 | loss_train: 0.0000 loss_val: 3.4477 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0070 | loss_train: 0.0000 loss_val: 3.4532 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0071 | loss_train: 0.0000 loss_val: 3.4588 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0072 | loss_train: 0.0000 loss_val: 3.4646 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Epoch: 0073 | loss_train: 0.0000 loss_val: 3.4705 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4547
Optimization Finished!
Train cost: 12.0601s
Loading 38th epoch
Test set results: loss= 3.8381 accuracy= 0.5490

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=60, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1763, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3140615
Epoch: 0001 | loss_train: 1.6296 loss_val: 1.5373 | acc_train: 0.1917 acc_val: 0.3625 | f1_train: 0.1435 f1_val: 0.1905
Epoch: 0002 | loss_train: 1.5634 loss_val: 1.4155 | acc_train: 0.3667 acc_val: 0.5375 | f1_train: 0.1797 f1_val: 0.2739
Epoch: 0003 | loss_train: 1.4446 loss_val: 1.2867 | acc_train: 0.4750 acc_val: 0.6000 | f1_train: 0.2039 f1_val: 0.2682
Epoch: 0004 | loss_train: 1.3126 loss_val: 1.1809 | acc_train: 0.5083 acc_val: 0.5750 | f1_train: 0.2156 f1_val: 0.2592
Epoch: 0005 | loss_train: 1.1897 loss_val: 1.1084 | acc_train: 0.5667 acc_val: 0.5750 | f1_train: 0.2591 f1_val: 0.2610
Epoch: 0006 | loss_train: 1.0883 loss_val: 1.0701 | acc_train: 0.6250 acc_val: 0.5625 | f1_train: 0.2930 f1_val: 0.2544
Epoch: 0007 | loss_train: 1.0066 loss_val: 1.0556 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3009 f1_val: 0.2532
Epoch: 0008 | loss_train: 0.9192 loss_val: 1.0445 | acc_train: 0.6333 acc_val: 0.5375 | f1_train: 0.2923 f1_val: 0.2433
Epoch: 0009 | loss_train: 0.8185 loss_val: 1.0210 | acc_train: 0.6333 acc_val: 0.6000 | f1_train: 0.2923 f1_val: 0.3571
Epoch: 0010 | loss_train: 0.7100 loss_val: 0.9902 | acc_train: 0.7333 acc_val: 0.6375 | f1_train: 0.5170 f1_val: 0.3919
Epoch: 0011 | loss_train: 0.6011 loss_val: 0.9463 | acc_train: 0.8500 acc_val: 0.6375 | f1_train: 0.7991 f1_val: 0.3844
Epoch: 0012 | loss_train: 0.5102 loss_val: 0.9069 | acc_train: 0.9000 acc_val: 0.6875 | f1_train: 0.8730 f1_val: 0.5505
Epoch: 0013 | loss_train: 0.4084 loss_val: 0.9568 | acc_train: 0.9167 acc_val: 0.6750 | f1_train: 0.8920 f1_val: 0.5301
Epoch: 0014 | loss_train: 0.3156 loss_val: 1.0371 | acc_train: 0.9333 acc_val: 0.6875 | f1_train: 0.8991 f1_val: 0.5621
Epoch: 0015 | loss_train: 0.2564 loss_val: 1.0601 | acc_train: 0.9583 acc_val: 0.6875 | f1_train: 0.9214 f1_val: 0.5546
Epoch: 0016 | loss_train: 0.1912 loss_val: 1.1189 | acc_train: 0.9667 acc_val: 0.7000 | f1_train: 0.9297 f1_val: 0.5608
Epoch: 0017 | loss_train: 0.1433 loss_val: 1.2353 | acc_train: 0.9750 acc_val: 0.6750 | f1_train: 0.9383 f1_val: 0.5454
Epoch: 0018 | loss_train: 0.1046 loss_val: 1.3444 | acc_train: 0.9833 acc_val: 0.6375 | f1_train: 0.9542 f1_val: 0.5085
Epoch: 0019 | loss_train: 0.0687 loss_val: 1.4358 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4454
Epoch: 0020 | loss_train: 0.0495 loss_val: 1.4782 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4803
Epoch: 0021 | loss_train: 0.0323 loss_val: 1.5645 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4950
Epoch: 0022 | loss_train: 0.0213 loss_val: 1.6743 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.5016
Epoch: 0023 | loss_train: 0.0201 loss_val: 1.7754 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4751
Epoch: 0024 | loss_train: 0.0097 loss_val: 1.9049 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0025 | loss_train: 0.0079 loss_val: 2.0581 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4548
Epoch: 0026 | loss_train: 0.0060 loss_val: 2.1913 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0027 | loss_train: 0.0044 loss_val: 2.3198 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0028 | loss_train: 0.0038 loss_val: 2.4287 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0029 | loss_train: 0.0025 loss_val: 2.5194 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0030 | loss_train: 0.0020 loss_val: 2.5954 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0031 | loss_train: 0.0016 loss_val: 2.6609 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0032 | loss_train: 0.0013 loss_val: 2.7174 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0033 | loss_train: 0.0009 loss_val: 2.7736 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0034 | loss_train: 0.0007 loss_val: 2.8266 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4567
Epoch: 0035 | loss_train: 0.0005 loss_val: 2.8793 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4640
Epoch: 0036 | loss_train: 0.0005 loss_val: 2.9301 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4640
Epoch: 0037 | loss_train: 0.0006 loss_val: 2.9753 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4640
Epoch: 0038 | loss_train: 0.0004 loss_val: 2.9997 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4640
Epoch: 0039 | loss_train: 0.0003 loss_val: 3.0231 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4640
Epoch: 0040 | loss_train: 0.0003 loss_val: 3.0054 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4640
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.9990 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4569
Epoch: 0042 | loss_train: 0.0003 loss_val: 3.0058 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4569
Epoch: 0043 | loss_train: 0.0004 loss_val: 3.0235 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4486
Epoch: 0044 | loss_train: 0.0004 loss_val: 3.0502 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4486
Epoch: 0045 | loss_train: 0.0003 loss_val: 3.0823 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4486
Epoch: 0046 | loss_train: 0.0004 loss_val: 3.1189 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4486
Optimization Finished!
Train cost: 11.0480s
Loading 16th epoch
Test set results: loss= 1.4382 accuracy= 0.5686

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=120, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1823, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3171335
Epoch: 0001 | loss_train: 1.6342 loss_val: 1.5512 | acc_train: 0.1917 acc_val: 0.3875 | f1_train: 0.1097 f1_val: 0.1638
Epoch: 0002 | loss_train: 1.5672 loss_val: 1.4459 | acc_train: 0.3417 acc_val: 0.5000 | f1_train: 0.1472 f1_val: 0.1345
Epoch: 0003 | loss_train: 1.4488 loss_val: 1.3280 | acc_train: 0.4333 acc_val: 0.5625 | f1_train: 0.1488 f1_val: 0.2387
Epoch: 0004 | loss_train: 1.3106 loss_val: 1.2267 | acc_train: 0.5667 acc_val: 0.5875 | f1_train: 0.2726 f1_val: 0.2669
Epoch: 0005 | loss_train: 1.1854 loss_val: 1.1472 | acc_train: 0.6333 acc_val: 0.5500 | f1_train: 0.3132 f1_val: 0.2473
Epoch: 0006 | loss_train: 1.0762 loss_val: 1.0875 | acc_train: 0.6583 acc_val: 0.5500 | f1_train: 0.3240 f1_val: 0.2473
Epoch: 0007 | loss_train: 0.9859 loss_val: 1.0496 | acc_train: 0.6500 acc_val: 0.5375 | f1_train: 0.3380 f1_val: 0.2452
Epoch: 0008 | loss_train: 0.8939 loss_val: 1.0236 | acc_train: 0.6750 acc_val: 0.5500 | f1_train: 0.4002 f1_val: 0.2919
Epoch: 0009 | loss_train: 0.7845 loss_val: 1.0006 | acc_train: 0.7083 acc_val: 0.5625 | f1_train: 0.4255 f1_val: 0.3308
Epoch: 0010 | loss_train: 0.6690 loss_val: 0.9726 | acc_train: 0.7750 acc_val: 0.6125 | f1_train: 0.5494 f1_val: 0.3729
Epoch: 0011 | loss_train: 0.5697 loss_val: 0.9141 | acc_train: 0.8500 acc_val: 0.6500 | f1_train: 0.7886 f1_val: 0.3966
Epoch: 0012 | loss_train: 0.4768 loss_val: 0.8576 | acc_train: 0.8917 acc_val: 0.6625 | f1_train: 0.8603 f1_val: 0.4156
Epoch: 0013 | loss_train: 0.3902 loss_val: 0.8960 | acc_train: 0.9167 acc_val: 0.6500 | f1_train: 0.8873 f1_val: 0.4854
Epoch: 0014 | loss_train: 0.3070 loss_val: 0.9861 | acc_train: 0.9333 acc_val: 0.6625 | f1_train: 0.8964 f1_val: 0.5525
Epoch: 0015 | loss_train: 0.2465 loss_val: 0.9985 | acc_train: 0.9500 acc_val: 0.6750 | f1_train: 0.9180 f1_val: 0.5138
Epoch: 0016 | loss_train: 0.1918 loss_val: 1.0939 | acc_train: 0.9583 acc_val: 0.6750 | f1_train: 0.9286 f1_val: 0.5322
Epoch: 0017 | loss_train: 0.1417 loss_val: 1.1590 | acc_train: 0.9833 acc_val: 0.6625 | f1_train: 0.9542 f1_val: 0.4589
Epoch: 0018 | loss_train: 0.1064 loss_val: 1.1932 | acc_train: 0.9917 acc_val: 0.6375 | f1_train: 0.9694 f1_val: 0.4452
Epoch: 0019 | loss_train: 0.0746 loss_val: 1.2603 | acc_train: 0.9833 acc_val: 0.6625 | f1_train: 0.9611 f1_val: 0.4604
Epoch: 0020 | loss_train: 0.0506 loss_val: 1.3467 | acc_train: 0.9917 acc_val: 0.6500 | f1_train: 0.9694 f1_val: 0.4541
Epoch: 0021 | loss_train: 0.0357 loss_val: 1.4312 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4607
Epoch: 0022 | loss_train: 0.0280 loss_val: 1.4936 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4607
Epoch: 0023 | loss_train: 0.0209 loss_val: 1.5472 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4742
Epoch: 0024 | loss_train: 0.0124 loss_val: 1.6224 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4696
Epoch: 0025 | loss_train: 0.0094 loss_val: 1.7055 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0026 | loss_train: 0.0074 loss_val: 1.7966 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0027 | loss_train: 0.0041 loss_val: 1.8912 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0028 | loss_train: 0.0031 loss_val: 1.9865 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0029 | loss_train: 0.0027 loss_val: 2.0754 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0030 | loss_train: 0.0025 loss_val: 2.1623 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0031 | loss_train: 0.0019 loss_val: 2.2360 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0032 | loss_train: 0.0014 loss_val: 2.3059 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0033 | loss_train: 0.0011 loss_val: 2.3739 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0034 | loss_train: 0.0009 loss_val: 2.4398 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0035 | loss_train: 0.0007 loss_val: 2.5022 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0036 | loss_train: 0.0005 loss_val: 2.5617 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0037 | loss_train: 0.0008 loss_val: 2.6124 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0038 | loss_train: 0.0004 loss_val: 2.6580 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0039 | loss_train: 0.0004 loss_val: 2.7000 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0040 | loss_train: 0.0004 loss_val: 2.7389 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4758
Epoch: 0041 | loss_train: 0.0003 loss_val: 2.7747 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0042 | loss_train: 0.0003 loss_val: 2.8073 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0043 | loss_train: 0.0003 loss_val: 2.8369 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4689
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.8646 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4573
Epoch: 0045 | loss_train: 0.0002 loss_val: 2.8894 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0046 | loss_train: 0.0002 loss_val: 2.9130 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0047 | loss_train: 0.0002 loss_val: 2.9350 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0048 | loss_train: 0.0002 loss_val: 2.9556 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0049 | loss_train: 0.0002 loss_val: 2.9753 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0050 | loss_train: 0.0002 loss_val: 2.9935 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0051 | loss_train: 0.0001 loss_val: 3.0105 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0052 | loss_train: 0.0001 loss_val: 3.0257 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0053 | loss_train: 0.0002 loss_val: 3.0395 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0054 | loss_train: 0.0001 loss_val: 3.0519 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0055 | loss_train: 0.0001 loss_val: 3.0631 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0056 | loss_train: 0.0001 loss_val: 3.0728 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0057 | loss_train: 0.0001 loss_val: 3.0811 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0058 | loss_train: 0.0001 loss_val: 3.0882 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0059 | loss_train: 0.0001 loss_val: 3.0944 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0060 | loss_train: 0.0001 loss_val: 3.1000 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0061 | loss_train: 0.0001 loss_val: 3.1053 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0062 | loss_train: 0.0000 loss_val: 3.1104 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0063 | loss_train: 0.0000 loss_val: 3.1150 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0064 | loss_train: 0.0000 loss_val: 3.1194 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0065 | loss_train: 0.0000 loss_val: 3.1239 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0066 | loss_train: 0.0000 loss_val: 3.1283 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0067 | loss_train: 0.0000 loss_val: 3.1326 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0068 | loss_train: 0.0000 loss_val: 3.1368 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0069 | loss_train: 0.0000 loss_val: 3.1405 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0070 | loss_train: 0.0000 loss_val: 3.1441 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0071 | loss_train: 0.0000 loss_val: 3.1477 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0072 | loss_train: 0.0000 loss_val: 3.1510 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0073 | loss_train: 0.0000 loss_val: 3.1541 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0074 | loss_train: 0.0000 loss_val: 3.1572 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0075 | loss_train: 0.0000 loss_val: 3.1604 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0076 | loss_train: 0.0000 loss_val: 3.1632 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0077 | loss_train: 0.0000 loss_val: 3.1660 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0078 | loss_train: 0.0000 loss_val: 3.1684 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0079 | loss_train: 0.0000 loss_val: 3.1709 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0080 | loss_train: 0.0000 loss_val: 3.1732 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0081 | loss_train: 0.0000 loss_val: 3.1750 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0082 | loss_train: 0.0000 loss_val: 3.1767 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0083 | loss_train: 0.0000 loss_val: 3.1783 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0084 | loss_train: 0.0000 loss_val: 3.1800 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0085 | loss_train: 0.0000 loss_val: 3.1814 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0086 | loss_train: 0.0000 loss_val: 3.1828 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0087 | loss_train: 0.0000 loss_val: 3.1843 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0088 | loss_train: 0.0000 loss_val: 3.1859 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0089 | loss_train: 0.0000 loss_val: 3.1873 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0090 | loss_train: 0.0000 loss_val: 3.1888 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0091 | loss_train: 0.0000 loss_val: 3.1904 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0092 | loss_train: 0.0000 loss_val: 3.1920 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0093 | loss_train: 0.0000 loss_val: 3.1936 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0094 | loss_train: 0.0000 loss_val: 3.1950 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0095 | loss_train: 0.0000 loss_val: 3.1963 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0096 | loss_train: 0.0000 loss_val: 3.1976 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0097 | loss_train: 0.0000 loss_val: 3.1989 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0098 | loss_train: 0.0000 loss_val: 3.2001 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0099 | loss_train: 0.0000 loss_val: 3.2011 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0100 | loss_train: 0.0000 loss_val: 3.2022 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0101 | loss_train: 0.0000 loss_val: 3.2032 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0102 | loss_train: 0.0000 loss_val: 3.2040 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0103 | loss_train: 0.0000 loss_val: 3.2050 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0104 | loss_train: 0.0000 loss_val: 3.2058 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0105 | loss_train: 0.0000 loss_val: 3.2067 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0106 | loss_train: 0.0000 loss_val: 3.2075 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0107 | loss_train: 0.0000 loss_val: 3.2085 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0108 | loss_train: 0.0000 loss_val: 3.2091 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0109 | loss_train: 0.0000 loss_val: 3.2097 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0110 | loss_train: 0.0000 loss_val: 3.2104 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0111 | loss_train: 0.0000 loss_val: 3.2110 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0112 | loss_train: 0.0000 loss_val: 3.2114 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0113 | loss_train: 0.0000 loss_val: 3.2118 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0114 | loss_train: 0.0000 loss_val: 3.2120 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0115 | loss_train: 0.0000 loss_val: 3.2124 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0116 | loss_train: 0.0000 loss_val: 3.2128 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0117 | loss_train: 0.0000 loss_val: 3.2135 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0118 | loss_train: 0.0000 loss_val: 3.2147 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0119 | loss_train: 0.0000 loss_val: 3.2159 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0120 | loss_train: 0.0000 loss_val: 3.2170 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0121 | loss_train: 0.0000 loss_val: 3.2182 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0122 | loss_train: 0.0000 loss_val: 3.2196 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0123 | loss_train: 0.0000 loss_val: 3.2206 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0124 | loss_train: 0.0000 loss_val: 3.2214 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0125 | loss_train: 0.0000 loss_val: 3.2222 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0126 | loss_train: 0.0000 loss_val: 3.2230 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0127 | loss_train: 0.0000 loss_val: 3.2237 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0128 | loss_train: 0.0000 loss_val: 3.2246 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0129 | loss_train: 0.0000 loss_val: 3.2254 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0130 | loss_train: 0.0000 loss_val: 3.2264 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0131 | loss_train: 0.0000 loss_val: 3.2272 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0132 | loss_train: 0.0000 loss_val: 3.2282 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0133 | loss_train: 0.0000 loss_val: 3.2291 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0134 | loss_train: 0.0000 loss_val: 3.2297 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0135 | loss_train: 0.0000 loss_val: 3.2307 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0136 | loss_train: 0.0000 loss_val: 3.2314 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0137 | loss_train: 0.0000 loss_val: 3.2324 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0138 | loss_train: 0.0000 loss_val: 3.2335 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0139 | loss_train: 0.0000 loss_val: 3.2349 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0140 | loss_train: 0.0000 loss_val: 3.2360 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0141 | loss_train: 0.0000 loss_val: 3.2374 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0142 | loss_train: 0.0000 loss_val: 3.2389 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0143 | loss_train: 0.0000 loss_val: 3.2404 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0144 | loss_train: 0.0000 loss_val: 3.2417 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0145 | loss_train: 0.0000 loss_val: 3.2432 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0146 | loss_train: 0.0000 loss_val: 3.2446 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0147 | loss_train: 0.0000 loss_val: 3.2457 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0148 | loss_train: 0.0000 loss_val: 3.2470 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0149 | loss_train: 0.0000 loss_val: 3.2478 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0150 | loss_train: 0.0000 loss_val: 3.2481 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0151 | loss_train: 0.0000 loss_val: 3.2485 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0152 | loss_train: 0.0000 loss_val: 3.2493 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0153 | loss_train: 0.0000 loss_val: 3.2498 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0154 | loss_train: 0.0000 loss_val: 3.2504 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0155 | loss_train: 0.0000 loss_val: 3.2512 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0156 | loss_train: 0.0000 loss_val: 3.2523 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0157 | loss_train: 0.0000 loss_val: 3.2534 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0158 | loss_train: 0.0000 loss_val: 3.2544 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0159 | loss_train: 0.0000 loss_val: 3.2553 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0160 | loss_train: 0.0000 loss_val: 3.2563 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0161 | loss_train: 0.0000 loss_val: 3.2571 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0162 | loss_train: 0.0000 loss_val: 3.2579 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0163 | loss_train: 0.0000 loss_val: 3.2589 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0164 | loss_train: 0.0000 loss_val: 3.2601 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0165 | loss_train: 0.0000 loss_val: 3.2611 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0166 | loss_train: 0.0000 loss_val: 3.2624 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0167 | loss_train: 0.0000 loss_val: 3.2637 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0168 | loss_train: 0.0000 loss_val: 3.2649 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0169 | loss_train: 0.0000 loss_val: 3.2664 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0170 | loss_train: 0.0000 loss_val: 3.2680 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0171 | loss_train: 0.0000 loss_val: 3.2695 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0172 | loss_train: 0.0000 loss_val: 3.2707 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0173 | loss_train: 0.0000 loss_val: 3.2721 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0174 | loss_train: 0.0000 loss_val: 3.2731 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0175 | loss_train: 0.0000 loss_val: 3.2744 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0176 | loss_train: 0.0000 loss_val: 3.2752 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0177 | loss_train: 0.0000 loss_val: 3.2763 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0178 | loss_train: 0.0000 loss_val: 3.2771 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0179 | loss_train: 0.0000 loss_val: 3.2778 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0180 | loss_train: 0.0000 loss_val: 3.2783 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0181 | loss_train: 0.0000 loss_val: 3.2789 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0182 | loss_train: 0.0000 loss_val: 3.2798 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0183 | loss_train: 0.0000 loss_val: 3.2808 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0184 | loss_train: 0.0000 loss_val: 3.2821 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0185 | loss_train: 0.0000 loss_val: 3.2838 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0186 | loss_train: 0.0000 loss_val: 3.2853 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0187 | loss_train: 0.0000 loss_val: 3.2870 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0188 | loss_train: 0.0000 loss_val: 3.2887 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0189 | loss_train: 0.0000 loss_val: 3.2904 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0190 | loss_train: 0.0000 loss_val: 3.2921 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0191 | loss_train: 0.0000 loss_val: 3.2939 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0192 | loss_train: 0.0000 loss_val: 3.2955 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0193 | loss_train: 0.0000 loss_val: 3.2975 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0194 | loss_train: 0.0000 loss_val: 3.2990 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0195 | loss_train: 0.0000 loss_val: 3.3004 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0196 | loss_train: 0.0000 loss_val: 3.3014 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0197 | loss_train: 0.0000 loss_val: 3.3019 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0198 | loss_train: 0.0000 loss_val: 3.3027 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0199 | loss_train: 0.0000 loss_val: 3.3037 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0200 | loss_train: 0.0000 loss_val: 3.3047 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0201 | loss_train: 0.0000 loss_val: 3.3058 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0202 | loss_train: 0.0000 loss_val: 3.3069 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0203 | loss_train: 0.0000 loss_val: 3.3083 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0204 | loss_train: 0.0000 loss_val: 3.3099 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0205 | loss_train: 0.0000 loss_val: 3.3112 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0206 | loss_train: 0.0000 loss_val: 3.3136 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0207 | loss_train: 0.0000 loss_val: 3.3153 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0208 | loss_train: 0.0000 loss_val: 3.3169 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0209 | loss_train: 0.0000 loss_val: 3.3185 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0210 | loss_train: 0.0000 loss_val: 3.3203 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0211 | loss_train: 0.0000 loss_val: 3.3221 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0212 | loss_train: 0.0000 loss_val: 3.3238 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0213 | loss_train: 0.0000 loss_val: 3.3255 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0214 | loss_train: 0.0000 loss_val: 3.3267 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0215 | loss_train: 0.0000 loss_val: 3.3275 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0216 | loss_train: 0.0000 loss_val: 3.3281 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0217 | loss_train: 0.0000 loss_val: 3.3287 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0218 | loss_train: 0.0000 loss_val: 3.3287 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0219 | loss_train: 0.0000 loss_val: 3.3290 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0220 | loss_train: 0.0000 loss_val: 3.3293 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0221 | loss_train: 0.0000 loss_val: 3.3295 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0222 | loss_train: 0.0000 loss_val: 3.3299 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0223 | loss_train: 0.0000 loss_val: 3.3305 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0224 | loss_train: 0.0000 loss_val: 3.3309 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0225 | loss_train: 0.0000 loss_val: 3.3319 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0226 | loss_train: 0.0000 loss_val: 3.3331 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0227 | loss_train: 0.0000 loss_val: 3.3341 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0228 | loss_train: 0.0000 loss_val: 3.3353 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0229 | loss_train: 0.0000 loss_val: 3.3366 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0230 | loss_train: 0.0000 loss_val: 3.3382 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4765
Epoch: 0231 | loss_train: 0.0000 loss_val: 3.3401 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0232 | loss_train: 0.0000 loss_val: 3.3414 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0233 | loss_train: 0.0000 loss_val: 3.3427 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0234 | loss_train: 0.0000 loss_val: 3.3443 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0235 | loss_train: 0.0000 loss_val: 3.3450 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0236 | loss_train: 0.0000 loss_val: 3.3459 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0237 | loss_train: 0.0000 loss_val: 3.3470 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0238 | loss_train: 0.0000 loss_val: 3.3486 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0239 | loss_train: 0.0000 loss_val: 3.3495 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0240 | loss_train: 0.0000 loss_val: 3.3510 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0241 | loss_train: 0.0000 loss_val: 3.3518 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0242 | loss_train: 0.0000 loss_val: 3.3526 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0243 | loss_train: 0.0000 loss_val: 3.3534 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0244 | loss_train: 0.0000 loss_val: 3.3551 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0245 | loss_train: 0.0000 loss_val: 3.3568 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0246 | loss_train: 0.0000 loss_val: 3.3583 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0247 | loss_train: 0.0000 loss_val: 3.3597 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0248 | loss_train: 0.0000 loss_val: 3.3610 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0249 | loss_train: 0.0000 loss_val: 3.3622 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0250 | loss_train: 0.0000 loss_val: 3.3626 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0251 | loss_train: 0.0000 loss_val: 3.3617 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0252 | loss_train: 0.0000 loss_val: 3.3613 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0253 | loss_train: 0.0000 loss_val: 3.3612 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0254 | loss_train: 0.0000 loss_val: 3.3614 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0255 | loss_train: 0.0000 loss_val: 3.3620 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0256 | loss_train: 0.0000 loss_val: 3.3632 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0257 | loss_train: 0.0000 loss_val: 3.3648 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0258 | loss_train: 0.0000 loss_val: 3.3665 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0259 | loss_train: 0.0000 loss_val: 3.3681 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0260 | loss_train: 0.0000 loss_val: 3.3697 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0261 | loss_train: 0.0000 loss_val: 3.3716 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0262 | loss_train: 0.0000 loss_val: 3.3740 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0263 | loss_train: 0.0000 loss_val: 3.3762 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0264 | loss_train: 0.0000 loss_val: 3.3782 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0265 | loss_train: 0.0000 loss_val: 3.3801 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0266 | loss_train: 0.0000 loss_val: 3.3821 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0267 | loss_train: 0.0000 loss_val: 3.3836 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0268 | loss_train: 0.0000 loss_val: 3.3849 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0269 | loss_train: 0.0000 loss_val: 3.3856 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0270 | loss_train: 0.0000 loss_val: 3.3862 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0271 | loss_train: 0.0000 loss_val: 3.3869 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0272 | loss_train: 0.0000 loss_val: 3.3882 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0273 | loss_train: 0.0000 loss_val: 3.3895 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0274 | loss_train: 0.0000 loss_val: 3.3908 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0275 | loss_train: 0.0000 loss_val: 3.3918 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0276 | loss_train: 0.0000 loss_val: 3.3927 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0277 | loss_train: 0.0000 loss_val: 3.3945 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0278 | loss_train: 0.0000 loss_val: 3.3962 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0279 | loss_train: 0.0000 loss_val: 3.3975 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0280 | loss_train: 0.0000 loss_val: 3.3986 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0281 | loss_train: 0.0000 loss_val: 3.3995 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0282 | loss_train: 0.0000 loss_val: 3.4005 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0283 | loss_train: 0.0000 loss_val: 3.4021 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0284 | loss_train: 0.0000 loss_val: 3.4037 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0285 | loss_train: 0.0000 loss_val: 3.4054 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0286 | loss_train: 0.0000 loss_val: 3.4065 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0287 | loss_train: 0.0000 loss_val: 3.4070 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0288 | loss_train: 0.0000 loss_val: 3.4073 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0289 | loss_train: 0.0000 loss_val: 3.4073 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0290 | loss_train: 0.0000 loss_val: 3.4074 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0291 | loss_train: 0.0000 loss_val: 3.4078 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0292 | loss_train: 0.0000 loss_val: 3.4080 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0293 | loss_train: 0.0000 loss_val: 3.4097 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0294 | loss_train: 0.0000 loss_val: 3.4117 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0295 | loss_train: 0.0000 loss_val: 3.4142 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0296 | loss_train: 0.0000 loss_val: 3.4183 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0297 | loss_train: 0.0000 loss_val: 3.4227 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0298 | loss_train: 0.0000 loss_val: 3.4268 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0299 | loss_train: 0.0000 loss_val: 3.4298 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0300 | loss_train: 0.0000 loss_val: 3.4325 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0301 | loss_train: 0.0000 loss_val: 3.4343 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0302 | loss_train: 0.0000 loss_val: 3.4351 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0303 | loss_train: 0.0000 loss_val: 3.4347 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0304 | loss_train: 0.0000 loss_val: 3.4328 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0305 | loss_train: 0.0000 loss_val: 3.4311 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0306 | loss_train: 0.0000 loss_val: 3.4286 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0307 | loss_train: 0.0000 loss_val: 3.4272 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0308 | loss_train: 0.0000 loss_val: 3.4270 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0309 | loss_train: 0.0000 loss_val: 3.4293 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0310 | loss_train: 0.0000 loss_val: 3.4323 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0311 | loss_train: 0.0000 loss_val: 3.4364 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0312 | loss_train: 0.0000 loss_val: 3.4410 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0313 | loss_train: 0.0000 loss_val: 3.4459 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0314 | loss_train: 0.0000 loss_val: 3.4511 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0315 | loss_train: 0.0000 loss_val: 3.4534 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0316 | loss_train: 0.0000 loss_val: 3.4550 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0317 | loss_train: 0.0000 loss_val: 3.4540 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0318 | loss_train: 0.0000 loss_val: 3.4535 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0319 | loss_train: 0.0000 loss_val: 3.4504 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0320 | loss_train: 0.0000 loss_val: 3.4484 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0321 | loss_train: 0.0000 loss_val: 3.4472 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0322 | loss_train: 0.0000 loss_val: 3.4468 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0323 | loss_train: 0.0000 loss_val: 3.4473 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0324 | loss_train: 0.0000 loss_val: 3.4479 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0325 | loss_train: 0.0000 loss_val: 3.4491 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0326 | loss_train: 0.0000 loss_val: 3.4512 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0327 | loss_train: 0.0000 loss_val: 3.4534 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0328 | loss_train: 0.0000 loss_val: 3.4559 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0329 | loss_train: 0.0000 loss_val: 3.4581 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0330 | loss_train: 0.0000 loss_val: 3.4591 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0331 | loss_train: 0.0000 loss_val: 3.4603 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0332 | loss_train: 0.0000 loss_val: 3.4620 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0333 | loss_train: 0.0000 loss_val: 3.4636 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0334 | loss_train: 0.0000 loss_val: 3.4659 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0335 | loss_train: 0.0000 loss_val: 3.4678 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0336 | loss_train: 0.0000 loss_val: 3.4700 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0337 | loss_train: 0.0000 loss_val: 3.4715 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0338 | loss_train: 0.0000 loss_val: 3.4737 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0339 | loss_train: 0.0000 loss_val: 3.4758 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0340 | loss_train: 0.0000 loss_val: 3.4774 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0341 | loss_train: 0.0000 loss_val: 3.4787 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0342 | loss_train: 0.0000 loss_val: 3.4803 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0343 | loss_train: 0.0000 loss_val: 3.4807 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0344 | loss_train: 0.0000 loss_val: 3.4810 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0345 | loss_train: 0.0000 loss_val: 3.4809 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0346 | loss_train: 0.0000 loss_val: 3.4811 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0347 | loss_train: 0.0000 loss_val: 3.4817 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0348 | loss_train: 0.0000 loss_val: 3.4821 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0349 | loss_train: 0.0000 loss_val: 3.4831 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0350 | loss_train: 0.0000 loss_val: 3.4843 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0351 | loss_train: 0.0000 loss_val: 3.4853 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0352 | loss_train: 0.0000 loss_val: 3.4866 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0353 | loss_train: 0.0000 loss_val: 3.4875 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0354 | loss_train: 0.0000 loss_val: 3.4883 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0355 | loss_train: 0.0000 loss_val: 3.4891 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0356 | loss_train: 0.0000 loss_val: 3.4899 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0357 | loss_train: 0.0000 loss_val: 3.4907 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0358 | loss_train: 0.0000 loss_val: 3.4912 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0359 | loss_train: 0.0000 loss_val: 3.4922 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0360 | loss_train: 0.0000 loss_val: 3.4922 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0361 | loss_train: 0.0000 loss_val: 3.4930 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0362 | loss_train: 0.0000 loss_val: 3.4941 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0363 | loss_train: 0.0000 loss_val: 3.4954 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0364 | loss_train: 0.0000 loss_val: 3.4975 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0365 | loss_train: 0.0000 loss_val: 3.5003 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0366 | loss_train: 0.0000 loss_val: 3.5038 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0367 | loss_train: 0.0000 loss_val: 3.5073 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0368 | loss_train: 0.0000 loss_val: 3.5105 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0369 | loss_train: 0.0000 loss_val: 3.5137 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0370 | loss_train: 0.0000 loss_val: 3.5158 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0371 | loss_train: 0.0000 loss_val: 3.5172 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0372 | loss_train: 0.0000 loss_val: 3.5183 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0373 | loss_train: 0.0000 loss_val: 3.5173 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0374 | loss_train: 0.0000 loss_val: 3.5162 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0375 | loss_train: 0.0000 loss_val: 3.5144 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0376 | loss_train: 0.0000 loss_val: 3.5134 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0377 | loss_train: 0.0000 loss_val: 3.5132 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0378 | loss_train: 0.0000 loss_val: 3.5150 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0379 | loss_train: 0.0000 loss_val: 3.5174 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0380 | loss_train: 0.0000 loss_val: 3.5206 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0381 | loss_train: 0.0000 loss_val: 3.5239 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0382 | loss_train: 0.0000 loss_val: 3.5267 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0383 | loss_train: 0.0000 loss_val: 3.5267 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0384 | loss_train: 0.0000 loss_val: 3.5264 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0385 | loss_train: 0.0000 loss_val: 3.5263 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0386 | loss_train: 0.0000 loss_val: 3.5273 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0387 | loss_train: 0.0000 loss_val: 3.5286 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0388 | loss_train: 0.0000 loss_val: 3.5305 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0389 | loss_train: 0.0000 loss_val: 3.5335 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0390 | loss_train: 0.0000 loss_val: 3.5367 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0391 | loss_train: 0.0000 loss_val: 3.5398 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0392 | loss_train: 0.0000 loss_val: 3.5428 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0393 | loss_train: 0.0000 loss_val: 3.5453 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0394 | loss_train: 0.0000 loss_val: 3.5474 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0395 | loss_train: 0.0000 loss_val: 3.5492 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0396 | loss_train: 0.0000 loss_val: 3.5501 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0397 | loss_train: 0.0000 loss_val: 3.5503 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0398 | loss_train: 0.0000 loss_val: 3.5507 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0399 | loss_train: 0.0000 loss_val: 3.5502 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0400 | loss_train: 0.0000 loss_val: 3.5498 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0401 | loss_train: 0.0000 loss_val: 3.5494 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0402 | loss_train: 0.0000 loss_val: 3.5494 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0403 | loss_train: 0.0000 loss_val: 3.5496 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0404 | loss_train: 0.0000 loss_val: 3.5501 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0405 | loss_train: 0.0000 loss_val: 3.5509 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0406 | loss_train: 0.0000 loss_val: 3.5525 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0407 | loss_train: 0.0000 loss_val: 3.5545 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0408 | loss_train: 0.0000 loss_val: 3.5565 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0409 | loss_train: 0.0000 loss_val: 3.5584 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0410 | loss_train: 0.0000 loss_val: 3.5602 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0411 | loss_train: 0.0000 loss_val: 3.5619 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0412 | loss_train: 0.0000 loss_val: 3.5639 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0413 | loss_train: 0.0000 loss_val: 3.5657 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0414 | loss_train: 0.0000 loss_val: 3.5669 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0415 | loss_train: 0.0000 loss_val: 3.5679 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0416 | loss_train: 0.0000 loss_val: 3.5683 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0417 | loss_train: 0.0000 loss_val: 3.5684 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0418 | loss_train: 0.0000 loss_val: 3.5684 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0419 | loss_train: 0.0000 loss_val: 3.5684 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0420 | loss_train: 0.0000 loss_val: 3.5681 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0421 | loss_train: 0.0000 loss_val: 3.5684 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0422 | loss_train: 0.0000 loss_val: 3.5693 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0423 | loss_train: 0.0000 loss_val: 3.5702 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0424 | loss_train: 0.0000 loss_val: 3.5713 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0425 | loss_train: 0.0000 loss_val: 3.5721 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0426 | loss_train: 0.0000 loss_val: 3.5732 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0427 | loss_train: 0.0000 loss_val: 3.5749 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0428 | loss_train: 0.0000 loss_val: 3.5768 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0429 | loss_train: 0.0000 loss_val: 3.5785 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0430 | loss_train: 0.0000 loss_val: 3.5796 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0431 | loss_train: 0.0000 loss_val: 3.5819 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0432 | loss_train: 0.0000 loss_val: 3.5839 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0433 | loss_train: 0.0000 loss_val: 3.5849 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0434 | loss_train: 0.0000 loss_val: 3.5855 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0435 | loss_train: 0.0000 loss_val: 3.5858 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0436 | loss_train: 0.0000 loss_val: 3.5860 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0437 | loss_train: 0.0000 loss_val: 3.5857 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0438 | loss_train: 0.0000 loss_val: 3.5858 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0439 | loss_train: 0.0000 loss_val: 3.5860 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0440 | loss_train: 0.0000 loss_val: 3.5863 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0441 | loss_train: 0.0000 loss_val: 3.5871 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0442 | loss_train: 0.0000 loss_val: 3.5879 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0443 | loss_train: 0.0000 loss_val: 3.5887 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0444 | loss_train: 0.0000 loss_val: 3.5898 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0445 | loss_train: 0.0000 loss_val: 3.5910 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0446 | loss_train: 0.0000 loss_val: 3.5923 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0447 | loss_train: 0.0000 loss_val: 3.5935 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0448 | loss_train: 0.0000 loss_val: 3.5948 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0449 | loss_train: 0.0000 loss_val: 3.5963 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0450 | loss_train: 0.0000 loss_val: 3.5978 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0451 | loss_train: 0.0000 loss_val: 3.5995 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0452 | loss_train: 0.0000 loss_val: 3.6008 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0453 | loss_train: 0.0000 loss_val: 3.6018 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0454 | loss_train: 0.0000 loss_val: 3.6030 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0455 | loss_train: 0.0000 loss_val: 3.6037 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0456 | loss_train: 0.0000 loss_val: 3.6043 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0457 | loss_train: 0.0000 loss_val: 3.6048 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0458 | loss_train: 0.0000 loss_val: 3.6057 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0459 | loss_train: 0.0000 loss_val: 3.6067 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0460 | loss_train: 0.0000 loss_val: 3.6077 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0461 | loss_train: 0.0000 loss_val: 3.6085 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0462 | loss_train: 0.0000 loss_val: 3.6089 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0463 | loss_train: 0.0000 loss_val: 3.6088 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0464 | loss_train: 0.0000 loss_val: 3.6087 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0465 | loss_train: 0.0000 loss_val: 3.6086 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0466 | loss_train: 0.0000 loss_val: 3.6087 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0467 | loss_train: 0.0000 loss_val: 3.6086 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0468 | loss_train: 0.0000 loss_val: 3.6084 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0469 | loss_train: 0.0000 loss_val: 3.6087 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0470 | loss_train: 0.0000 loss_val: 3.6091 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0471 | loss_train: 0.0000 loss_val: 3.6103 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0472 | loss_train: 0.0000 loss_val: 3.6114 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0473 | loss_train: 0.0000 loss_val: 3.6127 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0474 | loss_train: 0.0000 loss_val: 3.6142 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0475 | loss_train: 0.0000 loss_val: 3.6158 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0476 | loss_train: 0.0000 loss_val: 3.6172 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0477 | loss_train: 0.0000 loss_val: 3.6184 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0478 | loss_train: 0.0000 loss_val: 3.6200 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0479 | loss_train: 0.0000 loss_val: 3.6215 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0480 | loss_train: 0.0000 loss_val: 3.6228 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0481 | loss_train: 0.0000 loss_val: 3.6241 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0482 | loss_train: 0.0000 loss_val: 3.6252 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0483 | loss_train: 0.0000 loss_val: 3.6266 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0484 | loss_train: 0.0000 loss_val: 3.6279 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0485 | loss_train: 0.0000 loss_val: 3.6287 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0486 | loss_train: 0.0000 loss_val: 3.6295 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0487 | loss_train: 0.0000 loss_val: 3.6301 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0488 | loss_train: 0.0000 loss_val: 3.6305 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0489 | loss_train: 0.0000 loss_val: 3.6302 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0490 | loss_train: 0.0000 loss_val: 3.6301 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0491 | loss_train: 0.0000 loss_val: 3.6303 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0492 | loss_train: 0.0000 loss_val: 3.6309 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0493 | loss_train: 0.0000 loss_val: 3.6318 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0494 | loss_train: 0.0000 loss_val: 3.6327 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0495 | loss_train: 0.0000 loss_val: 3.6341 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0496 | loss_train: 0.0000 loss_val: 3.6353 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0497 | loss_train: 0.0000 loss_val: 3.6365 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0498 | loss_train: 0.0000 loss_val: 3.6372 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0499 | loss_train: 0.0000 loss_val: 3.6380 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0500 | loss_train: 0.0000 loss_val: 3.6388 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0501 | loss_train: 0.0000 loss_val: 3.6384 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0502 | loss_train: 0.0000 loss_val: 3.6379 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0503 | loss_train: 0.0000 loss_val: 3.6379 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0504 | loss_train: 0.0000 loss_val: 3.6382 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0505 | loss_train: 0.0000 loss_val: 3.6386 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0506 | loss_train: 0.0000 loss_val: 3.6390 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0507 | loss_train: 0.0000 loss_val: 3.6393 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0508 | loss_train: 0.0000 loss_val: 3.6400 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0509 | loss_train: 0.0000 loss_val: 3.6401 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0510 | loss_train: 0.0000 loss_val: 3.6410 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0511 | loss_train: 0.0000 loss_val: 3.6426 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0512 | loss_train: 0.0000 loss_val: 3.6436 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0513 | loss_train: 0.0000 loss_val: 3.6442 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0514 | loss_train: 0.0000 loss_val: 3.6449 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0515 | loss_train: 0.0000 loss_val: 3.6457 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0516 | loss_train: 0.0000 loss_val: 3.6468 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0517 | loss_train: 0.0000 loss_val: 3.6474 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0518 | loss_train: 0.0000 loss_val: 3.6481 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0519 | loss_train: 0.0000 loss_val: 3.6485 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0520 | loss_train: 0.0000 loss_val: 3.6491 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0521 | loss_train: 0.0000 loss_val: 3.6504 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0522 | loss_train: 0.0000 loss_val: 3.6514 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0523 | loss_train: 0.0000 loss_val: 3.6528 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0524 | loss_train: 0.0000 loss_val: 3.6539 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0525 | loss_train: 0.0000 loss_val: 3.6546 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0526 | loss_train: 0.0000 loss_val: 3.6550 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0527 | loss_train: 0.0000 loss_val: 3.6553 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0528 | loss_train: 0.0000 loss_val: 3.6552 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0529 | loss_train: 0.0000 loss_val: 3.6552 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0530 | loss_train: 0.0000 loss_val: 3.6551 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0531 | loss_train: 0.0000 loss_val: 3.6552 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0532 | loss_train: 0.0000 loss_val: 3.6549 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0533 | loss_train: 0.0000 loss_val: 3.6547 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0534 | loss_train: 0.0000 loss_val: 3.6548 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0535 | loss_train: 0.0000 loss_val: 3.6557 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0536 | loss_train: 0.0000 loss_val: 3.6567 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0537 | loss_train: 0.0000 loss_val: 3.6576 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0538 | loss_train: 0.0000 loss_val: 3.6594 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0539 | loss_train: 0.0000 loss_val: 3.6612 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0540 | loss_train: 0.0000 loss_val: 3.6633 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0541 | loss_train: 0.0000 loss_val: 3.6653 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0542 | loss_train: 0.0000 loss_val: 3.6676 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0543 | loss_train: 0.0000 loss_val: 3.6698 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0544 | loss_train: 0.0000 loss_val: 3.6716 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0545 | loss_train: 0.0000 loss_val: 3.6730 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0546 | loss_train: 0.0000 loss_val: 3.6739 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0547 | loss_train: 0.0000 loss_val: 3.6747 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0548 | loss_train: 0.0000 loss_val: 3.6749 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0549 | loss_train: 0.0000 loss_val: 3.6744 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0550 | loss_train: 0.0000 loss_val: 3.6735 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0551 | loss_train: 0.0000 loss_val: 3.6727 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0552 | loss_train: 0.0000 loss_val: 3.6724 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0553 | loss_train: 0.0000 loss_val: 3.6723 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0554 | loss_train: 0.0000 loss_val: 3.6720 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0555 | loss_train: 0.0000 loss_val: 3.6723 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0556 | loss_train: 0.0000 loss_val: 3.6721 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0557 | loss_train: 0.0000 loss_val: 3.6719 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0558 | loss_train: 0.0000 loss_val: 3.6726 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0559 | loss_train: 0.0000 loss_val: 3.6727 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0560 | loss_train: 0.0000 loss_val: 3.6727 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0561 | loss_train: 0.0000 loss_val: 3.6734 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0562 | loss_train: 0.0000 loss_val: 3.6738 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0563 | loss_train: 0.0000 loss_val: 3.6743 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0564 | loss_train: 0.0000 loss_val: 3.6747 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0565 | loss_train: 0.0000 loss_val: 3.6751 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0566 | loss_train: 0.0000 loss_val: 3.6756 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0567 | loss_train: 0.0000 loss_val: 3.6762 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0568 | loss_train: 0.0000 loss_val: 3.6766 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0569 | loss_train: 0.0000 loss_val: 3.6769 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0570 | loss_train: 0.0000 loss_val: 3.6772 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0571 | loss_train: 0.0000 loss_val: 3.6770 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0572 | loss_train: 0.0000 loss_val: 3.6770 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0573 | loss_train: 0.0000 loss_val: 3.6775 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0574 | loss_train: 0.0000 loss_val: 3.6781 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0575 | loss_train: 0.0000 loss_val: 3.6786 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0576 | loss_train: 0.0000 loss_val: 3.6794 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0577 | loss_train: 0.0000 loss_val: 3.6803 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0578 | loss_train: 0.0000 loss_val: 3.6812 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0579 | loss_train: 0.0000 loss_val: 3.6821 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0580 | loss_train: 0.0000 loss_val: 3.6830 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0581 | loss_train: 0.0000 loss_val: 3.6840 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0582 | loss_train: 0.0000 loss_val: 3.6835 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0583 | loss_train: 0.0000 loss_val: 3.6832 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0584 | loss_train: 0.0000 loss_val: 3.6831 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0585 | loss_train: 0.0000 loss_val: 3.6831 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0586 | loss_train: 0.0000 loss_val: 3.6838 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0587 | loss_train: 0.0000 loss_val: 3.6846 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0588 | loss_train: 0.0000 loss_val: 3.6862 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0589 | loss_train: 0.0000 loss_val: 3.6882 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0590 | loss_train: 0.0000 loss_val: 3.6905 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0591 | loss_train: 0.0000 loss_val: 3.6928 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0592 | loss_train: 0.0000 loss_val: 3.6951 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0593 | loss_train: 0.0000 loss_val: 3.6974 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0594 | loss_train: 0.0000 loss_val: 3.6995 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0595 | loss_train: 0.0000 loss_val: 3.7015 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0596 | loss_train: 0.0000 loss_val: 3.7032 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0597 | loss_train: 0.0000 loss_val: 3.7042 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0598 | loss_train: 0.0000 loss_val: 3.7051 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0599 | loss_train: 0.0000 loss_val: 3.7056 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0600 | loss_train: 0.0000 loss_val: 3.7060 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0601 | loss_train: 0.0000 loss_val: 3.7054 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0602 | loss_train: 0.0000 loss_val: 3.7048 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0603 | loss_train: 0.0000 loss_val: 3.7043 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0604 | loss_train: 0.0000 loss_val: 3.7033 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0605 | loss_train: 0.0000 loss_val: 3.7024 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0606 | loss_train: 0.0000 loss_val: 3.7018 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0607 | loss_train: 0.0000 loss_val: 3.7011 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0608 | loss_train: 0.0000 loss_val: 3.7009 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0609 | loss_train: 0.0000 loss_val: 3.7004 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0610 | loss_train: 0.0000 loss_val: 3.6998 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0611 | loss_train: 0.0000 loss_val: 3.6989 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0612 | loss_train: 0.0000 loss_val: 3.6981 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0613 | loss_train: 0.0000 loss_val: 3.6978 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0614 | loss_train: 0.0000 loss_val: 3.6976 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0615 | loss_train: 0.0000 loss_val: 3.6978 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0616 | loss_train: 0.0000 loss_val: 3.6981 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0617 | loss_train: 0.0000 loss_val: 3.6995 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0618 | loss_train: 0.0000 loss_val: 3.7010 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0619 | loss_train: 0.0000 loss_val: 3.7025 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0620 | loss_train: 0.0000 loss_val: 3.7042 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0621 | loss_train: 0.0000 loss_val: 3.7060 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0622 | loss_train: 0.0000 loss_val: 3.7076 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0623 | loss_train: 0.0000 loss_val: 3.7090 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0624 | loss_train: 0.0000 loss_val: 3.7106 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0625 | loss_train: 0.0000 loss_val: 3.7114 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0626 | loss_train: 0.0000 loss_val: 3.7115 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0627 | loss_train: 0.0000 loss_val: 3.7107 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0628 | loss_train: 0.0000 loss_val: 3.7092 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0629 | loss_train: 0.0000 loss_val: 3.7081 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0630 | loss_train: 0.0000 loss_val: 3.7067 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0631 | loss_train: 0.0000 loss_val: 3.7052 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0632 | loss_train: 0.0000 loss_val: 3.7042 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0633 | loss_train: 0.0000 loss_val: 3.7035 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0634 | loss_train: 0.0000 loss_val: 3.7032 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0635 | loss_train: 0.0000 loss_val: 3.7032 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0636 | loss_train: 0.0000 loss_val: 3.7032 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0637 | loss_train: 0.0000 loss_val: 3.7036 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0638 | loss_train: 0.0000 loss_val: 3.7041 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0639 | loss_train: 0.0000 loss_val: 3.7052 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0640 | loss_train: 0.0000 loss_val: 3.7065 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0641 | loss_train: 0.0000 loss_val: 3.7079 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0642 | loss_train: 0.0000 loss_val: 3.7092 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0643 | loss_train: 0.0000 loss_val: 3.7103 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0644 | loss_train: 0.0000 loss_val: 3.7112 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4835
Epoch: 0645 | loss_train: 0.0000 loss_val: 3.7124 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0646 | loss_train: 0.0000 loss_val: 3.7136 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0647 | loss_train: 0.0000 loss_val: 3.7142 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0648 | loss_train: 0.0000 loss_val: 3.7146 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0649 | loss_train: 0.0000 loss_val: 3.7154 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0650 | loss_train: 0.0000 loss_val: 3.7164 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0651 | loss_train: 0.0000 loss_val: 3.7174 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0652 | loss_train: 0.0000 loss_val: 3.7179 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0653 | loss_train: 0.0000 loss_val: 3.7183 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0654 | loss_train: 0.0000 loss_val: 3.7187 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0655 | loss_train: 0.0000 loss_val: 3.7193 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0656 | loss_train: 0.0000 loss_val: 3.7199 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0657 | loss_train: 0.0000 loss_val: 3.7204 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0658 | loss_train: 0.0000 loss_val: 3.7207 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0659 | loss_train: 0.0000 loss_val: 3.7211 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0660 | loss_train: 0.0000 loss_val: 3.7210 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0661 | loss_train: 0.0000 loss_val: 3.7210 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0662 | loss_train: 0.0000 loss_val: 3.7209 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0663 | loss_train: 0.0000 loss_val: 3.7210 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0664 | loss_train: 0.0000 loss_val: 3.7213 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0665 | loss_train: 0.0000 loss_val: 3.7218 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0666 | loss_train: 0.0000 loss_val: 3.7224 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0667 | loss_train: 0.0000 loss_val: 3.7230 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0668 | loss_train: 0.0000 loss_val: 3.7241 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0669 | loss_train: 0.0000 loss_val: 3.7251 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0670 | loss_train: 0.0000 loss_val: 3.7263 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0671 | loss_train: 0.0000 loss_val: 3.7273 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0672 | loss_train: 0.0000 loss_val: 3.7281 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0673 | loss_train: 0.0000 loss_val: 3.7287 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Epoch: 0674 | loss_train: 0.0000 loss_val: 3.7298 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4722
Optimization Finished!
Train cost: 22.1407s
Loading 231th epoch
Test set results: loss= 4.7850 accuracy= 0.5490

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/pe_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=240, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1943, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3232775
Epoch: 0001 | loss_train: 1.6620 loss_val: 1.5731 | acc_train: 0.0917 acc_val: 0.3125 | f1_train: 0.0520 f1_val: 0.1753
Epoch: 0002 | loss_train: 1.5890 loss_val: 1.4493 | acc_train: 0.2167 acc_val: 0.5500 | f1_train: 0.1379 f1_val: 0.2774
Epoch: 0003 | loss_train: 1.4653 loss_val: 1.3171 | acc_train: 0.4500 acc_val: 0.6000 | f1_train: 0.2272 f1_val: 0.2644
Epoch: 0004 | loss_train: 1.3292 loss_val: 1.2116 | acc_train: 0.5333 acc_val: 0.6625 | f1_train: 0.2825 f1_val: 0.3041
Epoch: 0005 | loss_train: 1.2049 loss_val: 1.1379 | acc_train: 0.5833 acc_val: 0.5875 | f1_train: 0.2889 f1_val: 0.2667
Epoch: 0006 | loss_train: 1.1026 loss_val: 1.0902 | acc_train: 0.5833 acc_val: 0.5375 | f1_train: 0.2910 f1_val: 0.2421
Epoch: 0007 | loss_train: 1.0236 loss_val: 1.0573 | acc_train: 0.6000 acc_val: 0.5375 | f1_train: 0.3015 f1_val: 0.2441
Epoch: 0008 | loss_train: 0.9322 loss_val: 1.0337 | acc_train: 0.6500 acc_val: 0.5500 | f1_train: 0.3409 f1_val: 0.2507
Epoch: 0009 | loss_train: 0.8251 loss_val: 1.0178 | acc_train: 0.7083 acc_val: 0.6125 | f1_train: 0.4723 f1_val: 0.3677
Epoch: 0010 | loss_train: 0.7154 loss_val: 1.0109 | acc_train: 0.7667 acc_val: 0.6000 | f1_train: 0.5637 f1_val: 0.3602
Epoch: 0011 | loss_train: 0.6215 loss_val: 0.9724 | acc_train: 0.8000 acc_val: 0.5875 | f1_train: 0.7729 f1_val: 0.3959
Epoch: 0012 | loss_train: 0.5240 loss_val: 0.8985 | acc_train: 0.8667 acc_val: 0.6125 | f1_train: 0.8598 f1_val: 0.3851
Epoch: 0013 | loss_train: 0.4212 loss_val: 0.8754 | acc_train: 0.8917 acc_val: 0.6750 | f1_train: 0.8722 f1_val: 0.4720
Epoch: 0014 | loss_train: 0.3461 loss_val: 0.9447 | acc_train: 0.9167 acc_val: 0.6750 | f1_train: 0.8919 f1_val: 0.5762
Epoch: 0015 | loss_train: 0.2761 loss_val: 1.0368 | acc_train: 0.9333 acc_val: 0.6625 | f1_train: 0.9051 f1_val: 0.5696
Epoch: 0016 | loss_train: 0.2151 loss_val: 1.0690 | acc_train: 0.9500 acc_val: 0.6625 | f1_train: 0.9102 f1_val: 0.5249
Epoch: 0017 | loss_train: 0.1511 loss_val: 1.1238 | acc_train: 0.9583 acc_val: 0.6500 | f1_train: 0.9191 f1_val: 0.5170
Epoch: 0018 | loss_train: 0.1172 loss_val: 1.1959 | acc_train: 0.9750 acc_val: 0.6625 | f1_train: 0.9383 f1_val: 0.5232
Epoch: 0019 | loss_train: 0.0791 loss_val: 1.2826 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5247
Epoch: 0020 | loss_train: 0.0562 loss_val: 1.4015 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.5110
Epoch: 0021 | loss_train: 0.0396 loss_val: 1.5632 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4376
Epoch: 0022 | loss_train: 0.0271 loss_val: 1.7032 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4345
Epoch: 0023 | loss_train: 0.0162 loss_val: 1.8089 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4408
Epoch: 0024 | loss_train: 0.0114 loss_val: 1.9123 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4241
Epoch: 0025 | loss_train: 0.0076 loss_val: 2.0531 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4241
Epoch: 0026 | loss_train: 0.0048 loss_val: 2.1823 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4241
Epoch: 0027 | loss_train: 0.0038 loss_val: 2.2819 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4241
Epoch: 0028 | loss_train: 0.0037 loss_val: 2.3523 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4241
Epoch: 0029 | loss_train: 0.0031 loss_val: 2.4027 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4241
Epoch: 0030 | loss_train: 0.0025 loss_val: 2.4478 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4303
Epoch: 0031 | loss_train: 0.0017 loss_val: 2.4997 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0032 | loss_train: 0.0014 loss_val: 2.5613 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4613
Epoch: 0033 | loss_train: 0.0009 loss_val: 2.6293 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4434
Epoch: 0034 | loss_train: 0.0008 loss_val: 2.6985 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4498
Epoch: 0035 | loss_train: 0.0007 loss_val: 2.7505 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4498
Epoch: 0036 | loss_train: 0.0004 loss_val: 2.7984 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4498
Epoch: 0037 | loss_train: 0.0003 loss_val: 2.8418 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4498
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.8818 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4498
Epoch: 0039 | loss_train: 0.0002 loss_val: 2.9188 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4498
Epoch: 0040 | loss_train: 0.0002 loss_val: 2.9532 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4498
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.9854 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4462
Epoch: 0042 | loss_train: 0.0002 loss_val: 3.0160 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4462
Epoch: 0043 | loss_train: 0.0001 loss_val: 3.0453 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4462
Epoch: 0044 | loss_train: 0.0002 loss_val: 3.0708 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4551
Optimization Finished!
Train cost: 8.1144s
Loading 13th epoch
Test set results: loss= 1.1909 accuracy= 0.5490

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='pe_dim', log_path='log/nagphormer/wisconsin/pe_dim', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/wisconsin/pe_dim')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/n_layers', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9644 loss_val: 1.8685 | acc_train: 0.1500 acc_val: 0.3380 | f1_train: 0.0961 f1_val: 0.1805
Epoch: 0002 | loss_train: 1.9218 loss_val: 1.8201 | acc_train: 0.2429 acc_val: 0.4180 | f1_train: 0.1645 f1_val: 0.2550
Epoch: 0003 | loss_train: 1.8483 loss_val: 1.7494 | acc_train: 0.3143 acc_val: 0.5420 | f1_train: 0.2601 f1_val: 0.4715
Epoch: 0004 | loss_train: 1.7314 loss_val: 1.6593 | acc_train: 0.6143 acc_val: 0.6300 | f1_train: 0.6107 f1_val: 0.5859
Epoch: 0005 | loss_train: 1.5866 loss_val: 1.5494 | acc_train: 0.7643 acc_val: 0.6940 | f1_train: 0.7638 f1_val: 0.6637
Epoch: 0006 | loss_train: 1.4234 loss_val: 1.4177 | acc_train: 0.8429 acc_val: 0.7260 | f1_train: 0.8451 f1_val: 0.7061
Epoch: 0007 | loss_train: 1.2424 loss_val: 1.2689 | acc_train: 0.8929 acc_val: 0.7400 | f1_train: 0.8946 f1_val: 0.7335
Epoch: 0008 | loss_train: 1.0407 loss_val: 1.1175 | acc_train: 0.9214 acc_val: 0.7560 | f1_train: 0.9217 f1_val: 0.7481
Epoch: 0009 | loss_train: 0.8483 loss_val: 0.9828 | acc_train: 0.9500 acc_val: 0.7660 | f1_train: 0.9508 f1_val: 0.7615
Epoch: 0010 | loss_train: 0.6650 loss_val: 0.8742 | acc_train: 0.9429 acc_val: 0.7660 | f1_train: 0.9426 f1_val: 0.7614
Epoch: 0011 | loss_train: 0.5059 loss_val: 0.7956 | acc_train: 0.9714 acc_val: 0.7640 | f1_train: 0.9712 f1_val: 0.7592
Epoch: 0012 | loss_train: 0.3727 loss_val: 0.7420 | acc_train: 0.9786 acc_val: 0.7620 | f1_train: 0.9784 f1_val: 0.7596
Epoch: 0013 | loss_train: 0.2600 loss_val: 0.7074 | acc_train: 0.9857 acc_val: 0.7720 | f1_train: 0.9855 f1_val: 0.7715
Epoch: 0014 | loss_train: 0.1732 loss_val: 0.6932 | acc_train: 0.9929 acc_val: 0.7720 | f1_train: 0.9929 f1_val: 0.7727
Epoch: 0015 | loss_train: 0.1114 loss_val: 0.7023 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7773
Epoch: 0016 | loss_train: 0.0693 loss_val: 0.7350 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7769
Epoch: 0017 | loss_train: 0.0415 loss_val: 0.7858 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0018 | loss_train: 0.0253 loss_val: 0.8479 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7655
Epoch: 0019 | loss_train: 0.0163 loss_val: 0.9169 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0020 | loss_train: 0.0101 loss_val: 0.9886 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7511
Epoch: 0021 | loss_train: 0.0070 loss_val: 1.0602 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0022 | loss_train: 0.0047 loss_val: 1.1301 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7422
Epoch: 0023 | loss_train: 0.0033 loss_val: 1.1974 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7394
Epoch: 0024 | loss_train: 0.0024 loss_val: 1.2620 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7351
Epoch: 0025 | loss_train: 0.0017 loss_val: 1.3238 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7355
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.3828 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7391
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.4388 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7396
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.4920 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7426
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.5427 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7427
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5906 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7439
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6363 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7467
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6800 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7431
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.7215 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7610 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7452
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7988 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7492
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.8345 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7510
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8685 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.9008 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9313 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9601 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9873 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7542
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.0127 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7551
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.0368 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7504
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.0593 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7502
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.0804 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7507
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.0999 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7506
Optimization Finished!
Train cost: 7.9255s
Loading 15th epoch
Test set results: loss= 0.6384 accuracy= 0.7910

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/n_layers', n_heads=8, n_layers=2, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 5076489
Epoch: 0001 | loss_train: 1.9429 loss_val: 1.9469 | acc_train: 0.1214 acc_val: 0.0940 | f1_train: 0.0547 f1_val: 0.0671
Epoch: 0002 | loss_train: 1.9091 loss_val: 1.9017 | acc_train: 0.1929 acc_val: 0.2800 | f1_train: 0.1227 f1_val: 0.3242
Epoch: 0003 | loss_train: 1.8380 loss_val: 1.8358 | acc_train: 0.4786 acc_val: 0.5120 | f1_train: 0.4713 f1_val: 0.5408
Epoch: 0004 | loss_train: 1.7425 loss_val: 1.7461 | acc_train: 0.7714 acc_val: 0.6420 | f1_train: 0.7676 f1_val: 0.6503
Epoch: 0005 | loss_train: 1.6170 loss_val: 1.6305 | acc_train: 0.8429 acc_val: 0.7080 | f1_train: 0.8437 f1_val: 0.7132
Epoch: 0006 | loss_train: 1.4621 loss_val: 1.4952 | acc_train: 0.9000 acc_val: 0.7340 | f1_train: 0.9003 f1_val: 0.7305
Epoch: 0007 | loss_train: 1.2927 loss_val: 1.3550 | acc_train: 0.9357 acc_val: 0.7580 | f1_train: 0.9347 f1_val: 0.7522
Epoch: 0008 | loss_train: 1.1125 loss_val: 1.2195 | acc_train: 0.9429 acc_val: 0.7640 | f1_train: 0.9420 f1_val: 0.7578
Epoch: 0009 | loss_train: 0.9378 loss_val: 1.0934 | acc_train: 0.9429 acc_val: 0.7640 | f1_train: 0.9419 f1_val: 0.7583
Epoch: 0010 | loss_train: 0.7654 loss_val: 0.9857 | acc_train: 0.9571 acc_val: 0.7740 | f1_train: 0.9561 f1_val: 0.7702
Epoch: 0011 | loss_train: 0.6108 loss_val: 0.9017 | acc_train: 0.9643 acc_val: 0.7780 | f1_train: 0.9637 f1_val: 0.7739
Epoch: 0012 | loss_train: 0.4708 loss_val: 0.8443 | acc_train: 0.9786 acc_val: 0.7820 | f1_train: 0.9784 f1_val: 0.7789
Epoch: 0013 | loss_train: 0.3533 loss_val: 0.8001 | acc_train: 0.9857 acc_val: 0.7840 | f1_train: 0.9857 f1_val: 0.7853
Epoch: 0014 | loss_train: 0.2574 loss_val: 0.7720 | acc_train: 1.0000 acc_val: 0.7840 | f1_train: 1.0000 f1_val: 0.7840
Epoch: 0015 | loss_train: 0.1816 loss_val: 0.7592 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7751
Epoch: 0016 | loss_train: 0.1250 loss_val: 0.7590 | acc_train: 1.0000 acc_val: 0.7780 | f1_train: 1.0000 f1_val: 0.7742
Epoch: 0017 | loss_train: 0.0840 loss_val: 0.7761 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7662
Epoch: 0018 | loss_train: 0.0559 loss_val: 0.8093 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7598
Epoch: 0019 | loss_train: 0.0363 loss_val: 0.8521 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7583
Epoch: 0020 | loss_train: 0.0235 loss_val: 0.9011 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0021 | loss_train: 0.0152 loss_val: 0.9543 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7549
Epoch: 0022 | loss_train: 0.0101 loss_val: 1.0108 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7556
Epoch: 0023 | loss_train: 0.0066 loss_val: 1.0704 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7491
Epoch: 0024 | loss_train: 0.0045 loss_val: 1.1314 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7469
Epoch: 0025 | loss_train: 0.0031 loss_val: 1.1932 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7420
Epoch: 0026 | loss_train: 0.0021 loss_val: 1.2558 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7372
Epoch: 0027 | loss_train: 0.0015 loss_val: 1.3190 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7326
Epoch: 0028 | loss_train: 0.0011 loss_val: 1.3822 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7276
Epoch: 0029 | loss_train: 0.0007 loss_val: 1.4449 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7254
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5073 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7224
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.5689 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7224
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6292 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7236
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.6877 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7242
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7443 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7241
Epoch: 0035 | loss_train: 0.0001 loss_val: 1.7988 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7243
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.8514 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7222
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.9016 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7222
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.9497 | acc_train: 1.0000 acc_val: 0.7160 | f1_train: 1.0000 f1_val: 0.7193
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9954 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7169
Epoch: 0040 | loss_train: 0.0001 loss_val: 2.0390 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7169
Epoch: 0041 | loss_train: 0.0000 loss_val: 2.0801 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7168
Epoch: 0042 | loss_train: 0.0000 loss_val: 2.1192 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7159
Epoch: 0043 | loss_train: 0.0000 loss_val: 2.1559 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7159
Epoch: 0044 | loss_train: 0.0000 loss_val: 2.1906 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7154
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.2231 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7151
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.2538 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7151
Optimization Finished!
Train cost: 7.5364s
Loading 14th epoch
Test set results: loss= 0.7422 accuracy= 0.7770

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/n_layers', n_heads=8, n_layers=3, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 7179273
Epoch: 0001 | loss_train: 1.9574 loss_val: 1.9424 | acc_train: 0.0929 acc_val: 0.1800 | f1_train: 0.0638 f1_val: 0.1409
Epoch: 0002 | loss_train: 1.9260 loss_val: 1.8991 | acc_train: 0.2571 acc_val: 0.4160 | f1_train: 0.2027 f1_val: 0.4051
Epoch: 0003 | loss_train: 1.8601 loss_val: 1.8411 | acc_train: 0.6429 acc_val: 0.6160 | f1_train: 0.6151 f1_val: 0.6220
Epoch: 0004 | loss_train: 1.7733 loss_val: 1.7641 | acc_train: 0.8571 acc_val: 0.6940 | f1_train: 0.8589 f1_val: 0.6993
Epoch: 0005 | loss_train: 1.6648 loss_val: 1.6660 | acc_train: 0.9071 acc_val: 0.7300 | f1_train: 0.9060 f1_val: 0.7289
Epoch: 0006 | loss_train: 1.5290 loss_val: 1.5559 | acc_train: 0.9357 acc_val: 0.7360 | f1_train: 0.9348 f1_val: 0.7329
Epoch: 0007 | loss_train: 1.3817 loss_val: 1.4394 | acc_train: 0.9286 acc_val: 0.7400 | f1_train: 0.9260 f1_val: 0.7367
Epoch: 0008 | loss_train: 1.2287 loss_val: 1.3231 | acc_train: 0.9214 acc_val: 0.7520 | f1_train: 0.9181 f1_val: 0.7479
Epoch: 0009 | loss_train: 1.0687 loss_val: 1.2146 | acc_train: 0.9357 acc_val: 0.7640 | f1_train: 0.9339 f1_val: 0.7584
Epoch: 0010 | loss_train: 0.9135 loss_val: 1.1218 | acc_train: 0.9500 acc_val: 0.7700 | f1_train: 0.9488 f1_val: 0.7665
Epoch: 0011 | loss_train: 0.7663 loss_val: 1.0424 | acc_train: 0.9643 acc_val: 0.7600 | f1_train: 0.9637 f1_val: 0.7566
Epoch: 0012 | loss_train: 0.6299 loss_val: 0.9836 | acc_train: 0.9857 acc_val: 0.7580 | f1_train: 0.9855 f1_val: 0.7557
Epoch: 0013 | loss_train: 0.5038 loss_val: 0.9366 | acc_train: 0.9929 acc_val: 0.7620 | f1_train: 0.9929 f1_val: 0.7567
Epoch: 0014 | loss_train: 0.3948 loss_val: 0.8737 | acc_train: 0.9929 acc_val: 0.7580 | f1_train: 0.9929 f1_val: 0.7508
Epoch: 0015 | loss_train: 0.2963 loss_val: 0.8282 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7590
Epoch: 0016 | loss_train: 0.2154 loss_val: 0.8327 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7607
Epoch: 0017 | loss_train: 0.1511 loss_val: 0.8305 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7614
Epoch: 0018 | loss_train: 0.1031 loss_val: 0.8247 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7643
Epoch: 0019 | loss_train: 0.0683 loss_val: 0.8512 | acc_train: 1.0000 acc_val: 0.7620 | f1_train: 1.0000 f1_val: 0.7558
Epoch: 0020 | loss_train: 0.0441 loss_val: 0.9052 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7542
Epoch: 0021 | loss_train: 0.0280 loss_val: 0.9689 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7443
Epoch: 0022 | loss_train: 0.0177 loss_val: 1.0311 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7415
Epoch: 0023 | loss_train: 0.0111 loss_val: 1.0901 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7394
Epoch: 0024 | loss_train: 0.0070 loss_val: 1.1484 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7414
Epoch: 0025 | loss_train: 0.0044 loss_val: 1.2063 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7412
Epoch: 0026 | loss_train: 0.0028 loss_val: 1.2644 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7443
Epoch: 0027 | loss_train: 0.0019 loss_val: 1.3224 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7429
Epoch: 0028 | loss_train: 0.0012 loss_val: 1.3800 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7422
Epoch: 0029 | loss_train: 0.0008 loss_val: 1.4365 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7405
Epoch: 0030 | loss_train: 0.0006 loss_val: 1.4920 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7328
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.5459 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7303
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.5982 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7356
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.6463 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7319
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.6918 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7370
Epoch: 0035 | loss_train: 0.0001 loss_val: 1.7359 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7320
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.7784 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7354
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8193 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7354
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.8586 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7354
Epoch: 0039 | loss_train: 0.0000 loss_val: 1.8961 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7354
Epoch: 0040 | loss_train: 0.0000 loss_val: 1.9319 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7354
Epoch: 0041 | loss_train: 0.0000 loss_val: 1.9660 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0042 | loss_train: 0.0000 loss_val: 1.9983 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0043 | loss_train: 0.0000 loss_val: 2.0288 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0044 | loss_train: 0.0000 loss_val: 2.0575 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.0846 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.1101 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7314
Epoch: 0047 | loss_train: 0.0000 loss_val: 2.1341 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7288
Epoch: 0048 | loss_train: 0.0000 loss_val: 2.1566 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7288
Optimization Finished!
Train cost: 8.3121s
Loading 18th epoch
Test set results: loss= 0.7585 accuracy= 0.7850

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/n_layers', n_heads=8, n_layers=4, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 9282057
Epoch: 0001 | loss_train: 1.9486 loss_val: 1.9333 | acc_train: 0.1000 acc_val: 0.2200 | f1_train: 0.0354 f1_val: 0.2492
Epoch: 0002 | loss_train: 1.9136 loss_val: 1.8915 | acc_train: 0.3286 acc_val: 0.5420 | f1_train: 0.2903 f1_val: 0.5436
Epoch: 0003 | loss_train: 1.8510 loss_val: 1.8372 | acc_train: 0.7643 acc_val: 0.6240 | f1_train: 0.7518 f1_val: 0.6263
Epoch: 0004 | loss_train: 1.7665 loss_val: 1.7675 | acc_train: 0.8929 acc_val: 0.6780 | f1_train: 0.8908 f1_val: 0.6682
Epoch: 0005 | loss_train: 1.6648 loss_val: 1.6820 | acc_train: 0.9071 acc_val: 0.7160 | f1_train: 0.9054 f1_val: 0.7054
Epoch: 0006 | loss_train: 1.5504 loss_val: 1.5874 | acc_train: 0.9071 acc_val: 0.7180 | f1_train: 0.9033 f1_val: 0.7061
Epoch: 0007 | loss_train: 1.4297 loss_val: 1.4935 | acc_train: 0.9071 acc_val: 0.7360 | f1_train: 0.9035 f1_val: 0.7267
Epoch: 0008 | loss_train: 1.3043 loss_val: 1.4063 | acc_train: 0.9286 acc_val: 0.7360 | f1_train: 0.9270 f1_val: 0.7305
Epoch: 0009 | loss_train: 1.1713 loss_val: 1.3082 | acc_train: 0.9571 acc_val: 0.7600 | f1_train: 0.9563 f1_val: 0.7537
Epoch: 0010 | loss_train: 1.0417 loss_val: 1.2210 | acc_train: 0.9714 acc_val: 0.7720 | f1_train: 0.9710 f1_val: 0.7646
Epoch: 0011 | loss_train: 0.9105 loss_val: 1.1469 | acc_train: 0.9857 acc_val: 0.7720 | f1_train: 0.9857 f1_val: 0.7614
Epoch: 0012 | loss_train: 0.7801 loss_val: 1.0794 | acc_train: 0.9929 acc_val: 0.7700 | f1_train: 0.9929 f1_val: 0.7600
Epoch: 0013 | loss_train: 0.6553 loss_val: 1.0061 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7686
Epoch: 0014 | loss_train: 0.5491 loss_val: 1.0164 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7266
Epoch: 0015 | loss_train: 0.4387 loss_val: 0.9084 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7494
Epoch: 0016 | loss_train: 0.3227 loss_val: 0.8385 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7581
Epoch: 0017 | loss_train: 0.2406 loss_val: 0.8512 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7319
Epoch: 0018 | loss_train: 0.1626 loss_val: 0.8931 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7211
Epoch: 0019 | loss_train: 0.1107 loss_val: 0.8966 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7420
Epoch: 0020 | loss_train: 0.0705 loss_val: 0.9039 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7373
Epoch: 0021 | loss_train: 0.0452 loss_val: 0.9262 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0022 | loss_train: 0.0279 loss_val: 0.9737 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7248
Epoch: 0023 | loss_train: 0.0170 loss_val: 1.0405 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7250
Epoch: 0024 | loss_train: 0.0103 loss_val: 1.1201 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7261
Epoch: 0025 | loss_train: 0.0063 loss_val: 1.2077 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7184
Epoch: 0026 | loss_train: 0.0039 loss_val: 1.3003 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7162
Epoch: 0027 | loss_train: 0.0025 loss_val: 1.3952 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7207
Epoch: 0028 | loss_train: 0.0017 loss_val: 1.4903 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7204
Epoch: 0029 | loss_train: 0.0011 loss_val: 1.5850 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7155
Epoch: 0030 | loss_train: 0.0007 loss_val: 1.6789 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7115
Epoch: 0031 | loss_train: 0.0005 loss_val: 1.7712 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7178
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.8614 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7152
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.9492 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7152
Epoch: 0034 | loss_train: 0.0002 loss_val: 2.0337 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7140
Epoch: 0035 | loss_train: 0.0001 loss_val: 2.1151 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7113
Epoch: 0036 | loss_train: 0.0001 loss_val: 2.1928 | acc_train: 1.0000 acc_val: 0.7180 | f1_train: 1.0000 f1_val: 0.7089
Epoch: 0037 | loss_train: 0.0001 loss_val: 2.2664 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7052
Epoch: 0038 | loss_train: 0.0001 loss_val: 2.3362 | acc_train: 1.0000 acc_val: 0.7100 | f1_train: 1.0000 f1_val: 0.6984
Epoch: 0039 | loss_train: 0.0000 loss_val: 2.4020 | acc_train: 1.0000 acc_val: 0.7080 | f1_train: 1.0000 f1_val: 0.6964
Epoch: 0040 | loss_train: 0.0000 loss_val: 2.4639 | acc_train: 1.0000 acc_val: 0.7100 | f1_train: 1.0000 f1_val: 0.6985
Epoch: 0041 | loss_train: 0.0000 loss_val: 2.5215 | acc_train: 1.0000 acc_val: 0.7100 | f1_train: 1.0000 f1_val: 0.6985
Epoch: 0042 | loss_train: 0.0000 loss_val: 2.5751 | acc_train: 1.0000 acc_val: 0.7120 | f1_train: 1.0000 f1_val: 0.7009
Epoch: 0043 | loss_train: 0.0000 loss_val: 2.6247 | acc_train: 1.0000 acc_val: 0.7120 | f1_train: 1.0000 f1_val: 0.7009
Epoch: 0044 | loss_train: 0.0000 loss_val: 2.6707 | acc_train: 1.0000 acc_val: 0.7100 | f1_train: 1.0000 f1_val: 0.6994
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.7134 | acc_train: 1.0000 acc_val: 0.7100 | f1_train: 1.0000 f1_val: 0.6994
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.7530 | acc_train: 1.0000 acc_val: 0.7100 | f1_train: 1.0000 f1_val: 0.6992
Optimization Finished!
Train cost: 8.6422s
Loading 13th epoch
Test set results: loss= 0.9807 accuracy= 0.7850

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/n_layers', n_heads=8, n_layers=5, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 11384841
Epoch: 0001 | loss_train: 1.9477 loss_val: 1.9318 | acc_train: 0.1643 acc_val: 0.2080 | f1_train: 0.0980 f1_val: 0.1915
Epoch: 0002 | loss_train: 1.9138 loss_val: 1.8927 | acc_train: 0.4000 acc_val: 0.4480 | f1_train: 0.3299 f1_val: 0.4370
Epoch: 0003 | loss_train: 1.8530 loss_val: 1.8356 | acc_train: 0.7143 acc_val: 0.6200 | f1_train: 0.6995 f1_val: 0.6076
Epoch: 0004 | loss_train: 1.7675 loss_val: 1.7574 | acc_train: 0.8786 acc_val: 0.6880 | f1_train: 0.8764 f1_val: 0.6781
Epoch: 0005 | loss_train: 1.6642 loss_val: 1.6714 | acc_train: 0.9286 acc_val: 0.7020 | f1_train: 0.9278 f1_val: 0.6960
Epoch: 0006 | loss_train: 1.5546 loss_val: 1.5880 | acc_train: 0.9214 acc_val: 0.7140 | f1_train: 0.9192 f1_val: 0.7074
Epoch: 0007 | loss_train: 1.4398 loss_val: 1.5060 | acc_train: 0.9500 acc_val: 0.7380 | f1_train: 0.9487 f1_val: 0.7325
Epoch: 0008 | loss_train: 1.3231 loss_val: 1.4224 | acc_train: 0.9500 acc_val: 0.7620 | f1_train: 0.9487 f1_val: 0.7502
Epoch: 0009 | loss_train: 1.2075 loss_val: 1.3570 | acc_train: 0.9571 acc_val: 0.7700 | f1_train: 0.9552 f1_val: 0.7643
Epoch: 0010 | loss_train: 1.0923 loss_val: 1.2845 | acc_train: 0.9714 acc_val: 0.7620 | f1_train: 0.9710 f1_val: 0.7598
Epoch: 0011 | loss_train: 0.9873 loss_val: 1.2305 | acc_train: 0.9714 acc_val: 0.7600 | f1_train: 0.9710 f1_val: 0.7556
Epoch: 0012 | loss_train: 0.8680 loss_val: 1.1314 | acc_train: 0.9929 acc_val: 0.7540 | f1_train: 0.9929 f1_val: 0.7459
Epoch: 0013 | loss_train: 0.7301 loss_val: 1.0475 | acc_train: 0.9929 acc_val: 0.7800 | f1_train: 0.9929 f1_val: 0.7722
Epoch: 0014 | loss_train: 0.6114 loss_val: 1.0138 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7600
Epoch: 0015 | loss_train: 0.4874 loss_val: 0.9474 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7380
Epoch: 0016 | loss_train: 0.3843 loss_val: 0.9413 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7481
Epoch: 0017 | loss_train: 0.2980 loss_val: 0.9236 | acc_train: 1.0000 acc_val: 0.7200 | f1_train: 1.0000 f1_val: 0.7192
Epoch: 0018 | loss_train: 0.2030 loss_val: 0.9006 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7213
Epoch: 0019 | loss_train: 0.1389 loss_val: 0.8863 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7169
Epoch: 0020 | loss_train: 0.0885 loss_val: 0.9079 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7204
Epoch: 0021 | loss_train: 0.0554 loss_val: 0.9533 | acc_train: 1.0000 acc_val: 0.7180 | f1_train: 1.0000 f1_val: 0.7091
Epoch: 0022 | loss_train: 0.0345 loss_val: 1.0107 | acc_train: 1.0000 acc_val: 0.7180 | f1_train: 1.0000 f1_val: 0.7104
Epoch: 0023 | loss_train: 0.0217 loss_val: 1.0765 | acc_train: 1.0000 acc_val: 0.7160 | f1_train: 1.0000 f1_val: 0.7087
Epoch: 0024 | loss_train: 0.0137 loss_val: 1.1523 | acc_train: 1.0000 acc_val: 0.7100 | f1_train: 1.0000 f1_val: 0.6991
Epoch: 0025 | loss_train: 0.0086 loss_val: 1.2408 | acc_train: 1.0000 acc_val: 0.7060 | f1_train: 1.0000 f1_val: 0.6931
Epoch: 0026 | loss_train: 0.0053 loss_val: 1.3415 | acc_train: 1.0000 acc_val: 0.7040 | f1_train: 1.0000 f1_val: 0.6880
Epoch: 0027 | loss_train: 0.0033 loss_val: 1.4467 | acc_train: 1.0000 acc_val: 0.7040 | f1_train: 1.0000 f1_val: 0.6878
Epoch: 0028 | loss_train: 0.0021 loss_val: 1.5552 | acc_train: 1.0000 acc_val: 0.6980 | f1_train: 1.0000 f1_val: 0.6801
Epoch: 0029 | loss_train: 0.0014 loss_val: 1.6637 | acc_train: 1.0000 acc_val: 0.6960 | f1_train: 1.0000 f1_val: 0.6788
Epoch: 0030 | loss_train: 0.0009 loss_val: 1.7717 | acc_train: 1.0000 acc_val: 0.6900 | f1_train: 1.0000 f1_val: 0.6735
Epoch: 0031 | loss_train: 0.0006 loss_val: 1.8777 | acc_train: 1.0000 acc_val: 0.6880 | f1_train: 1.0000 f1_val: 0.6747
Epoch: 0032 | loss_train: 0.0004 loss_val: 1.9816 | acc_train: 1.0000 acc_val: 0.6860 | f1_train: 1.0000 f1_val: 0.6723
Epoch: 0033 | loss_train: 0.0003 loss_val: 2.0832 | acc_train: 1.0000 acc_val: 0.6840 | f1_train: 1.0000 f1_val: 0.6702
Epoch: 0034 | loss_train: 0.0002 loss_val: 2.1815 | acc_train: 1.0000 acc_val: 0.6860 | f1_train: 1.0000 f1_val: 0.6725
Epoch: 0035 | loss_train: 0.0001 loss_val: 2.2766 | acc_train: 1.0000 acc_val: 0.6860 | f1_train: 1.0000 f1_val: 0.6742
Epoch: 0036 | loss_train: 0.0001 loss_val: 2.3677 | acc_train: 1.0000 acc_val: 0.6860 | f1_train: 1.0000 f1_val: 0.6776
Epoch: 0037 | loss_train: 0.0001 loss_val: 2.4547 | acc_train: 1.0000 acc_val: 0.6800 | f1_train: 1.0000 f1_val: 0.6728
Epoch: 0038 | loss_train: 0.0001 loss_val: 2.5376 | acc_train: 1.0000 acc_val: 0.6800 | f1_train: 1.0000 f1_val: 0.6721
Epoch: 0039 | loss_train: 0.0000 loss_val: 2.6163 | acc_train: 1.0000 acc_val: 0.6780 | f1_train: 1.0000 f1_val: 0.6706
Epoch: 0040 | loss_train: 0.0000 loss_val: 2.6908 | acc_train: 1.0000 acc_val: 0.6740 | f1_train: 1.0000 f1_val: 0.6663
Epoch: 0041 | loss_train: 0.0000 loss_val: 2.7608 | acc_train: 1.0000 acc_val: 0.6680 | f1_train: 1.0000 f1_val: 0.6614
Epoch: 0042 | loss_train: 0.0000 loss_val: 2.8266 | acc_train: 1.0000 acc_val: 0.6680 | f1_train: 1.0000 f1_val: 0.6625
Epoch: 0043 | loss_train: 0.0000 loss_val: 2.8881 | acc_train: 1.0000 acc_val: 0.6660 | f1_train: 1.0000 f1_val: 0.6615
Epoch: 0044 | loss_train: 0.0000 loss_val: 2.9459 | acc_train: 1.0000 acc_val: 0.6660 | f1_train: 1.0000 f1_val: 0.6615
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.9997 | acc_train: 1.0000 acc_val: 0.6660 | f1_train: 1.0000 f1_val: 0.6615
Epoch: 0046 | loss_train: 0.0000 loss_val: 3.0501 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6601
Epoch: 0047 | loss_train: 0.0000 loss_val: 3.0968 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6598
Epoch: 0048 | loss_train: 0.0000 loss_val: 3.1403 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6598
Epoch: 0049 | loss_train: 0.0000 loss_val: 3.1806 | acc_train: 1.0000 acc_val: 0.6680 | f1_train: 1.0000 f1_val: 0.6658
Optimization Finished!
Train cost: 9.9579s
Loading 13th epoch
Test set results: loss= 1.0163 accuracy= 0.7750

>>> run.py: Namespace(dataset='cora', device=1, experiment='n_layers', log_path='log/nagphormer/cora/n_layers', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/cora/n_layers')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/n_layers', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6315 loss_val: 1.5466 | acc_train: 0.1500 acc_val: 0.3000 | f1_train: 0.1211 f1_val: 0.1308
Epoch: 0002 | loss_train: 1.5652 loss_val: 1.4203 | acc_train: 0.2833 acc_val: 0.5000 | f1_train: 0.1653 f1_val: 0.1644
Epoch: 0003 | loss_train: 1.4443 loss_val: 1.2855 | acc_train: 0.4333 acc_val: 0.5125 | f1_train: 0.1550 f1_val: 0.1526
Epoch: 0004 | loss_train: 1.3147 loss_val: 1.1833 | acc_train: 0.4333 acc_val: 0.5750 | f1_train: 0.1390 f1_val: 0.2358
Epoch: 0005 | loss_train: 1.2048 loss_val: 1.1209 | acc_train: 0.4583 acc_val: 0.6125 | f1_train: 0.1674 f1_val: 0.2715
Epoch: 0006 | loss_train: 1.1178 loss_val: 1.0844 | acc_train: 0.5333 acc_val: 0.5625 | f1_train: 0.2372 f1_val: 0.2529
Epoch: 0007 | loss_train: 1.0281 loss_val: 1.0654 | acc_train: 0.5750 acc_val: 0.5500 | f1_train: 0.2665 f1_val: 0.2474
Epoch: 0008 | loss_train: 0.9321 loss_val: 1.0565 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3003 f1_val: 0.2557
Epoch: 0009 | loss_train: 0.8323 loss_val: 1.0307 | acc_train: 0.6667 acc_val: 0.6125 | f1_train: 0.3606 f1_val: 0.3566
Epoch: 0010 | loss_train: 0.7215 loss_val: 0.9827 | acc_train: 0.7583 acc_val: 0.6000 | f1_train: 0.5366 f1_val: 0.3628
Epoch: 0011 | loss_train: 0.6194 loss_val: 0.9135 | acc_train: 0.8167 acc_val: 0.6500 | f1_train: 0.7483 f1_val: 0.4362
Epoch: 0012 | loss_train: 0.5123 loss_val: 0.8581 | acc_train: 0.8833 acc_val: 0.6500 | f1_train: 0.8614 f1_val: 0.4708
Epoch: 0013 | loss_train: 0.4123 loss_val: 0.8767 | acc_train: 0.9167 acc_val: 0.7000 | f1_train: 0.8844 f1_val: 0.5609
Epoch: 0014 | loss_train: 0.3231 loss_val: 0.9327 | acc_train: 0.9500 acc_val: 0.7000 | f1_train: 0.9142 f1_val: 0.5516
Epoch: 0015 | loss_train: 0.2508 loss_val: 0.9685 | acc_train: 0.9500 acc_val: 0.6875 | f1_train: 0.9209 f1_val: 0.4886
Epoch: 0016 | loss_train: 0.1914 loss_val: 1.0292 | acc_train: 0.9583 acc_val: 0.7000 | f1_train: 0.9259 f1_val: 0.4882
Epoch: 0017 | loss_train: 0.1423 loss_val: 1.1019 | acc_train: 0.9667 acc_val: 0.7000 | f1_train: 0.9297 f1_val: 0.4825
Epoch: 0018 | loss_train: 0.1004 loss_val: 1.1785 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9694 f1_val: 0.4658
Epoch: 0019 | loss_train: 0.0704 loss_val: 1.2618 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4528
Epoch: 0020 | loss_train: 0.0536 loss_val: 1.3591 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0021 | loss_train: 0.0349 loss_val: 1.4517 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0022 | loss_train: 0.0250 loss_val: 1.5643 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0023 | loss_train: 0.0185 loss_val: 1.6873 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0024 | loss_train: 0.0115 loss_val: 1.7941 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0025 | loss_train: 0.0134 loss_val: 1.8016 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0026 | loss_train: 0.0071 loss_val: 1.8700 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0027 | loss_train: 0.0042 loss_val: 1.9669 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4397
Epoch: 0028 | loss_train: 0.0032 loss_val: 2.0669 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4652
Epoch: 0029 | loss_train: 0.0030 loss_val: 2.1614 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0030 | loss_train: 0.0027 loss_val: 2.2514 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4694
Epoch: 0031 | loss_train: 0.0019 loss_val: 2.3389 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0032 | loss_train: 0.0016 loss_val: 2.4217 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0033 | loss_train: 0.0012 loss_val: 2.4973 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0034 | loss_train: 0.0011 loss_val: 2.5645 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0035 | loss_train: 0.0008 loss_val: 2.6212 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.6685 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.7098 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.7466 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0039 | loss_train: 0.0004 loss_val: 2.7690 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.7881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.8042 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.8187 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.8326 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.8465 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.8602 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.8742 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.8881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4756
Optimization Finished!
Train cost: 7.2877s
Loading 13th epoch
Test set results: loss= 1.1536 accuracy= 0.5686

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/n_layers', n_heads=8, n_layers=2, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 5214215
Epoch: 0001 | loss_train: 1.5998 loss_val: 1.5528 | acc_train: 0.2083 acc_val: 0.4375 | f1_train: 0.1631 f1_val: 0.2690
Epoch: 0002 | loss_train: 1.5389 loss_val: 1.4422 | acc_train: 0.5167 acc_val: 0.5375 | f1_train: 0.3382 f1_val: 0.2421
Epoch: 0003 | loss_train: 1.4339 loss_val: 1.3286 | acc_train: 0.6333 acc_val: 0.5375 | f1_train: 0.3448 f1_val: 0.2398
Epoch: 0004 | loss_train: 1.3180 loss_val: 1.2332 | acc_train: 0.6000 acc_val: 0.5375 | f1_train: 0.2768 f1_val: 0.2405
Epoch: 0005 | loss_train: 1.2164 loss_val: 1.1637 | acc_train: 0.5917 acc_val: 0.5250 | f1_train: 0.2731 f1_val: 0.2347
Epoch: 0006 | loss_train: 1.1249 loss_val: 1.1173 | acc_train: 0.5917 acc_val: 0.5500 | f1_train: 0.2730 f1_val: 0.2474
Epoch: 0007 | loss_train: 1.0313 loss_val: 1.0822 | acc_train: 0.6167 acc_val: 0.5875 | f1_train: 0.2849 f1_val: 0.2683
Epoch: 0008 | loss_train: 0.9129 loss_val: 1.0587 | acc_train: 0.6500 acc_val: 0.6000 | f1_train: 0.3975 f1_val: 0.3110
Epoch: 0009 | loss_train: 0.8156 loss_val: 0.9840 | acc_train: 0.7667 acc_val: 0.5875 | f1_train: 0.6736 f1_val: 0.3282
Epoch: 0010 | loss_train: 0.6884 loss_val: 0.9145 | acc_train: 0.8417 acc_val: 0.6250 | f1_train: 0.8102 f1_val: 0.3453
Epoch: 0011 | loss_train: 0.5725 loss_val: 0.9560 | acc_train: 0.8917 acc_val: 0.6375 | f1_train: 0.8509 f1_val: 0.4716
Epoch: 0012 | loss_train: 0.4930 loss_val: 0.8916 | acc_train: 0.9083 acc_val: 0.6875 | f1_train: 0.8704 f1_val: 0.4954
Epoch: 0013 | loss_train: 0.4312 loss_val: 1.1650 | acc_train: 0.9083 acc_val: 0.6125 | f1_train: 0.8862 f1_val: 0.4526
Epoch: 0014 | loss_train: 0.3787 loss_val: 1.0888 | acc_train: 0.9000 acc_val: 0.6500 | f1_train: 0.8804 f1_val: 0.5021
Epoch: 0015 | loss_train: 0.2729 loss_val: 1.0534 | acc_train: 0.9583 acc_val: 0.6500 | f1_train: 0.9286 f1_val: 0.4526
Epoch: 0016 | loss_train: 0.2406 loss_val: 1.3515 | acc_train: 0.9333 acc_val: 0.5750 | f1_train: 0.8922 f1_val: 0.3907
Epoch: 0017 | loss_train: 0.1647 loss_val: 1.6992 | acc_train: 0.9750 acc_val: 0.5500 | f1_train: 0.9418 f1_val: 0.3753
Epoch: 0018 | loss_train: 0.1476 loss_val: 1.1535 | acc_train: 0.9667 acc_val: 0.6500 | f1_train: 0.9597 f1_val: 0.4433
Epoch: 0019 | loss_train: 0.1037 loss_val: 1.0732 | acc_train: 0.9917 acc_val: 0.7250 | f1_train: 0.9954 f1_val: 0.5071
Epoch: 0020 | loss_train: 0.0919 loss_val: 1.1913 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9954 f1_val: 0.4670
Epoch: 0021 | loss_train: 0.0544 loss_val: 1.3537 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4591
Epoch: 0022 | loss_train: 0.0389 loss_val: 1.4808 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4487
Epoch: 0023 | loss_train: 0.0248 loss_val: 1.5695 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4487
Epoch: 0024 | loss_train: 0.0211 loss_val: 1.5819 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.5377
Epoch: 0025 | loss_train: 0.0111 loss_val: 1.5867 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.6358
Epoch: 0026 | loss_train: 0.0080 loss_val: 1.6154 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.6533
Epoch: 0027 | loss_train: 0.0060 loss_val: 1.6715 | acc_train: 1.0000 acc_val: 0.7625 | f1_train: 1.0000 f1_val: 0.6583
Epoch: 0028 | loss_train: 0.0059 loss_val: 1.7348 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.6319
Epoch: 0029 | loss_train: 0.0037 loss_val: 1.8052 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.6319
Epoch: 0030 | loss_train: 0.0027 loss_val: 1.8727 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.6319
Epoch: 0031 | loss_train: 0.0021 loss_val: 1.9380 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.6319
Epoch: 0032 | loss_train: 0.0019 loss_val: 2.0031 | acc_train: 1.0000 acc_val: 0.7375 | f1_train: 1.0000 f1_val: 0.6251
Epoch: 0033 | loss_train: 0.0013 loss_val: 2.0655 | acc_train: 1.0000 acc_val: 0.7375 | f1_train: 1.0000 f1_val: 0.6251
Epoch: 0034 | loss_train: 0.0021 loss_val: 2.1295 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5691
Epoch: 0035 | loss_train: 0.0008 loss_val: 2.1924 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5691
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.2532 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5691
Epoch: 0037 | loss_train: 0.0006 loss_val: 2.3124 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5691
Epoch: 0038 | loss_train: 0.0004 loss_val: 2.3681 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5707
Epoch: 0039 | loss_train: 0.0004 loss_val: 2.4197 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5707
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.4684 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5707
Epoch: 0041 | loss_train: 0.0003 loss_val: 2.5138 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5724
Epoch: 0042 | loss_train: 0.0003 loss_val: 2.5556 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5724
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.5941 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5724
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.6289 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5621
Epoch: 0045 | loss_train: 0.0002 loss_val: 2.6598 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5621
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.6876 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5621
Epoch: 0047 | loss_train: 0.0002 loss_val: 2.7140 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5621
Epoch: 0048 | loss_train: 0.0001 loss_val: 2.7396 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5621
Epoch: 0049 | loss_train: 0.0001 loss_val: 2.7655 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5621
Epoch: 0050 | loss_train: 0.0001 loss_val: 2.7915 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.5621
Epoch: 0051 | loss_train: 0.0001 loss_val: 2.8172 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5693
Epoch: 0052 | loss_train: 0.0001 loss_val: 2.8427 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5693
Epoch: 0053 | loss_train: 0.0001 loss_val: 2.8672 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5693
Epoch: 0054 | loss_train: 0.0001 loss_val: 2.8909 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5672
Epoch: 0055 | loss_train: 0.0001 loss_val: 2.9138 | acc_train: 1.0000 acc_val: 0.7250 | f1_train: 1.0000 f1_val: 0.5672
Epoch: 0056 | loss_train: 0.0000 loss_val: 2.9354 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4985
Epoch: 0057 | loss_train: 0.0000 loss_val: 2.9556 | acc_train: 1.0000 acc_val: 0.7125 | f1_train: 1.0000 f1_val: 0.4985
Optimization Finished!
Train cost: 6.7473s
Loading 26th epoch
Test set results: loss= 2.8114 accuracy= 0.5294

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/n_layers', n_heads=8, n_layers=3, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 7316999
Epoch: 0001 | loss_train: 1.5899 loss_val: 1.5220 | acc_train: 0.2333 acc_val: 0.5500 | f1_train: 0.1515 f1_val: 0.3216
Epoch: 0002 | loss_train: 1.5320 loss_val: 1.4271 | acc_train: 0.4750 acc_val: 0.5125 | f1_train: 0.2727 f1_val: 0.2248
Epoch: 0003 | loss_train: 1.4433 loss_val: 1.3348 | acc_train: 0.5333 acc_val: 0.5375 | f1_train: 0.2455 f1_val: 0.2389
Epoch: 0004 | loss_train: 1.3486 loss_val: 1.2578 | acc_train: 0.5417 acc_val: 0.5250 | f1_train: 0.2496 f1_val: 0.2347
Epoch: 0005 | loss_train: 1.2634 loss_val: 1.1963 | acc_train: 0.5583 acc_val: 0.5250 | f1_train: 0.2597 f1_val: 0.2352
Epoch: 0006 | loss_train: 1.1752 loss_val: 1.1577 | acc_train: 0.5750 acc_val: 0.5500 | f1_train: 0.2669 f1_val: 0.2500
Epoch: 0007 | loss_train: 1.0658 loss_val: 1.1392 | acc_train: 0.6417 acc_val: 0.5750 | f1_train: 0.3337 f1_val: 0.2662
Epoch: 0008 | loss_train: 0.9799 loss_val: 1.0369 | acc_train: 0.7500 acc_val: 0.5625 | f1_train: 0.6545 f1_val: 0.3152
Epoch: 0009 | loss_train: 0.8538 loss_val: 0.9666 | acc_train: 0.7917 acc_val: 0.5750 | f1_train: 0.6865 f1_val: 0.3128
Epoch: 0010 | loss_train: 0.7595 loss_val: 0.9970 | acc_train: 0.8417 acc_val: 0.5875 | f1_train: 0.7739 f1_val: 0.3889
Epoch: 0011 | loss_train: 0.6698 loss_val: 0.9495 | acc_train: 0.8583 acc_val: 0.6375 | f1_train: 0.7357 f1_val: 0.4098
Epoch: 0012 | loss_train: 0.5718 loss_val: 1.0229 | acc_train: 0.8667 acc_val: 0.6000 | f1_train: 0.7604 f1_val: 0.3914
Epoch: 0013 | loss_train: 0.4715 loss_val: 1.0228 | acc_train: 0.9083 acc_val: 0.6250 | f1_train: 0.8356 f1_val: 0.3954
Epoch: 0014 | loss_train: 0.4154 loss_val: 1.3394 | acc_train: 0.9000 acc_val: 0.5375 | f1_train: 0.8189 f1_val: 0.3598
Epoch: 0015 | loss_train: 0.3558 loss_val: 1.0290 | acc_train: 0.9167 acc_val: 0.6375 | f1_train: 0.8474 f1_val: 0.4398
Epoch: 0016 | loss_train: 0.3901 loss_val: 1.1818 | acc_train: 0.8667 acc_val: 0.5875 | f1_train: 0.8599 f1_val: 0.3950
Epoch: 0017 | loss_train: 0.2087 loss_val: 1.3736 | acc_train: 0.9750 acc_val: 0.5750 | f1_train: 0.9602 f1_val: 0.3904
Epoch: 0018 | loss_train: 0.2619 loss_val: 1.3896 | acc_train: 0.9417 acc_val: 0.6000 | f1_train: 0.9340 f1_val: 0.4067
Epoch: 0019 | loss_train: 0.1708 loss_val: 1.3776 | acc_train: 0.9667 acc_val: 0.6000 | f1_train: 0.9778 f1_val: 0.4094
Epoch: 0020 | loss_train: 0.1139 loss_val: 1.4377 | acc_train: 0.9917 acc_val: 0.6250 | f1_train: 0.9657 f1_val: 0.4167
Epoch: 0021 | loss_train: 0.1204 loss_val: 1.5073 | acc_train: 0.9917 acc_val: 0.6000 | f1_train: 0.9954 f1_val: 0.4077
Epoch: 0022 | loss_train: 0.0587 loss_val: 1.6685 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4087
Epoch: 0023 | loss_train: 0.0492 loss_val: 1.8460 | acc_train: 0.9917 acc_val: 0.5875 | f1_train: 0.9954 f1_val: 0.4103
Epoch: 0024 | loss_train: 0.0439 loss_val: 1.9912 | acc_train: 0.9917 acc_val: 0.5875 | f1_train: 0.9954 f1_val: 0.4245
Epoch: 0025 | loss_train: 0.0351 loss_val: 1.6758 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9923 f1_val: 0.4576
Epoch: 0026 | loss_train: 0.0215 loss_val: 1.6198 | acc_train: 1.0000 acc_val: 0.7000 | f1_train: 1.0000 f1_val: 0.4776
Epoch: 0027 | loss_train: 0.0362 loss_val: 1.5373 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9920 f1_val: 0.4568
Epoch: 0028 | loss_train: 0.0106 loss_val: 1.6298 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4787
Epoch: 0029 | loss_train: 0.0099 loss_val: 1.8902 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4730
Epoch: 0030 | loss_train: 0.0137 loss_val: 2.0983 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4730
Epoch: 0031 | loss_train: 0.0052 loss_val: 2.2494 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4916
Epoch: 0032 | loss_train: 0.0023 loss_val: 2.3438 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4906
Epoch: 0033 | loss_train: 0.0022 loss_val: 2.4558 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4963
Epoch: 0034 | loss_train: 0.0029 loss_val: 2.5672 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.5025
Epoch: 0035 | loss_train: 0.0035 loss_val: 2.6778 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.5025
Epoch: 0036 | loss_train: 0.0019 loss_val: 2.7839 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.5025
Epoch: 0037 | loss_train: 0.0012 loss_val: 2.8768 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.5025
Epoch: 0038 | loss_train: 0.0008 loss_val: 2.9604 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5244
Epoch: 0039 | loss_train: 0.0010 loss_val: 3.0470 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5244
Epoch: 0040 | loss_train: 0.0012 loss_val: 3.1419 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5252
Epoch: 0041 | loss_train: 0.0012 loss_val: 3.2426 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5252
Epoch: 0042 | loss_train: 0.0009 loss_val: 3.3318 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5252
Epoch: 0043 | loss_train: 0.0006 loss_val: 3.4170 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5252
Epoch: 0044 | loss_train: 0.0004 loss_val: 3.4932 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.5186
Epoch: 0045 | loss_train: 0.0003 loss_val: 3.5644 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.5128
Epoch: 0046 | loss_train: 0.0003 loss_val: 3.6390 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.5030
Epoch: 0047 | loss_train: 0.0002 loss_val: 3.7120 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.5030
Epoch: 0048 | loss_train: 0.0003 loss_val: 3.7826 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.5030
Epoch: 0049 | loss_train: 0.0003 loss_val: 3.8468 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4964
Epoch: 0050 | loss_train: 0.0003 loss_val: 3.9070 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4964
Epoch: 0051 | loss_train: 0.0002 loss_val: 3.9633 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4964
Epoch: 0052 | loss_train: 0.0002 loss_val: 4.0163 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4964
Epoch: 0053 | loss_train: 0.0002 loss_val: 4.0635 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4964
Epoch: 0054 | loss_train: 0.0001 loss_val: 4.1079 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4845
Epoch: 0055 | loss_train: 0.0001 loss_val: 4.1490 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4845
Epoch: 0056 | loss_train: 0.0001 loss_val: 4.1874 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4845
Optimization Finished!
Train cost: 9.1318s
Loading 26th epoch
Test set results: loss= 2.6624 accuracy= 0.6078

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/n_layers', n_heads=8, n_layers=4, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 9419783
Epoch: 0001 | loss_train: 1.6114 loss_val: 1.5292 | acc_train: 0.2917 acc_val: 0.5125 | f1_train: 0.1607 f1_val: 0.1355
Epoch: 0002 | loss_train: 1.5473 loss_val: 1.4326 | acc_train: 0.4250 acc_val: 0.5125 | f1_train: 0.1524 f1_val: 0.1355
Epoch: 0003 | loss_train: 1.4600 loss_val: 1.3472 | acc_train: 0.4167 acc_val: 0.5625 | f1_train: 0.1176 f1_val: 0.2068
Epoch: 0004 | loss_train: 1.3793 loss_val: 1.2802 | acc_train: 0.4333 acc_val: 0.6125 | f1_train: 0.1390 f1_val: 0.2681
Epoch: 0005 | loss_train: 1.3041 loss_val: 1.2273 | acc_train: 0.4917 acc_val: 0.5625 | f1_train: 0.2000 f1_val: 0.2553
Epoch: 0006 | loss_train: 1.2173 loss_val: 1.2082 | acc_train: 0.6417 acc_val: 0.5750 | f1_train: 0.3503 f1_val: 0.2621
Epoch: 0007 | loss_train: 1.1344 loss_val: 1.1213 | acc_train: 0.7000 acc_val: 0.5750 | f1_train: 0.5223 f1_val: 0.2658
Epoch: 0008 | loss_train: 1.0232 loss_val: 1.0563 | acc_train: 0.7417 acc_val: 0.6125 | f1_train: 0.5347 f1_val: 0.3169
Epoch: 0009 | loss_train: 0.9150 loss_val: 1.0170 | acc_train: 0.8583 acc_val: 0.6375 | f1_train: 0.7112 f1_val: 0.3727
Epoch: 0010 | loss_train: 0.8324 loss_val: 0.9623 | acc_train: 0.8583 acc_val: 0.6375 | f1_train: 0.7743 f1_val: 0.3252
Epoch: 0011 | loss_train: 0.7801 loss_val: 1.1495 | acc_train: 0.7583 acc_val: 0.5375 | f1_train: 0.6786 f1_val: 0.3218
Epoch: 0012 | loss_train: 0.7417 loss_val: 0.9737 | acc_train: 0.7917 acc_val: 0.6250 | f1_train: 0.7460 f1_val: 0.3685
Epoch: 0013 | loss_train: 0.5651 loss_val: 0.9397 | acc_train: 0.9000 acc_val: 0.6375 | f1_train: 0.8511 f1_val: 0.4001
Epoch: 0014 | loss_train: 0.5799 loss_val: 1.0177 | acc_train: 0.8500 acc_val: 0.6000 | f1_train: 0.8096 f1_val: 0.3773
Epoch: 0015 | loss_train: 0.4236 loss_val: 1.1621 | acc_train: 0.9167 acc_val: 0.5375 | f1_train: 0.8673 f1_val: 0.3437
Epoch: 0016 | loss_train: 0.3942 loss_val: 1.1737 | acc_train: 0.9000 acc_val: 0.5625 | f1_train: 0.8753 f1_val: 0.3770
Epoch: 0017 | loss_train: 0.2868 loss_val: 1.1049 | acc_train: 0.9667 acc_val: 0.6250 | f1_train: 0.9337 f1_val: 0.4344
Epoch: 0018 | loss_train: 0.2364 loss_val: 1.1660 | acc_train: 0.9667 acc_val: 0.6250 | f1_train: 0.9488 f1_val: 0.4548
Epoch: 0019 | loss_train: 0.1821 loss_val: 1.3630 | acc_train: 0.9750 acc_val: 0.6250 | f1_train: 0.9752 f1_val: 0.5012
Epoch: 0020 | loss_train: 0.1291 loss_val: 1.5122 | acc_train: 0.9917 acc_val: 0.5875 | f1_train: 0.9954 f1_val: 0.4119
Epoch: 0021 | loss_train: 0.1003 loss_val: 1.5158 | acc_train: 0.9917 acc_val: 0.6250 | f1_train: 0.9954 f1_val: 0.4467
Epoch: 0022 | loss_train: 0.0620 loss_val: 1.5256 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4367
Epoch: 0023 | loss_train: 0.0461 loss_val: 1.6178 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4431
Epoch: 0024 | loss_train: 0.0328 loss_val: 1.7328 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4492
Epoch: 0025 | loss_train: 0.0218 loss_val: 1.8443 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0026 | loss_train: 0.0150 loss_val: 1.9529 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4653
Epoch: 0027 | loss_train: 0.0096 loss_val: 2.0560 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4594
Epoch: 0028 | loss_train: 0.0066 loss_val: 2.1500 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4594
Epoch: 0029 | loss_train: 0.0045 loss_val: 2.2377 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4594
Epoch: 0030 | loss_train: 0.0031 loss_val: 2.3216 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4658
Epoch: 0031 | loss_train: 0.0025 loss_val: 2.4006 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4686
Epoch: 0032 | loss_train: 0.0016 loss_val: 2.4874 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4695
Epoch: 0033 | loss_train: 0.0011 loss_val: 2.5775 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4695
Epoch: 0034 | loss_train: 0.0008 loss_val: 2.6711 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4628
Epoch: 0035 | loss_train: 0.0007 loss_val: 2.7676 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4628
Epoch: 0036 | loss_train: 0.0005 loss_val: 2.8643 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4825
Epoch: 0037 | loss_train: 0.0004 loss_val: 2.9588 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4825
Epoch: 0038 | loss_train: 0.0004 loss_val: 3.0510 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4854
Epoch: 0039 | loss_train: 0.0003 loss_val: 3.1407 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4854
Epoch: 0040 | loss_train: 0.0003 loss_val: 3.2285 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4854
Epoch: 0041 | loss_train: 0.0002 loss_val: 3.3135 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4854
Epoch: 0042 | loss_train: 0.0002 loss_val: 3.3970 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4757
Epoch: 0043 | loss_train: 0.0002 loss_val: 3.4766 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4757
Epoch: 0044 | loss_train: 0.0001 loss_val: 3.5523 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4666
Epoch: 0045 | loss_train: 0.0001 loss_val: 3.6225 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4666
Epoch: 0046 | loss_train: 0.0001 loss_val: 3.6882 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4666
Epoch: 0047 | loss_train: 0.0001 loss_val: 3.7509 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4666
Epoch: 0048 | loss_train: 0.0001 loss_val: 3.8051 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4666
Epoch: 0049 | loss_train: 0.0001 loss_val: 3.8503 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4666
Epoch: 0050 | loss_train: 0.0001 loss_val: 3.8921 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4596
Epoch: 0051 | loss_train: 0.0001 loss_val: 3.9302 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4596
Epoch: 0052 | loss_train: 0.0000 loss_val: 3.9666 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4538
Epoch: 0053 | loss_train: 0.0000 loss_val: 3.9999 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4538
Epoch: 0054 | loss_train: 0.0000 loss_val: 4.0287 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4538
Epoch: 0055 | loss_train: 0.0000 loss_val: 4.0546 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4596
Epoch: 0056 | loss_train: 0.0000 loss_val: 4.0777 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4596
Epoch: 0057 | loss_train: 0.0000 loss_val: 4.0993 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4596
Epoch: 0058 | loss_train: 0.0000 loss_val: 4.1192 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4688
Epoch: 0059 | loss_train: 0.0000 loss_val: 4.1372 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4688
Epoch: 0060 | loss_train: 0.0000 loss_val: 4.1549 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4688
Epoch: 0061 | loss_train: 0.0000 loss_val: 4.1717 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4688
Epoch: 0062 | loss_train: 0.0000 loss_val: 4.1880 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0063 | loss_train: 0.0000 loss_val: 4.2042 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0064 | loss_train: 0.0000 loss_val: 4.2198 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0065 | loss_train: 0.0000 loss_val: 4.2348 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0066 | loss_train: 0.0000 loss_val: 4.2486 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0067 | loss_train: 0.0000 loss_val: 4.2615 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0068 | loss_train: 0.0000 loss_val: 4.2734 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0069 | loss_train: 0.0000 loss_val: 4.2840 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0070 | loss_train: 0.0000 loss_val: 4.2935 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Epoch: 0071 | loss_train: 0.0000 loss_val: 4.3026 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.5420
Optimization Finished!
Train cost: 9.5485s
Loading 31th epoch
Test set results: loss= 3.1931 accuracy= 0.6078

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/n_layers', n_heads=8, n_layers=5, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (1): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (2): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (3): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
    (4): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 11522567
Epoch: 0001 | loss_train: 1.6176 loss_val: 1.5597 | acc_train: 0.1667 acc_val: 0.5375 | f1_train: 0.1290 f1_val: 0.2440
Epoch: 0002 | loss_train: 1.5633 loss_val: 1.4790 | acc_train: 0.5750 acc_val: 0.6000 | f1_train: 0.3371 f1_val: 0.2569
Epoch: 0003 | loss_train: 1.4838 loss_val: 1.4062 | acc_train: 0.5500 acc_val: 0.6250 | f1_train: 0.2459 f1_val: 0.2778
Epoch: 0004 | loss_train: 1.4106 loss_val: 1.3491 | acc_train: 0.5583 acc_val: 0.5625 | f1_train: 0.2530 f1_val: 0.2519
Epoch: 0005 | loss_train: 1.3368 loss_val: 1.3023 | acc_train: 0.5833 acc_val: 0.5500 | f1_train: 0.2695 f1_val: 0.2494
Epoch: 0006 | loss_train: 1.2522 loss_val: 1.2631 | acc_train: 0.6500 acc_val: 0.5500 | f1_train: 0.3373 f1_val: 0.2525
Epoch: 0007 | loss_train: 1.1738 loss_val: 1.1884 | acc_train: 0.7333 acc_val: 0.5875 | f1_train: 0.6666 f1_val: 0.2697
Epoch: 0008 | loss_train: 1.0810 loss_val: 1.1792 | acc_train: 0.8000 acc_val: 0.5875 | f1_train: 0.7154 f1_val: 0.3230
Epoch: 0009 | loss_train: 1.0077 loss_val: 1.0809 | acc_train: 0.7833 acc_val: 0.6625 | f1_train: 0.5625 f1_val: 0.3970
Epoch: 0010 | loss_train: 0.9825 loss_val: 1.1155 | acc_train: 0.7167 acc_val: 0.5625 | f1_train: 0.6668 f1_val: 0.3251
Epoch: 0011 | loss_train: 0.8462 loss_val: 1.1087 | acc_train: 0.8417 acc_val: 0.5625 | f1_train: 0.7967 f1_val: 0.3248
Epoch: 0012 | loss_train: 0.7546 loss_val: 0.9837 | acc_train: 0.8583 acc_val: 0.6125 | f1_train: 0.8213 f1_val: 0.3388
Epoch: 0013 | loss_train: 0.6869 loss_val: 0.9986 | acc_train: 0.8250 acc_val: 0.5875 | f1_train: 0.7483 f1_val: 0.3607
Epoch: 0014 | loss_train: 0.5542 loss_val: 1.1408 | acc_train: 0.8917 acc_val: 0.5625 | f1_train: 0.8505 f1_val: 0.3446
Epoch: 0015 | loss_train: 0.4733 loss_val: 1.1739 | acc_train: 0.9083 acc_val: 0.5375 | f1_train: 0.8568 f1_val: 0.3305
Epoch: 0016 | loss_train: 0.3744 loss_val: 1.0864 | acc_train: 0.9333 acc_val: 0.5750 | f1_train: 0.8891 f1_val: 0.3961
Epoch: 0017 | loss_train: 0.3218 loss_val: 1.2144 | acc_train: 0.9583 acc_val: 0.5750 | f1_train: 0.9286 f1_val: 0.4005
Epoch: 0018 | loss_train: 0.2439 loss_val: 1.3155 | acc_train: 0.9833 acc_val: 0.5500 | f1_train: 0.9608 f1_val: 0.3786
Epoch: 0019 | loss_train: 0.1725 loss_val: 1.4103 | acc_train: 0.9833 acc_val: 0.5625 | f1_train: 0.9801 f1_val: 0.3705
Epoch: 0020 | loss_train: 0.1422 loss_val: 1.3457 | acc_train: 0.9833 acc_val: 0.6000 | f1_train: 0.9767 f1_val: 0.4075
Epoch: 0021 | loss_train: 0.0937 loss_val: 1.4064 | acc_train: 0.9917 acc_val: 0.6625 | f1_train: 0.9913 f1_val: 0.4520
Epoch: 0022 | loss_train: 0.0774 loss_val: 1.5010 | acc_train: 0.9917 acc_val: 0.6875 | f1_train: 0.9913 f1_val: 0.4795
Epoch: 0023 | loss_train: 0.0501 loss_val: 1.6218 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4529
Epoch: 0024 | loss_train: 0.0380 loss_val: 1.7308 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4382
Epoch: 0025 | loss_train: 0.0268 loss_val: 1.8074 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4481
Epoch: 0026 | loss_train: 0.0175 loss_val: 1.8897 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4686
Epoch: 0027 | loss_train: 0.0117 loss_val: 1.9946 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4471
Epoch: 0028 | loss_train: 0.0084 loss_val: 2.1197 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4471
Epoch: 0029 | loss_train: 0.0103 loss_val: 2.2379 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4263
Epoch: 0030 | loss_train: 0.0064 loss_val: 2.3979 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4478
Epoch: 0031 | loss_train: 0.0033 loss_val: 2.5350 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4413
Epoch: 0032 | loss_train: 0.0030 loss_val: 2.6614 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4767
Epoch: 0033 | loss_train: 0.0029 loss_val: 2.8321 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4767
Epoch: 0034 | loss_train: 0.0016 loss_val: 3.0084 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4988
Epoch: 0035 | loss_train: 0.0015 loss_val: 3.1920 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4857
Epoch: 0036 | loss_train: 0.0016 loss_val: 3.3398 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4857
Epoch: 0037 | loss_train: 0.0011 loss_val: 3.4543 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4750
Epoch: 0038 | loss_train: 0.0007 loss_val: 3.5409 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4750
Epoch: 0039 | loss_train: 0.0005 loss_val: 3.6093 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4798
Epoch: 0040 | loss_train: 0.0011 loss_val: 3.8177 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4663
Epoch: 0041 | loss_train: 0.0004 loss_val: 4.0917 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.5254
Epoch: 0042 | loss_train: 0.0004 loss_val: 4.2950 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.5254
Epoch: 0043 | loss_train: 0.0480 loss_val: 4.3510 | acc_train: 0.9917 acc_val: 0.6250 | f1_train: 0.9923 f1_val: 0.3630
Epoch: 0044 | loss_train: 1.4637 loss_val: 7.3924 | acc_train: 0.8667 acc_val: 0.2125 | f1_train: 0.7724 f1_val: 0.2077
Epoch: 0045 | loss_train: 4.5729 loss_val: 7.7649 | acc_train: 0.3667 acc_val: 0.1375 | f1_train: 0.4509 f1_val: 0.1421
Epoch: 0046 | loss_train: 7.8039 loss_val: 5.8437 | acc_train: 0.1333 acc_val: 0.1375 | f1_train: 0.1911 f1_val: 0.1635
Epoch: 0047 | loss_train: 5.8769 loss_val: 3.6172 | acc_train: 0.1333 acc_val: 0.2500 | f1_train: 0.1667 f1_val: 0.2993
Epoch: 0048 | loss_train: 2.9840 loss_val: 2.5186 | acc_train: 0.2583 acc_val: 0.4750 | f1_train: 0.3303 f1_val: 0.2801
Epoch: 0049 | loss_train: 1.6844 loss_val: 2.2815 | acc_train: 0.4917 acc_val: 0.6125 | f1_train: 0.3966 f1_val: 0.3551
Epoch: 0050 | loss_train: 0.9131 loss_val: 2.3997 | acc_train: 0.7167 acc_val: 0.4875 | f1_train: 0.5664 f1_val: 0.3031
Epoch: 0051 | loss_train: 0.9729 loss_val: 2.0251 | acc_train: 0.6417 acc_val: 0.6125 | f1_train: 0.4847 f1_val: 0.4365
Epoch: 0052 | loss_train: 0.6098 loss_val: 2.1641 | acc_train: 0.7750 acc_val: 0.5875 | f1_train: 0.5760 f1_val: 0.3869
Optimization Finished!
Train cost: 11.2900s
Loading 22th epoch
Test set results: loss= 2.0801 accuracy= 0.5686

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='n_layers', log_path='log/nagphormer/wisconsin/n_layers', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/wisconsin/n_layers')

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/n_layers', lr=0.001, max_epochs=1000, n_layers=1, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9365, Val loss: 1.9462 | Train F1: 0.25, Val F1: 0.09
Epoch: 001 | Train loss: 1.9262, Val loss: 1.9433 | Train F1: 0.39, Val F1: 0.13
Epoch: 002 | Train loss: 1.9152, Val loss: 1.9404 | Train F1: 0.51, Val F1: 0.17
Epoch: 003 | Train loss: 1.9034, Val loss: 1.9373 | Train F1: 0.73, Val F1: 0.21
Epoch: 004 | Train loss: 1.8907, Val loss: 1.9341 | Train F1: 0.83, Val F1: 0.25
Epoch: 005 | Train loss: 1.8771, Val loss: 1.9308 | Train F1: 0.91, Val F1: 0.33
Epoch: 006 | Train loss: 1.8623, Val loss: 1.9273 | Train F1: 0.93, Val F1: 0.36
Epoch: 007 | Train loss: 1.8465, Val loss: 1.9236 | Train F1: 0.96, Val F1: 0.41
Epoch: 008 | Train loss: 1.8296, Val loss: 1.9197 | Train F1: 0.97, Val F1: 0.44
Epoch: 009 | Train loss: 1.8116, Val loss: 1.9157 | Train F1: 0.97, Val F1: 0.48
Epoch: 010 | Train loss: 1.7925, Val loss: 1.9114 | Train F1: 0.97, Val F1: 0.50
Epoch: 011 | Train loss: 1.7723, Val loss: 1.9068 | Train F1: 0.98, Val F1: 0.52
Epoch: 012 | Train loss: 1.7512, Val loss: 1.9020 | Train F1: 0.98, Val F1: 0.54
Epoch: 013 | Train loss: 1.7291, Val loss: 1.8969 | Train F1: 0.98, Val F1: 0.54
Epoch: 014 | Train loss: 1.7063, Val loss: 1.8916 | Train F1: 0.98, Val F1: 0.55
Epoch: 015 | Train loss: 1.6828, Val loss: 1.8859 | Train F1: 0.98, Val F1: 0.57
Epoch: 016 | Train loss: 1.6588, Val loss: 1.8799 | Train F1: 0.99, Val F1: 0.59
Epoch: 017 | Train loss: 1.6344, Val loss: 1.8736 | Train F1: 0.99, Val F1: 0.62
Epoch: 018 | Train loss: 1.6097, Val loss: 1.8670 | Train F1: 0.99, Val F1: 0.63
Epoch: 019 | Train loss: 1.5849, Val loss: 1.8601 | Train F1: 1.00, Val F1: 0.64
Epoch: 020 | Train loss: 1.5600, Val loss: 1.8528 | Train F1: 1.00, Val F1: 0.65
Epoch: 021 | Train loss: 1.5353, Val loss: 1.8453 | Train F1: 1.00, Val F1: 0.66
Epoch: 022 | Train loss: 1.5110, Val loss: 1.8374 | Train F1: 1.00, Val F1: 0.67
Epoch: 023 | Train loss: 1.4871, Val loss: 1.8294 | Train F1: 1.00, Val F1: 0.67
Epoch: 024 | Train loss: 1.4638, Val loss: 1.8211 | Train F1: 1.00, Val F1: 0.67
Epoch: 025 | Train loss: 1.4413, Val loss: 1.8127 | Train F1: 1.00, Val F1: 0.69
Epoch: 026 | Train loss: 1.4197, Val loss: 1.8041 | Train F1: 1.00, Val F1: 0.69
Epoch: 027 | Train loss: 1.3990, Val loss: 1.7954 | Train F1: 1.00, Val F1: 0.70
Epoch: 028 | Train loss: 1.3795, Val loss: 1.7866 | Train F1: 1.00, Val F1: 0.70
Epoch: 029 | Train loss: 1.3611, Val loss: 1.7778 | Train F1: 1.00, Val F1: 0.71
Epoch: 030 | Train loss: 1.3439, Val loss: 1.7691 | Train F1: 1.00, Val F1: 0.71
Epoch: 031 | Train loss: 1.3280, Val loss: 1.7604 | Train F1: 1.00, Val F1: 0.72
Epoch: 032 | Train loss: 1.3132, Val loss: 1.7519 | Train F1: 1.00, Val F1: 0.71
Epoch: 033 | Train loss: 1.2997, Val loss: 1.7435 | Train F1: 1.00, Val F1: 0.71
Epoch: 034 | Train loss: 1.2874, Val loss: 1.7353 | Train F1: 1.00, Val F1: 0.72
Epoch: 035 | Train loss: 1.2761, Val loss: 1.7274 | Train F1: 1.00, Val F1: 0.71
Epoch: 036 | Train loss: 1.2659, Val loss: 1.7197 | Train F1: 1.00, Val F1: 0.71
Epoch: 037 | Train loss: 1.2567, Val loss: 1.7123 | Train F1: 1.00, Val F1: 0.71
Epoch: 038 | Train loss: 1.2484, Val loss: 1.7052 | Train F1: 1.00, Val F1: 0.71
Epoch: 039 | Train loss: 1.2409, Val loss: 1.6983 | Train F1: 1.00, Val F1: 0.71
Epoch: 040 | Train loss: 1.2341, Val loss: 1.6918 | Train F1: 1.00, Val F1: 0.71
Epoch: 041 | Train loss: 1.2280, Val loss: 1.6856 | Train F1: 1.00, Val F1: 0.71
Epoch: 042 | Train loss: 1.2225, Val loss: 1.6797 | Train F1: 1.00, Val F1: 0.71
Epoch: 043 | Train loss: 1.2176, Val loss: 1.6741 | Train F1: 1.00, Val F1: 0.71
Epoch: 044 | Train loss: 1.2132, Val loss: 1.6687 | Train F1: 1.00, Val F1: 0.71
Epoch: 045 | Train loss: 1.2092, Val loss: 1.6637 | Train F1: 1.00, Val F1: 0.71
Epoch: 046 | Train loss: 1.2056, Val loss: 1.6589 | Train F1: 1.00, Val F1: 0.72
Epoch: 047 | Train loss: 1.2023, Val loss: 1.6544 | Train F1: 1.00, Val F1: 0.72
Epoch: 048 | Train loss: 1.1994, Val loss: 1.6502 | Train F1: 1.00, Val F1: 0.72
Epoch: 049 | Train loss: 1.1968, Val loss: 1.6462 | Train F1: 1.00, Val F1: 0.72
Epoch: 050 | Train loss: 1.1944, Val loss: 1.6424 | Train F1: 1.00, Val F1: 0.72
Epoch: 051 | Train loss: 1.1923, Val loss: 1.6389 | Train F1: 1.00, Val F1: 0.72
Epoch: 052 | Train loss: 1.1903, Val loss: 1.6357 | Train F1: 1.00, Val F1: 0.72
Epoch: 053 | Train loss: 1.1886, Val loss: 1.6326 | Train F1: 1.00, Val F1: 0.72
Epoch: 054 | Train loss: 1.1870, Val loss: 1.6297 | Train F1: 1.00, Val F1: 0.72
Epoch: 055 | Train loss: 1.1855, Val loss: 1.6270 | Train F1: 1.00, Val F1: 0.72
Epoch: 056 | Train loss: 1.1842, Val loss: 1.6244 | Train F1: 1.00, Val F1: 0.73
Epoch: 057 | Train loss: 1.1831, Val loss: 1.6220 | Train F1: 1.00, Val F1: 0.73
Epoch: 058 | Train loss: 1.1820, Val loss: 1.6197 | Train F1: 1.00, Val F1: 0.73
Epoch: 059 | Train loss: 1.1810, Val loss: 1.6176 | Train F1: 1.00, Val F1: 0.73
Epoch: 060 | Train loss: 1.1801, Val loss: 1.6156 | Train F1: 1.00, Val F1: 0.73
Epoch: 061 | Train loss: 1.1793, Val loss: 1.6137 | Train F1: 1.00, Val F1: 0.73
Epoch: 062 | Train loss: 1.1785, Val loss: 1.6120 | Train F1: 1.00, Val F1: 0.73
Epoch: 063 | Train loss: 1.1779, Val loss: 1.6103 | Train F1: 1.00, Val F1: 0.72
Epoch: 064 | Train loss: 1.1772, Val loss: 1.6087 | Train F1: 1.00, Val F1: 0.72
Epoch: 065 | Train loss: 1.1767, Val loss: 1.6072 | Train F1: 1.00, Val F1: 0.72
Epoch: 066 | Train loss: 1.1761, Val loss: 1.6058 | Train F1: 1.00, Val F1: 0.72
Epoch: 067 | Train loss: 1.1756, Val loss: 1.6045 | Train F1: 1.00, Val F1: 0.72
Epoch: 068 | Train loss: 1.1752, Val loss: 1.6033 | Train F1: 1.00, Val F1: 0.72
Epoch: 069 | Train loss: 1.1748, Val loss: 1.6021 | Train F1: 1.00, Val F1: 0.73
Epoch: 070 | Train loss: 1.1744, Val loss: 1.6009 | Train F1: 1.00, Val F1: 0.73
Epoch: 071 | Train loss: 1.1741, Val loss: 1.5999 | Train F1: 1.00, Val F1: 0.73
Epoch: 072 | Train loss: 1.1737, Val loss: 1.5988 | Train F1: 1.00, Val F1: 0.73
Epoch: 073 | Train loss: 1.1734, Val loss: 1.5979 | Train F1: 1.00, Val F1: 0.73
Epoch: 074 | Train loss: 1.1732, Val loss: 1.5970 | Train F1: 1.00, Val F1: 0.72
Epoch: 075 | Train loss: 1.1729, Val loss: 1.5961 | Train F1: 1.00, Val F1: 0.72
Epoch: 076 | Train loss: 1.1726, Val loss: 1.5952 | Train F1: 1.00, Val F1: 0.72
Epoch: 077 | Train loss: 1.1724, Val loss: 1.5944 | Train F1: 1.00, Val F1: 0.72
Epoch: 078 | Train loss: 1.1722, Val loss: 1.5937 | Train F1: 1.00, Val F1: 0.72
Epoch: 079 | Train loss: 1.1720, Val loss: 1.5929 | Train F1: 1.00, Val F1: 0.72
Epoch: 080 | Train loss: 1.1718, Val loss: 1.5922 | Train F1: 1.00, Val F1: 0.72
Epoch: 081 | Train loss: 1.1716, Val loss: 1.5915 | Train F1: 1.00, Val F1: 0.72
Epoch: 082 | Train loss: 1.1715, Val loss: 1.5908 | Train F1: 1.00, Val F1: 0.72
Epoch: 083 | Train loss: 1.1713, Val loss: 1.5902 | Train F1: 1.00, Val F1: 0.72
Epoch: 084 | Train loss: 1.1712, Val loss: 1.5895 | Train F1: 1.00, Val F1: 0.72
Epoch: 085 | Train loss: 1.1710, Val loss: 1.5889 | Train F1: 1.00, Val F1: 0.72
Epoch: 086 | Train loss: 1.1709, Val loss: 1.5883 | Train F1: 1.00, Val F1: 0.72
Epoch: 087 | Train loss: 1.1708, Val loss: 1.5877 | Train F1: 1.00, Val F1: 0.72
Epoch: 088 | Train loss: 1.1706, Val loss: 1.5872 | Train F1: 1.00, Val F1: 0.72
Epoch: 089 | Train loss: 1.1705, Val loss: 1.5866 | Train F1: 1.00, Val F1: 0.72
Epoch: 090 | Train loss: 1.1704, Val loss: 1.5860 | Train F1: 1.00, Val F1: 0.72
Epoch: 091 | Train loss: 1.1703, Val loss: 1.5855 | Train F1: 1.00, Val F1: 0.72
Epoch: 092 | Train loss: 1.1702, Val loss: 1.5850 | Train F1: 1.00, Val F1: 0.72
Epoch: 093 | Train loss: 1.1701, Val loss: 1.5845 | Train F1: 1.00, Val F1: 0.72
Epoch: 094 | Train loss: 1.1700, Val loss: 1.5840 | Train F1: 1.00, Val F1: 0.72
Epoch: 095 | Train loss: 1.1699, Val loss: 1.5835 | Train F1: 1.00, Val F1: 0.72
Epoch: 096 | Train loss: 1.1699, Val loss: 1.5831 | Train F1: 1.00, Val F1: 0.72
Epoch: 097 | Train loss: 1.1698, Val loss: 1.5826 | Train F1: 1.00, Val F1: 0.72
Epoch: 098 | Train loss: 1.1697, Val loss: 1.5821 | Train F1: 1.00, Val F1: 0.72
Epoch: 099 | Train loss: 1.1696, Val loss: 1.5817 | Train F1: 1.00, Val F1: 0.72
Epoch: 100 | Train loss: 1.1695, Val loss: 1.5812 | Train F1: 1.00, Val F1: 0.72
Epoch: 101 | Train loss: 1.1695, Val loss: 1.5808 | Train F1: 1.00, Val F1: 0.72
Epoch: 102 | Train loss: 1.1694, Val loss: 1.5803 | Train F1: 1.00, Val F1: 0.72
Epoch: 103 | Train loss: 1.1693, Val loss: 1.5799 | Train F1: 1.00, Val F1: 0.72
Epoch: 104 | Train loss: 1.1693, Val loss: 1.5795 | Train F1: 1.00, Val F1: 0.72
Epoch: 105 | Train loss: 1.1692, Val loss: 1.5791 | Train F1: 1.00, Val F1: 0.72
Epoch: 106 | Train loss: 1.1692, Val loss: 1.5787 | Train F1: 1.00, Val F1: 0.72
Epoch: 107 | Train loss: 1.1691, Val loss: 1.5783 | Train F1: 1.00, Val F1: 0.72
Epoch: 108 | Train loss: 1.1690, Val loss: 1.5779 | Train F1: 1.00, Val F1: 0.72
Epoch: 109 | Train loss: 1.1690, Val loss: 1.5776 | Train F1: 1.00, Val F1: 0.72
Epoch: 110 | Train loss: 1.1689, Val loss: 1.5772 | Train F1: 1.00, Val F1: 0.72
Epoch: 111 | Train loss: 1.1689, Val loss: 1.5768 | Train F1: 1.00, Val F1: 0.72
Epoch: 112 | Train loss: 1.1688, Val loss: 1.5765 | Train F1: 1.00, Val F1: 0.72
Epoch: 113 | Train loss: 1.1688, Val loss: 1.5761 | Train F1: 1.00, Val F1: 0.72
Epoch: 114 | Train loss: 1.1687, Val loss: 1.5757 | Train F1: 1.00, Val F1: 0.72
Epoch: 115 | Train loss: 1.1687, Val loss: 1.5754 | Train F1: 1.00, Val F1: 0.72
Epoch: 116 | Train loss: 1.1686, Val loss: 1.5750 | Train F1: 1.00, Val F1: 0.72
Epoch: 117 | Train loss: 1.1686, Val loss: 1.5747 | Train F1: 1.00, Val F1: 0.72
Epoch: 118 | Train loss: 1.1686, Val loss: 1.5744 | Train F1: 1.00, Val F1: 0.72
Epoch: 119 | Train loss: 1.1685, Val loss: 1.5740 | Train F1: 1.00, Val F1: 0.72
Epoch: 120 | Train loss: 1.1685, Val loss: 1.5737 | Train F1: 1.00, Val F1: 0.72
Epoch: 121 | Train loss: 1.1684, Val loss: 1.5734 | Train F1: 1.00, Val F1: 0.72
Epoch: 122 | Train loss: 1.1684, Val loss: 1.5731 | Train F1: 1.00, Val F1: 0.72
Epoch: 123 | Train loss: 1.1684, Val loss: 1.5727 | Train F1: 1.00, Val F1: 0.72
Epoch: 124 | Train loss: 1.1683, Val loss: 1.5724 | Train F1: 1.00, Val F1: 0.72
Epoch: 125 | Train loss: 1.1683, Val loss: 1.5721 | Train F1: 1.00, Val F1: 0.72
Epoch: 126 | Train loss: 1.1682, Val loss: 1.5718 | Train F1: 1.00, Val F1: 0.72
Epoch: 127 | Train loss: 1.1682, Val loss: 1.5715 | Train F1: 1.00, Val F1: 0.72
Epoch: 128 | Train loss: 1.1682, Val loss: 1.5712 | Train F1: 1.00, Val F1: 0.72
Epoch: 129 | Train loss: 1.1681, Val loss: 1.5709 | Train F1: 1.00, Val F1: 0.72
Epoch: 130 | Train loss: 1.1681, Val loss: 1.5706 | Train F1: 1.00, Val F1: 0.72
Epoch: 131 | Train loss: 1.1681, Val loss: 1.5703 | Train F1: 1.00, Val F1: 0.72
Epoch: 132 | Train loss: 1.1680, Val loss: 1.5700 | Train F1: 1.00, Val F1: 0.72
Epoch: 133 | Train loss: 1.1680, Val loss: 1.5697 | Train F1: 1.00, Val F1: 0.72
Epoch: 134 | Train loss: 1.1680, Val loss: 1.5694 | Train F1: 1.00, Val F1: 0.72
Epoch: 135 | Train loss: 1.1680, Val loss: 1.5691 | Train F1: 1.00, Val F1: 0.72
Epoch: 136 | Train loss: 1.1679, Val loss: 1.5688 | Train F1: 1.00, Val F1: 0.72
Epoch: 137 | Train loss: 1.1679, Val loss: 1.5685 | Train F1: 1.00, Val F1: 0.72
Epoch: 138 | Train loss: 1.1679, Val loss: 1.5683 | Train F1: 1.00, Val F1: 0.72
Epoch: 139 | Train loss: 1.1678, Val loss: 1.5680 | Train F1: 1.00, Val F1: 0.72
Epoch: 140 | Train loss: 1.1678, Val loss: 1.5677 | Train F1: 1.00, Val F1: 0.72
Epoch: 141 | Train loss: 1.1678, Val loss: 1.5674 | Train F1: 1.00, Val F1: 0.72
Epoch: 142 | Train loss: 1.1678, Val loss: 1.5671 | Train F1: 1.00, Val F1: 0.72
Epoch: 143 | Train loss: 1.1677, Val loss: 1.5669 | Train F1: 1.00, Val F1: 0.72
Epoch: 144 | Train loss: 1.1677, Val loss: 1.5666 | Train F1: 1.00, Val F1: 0.72
Epoch: 145 | Train loss: 1.1677, Val loss: 1.5663 | Train F1: 1.00, Val F1: 0.72
Epoch: 146 | Train loss: 1.1677, Val loss: 1.5660 | Train F1: 1.00, Val F1: 0.72
Epoch: 147 | Train loss: 1.1676, Val loss: 1.5658 | Train F1: 1.00, Val F1: 0.72
Epoch: 148 | Train loss: 1.1676, Val loss: 1.5655 | Train F1: 1.00, Val F1: 0.72
Epoch: 149 | Train loss: 1.1676, Val loss: 1.5652 | Train F1: 1.00, Val F1: 0.72
Epoch: 150 | Train loss: 1.1676, Val loss: 1.5650 | Train F1: 1.00, Val F1: 0.72
Best model:
Train loss: 1.1685, Val loss: 1.5737, Test loss: 1.5758
Train F1: 1.00, Val F1: 0.72, Test F1: 0.71

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/n_layers', lr=0.001, max_epochs=1000, n_layers=2, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9392, Val loss: 1.9432 | Train F1: 0.35, Val F1: 0.20
Epoch: 001 | Train loss: 1.9318, Val loss: 1.9407 | Train F1: 0.65, Val F1: 0.29
Epoch: 002 | Train loss: 1.9237, Val loss: 1.9380 | Train F1: 0.86, Val F1: 0.39
Epoch: 003 | Train loss: 1.9145, Val loss: 1.9350 | Train F1: 0.94, Val F1: 0.51
Epoch: 004 | Train loss: 1.9041, Val loss: 1.9318 | Train F1: 0.97, Val F1: 0.55
Epoch: 005 | Train loss: 1.8923, Val loss: 1.9280 | Train F1: 0.98, Val F1: 0.61
Epoch: 006 | Train loss: 1.8788, Val loss: 1.9238 | Train F1: 0.99, Val F1: 0.62
Epoch: 007 | Train loss: 1.8633, Val loss: 1.9190 | Train F1: 0.99, Val F1: 0.63
Epoch: 008 | Train loss: 1.8458, Val loss: 1.9137 | Train F1: 0.99, Val F1: 0.64
Epoch: 009 | Train loss: 1.8258, Val loss: 1.9078 | Train F1: 0.99, Val F1: 0.65
Epoch: 010 | Train loss: 1.8033, Val loss: 1.9011 | Train F1: 0.99, Val F1: 0.65
Epoch: 011 | Train loss: 1.7782, Val loss: 1.8937 | Train F1: 0.99, Val F1: 0.66
Epoch: 012 | Train loss: 1.7504, Val loss: 1.8854 | Train F1: 0.99, Val F1: 0.67
Epoch: 013 | Train loss: 1.7201, Val loss: 1.8761 | Train F1: 0.99, Val F1: 0.67
Epoch: 014 | Train loss: 1.6875, Val loss: 1.8656 | Train F1: 0.99, Val F1: 0.68
Epoch: 015 | Train loss: 1.6529, Val loss: 1.8541 | Train F1: 0.99, Val F1: 0.68
Epoch: 016 | Train loss: 1.6168, Val loss: 1.8414 | Train F1: 0.99, Val F1: 0.69
Epoch: 017 | Train loss: 1.5798, Val loss: 1.8274 | Train F1: 0.99, Val F1: 0.70
Epoch: 018 | Train loss: 1.5423, Val loss: 1.8121 | Train F1: 0.99, Val F1: 0.70
Epoch: 019 | Train loss: 1.5050, Val loss: 1.7957 | Train F1: 0.99, Val F1: 0.71
Epoch: 020 | Train loss: 1.4685, Val loss: 1.7782 | Train F1: 0.99, Val F1: 0.72
Epoch: 021 | Train loss: 1.4334, Val loss: 1.7597 | Train F1: 0.99, Val F1: 0.73
Epoch: 022 | Train loss: 1.4001, Val loss: 1.7405 | Train F1: 0.99, Val F1: 0.73
Epoch: 023 | Train loss: 1.3691, Val loss: 1.7207 | Train F1: 0.99, Val F1: 0.73
Epoch: 024 | Train loss: 1.3407, Val loss: 1.7007 | Train F1: 0.99, Val F1: 0.74
Epoch: 025 | Train loss: 1.3150, Val loss: 1.6805 | Train F1: 0.99, Val F1: 0.75
Epoch: 026 | Train loss: 1.2921, Val loss: 1.6606 | Train F1: 0.99, Val F1: 0.75
Epoch: 027 | Train loss: 1.2721, Val loss: 1.6412 | Train F1: 0.99, Val F1: 0.76
Epoch: 028 | Train loss: 1.2547, Val loss: 1.6224 | Train F1: 0.99, Val F1: 0.77
Epoch: 029 | Train loss: 1.2398, Val loss: 1.6045 | Train F1: 0.99, Val F1: 0.77
Epoch: 030 | Train loss: 1.2273, Val loss: 1.5876 | Train F1: 0.99, Val F1: 0.77
Epoch: 031 | Train loss: 1.2169, Val loss: 1.5720 | Train F1: 0.99, Val F1: 0.78
Epoch: 032 | Train loss: 1.2083, Val loss: 1.5578 | Train F1: 0.99, Val F1: 0.78
Epoch: 033 | Train loss: 1.2012, Val loss: 1.5449 | Train F1: 0.99, Val F1: 0.78
Epoch: 034 | Train loss: 1.1953, Val loss: 1.5334 | Train F1: 0.99, Val F1: 0.77
Epoch: 035 | Train loss: 1.1905, Val loss: 1.5231 | Train F1: 1.00, Val F1: 0.78
Epoch: 036 | Train loss: 1.1865, Val loss: 1.5141 | Train F1: 1.00, Val F1: 0.78
Epoch: 037 | Train loss: 1.1831, Val loss: 1.5061 | Train F1: 1.00, Val F1: 0.78
Epoch: 038 | Train loss: 1.1803, Val loss: 1.4992 | Train F1: 1.00, Val F1: 0.77
Epoch: 039 | Train loss: 1.1779, Val loss: 1.4932 | Train F1: 1.00, Val F1: 0.77
Epoch: 040 | Train loss: 1.1758, Val loss: 1.4879 | Train F1: 1.00, Val F1: 0.77
Epoch: 041 | Train loss: 1.1741, Val loss: 1.4834 | Train F1: 1.00, Val F1: 0.76
Epoch: 042 | Train loss: 1.1727, Val loss: 1.4795 | Train F1: 1.00, Val F1: 0.77
Epoch: 043 | Train loss: 1.1715, Val loss: 1.4762 | Train F1: 1.00, Val F1: 0.77
Epoch: 044 | Train loss: 1.1706, Val loss: 1.4734 | Train F1: 1.00, Val F1: 0.77
Epoch: 045 | Train loss: 1.1698, Val loss: 1.4710 | Train F1: 1.00, Val F1: 0.76
Epoch: 046 | Train loss: 1.1692, Val loss: 1.4690 | Train F1: 1.00, Val F1: 0.77
Epoch: 047 | Train loss: 1.1688, Val loss: 1.4672 | Train F1: 1.00, Val F1: 0.77
Epoch: 048 | Train loss: 1.1684, Val loss: 1.4656 | Train F1: 1.00, Val F1: 0.77
Epoch: 049 | Train loss: 1.1681, Val loss: 1.4642 | Train F1: 1.00, Val F1: 0.77
Epoch: 050 | Train loss: 1.1678, Val loss: 1.4629 | Train F1: 1.00, Val F1: 0.77
Epoch: 051 | Train loss: 1.1676, Val loss: 1.4617 | Train F1: 1.00, Val F1: 0.77
Epoch: 052 | Train loss: 1.1674, Val loss: 1.4606 | Train F1: 1.00, Val F1: 0.77
Epoch: 053 | Train loss: 1.1672, Val loss: 1.4595 | Train F1: 1.00, Val F1: 0.77
Epoch: 054 | Train loss: 1.1671, Val loss: 1.4586 | Train F1: 1.00, Val F1: 0.77
Epoch: 055 | Train loss: 1.1670, Val loss: 1.4576 | Train F1: 1.00, Val F1: 0.77
Epoch: 056 | Train loss: 1.1669, Val loss: 1.4568 | Train F1: 1.00, Val F1: 0.76
Epoch: 057 | Train loss: 1.1668, Val loss: 1.4560 | Train F1: 1.00, Val F1: 0.76
Epoch: 058 | Train loss: 1.1667, Val loss: 1.4552 | Train F1: 1.00, Val F1: 0.76
Epoch: 059 | Train loss: 1.1666, Val loss: 1.4545 | Train F1: 1.00, Val F1: 0.76
Epoch: 060 | Train loss: 1.1666, Val loss: 1.4538 | Train F1: 1.00, Val F1: 0.76
Epoch: 061 | Train loss: 1.1665, Val loss: 1.4532 | Train F1: 1.00, Val F1: 0.76
Epoch: 062 | Train loss: 1.1664, Val loss: 1.4526 | Train F1: 1.00, Val F1: 0.76
Epoch: 063 | Train loss: 1.1664, Val loss: 1.4520 | Train F1: 1.00, Val F1: 0.76
Epoch: 064 | Train loss: 1.1664, Val loss: 1.4514 | Train F1: 1.00, Val F1: 0.76
Epoch: 065 | Train loss: 1.1663, Val loss: 1.4509 | Train F1: 1.00, Val F1: 0.76
Epoch: 066 | Train loss: 1.1663, Val loss: 1.4504 | Train F1: 1.00, Val F1: 0.76
Epoch: 067 | Train loss: 1.1663, Val loss: 1.4500 | Train F1: 1.00, Val F1: 0.76
Epoch: 068 | Train loss: 1.1662, Val loss: 1.4495 | Train F1: 1.00, Val F1: 0.76
Epoch: 069 | Train loss: 1.1662, Val loss: 1.4491 | Train F1: 1.00, Val F1: 0.76
Epoch: 070 | Train loss: 1.1662, Val loss: 1.4487 | Train F1: 1.00, Val F1: 0.76
Epoch: 071 | Train loss: 1.1662, Val loss: 1.4483 | Train F1: 1.00, Val F1: 0.76
Epoch: 072 | Train loss: 1.1661, Val loss: 1.4479 | Train F1: 1.00, Val F1: 0.76
Epoch: 073 | Train loss: 1.1661, Val loss: 1.4476 | Train F1: 1.00, Val F1: 0.75
Epoch: 074 | Train loss: 1.1661, Val loss: 1.4472 | Train F1: 1.00, Val F1: 0.75
Epoch: 075 | Train loss: 1.1661, Val loss: 1.4469 | Train F1: 1.00, Val F1: 0.75
Epoch: 076 | Train loss: 1.1661, Val loss: 1.4466 | Train F1: 1.00, Val F1: 0.75
Epoch: 077 | Train loss: 1.1661, Val loss: 1.4463 | Train F1: 1.00, Val F1: 0.75
Epoch: 078 | Train loss: 1.1660, Val loss: 1.4460 | Train F1: 1.00, Val F1: 0.75
Epoch: 079 | Train loss: 1.1660, Val loss: 1.4458 | Train F1: 1.00, Val F1: 0.75
Epoch: 080 | Train loss: 1.1660, Val loss: 1.4455 | Train F1: 1.00, Val F1: 0.76
Epoch: 081 | Train loss: 1.1660, Val loss: 1.4453 | Train F1: 1.00, Val F1: 0.76
Epoch: 082 | Train loss: 1.1660, Val loss: 1.4451 | Train F1: 1.00, Val F1: 0.76
Epoch: 083 | Train loss: 1.1660, Val loss: 1.4449 | Train F1: 1.00, Val F1: 0.76
Epoch: 084 | Train loss: 1.1660, Val loss: 1.4447 | Train F1: 1.00, Val F1: 0.76
Epoch: 085 | Train loss: 1.1660, Val loss: 1.4445 | Train F1: 1.00, Val F1: 0.76
Epoch: 086 | Train loss: 1.1660, Val loss: 1.4444 | Train F1: 1.00, Val F1: 0.76
Epoch: 087 | Train loss: 1.1660, Val loss: 1.4442 | Train F1: 1.00, Val F1: 0.76
Epoch: 088 | Train loss: 1.1659, Val loss: 1.4440 | Train F1: 1.00, Val F1: 0.76
Epoch: 089 | Train loss: 1.1659, Val loss: 1.4439 | Train F1: 1.00, Val F1: 0.76
Epoch: 090 | Train loss: 1.1659, Val loss: 1.4437 | Train F1: 1.00, Val F1: 0.76
Epoch: 091 | Train loss: 1.1659, Val loss: 1.4436 | Train F1: 1.00, Val F1: 0.76
Epoch: 092 | Train loss: 1.1659, Val loss: 1.4434 | Train F1: 1.00, Val F1: 0.76
Epoch: 093 | Train loss: 1.1659, Val loss: 1.4433 | Train F1: 1.00, Val F1: 0.76
Epoch: 094 | Train loss: 1.1659, Val loss: 1.4431 | Train F1: 1.00, Val F1: 0.76
Epoch: 095 | Train loss: 1.1659, Val loss: 1.4430 | Train F1: 1.00, Val F1: 0.76
Epoch: 096 | Train loss: 1.1659, Val loss: 1.4429 | Train F1: 1.00, Val F1: 0.76
Epoch: 097 | Train loss: 1.1659, Val loss: 1.4428 | Train F1: 1.00, Val F1: 0.76
Epoch: 098 | Train loss: 1.1659, Val loss: 1.4427 | Train F1: 1.00, Val F1: 0.76
Epoch: 099 | Train loss: 1.1659, Val loss: 1.4426 | Train F1: 1.00, Val F1: 0.76
Epoch: 100 | Train loss: 1.1659, Val loss: 1.4425 | Train F1: 1.00, Val F1: 0.76
Epoch: 101 | Train loss: 1.1659, Val loss: 1.4424 | Train F1: 1.00, Val F1: 0.76
Best model:
Train loss: 1.1662, Val loss: 1.4483, Test loss: 1.4338
Train F1: 1.00, Val F1: 0.76, Test F1: 0.78

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/n_layers', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9407, Val loss: 1.9455 | Train F1: 0.12, Val F1: 0.08
Epoch: 001 | Train loss: 1.9352, Val loss: 1.9436 | Train F1: 0.37, Val F1: 0.12
Epoch: 002 | Train loss: 1.9288, Val loss: 1.9413 | Train F1: 0.60, Val F1: 0.21
Epoch: 003 | Train loss: 1.9211, Val loss: 1.9385 | Train F1: 0.70, Val F1: 0.35
Epoch: 004 | Train loss: 1.9117, Val loss: 1.9352 | Train F1: 0.80, Val F1: 0.42
Epoch: 005 | Train loss: 1.9004, Val loss: 1.9313 | Train F1: 0.84, Val F1: 0.47
Epoch: 006 | Train loss: 1.8865, Val loss: 1.9266 | Train F1: 0.91, Val F1: 0.49
Epoch: 007 | Train loss: 1.8697, Val loss: 1.9209 | Train F1: 0.95, Val F1: 0.50
Epoch: 008 | Train loss: 1.8494, Val loss: 1.9141 | Train F1: 0.96, Val F1: 0.53
Epoch: 009 | Train loss: 1.8250, Val loss: 1.9060 | Train F1: 0.97, Val F1: 0.57
Epoch: 010 | Train loss: 1.7962, Val loss: 1.8963 | Train F1: 0.98, Val F1: 0.60
Epoch: 011 | Train loss: 1.7628, Val loss: 1.8848 | Train F1: 0.99, Val F1: 0.61
Epoch: 012 | Train loss: 1.7251, Val loss: 1.8711 | Train F1: 0.99, Val F1: 0.63
Epoch: 013 | Train loss: 1.6836, Val loss: 1.8551 | Train F1: 0.99, Val F1: 0.66
Epoch: 014 | Train loss: 1.6393, Val loss: 1.8365 | Train F1: 0.99, Val F1: 0.67
Epoch: 015 | Train loss: 1.5931, Val loss: 1.8151 | Train F1: 0.99, Val F1: 0.68
Epoch: 016 | Train loss: 1.5458, Val loss: 1.7910 | Train F1: 0.99, Val F1: 0.69
Epoch: 017 | Train loss: 1.4983, Val loss: 1.7645 | Train F1: 0.99, Val F1: 0.69
Epoch: 018 | Train loss: 1.4517, Val loss: 1.7361 | Train F1: 0.99, Val F1: 0.69
Epoch: 019 | Train loss: 1.4072, Val loss: 1.7063 | Train F1: 0.99, Val F1: 0.71
Epoch: 020 | Train loss: 1.3658, Val loss: 1.6763 | Train F1: 0.99, Val F1: 0.72
Epoch: 021 | Train loss: 1.3285, Val loss: 1.6469 | Train F1: 0.99, Val F1: 0.73
Epoch: 022 | Train loss: 1.2955, Val loss: 1.6188 | Train F1: 0.99, Val F1: 0.75
Epoch: 023 | Train loss: 1.2671, Val loss: 1.5919 | Train F1: 0.99, Val F1: 0.75
Epoch: 024 | Train loss: 1.2434, Val loss: 1.5666 | Train F1: 0.99, Val F1: 0.76
Epoch: 025 | Train loss: 1.2244, Val loss: 1.5435 | Train F1: 0.99, Val F1: 0.76
Epoch: 026 | Train loss: 1.2099, Val loss: 1.5230 | Train F1: 0.99, Val F1: 0.77
Epoch: 027 | Train loss: 1.1991, Val loss: 1.5051 | Train F1: 0.99, Val F1: 0.77
Epoch: 028 | Train loss: 1.1913, Val loss: 1.4903 | Train F1: 0.99, Val F1: 0.78
Epoch: 029 | Train loss: 1.1858, Val loss: 1.4784 | Train F1: 0.99, Val F1: 0.77
Epoch: 030 | Train loss: 1.1818, Val loss: 1.4688 | Train F1: 0.99, Val F1: 0.77
Epoch: 031 | Train loss: 1.1789, Val loss: 1.4612 | Train F1: 0.99, Val F1: 0.76
Epoch: 032 | Train loss: 1.1768, Val loss: 1.4551 | Train F1: 0.99, Val F1: 0.77
Epoch: 033 | Train loss: 1.1752, Val loss: 1.4505 | Train F1: 0.99, Val F1: 0.77
Epoch: 034 | Train loss: 1.1738, Val loss: 1.4471 | Train F1: 0.99, Val F1: 0.77
Epoch: 035 | Train loss: 1.1724, Val loss: 1.4447 | Train F1: 0.99, Val F1: 0.77
Epoch: 036 | Train loss: 1.1712, Val loss: 1.4428 | Train F1: 1.00, Val F1: 0.76
Epoch: 037 | Train loss: 1.1698, Val loss: 1.4418 | Train F1: 1.00, Val F1: 0.76
Epoch: 038 | Train loss: 1.1686, Val loss: 1.4415 | Train F1: 1.00, Val F1: 0.76
Epoch: 039 | Train loss: 1.1676, Val loss: 1.4417 | Train F1: 1.00, Val F1: 0.75
Epoch: 040 | Train loss: 1.1670, Val loss: 1.4422 | Train F1: 1.00, Val F1: 0.76
Epoch: 041 | Train loss: 1.1666, Val loss: 1.4430 | Train F1: 1.00, Val F1: 0.75
Epoch: 042 | Train loss: 1.1664, Val loss: 1.4438 | Train F1: 1.00, Val F1: 0.75
Epoch: 043 | Train loss: 1.1663, Val loss: 1.4445 | Train F1: 1.00, Val F1: 0.75
Epoch: 044 | Train loss: 1.1662, Val loss: 1.4451 | Train F1: 1.00, Val F1: 0.75
Epoch: 045 | Train loss: 1.1661, Val loss: 1.4456 | Train F1: 1.00, Val F1: 0.76
Epoch: 046 | Train loss: 1.1660, Val loss: 1.4457 | Train F1: 1.00, Val F1: 0.75
Epoch: 047 | Train loss: 1.1660, Val loss: 1.4454 | Train F1: 1.00, Val F1: 0.75
Epoch: 048 | Train loss: 1.1659, Val loss: 1.4450 | Train F1: 1.00, Val F1: 0.75
Epoch: 049 | Train loss: 1.1658, Val loss: 1.4442 | Train F1: 1.00, Val F1: 0.75
Epoch: 050 | Train loss: 1.1658, Val loss: 1.4432 | Train F1: 1.00, Val F1: 0.75
Epoch: 051 | Train loss: 1.1657, Val loss: 1.4421 | Train F1: 1.00, Val F1: 0.75
Epoch: 052 | Train loss: 1.1657, Val loss: 1.4410 | Train F1: 1.00, Val F1: 0.75
Epoch: 053 | Train loss: 1.1657, Val loss: 1.4399 | Train F1: 1.00, Val F1: 0.75
Epoch: 054 | Train loss: 1.1656, Val loss: 1.4388 | Train F1: 1.00, Val F1: 0.75
Epoch: 055 | Train loss: 1.1656, Val loss: 1.4378 | Train F1: 1.00, Val F1: 0.75
Epoch: 056 | Train loss: 1.1656, Val loss: 1.4367 | Train F1: 1.00, Val F1: 0.75
Epoch: 057 | Train loss: 1.1656, Val loss: 1.4358 | Train F1: 1.00, Val F1: 0.75
Epoch: 058 | Train loss: 1.1656, Val loss: 1.4348 | Train F1: 1.00, Val F1: 0.75
Epoch: 059 | Train loss: 1.1656, Val loss: 1.4340 | Train F1: 1.00, Val F1: 0.75
Epoch: 060 | Train loss: 1.1655, Val loss: 1.4331 | Train F1: 1.00, Val F1: 0.75
Epoch: 061 | Train loss: 1.1655, Val loss: 1.4324 | Train F1: 1.00, Val F1: 0.75
Epoch: 062 | Train loss: 1.1655, Val loss: 1.4316 | Train F1: 1.00, Val F1: 0.75
Epoch: 063 | Train loss: 1.1655, Val loss: 1.4308 | Train F1: 1.00, Val F1: 0.75
Epoch: 064 | Train loss: 1.1655, Val loss: 1.4301 | Train F1: 1.00, Val F1: 0.75
Epoch: 065 | Train loss: 1.1655, Val loss: 1.4294 | Train F1: 1.00, Val F1: 0.76
Epoch: 066 | Train loss: 1.1655, Val loss: 1.4287 | Train F1: 1.00, Val F1: 0.76
Epoch: 067 | Train loss: 1.1655, Val loss: 1.4281 | Train F1: 1.00, Val F1: 0.76
Epoch: 068 | Train loss: 1.1655, Val loss: 1.4276 | Train F1: 1.00, Val F1: 0.76
Epoch: 069 | Train loss: 1.1655, Val loss: 1.4270 | Train F1: 1.00, Val F1: 0.76
Epoch: 070 | Train loss: 1.1655, Val loss: 1.4265 | Train F1: 1.00, Val F1: 0.76
Epoch: 071 | Train loss: 1.1655, Val loss: 1.4260 | Train F1: 1.00, Val F1: 0.76
Epoch: 072 | Train loss: 1.1655, Val loss: 1.4256 | Train F1: 1.00, Val F1: 0.76
Epoch: 073 | Train loss: 1.1655, Val loss: 1.4252 | Train F1: 1.00, Val F1: 0.76
Epoch: 074 | Train loss: 1.1655, Val loss: 1.4248 | Train F1: 1.00, Val F1: 0.76
Epoch: 075 | Train loss: 1.1655, Val loss: 1.4244 | Train F1: 1.00, Val F1: 0.76
Epoch: 076 | Train loss: 1.1655, Val loss: 1.4241 | Train F1: 1.00, Val F1: 0.75
Epoch: 077 | Train loss: 1.1655, Val loss: 1.4238 | Train F1: 1.00, Val F1: 0.75
Epoch: 078 | Train loss: 1.1655, Val loss: 1.4235 | Train F1: 1.00, Val F1: 0.75
Epoch: 079 | Train loss: 1.1655, Val loss: 1.4233 | Train F1: 1.00, Val F1: 0.75
Epoch: 080 | Train loss: 1.1655, Val loss: 1.4230 | Train F1: 1.00, Val F1: 0.75
Epoch: 081 | Train loss: 1.1655, Val loss: 1.4228 | Train F1: 1.00, Val F1: 0.75
Epoch: 082 | Train loss: 1.1655, Val loss: 1.4225 | Train F1: 1.00, Val F1: 0.75
Epoch: 083 | Train loss: 1.1655, Val loss: 1.4223 | Train F1: 1.00, Val F1: 0.75
Epoch: 084 | Train loss: 1.1655, Val loss: 1.4221 | Train F1: 1.00, Val F1: 0.75
Epoch: 085 | Train loss: 1.1655, Val loss: 1.4219 | Train F1: 1.00, Val F1: 0.75
Epoch: 086 | Train loss: 1.1655, Val loss: 1.4217 | Train F1: 1.00, Val F1: 0.75
Epoch: 087 | Train loss: 1.1655, Val loss: 1.4215 | Train F1: 1.00, Val F1: 0.75
Epoch: 088 | Train loss: 1.1655, Val loss: 1.4213 | Train F1: 1.00, Val F1: 0.75
Epoch: 089 | Train loss: 1.1655, Val loss: 1.4211 | Train F1: 1.00, Val F1: 0.75
Epoch: 090 | Train loss: 1.1655, Val loss: 1.4209 | Train F1: 1.00, Val F1: 0.75
Epoch: 091 | Train loss: 1.1655, Val loss: 1.4208 | Train F1: 1.00, Val F1: 0.75
Epoch: 092 | Train loss: 1.1655, Val loss: 1.4206 | Train F1: 1.00, Val F1: 0.75
Epoch: 093 | Train loss: 1.1655, Val loss: 1.4205 | Train F1: 1.00, Val F1: 0.75
Epoch: 094 | Train loss: 1.1655, Val loss: 1.4203 | Train F1: 1.00, Val F1: 0.75
Epoch: 095 | Train loss: 1.1655, Val loss: 1.4202 | Train F1: 1.00, Val F1: 0.75
Best model:
Train loss: 1.1655, Val loss: 1.4294, Test loss: 1.4165
Train F1: 1.00, Val F1: 0.76, Test F1: 0.76

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/n_layers', lr=0.001, max_epochs=1000, n_layers=4, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9419, Val loss: 1.9417 | Train F1: 0.09, Val F1: 0.04
Epoch: 001 | Train loss: 1.9370, Val loss: 1.9395 | Train F1: 0.20, Val F1: 0.09
Epoch: 002 | Train loss: 1.9309, Val loss: 1.9369 | Train F1: 0.31, Val F1: 0.20
Epoch: 003 | Train loss: 1.9229, Val loss: 1.9334 | Train F1: 0.45, Val F1: 0.26
Epoch: 004 | Train loss: 1.9124, Val loss: 1.9289 | Train F1: 0.50, Val F1: 0.27
Epoch: 005 | Train loss: 1.8985, Val loss: 1.9231 | Train F1: 0.62, Val F1: 0.36
Epoch: 006 | Train loss: 1.8803, Val loss: 1.9155 | Train F1: 0.74, Val F1: 0.44
Epoch: 007 | Train loss: 1.8568, Val loss: 1.9057 | Train F1: 0.78, Val F1: 0.49
Epoch: 008 | Train loss: 1.8267, Val loss: 1.8934 | Train F1: 0.83, Val F1: 0.50
Epoch: 009 | Train loss: 1.7895, Val loss: 1.8780 | Train F1: 0.85, Val F1: 0.53
Epoch: 010 | Train loss: 1.7457, Val loss: 1.8591 | Train F1: 0.85, Val F1: 0.54
Epoch: 011 | Train loss: 1.6968, Val loss: 1.8361 | Train F1: 0.85, Val F1: 0.54
Epoch: 012 | Train loss: 1.6454, Val loss: 1.8087 | Train F1: 0.85, Val F1: 0.56
Epoch: 013 | Train loss: 1.5933, Val loss: 1.7767 | Train F1: 0.83, Val F1: 0.57
Epoch: 014 | Train loss: 1.5423, Val loss: 1.7409 | Train F1: 0.81, Val F1: 0.56
Epoch: 015 | Train loss: 1.4937, Val loss: 1.7022 | Train F1: 0.78, Val F1: 0.58
Epoch: 016 | Train loss: 1.4491, Val loss: 1.6622 | Train F1: 0.78, Val F1: 0.59
Epoch: 017 | Train loss: 1.4096, Val loss: 1.6229 | Train F1: 0.78, Val F1: 0.61
Epoch: 018 | Train loss: 1.3760, Val loss: 1.5874 | Train F1: 0.78, Val F1: 0.62
Epoch: 019 | Train loss: 1.3475, Val loss: 1.5579 | Train F1: 0.79, Val F1: 0.63
Epoch: 020 | Train loss: 1.3227, Val loss: 1.5337 | Train F1: 0.81, Val F1: 0.65
Epoch: 021 | Train loss: 1.2998, Val loss: 1.5131 | Train F1: 0.89, Val F1: 0.68
Epoch: 022 | Train loss: 1.2777, Val loss: 1.4949 | Train F1: 0.94, Val F1: 0.72
Epoch: 023 | Train loss: 1.2560, Val loss: 1.4779 | Train F1: 0.99, Val F1: 0.76
Epoch: 024 | Train loss: 1.2371, Val loss: 1.4635 | Train F1: 0.99, Val F1: 0.77
Epoch: 025 | Train loss: 1.2232, Val loss: 1.4522 | Train F1: 0.99, Val F1: 0.77
Epoch: 026 | Train loss: 1.2141, Val loss: 1.4449 | Train F1: 0.99, Val F1: 0.76
Epoch: 027 | Train loss: 1.2044, Val loss: 1.4370 | Train F1: 0.99, Val F1: 0.76
Epoch: 028 | Train loss: 1.1941, Val loss: 1.4283 | Train F1: 0.99, Val F1: 0.77
Epoch: 029 | Train loss: 1.1860, Val loss: 1.4212 | Train F1: 0.99, Val F1: 0.77
Epoch: 030 | Train loss: 1.1809, Val loss: 1.4167 | Train F1: 0.99, Val F1: 0.77
Epoch: 031 | Train loss: 1.1781, Val loss: 1.4144 | Train F1: 0.99, Val F1: 0.77
Epoch: 032 | Train loss: 1.1764, Val loss: 1.4132 | Train F1: 0.99, Val F1: 0.78
Epoch: 033 | Train loss: 1.1753, Val loss: 1.4125 | Train F1: 0.99, Val F1: 0.78
Epoch: 034 | Train loss: 1.1744, Val loss: 1.4119 | Train F1: 0.99, Val F1: 0.78
Epoch: 035 | Train loss: 1.1739, Val loss: 1.4113 | Train F1: 0.99, Val F1: 0.78
Epoch: 036 | Train loss: 1.1735, Val loss: 1.4109 | Train F1: 0.99, Val F1: 0.77
Epoch: 037 | Train loss: 1.1732, Val loss: 1.4104 | Train F1: 0.99, Val F1: 0.77
Epoch: 038 | Train loss: 1.1731, Val loss: 1.4097 | Train F1: 0.99, Val F1: 0.77
Epoch: 039 | Train loss: 1.1729, Val loss: 1.4091 | Train F1: 0.99, Val F1: 0.77
Epoch: 040 | Train loss: 1.1728, Val loss: 1.4085 | Train F1: 0.99, Val F1: 0.77
Epoch: 041 | Train loss: 1.1728, Val loss: 1.4080 | Train F1: 0.99, Val F1: 0.77
Epoch: 042 | Train loss: 1.1727, Val loss: 1.4075 | Train F1: 0.99, Val F1: 0.77
Epoch: 043 | Train loss: 1.1727, Val loss: 1.4071 | Train F1: 0.99, Val F1: 0.77
Epoch: 044 | Train loss: 1.1727, Val loss: 1.4067 | Train F1: 0.99, Val F1: 0.77
Epoch: 045 | Train loss: 1.1726, Val loss: 1.4063 | Train F1: 0.99, Val F1: 0.77
Epoch: 046 | Train loss: 1.1726, Val loss: 1.4059 | Train F1: 0.99, Val F1: 0.77
Epoch: 047 | Train loss: 1.1726, Val loss: 1.4054 | Train F1: 0.99, Val F1: 0.77
Epoch: 048 | Train loss: 1.1726, Val loss: 1.4049 | Train F1: 0.99, Val F1: 0.77
Epoch: 049 | Train loss: 1.1725, Val loss: 1.4045 | Train F1: 0.99, Val F1: 0.77
Epoch: 050 | Train loss: 1.1725, Val loss: 1.4041 | Train F1: 0.99, Val F1: 0.77
Epoch: 051 | Train loss: 1.1724, Val loss: 1.4036 | Train F1: 0.99, Val F1: 0.77
Epoch: 052 | Train loss: 1.1724, Val loss: 1.4033 | Train F1: 0.99, Val F1: 0.77
Epoch: 053 | Train loss: 1.1722, Val loss: 1.4030 | Train F1: 0.99, Val F1: 0.77
Epoch: 054 | Train loss: 1.1720, Val loss: 1.4027 | Train F1: 0.99, Val F1: 0.77
Epoch: 055 | Train loss: 1.1714, Val loss: 1.4026 | Train F1: 0.99, Val F1: 0.77
Epoch: 056 | Train loss: 1.1700, Val loss: 1.4026 | Train F1: 0.99, Val F1: 0.77
Epoch: 057 | Train loss: 1.1675, Val loss: 1.4026 | Train F1: 1.00, Val F1: 0.77
Epoch: 058 | Train loss: 1.1661, Val loss: 1.4022 | Train F1: 1.00, Val F1: 0.76
Epoch: 059 | Train loss: 1.1660, Val loss: 1.4018 | Train F1: 1.00, Val F1: 0.76
Epoch: 060 | Train loss: 1.1665, Val loss: 1.4013 | Train F1: 1.00, Val F1: 0.76
Epoch: 061 | Train loss: 1.1674, Val loss: 1.4011 | Train F1: 1.00, Val F1: 0.76
Epoch: 062 | Train loss: 1.1685, Val loss: 1.4012 | Train F1: 1.00, Val F1: 0.76
Epoch: 063 | Train loss: 1.1685, Val loss: 1.4009 | Train F1: 1.00, Val F1: 0.76
Epoch: 064 | Train loss: 1.1680, Val loss: 1.4004 | Train F1: 1.00, Val F1: 0.76
Epoch: 065 | Train loss: 1.1668, Val loss: 1.3999 | Train F1: 1.00, Val F1: 0.76
Epoch: 066 | Train loss: 1.1659, Val loss: 1.3997 | Train F1: 1.00, Val F1: 0.76
Epoch: 067 | Train loss: 1.1656, Val loss: 1.4002 | Train F1: 1.00, Val F1: 0.75
Epoch: 068 | Train loss: 1.1655, Val loss: 1.4012 | Train F1: 1.00, Val F1: 0.76
Epoch: 069 | Train loss: 1.1655, Val loss: 1.4021 | Train F1: 1.00, Val F1: 0.76
Epoch: 070 | Train loss: 1.1655, Val loss: 1.4029 | Train F1: 1.00, Val F1: 0.76
Epoch: 071 | Train loss: 1.1655, Val loss: 1.4035 | Train F1: 1.00, Val F1: 0.75
Epoch: 072 | Train loss: 1.1656, Val loss: 1.4041 | Train F1: 1.00, Val F1: 0.75
Epoch: 073 | Train loss: 1.1657, Val loss: 1.4047 | Train F1: 1.00, Val F1: 0.75
Epoch: 074 | Train loss: 1.1657, Val loss: 1.4051 | Train F1: 1.00, Val F1: 0.75
Epoch: 075 | Train loss: 1.1657, Val loss: 1.4052 | Train F1: 1.00, Val F1: 0.75
Epoch: 076 | Train loss: 1.1657, Val loss: 1.4052 | Train F1: 1.00, Val F1: 0.75
Epoch: 077 | Train loss: 1.1656, Val loss: 1.4052 | Train F1: 1.00, Val F1: 0.75
Epoch: 078 | Train loss: 1.1656, Val loss: 1.4052 | Train F1: 1.00, Val F1: 0.75
Epoch: 079 | Train loss: 1.1656, Val loss: 1.4049 | Train F1: 1.00, Val F1: 0.75
Epoch: 080 | Train loss: 1.1655, Val loss: 1.4046 | Train F1: 1.00, Val F1: 0.76
Epoch: 081 | Train loss: 1.1655, Val loss: 1.4043 | Train F1: 1.00, Val F1: 0.76
Epoch: 082 | Train loss: 1.1655, Val loss: 1.4040 | Train F1: 1.00, Val F1: 0.76
Epoch: 083 | Train loss: 1.1655, Val loss: 1.4038 | Train F1: 1.00, Val F1: 0.76
Epoch: 084 | Train loss: 1.1655, Val loss: 1.4036 | Train F1: 1.00, Val F1: 0.76
Epoch: 085 | Train loss: 1.1655, Val loss: 1.4034 | Train F1: 1.00, Val F1: 0.76
Epoch: 086 | Train loss: 1.1655, Val loss: 1.4032 | Train F1: 1.00, Val F1: 0.76
Epoch: 087 | Train loss: 1.1655, Val loss: 1.4031 | Train F1: 1.00, Val F1: 0.76
Epoch: 088 | Train loss: 1.1655, Val loss: 1.4030 | Train F1: 1.00, Val F1: 0.76
Epoch: 089 | Train loss: 1.1655, Val loss: 1.4029 | Train F1: 1.00, Val F1: 0.76
Epoch: 090 | Train loss: 1.1655, Val loss: 1.4028 | Train F1: 1.00, Val F1: 0.76
Epoch: 091 | Train loss: 1.1655, Val loss: 1.4027 | Train F1: 1.00, Val F1: 0.76
Epoch: 092 | Train loss: 1.1655, Val loss: 1.4025 | Train F1: 1.00, Val F1: 0.76
Epoch: 093 | Train loss: 1.1655, Val loss: 1.4025 | Train F1: 1.00, Val F1: 0.76
Best model:
Train loss: 1.1685, Val loss: 1.4009, Test loss: 1.3786
Train F1: 1.00, Val F1: 0.76, Test F1: 0.78

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/n_layers', lr=0.001, max_epochs=1000, n_layers=5, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9428, Val loss: 1.9436 | Train F1: 0.21, Val F1: 0.17
Epoch: 001 | Train loss: 1.9390, Val loss: 1.9423 | Train F1: 0.36, Val F1: 0.17
Epoch: 002 | Train loss: 1.9339, Val loss: 1.9404 | Train F1: 0.46, Val F1: 0.25
Epoch: 003 | Train loss: 1.9266, Val loss: 1.9373 | Train F1: 0.71, Val F1: 0.36
Epoch: 004 | Train loss: 1.9163, Val loss: 1.9330 | Train F1: 0.80, Val F1: 0.48
Epoch: 005 | Train loss: 1.9016, Val loss: 1.9271 | Train F1: 0.87, Val F1: 0.56
Epoch: 006 | Train loss: 1.8811, Val loss: 1.9192 | Train F1: 0.91, Val F1: 0.58
Epoch: 007 | Train loss: 1.8534, Val loss: 1.9085 | Train F1: 0.91, Val F1: 0.60
Epoch: 008 | Train loss: 1.8184, Val loss: 1.8946 | Train F1: 0.92, Val F1: 0.61
Epoch: 009 | Train loss: 1.7778, Val loss: 1.8762 | Train F1: 0.91, Val F1: 0.62
Epoch: 010 | Train loss: 1.7325, Val loss: 1.8523 | Train F1: 0.91, Val F1: 0.64
Epoch: 011 | Train loss: 1.6813, Val loss: 1.8222 | Train F1: 0.94, Val F1: 0.66
Epoch: 012 | Train loss: 1.6238, Val loss: 1.7854 | Train F1: 0.94, Val F1: 0.67
Epoch: 013 | Train loss: 1.5617, Val loss: 1.7426 | Train F1: 0.96, Val F1: 0.68
Epoch: 014 | Train loss: 1.4979, Val loss: 1.6944 | Train F1: 0.96, Val F1: 0.70
Epoch: 015 | Train loss: 1.4345, Val loss: 1.6418 | Train F1: 0.98, Val F1: 0.72
Epoch: 016 | Train loss: 1.3739, Val loss: 1.5864 | Train F1: 0.97, Val F1: 0.74
Epoch: 017 | Train loss: 1.3220, Val loss: 1.5353 | Train F1: 0.96, Val F1: 0.75
Epoch: 018 | Train loss: 1.2827, Val loss: 1.4943 | Train F1: 0.96, Val F1: 0.75
Epoch: 019 | Train loss: 1.2546, Val loss: 1.4636 | Train F1: 0.96, Val F1: 0.76
Epoch: 020 | Train loss: 1.2334, Val loss: 1.4426 | Train F1: 0.98, Val F1: 0.76
Epoch: 021 | Train loss: 1.2167, Val loss: 1.4281 | Train F1: 0.98, Val F1: 0.77
Epoch: 022 | Train loss: 1.2034, Val loss: 1.4180 | Train F1: 0.99, Val F1: 0.77
Epoch: 023 | Train loss: 1.1933, Val loss: 1.4106 | Train F1: 0.99, Val F1: 0.77
Epoch: 024 | Train loss: 1.1861, Val loss: 1.4053 | Train F1: 0.99, Val F1: 0.77
Epoch: 025 | Train loss: 1.1810, Val loss: 1.4010 | Train F1: 0.99, Val F1: 0.77
Epoch: 026 | Train loss: 1.1773, Val loss: 1.3996 | Train F1: 0.99, Val F1: 0.76
Epoch: 027 | Train loss: 1.1747, Val loss: 1.3976 | Train F1: 0.99, Val F1: 0.76
Epoch: 028 | Train loss: 1.1723, Val loss: 1.3971 | Train F1: 0.99, Val F1: 0.75
Epoch: 029 | Train loss: 1.1697, Val loss: 1.3980 | Train F1: 1.00, Val F1: 0.75
Epoch: 030 | Train loss: 1.1680, Val loss: 1.3982 | Train F1: 1.00, Val F1: 0.75
Epoch: 031 | Train loss: 1.1677, Val loss: 1.3989 | Train F1: 1.00, Val F1: 0.75
Epoch: 032 | Train loss: 1.1675, Val loss: 1.4001 | Train F1: 1.00, Val F1: 0.75
Epoch: 033 | Train loss: 1.1671, Val loss: 1.4015 | Train F1: 1.00, Val F1: 0.74
Epoch: 034 | Train loss: 1.1665, Val loss: 1.4027 | Train F1: 1.00, Val F1: 0.74
Epoch: 035 | Train loss: 1.1659, Val loss: 1.4039 | Train F1: 1.00, Val F1: 0.75
Epoch: 036 | Train loss: 1.1657, Val loss: 1.4056 | Train F1: 1.00, Val F1: 0.74
Epoch: 037 | Train loss: 1.1656, Val loss: 1.4076 | Train F1: 1.00, Val F1: 0.75
Epoch: 038 | Train loss: 1.1656, Val loss: 1.4092 | Train F1: 1.00, Val F1: 0.74
Epoch: 039 | Train loss: 1.1655, Val loss: 1.4106 | Train F1: 1.00, Val F1: 0.74
Epoch: 040 | Train loss: 1.1655, Val loss: 1.4119 | Train F1: 1.00, Val F1: 0.74
Epoch: 041 | Train loss: 1.1655, Val loss: 1.4133 | Train F1: 1.00, Val F1: 0.74
Epoch: 042 | Train loss: 1.1656, Val loss: 1.4145 | Train F1: 1.00, Val F1: 0.74
Epoch: 043 | Train loss: 1.1655, Val loss: 1.4146 | Train F1: 1.00, Val F1: 0.74
Epoch: 044 | Train loss: 1.1655, Val loss: 1.4148 | Train F1: 1.00, Val F1: 0.74
Epoch: 045 | Train loss: 1.1655, Val loss: 1.4149 | Train F1: 1.00, Val F1: 0.75
Epoch: 046 | Train loss: 1.1655, Val loss: 1.4151 | Train F1: 1.00, Val F1: 0.75
Epoch: 047 | Train loss: 1.1655, Val loss: 1.4153 | Train F1: 1.00, Val F1: 0.75
Epoch: 048 | Train loss: 1.1655, Val loss: 1.4155 | Train F1: 1.00, Val F1: 0.75
Epoch: 049 | Train loss: 1.1655, Val loss: 1.4156 | Train F1: 1.00, Val F1: 0.75
Epoch: 050 | Train loss: 1.1655, Val loss: 1.4157 | Train F1: 1.00, Val F1: 0.75
Epoch: 051 | Train loss: 1.1655, Val loss: 1.4177 | Train F1: 1.00, Val F1: 0.74
Epoch: 052 | Train loss: 1.1655, Val loss: 1.4194 | Train F1: 1.00, Val F1: 0.74
Epoch: 053 | Train loss: 1.1655, Val loss: 1.4208 | Train F1: 1.00, Val F1: 0.73
Epoch: 054 | Train loss: 1.1655, Val loss: 1.4215 | Train F1: 1.00, Val F1: 0.74
Epoch: 055 | Train loss: 1.1655, Val loss: 1.4176 | Train F1: 1.00, Val F1: 0.74
Epoch: 056 | Train loss: 1.1655, Val loss: 1.4131 | Train F1: 1.00, Val F1: 0.74
Epoch: 057 | Train loss: 1.1654, Val loss: 1.4098 | Train F1: 1.00, Val F1: 0.74
Epoch: 058 | Train loss: 1.1654, Val loss: 1.4076 | Train F1: 1.00, Val F1: 0.74
Epoch: 059 | Train loss: 1.1654, Val loss: 1.4060 | Train F1: 1.00, Val F1: 0.74
Epoch: 060 | Train loss: 1.1654, Val loss: 1.4052 | Train F1: 1.00, Val F1: 0.75
Epoch: 061 | Train loss: 1.1654, Val loss: 1.4049 | Train F1: 1.00, Val F1: 0.75
Epoch: 062 | Train loss: 1.1654, Val loss: 1.4050 | Train F1: 1.00, Val F1: 0.75
Epoch: 063 | Train loss: 1.1654, Val loss: 1.4050 | Train F1: 1.00, Val F1: 0.74
Epoch: 064 | Train loss: 1.1654, Val loss: 1.4051 | Train F1: 1.00, Val F1: 0.74
Epoch: 065 | Train loss: 1.1654, Val loss: 1.4051 | Train F1: 1.00, Val F1: 0.74
Epoch: 066 | Train loss: 1.1654, Val loss: 1.4052 | Train F1: 1.00, Val F1: 0.74
Epoch: 067 | Train loss: 1.1654, Val loss: 1.4052 | Train F1: 1.00, Val F1: 0.74
Epoch: 068 | Train loss: 1.1654, Val loss: 1.4053 | Train F1: 1.00, Val F1: 0.74
Epoch: 069 | Train loss: 1.1654, Val loss: 1.4053 | Train F1: 1.00, Val F1: 0.74
Epoch: 070 | Train loss: 1.1654, Val loss: 1.4054 | Train F1: 1.00, Val F1: 0.74
Epoch: 071 | Train loss: 1.1654, Val loss: 1.4055 | Train F1: 1.00, Val F1: 0.74
Epoch: 072 | Train loss: 1.1654, Val loss: 1.4055 | Train F1: 1.00, Val F1: 0.74
Epoch: 073 | Train loss: 1.1654, Val loss: 1.4055 | Train F1: 1.00, Val F1: 0.74
Epoch: 074 | Train loss: 1.1654, Val loss: 1.4054 | Train F1: 1.00, Val F1: 0.74
Epoch: 075 | Train loss: 1.1654, Val loss: 1.4053 | Train F1: 1.00, Val F1: 0.74
Epoch: 076 | Train loss: 1.1654, Val loss: 1.4051 | Train F1: 1.00, Val F1: 0.74
Epoch: 077 | Train loss: 1.1654, Val loss: 1.4048 | Train F1: 1.00, Val F1: 0.74
Epoch: 078 | Train loss: 1.1654, Val loss: 1.4044 | Train F1: 1.00, Val F1: 0.74
Epoch: 079 | Train loss: 1.1654, Val loss: 1.4039 | Train F1: 1.00, Val F1: 0.74
Epoch: 080 | Train loss: 1.1654, Val loss: 1.4034 | Train F1: 1.00, Val F1: 0.74
Epoch: 081 | Train loss: 1.1654, Val loss: 1.4030 | Train F1: 1.00, Val F1: 0.74
Epoch: 082 | Train loss: 1.1654, Val loss: 1.4025 | Train F1: 1.00, Val F1: 0.74
Epoch: 083 | Train loss: 1.1654, Val loss: 1.4020 | Train F1: 1.00, Val F1: 0.74
Epoch: 084 | Train loss: 1.1654, Val loss: 1.4015 | Train F1: 1.00, Val F1: 0.74
Epoch: 085 | Train loss: 1.1654, Val loss: 1.4011 | Train F1: 1.00, Val F1: 0.74
Epoch: 086 | Train loss: 1.1654, Val loss: 1.4006 | Train F1: 1.00, Val F1: 0.74
Epoch: 087 | Train loss: 1.1654, Val loss: 1.4002 | Train F1: 1.00, Val F1: 0.74
Epoch: 088 | Train loss: 1.1654, Val loss: 1.3998 | Train F1: 1.00, Val F1: 0.74
Epoch: 089 | Train loss: 1.1654, Val loss: 1.3994 | Train F1: 1.00, Val F1: 0.74
Epoch: 090 | Train loss: 1.1654, Val loss: 1.3991 | Train F1: 1.00, Val F1: 0.74
Best model:
Train loss: 1.1654, Val loss: 1.4052, Test loss: 1.4032
Train F1: 1.00, Val F1: 0.75, Test F1: 0.76

>>> run.py: Namespace(dataset='cora', device=1, experiment='n_layers', log_path='log/graphsage/cora/n_layers', method='graphsage', path='data/graphsage', plot_path='plots/graphsage/cora/n_layers')

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/n_layers', lr=0.001, max_epochs=1000, n_layers=1, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5421, Val loss: 1.5661 | Train F1: 0.43, Val F1: 0.25
Epoch: 001 | Train loss: 1.4665, Val loss: 1.5164 | Train F1: 0.46, Val F1: 0.28
Epoch: 002 | Train loss: 1.3947, Val loss: 1.4676 | Train F1: 0.47, Val F1: 0.37
Epoch: 003 | Train loss: 1.3328, Val loss: 1.4246 | Train F1: 0.49, Val F1: 0.39
Epoch: 004 | Train loss: 1.2811, Val loss: 1.3894 | Train F1: 0.49, Val F1: 0.39
Epoch: 005 | Train loss: 1.2373, Val loss: 1.3612 | Train F1: 0.51, Val F1: 0.39
Epoch: 006 | Train loss: 1.2000, Val loss: 1.3387 | Train F1: 0.58, Val F1: 0.39
Epoch: 007 | Train loss: 1.1685, Val loss: 1.3205 | Train F1: 0.59, Val F1: 0.43
Epoch: 008 | Train loss: 1.1422, Val loss: 1.3053 | Train F1: 0.62, Val F1: 0.43
Epoch: 009 | Train loss: 1.1205, Val loss: 1.2926 | Train F1: 0.62, Val F1: 0.44
Epoch: 010 | Train loss: 1.1021, Val loss: 1.2814 | Train F1: 0.65, Val F1: 0.45
Epoch: 011 | Train loss: 1.0863, Val loss: 1.2715 | Train F1: 0.66, Val F1: 0.45
Epoch: 012 | Train loss: 1.0722, Val loss: 1.2621 | Train F1: 0.66, Val F1: 0.45
Epoch: 013 | Train loss: 1.0596, Val loss: 1.2533 | Train F1: 0.66, Val F1: 0.45
Epoch: 014 | Train loss: 1.0486, Val loss: 1.2454 | Train F1: 0.68, Val F1: 0.45
Epoch: 015 | Train loss: 1.0390, Val loss: 1.2385 | Train F1: 0.76, Val F1: 0.45
Epoch: 016 | Train loss: 1.0304, Val loss: 1.2325 | Train F1: 0.76, Val F1: 0.45
Epoch: 017 | Train loss: 1.0229, Val loss: 1.2275 | Train F1: 0.78, Val F1: 0.46
Epoch: 018 | Train loss: 1.0163, Val loss: 1.2232 | Train F1: 0.78, Val F1: 0.45
Epoch: 019 | Train loss: 1.0105, Val loss: 1.2196 | Train F1: 0.80, Val F1: 0.45
Epoch: 020 | Train loss: 1.0055, Val loss: 1.2165 | Train F1: 0.81, Val F1: 0.45
Epoch: 021 | Train loss: 1.0011, Val loss: 1.2138 | Train F1: 0.80, Val F1: 0.45
Epoch: 022 | Train loss: 0.9973, Val loss: 1.2115 | Train F1: 0.80, Val F1: 0.45
Epoch: 023 | Train loss: 0.9940, Val loss: 1.2094 | Train F1: 0.80, Val F1: 0.45
Epoch: 024 | Train loss: 0.9909, Val loss: 1.2075 | Train F1: 0.82, Val F1: 0.45
Epoch: 025 | Train loss: 0.9881, Val loss: 1.2057 | Train F1: 0.82, Val F1: 0.45
Epoch: 026 | Train loss: 0.9852, Val loss: 1.2040 | Train F1: 0.82, Val F1: 0.45
Epoch: 027 | Train loss: 0.9821, Val loss: 1.2023 | Train F1: 0.88, Val F1: 0.45
Epoch: 028 | Train loss: 0.9788, Val loss: 1.2005 | Train F1: 0.88, Val F1: 0.46
Epoch: 029 | Train loss: 0.9755, Val loss: 1.1986 | Train F1: 0.88, Val F1: 0.46
Epoch: 030 | Train loss: 0.9725, Val loss: 1.1968 | Train F1: 0.88, Val F1: 0.46
Epoch: 031 | Train loss: 0.9699, Val loss: 1.1949 | Train F1: 0.88, Val F1: 0.46
Epoch: 032 | Train loss: 0.9677, Val loss: 1.1930 | Train F1: 0.92, Val F1: 0.45
Epoch: 033 | Train loss: 0.9658, Val loss: 1.1912 | Train F1: 0.92, Val F1: 0.45
Epoch: 034 | Train loss: 0.9639, Val loss: 1.1894 | Train F1: 0.92, Val F1: 0.45
Epoch: 035 | Train loss: 0.9622, Val loss: 1.1876 | Train F1: 0.92, Val F1: 0.45
Epoch: 036 | Train loss: 0.9606, Val loss: 1.1860 | Train F1: 0.92, Val F1: 0.46
Epoch: 037 | Train loss: 0.9591, Val loss: 1.1844 | Train F1: 0.92, Val F1: 0.46
Epoch: 038 | Train loss: 0.9577, Val loss: 1.1828 | Train F1: 0.92, Val F1: 0.46
Epoch: 039 | Train loss: 0.9565, Val loss: 1.1814 | Train F1: 0.92, Val F1: 0.46
Epoch: 040 | Train loss: 0.9554, Val loss: 1.1800 | Train F1: 0.94, Val F1: 0.47
Epoch: 041 | Train loss: 0.9545, Val loss: 1.1786 | Train F1: 0.94, Val F1: 0.47
Epoch: 042 | Train loss: 0.9536, Val loss: 1.1774 | Train F1: 0.94, Val F1: 0.48
Epoch: 043 | Train loss: 0.9528, Val loss: 1.1761 | Train F1: 0.94, Val F1: 0.48
Epoch: 044 | Train loss: 0.9521, Val loss: 1.1749 | Train F1: 0.94, Val F1: 0.48
Epoch: 045 | Train loss: 0.9515, Val loss: 1.1738 | Train F1: 0.94, Val F1: 0.48
Epoch: 046 | Train loss: 0.9509, Val loss: 1.1727 | Train F1: 0.94, Val F1: 0.47
Epoch: 047 | Train loss: 0.9503, Val loss: 1.1716 | Train F1: 0.94, Val F1: 0.48
Epoch: 048 | Train loss: 0.9498, Val loss: 1.1706 | Train F1: 0.94, Val F1: 0.48
Epoch: 049 | Train loss: 0.9493, Val loss: 1.1697 | Train F1: 0.94, Val F1: 0.48
Epoch: 050 | Train loss: 0.9487, Val loss: 1.1689 | Train F1: 0.94, Val F1: 0.48
Epoch: 051 | Train loss: 0.9482, Val loss: 1.1681 | Train F1: 0.94, Val F1: 0.47
Epoch: 052 | Train loss: 0.9477, Val loss: 1.1673 | Train F1: 0.94, Val F1: 0.47
Epoch: 053 | Train loss: 0.9471, Val loss: 1.1667 | Train F1: 0.94, Val F1: 0.47
Epoch: 054 | Train loss: 0.9465, Val loss: 1.1662 | Train F1: 0.94, Val F1: 0.46
Epoch: 055 | Train loss: 0.9458, Val loss: 1.1657 | Train F1: 0.94, Val F1: 0.46
Epoch: 056 | Train loss: 0.9451, Val loss: 1.1653 | Train F1: 0.94, Val F1: 0.46
Epoch: 057 | Train loss: 0.9443, Val loss: 1.1649 | Train F1: 0.94, Val F1: 0.46
Epoch: 058 | Train loss: 0.9433, Val loss: 1.1647 | Train F1: 0.94, Val F1: 0.46
Epoch: 059 | Train loss: 0.9423, Val loss: 1.1646 | Train F1: 0.95, Val F1: 0.46
Epoch: 060 | Train loss: 0.9412, Val loss: 1.1646 | Train F1: 0.95, Val F1: 0.46
Epoch: 061 | Train loss: 0.9402, Val loss: 1.1647 | Train F1: 0.95, Val F1: 0.46
Epoch: 062 | Train loss: 0.9393, Val loss: 1.1648 | Train F1: 0.95, Val F1: 0.46
Epoch: 063 | Train loss: 0.9385, Val loss: 1.1651 | Train F1: 0.95, Val F1: 0.46
Epoch: 064 | Train loss: 0.9379, Val loss: 1.1654 | Train F1: 0.95, Val F1: 0.51
Epoch: 065 | Train loss: 0.9374, Val loss: 1.1658 | Train F1: 0.95, Val F1: 0.51
Epoch: 066 | Train loss: 0.9370, Val loss: 1.1663 | Train F1: 0.95, Val F1: 0.48
Epoch: 067 | Train loss: 0.9366, Val loss: 1.1667 | Train F1: 0.95, Val F1: 0.48
Epoch: 068 | Train loss: 0.9363, Val loss: 1.1671 | Train F1: 0.95, Val F1: 0.48
Epoch: 069 | Train loss: 0.9359, Val loss: 1.1676 | Train F1: 0.95, Val F1: 0.48
Epoch: 070 | Train loss: 0.9356, Val loss: 1.1680 | Train F1: 0.96, Val F1: 0.48
Epoch: 071 | Train loss: 0.9352, Val loss: 1.1684 | Train F1: 0.96, Val F1: 0.48
Epoch: 072 | Train loss: 0.9347, Val loss: 1.1687 | Train F1: 0.96, Val F1: 0.48
Epoch: 073 | Train loss: 0.9342, Val loss: 1.1691 | Train F1: 0.96, Val F1: 0.51
Epoch: 074 | Train loss: 0.9335, Val loss: 1.1696 | Train F1: 0.96, Val F1: 0.53
Epoch: 075 | Train loss: 0.9327, Val loss: 1.1700 | Train F1: 0.96, Val F1: 0.53
Epoch: 076 | Train loss: 0.9317, Val loss: 1.1706 | Train F1: 0.96, Val F1: 0.53
Epoch: 077 | Train loss: 0.9307, Val loss: 1.1713 | Train F1: 0.97, Val F1: 0.53
Epoch: 078 | Train loss: 0.9296, Val loss: 1.1722 | Train F1: 0.97, Val F1: 0.49
Epoch: 079 | Train loss: 0.9287, Val loss: 1.1731 | Train F1: 0.97, Val F1: 0.49
Epoch: 080 | Train loss: 0.9280, Val loss: 1.1741 | Train F1: 0.97, Val F1: 0.49
Epoch: 081 | Train loss: 0.9275, Val loss: 1.1750 | Train F1: 0.97, Val F1: 0.48
Epoch: 082 | Train loss: 0.9271, Val loss: 1.1759 | Train F1: 0.97, Val F1: 0.47
Best model:
Train loss: 0.9477, Val loss: 1.1673, Test loss: 1.1972
Train F1: 0.94, Val F1: 0.47, Test F1: 0.52

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/n_layers', lr=0.001, max_epochs=1000, n_layers=2, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5729, Val loss: 1.5926 | Train F1: 0.38, Val F1: 0.20
Epoch: 001 | Train loss: 1.5215, Val loss: 1.5590 | Train F1: 0.51, Val F1: 0.29
Epoch: 002 | Train loss: 1.4684, Val loss: 1.5219 | Train F1: 0.46, Val F1: 0.28
Epoch: 003 | Train loss: 1.4165, Val loss: 1.4833 | Train F1: 0.46, Val F1: 0.27
Epoch: 004 | Train loss: 1.3694, Val loss: 1.4462 | Train F1: 0.45, Val F1: 0.31
Epoch: 005 | Train loss: 1.3287, Val loss: 1.4133 | Train F1: 0.45, Val F1: 0.33
Epoch: 006 | Train loss: 1.2930, Val loss: 1.3862 | Train F1: 0.47, Val F1: 0.34
Epoch: 007 | Train loss: 1.2602, Val loss: 1.3640 | Train F1: 0.48, Val F1: 0.33
Epoch: 008 | Train loss: 1.2287, Val loss: 1.3461 | Train F1: 0.49, Val F1: 0.34
Epoch: 009 | Train loss: 1.1980, Val loss: 1.3320 | Train F1: 0.51, Val F1: 0.35
Epoch: 010 | Train loss: 1.1693, Val loss: 1.3210 | Train F1: 0.61, Val F1: 0.36
Epoch: 011 | Train loss: 1.1438, Val loss: 1.3104 | Train F1: 0.62, Val F1: 0.38
Epoch: 012 | Train loss: 1.1226, Val loss: 1.3003 | Train F1: 0.63, Val F1: 0.37
Epoch: 013 | Train loss: 1.1054, Val loss: 1.2897 | Train F1: 0.64, Val F1: 0.39
Epoch: 014 | Train loss: 1.0920, Val loss: 1.2811 | Train F1: 0.65, Val F1: 0.38
Epoch: 015 | Train loss: 1.0818, Val loss: 1.2753 | Train F1: 0.65, Val F1: 0.38
Epoch: 016 | Train loss: 1.0734, Val loss: 1.2706 | Train F1: 0.65, Val F1: 0.38
Epoch: 017 | Train loss: 1.0657, Val loss: 1.2656 | Train F1: 0.64, Val F1: 0.42
Epoch: 018 | Train loss: 1.0583, Val loss: 1.2595 | Train F1: 0.64, Val F1: 0.41
Epoch: 019 | Train loss: 1.0515, Val loss: 1.2531 | Train F1: 0.64, Val F1: 0.39
Epoch: 020 | Train loss: 1.0457, Val loss: 1.2474 | Train F1: 0.65, Val F1: 0.39
Epoch: 021 | Train loss: 1.0410, Val loss: 1.2431 | Train F1: 0.65, Val F1: 0.39
Epoch: 022 | Train loss: 1.0370, Val loss: 1.2401 | Train F1: 0.65, Val F1: 0.41
Epoch: 023 | Train loss: 1.0333, Val loss: 1.2377 | Train F1: 0.65, Val F1: 0.41
Epoch: 024 | Train loss: 1.0293, Val loss: 1.2354 | Train F1: 0.68, Val F1: 0.41
Epoch: 025 | Train loss: 1.0251, Val loss: 1.2333 | Train F1: 0.68, Val F1: 0.42
Epoch: 026 | Train loss: 1.0211, Val loss: 1.2316 | Train F1: 0.69, Val F1: 0.42
Epoch: 027 | Train loss: 1.0172, Val loss: 1.2300 | Train F1: 0.70, Val F1: 0.42
Epoch: 028 | Train loss: 1.0132, Val loss: 1.2283 | Train F1: 0.70, Val F1: 0.42
Epoch: 029 | Train loss: 1.0096, Val loss: 1.2270 | Train F1: 0.70, Val F1: 0.42
Epoch: 030 | Train loss: 1.0062, Val loss: 1.2259 | Train F1: 0.78, Val F1: 0.42
Epoch: 031 | Train loss: 1.0028, Val loss: 1.2254 | Train F1: 0.79, Val F1: 0.42
Epoch: 032 | Train loss: 0.9992, Val loss: 1.2253 | Train F1: 0.82, Val F1: 0.42
Epoch: 033 | Train loss: 0.9954, Val loss: 1.2257 | Train F1: 0.82, Val F1: 0.42
Epoch: 034 | Train loss: 0.9915, Val loss: 1.2263 | Train F1: 0.82, Val F1: 0.42
Epoch: 035 | Train loss: 0.9877, Val loss: 1.2274 | Train F1: 0.83, Val F1: 0.42
Epoch: 036 | Train loss: 0.9838, Val loss: 1.2285 | Train F1: 0.89, Val F1: 0.46
Epoch: 037 | Train loss: 0.9803, Val loss: 1.2293 | Train F1: 0.89, Val F1: 0.46
Epoch: 038 | Train loss: 0.9772, Val loss: 1.2289 | Train F1: 0.89, Val F1: 0.45
Epoch: 039 | Train loss: 0.9749, Val loss: 1.2274 | Train F1: 0.89, Val F1: 0.46
Epoch: 040 | Train loss: 0.9730, Val loss: 1.2248 | Train F1: 0.89, Val F1: 0.43
Epoch: 041 | Train loss: 0.9712, Val loss: 1.2225 | Train F1: 0.89, Val F1: 0.43
Epoch: 042 | Train loss: 0.9695, Val loss: 1.2201 | Train F1: 0.89, Val F1: 0.42
Epoch: 043 | Train loss: 0.9677, Val loss: 1.2179 | Train F1: 0.89, Val F1: 0.42
Epoch: 044 | Train loss: 0.9659, Val loss: 1.2158 | Train F1: 0.93, Val F1: 0.48
Epoch: 045 | Train loss: 0.9637, Val loss: 1.2147 | Train F1: 0.93, Val F1: 0.43
Epoch: 046 | Train loss: 0.9613, Val loss: 1.2136 | Train F1: 0.93, Val F1: 0.43
Epoch: 047 | Train loss: 0.9585, Val loss: 1.2121 | Train F1: 0.93, Val F1: 0.43
Epoch: 048 | Train loss: 0.9555, Val loss: 1.2100 | Train F1: 0.94, Val F1: 0.43
Epoch: 049 | Train loss: 0.9529, Val loss: 1.2074 | Train F1: 0.94, Val F1: 0.43
Epoch: 050 | Train loss: 0.9509, Val loss: 1.2045 | Train F1: 0.94, Val F1: 0.43
Epoch: 051 | Train loss: 0.9492, Val loss: 1.2019 | Train F1: 0.94, Val F1: 0.43
Epoch: 052 | Train loss: 0.9476, Val loss: 1.2001 | Train F1: 0.94, Val F1: 0.43
Epoch: 053 | Train loss: 0.9458, Val loss: 1.1992 | Train F1: 0.94, Val F1: 0.43
Epoch: 054 | Train loss: 0.9437, Val loss: 1.1995 | Train F1: 0.94, Val F1: 0.43
Epoch: 055 | Train loss: 0.9415, Val loss: 1.2013 | Train F1: 0.95, Val F1: 0.43
Epoch: 056 | Train loss: 0.9401, Val loss: 1.2045 | Train F1: 0.95, Val F1: 0.43
Epoch: 057 | Train loss: 0.9394, Val loss: 1.2084 | Train F1: 0.95, Val F1: 0.41
Epoch: 058 | Train loss: 0.9392, Val loss: 1.2121 | Train F1: 0.95, Val F1: 0.43
Epoch: 059 | Train loss: 0.9390, Val loss: 1.2144 | Train F1: 0.95, Val F1: 0.49
Epoch: 060 | Train loss: 0.9383, Val loss: 1.2144 | Train F1: 0.95, Val F1: 0.49
Epoch: 061 | Train loss: 0.9374, Val loss: 1.2131 | Train F1: 0.95, Val F1: 0.50
Epoch: 062 | Train loss: 0.9364, Val loss: 1.2106 | Train F1: 0.95, Val F1: 0.50
Epoch: 063 | Train loss: 0.9354, Val loss: 1.2074 | Train F1: 0.95, Val F1: 0.50
Epoch: 064 | Train loss: 0.9346, Val loss: 1.2042 | Train F1: 0.95, Val F1: 0.50
Epoch: 065 | Train loss: 0.9339, Val loss: 1.2014 | Train F1: 0.95, Val F1: 0.49
Epoch: 066 | Train loss: 0.9333, Val loss: 1.1992 | Train F1: 0.95, Val F1: 0.48
Epoch: 067 | Train loss: 0.9327, Val loss: 1.1975 | Train F1: 0.95, Val F1: 0.49
Epoch: 068 | Train loss: 0.9320, Val loss: 1.1962 | Train F1: 0.95, Val F1: 0.48
Epoch: 069 | Train loss: 0.9312, Val loss: 1.1958 | Train F1: 0.95, Val F1: 0.48
Epoch: 070 | Train loss: 0.9303, Val loss: 1.1959 | Train F1: 0.95, Val F1: 0.48
Epoch: 071 | Train loss: 0.9293, Val loss: 1.1967 | Train F1: 0.95, Val F1: 0.48
Epoch: 072 | Train loss: 0.9281, Val loss: 1.1979 | Train F1: 0.98, Val F1: 0.48
Epoch: 073 | Train loss: 0.9267, Val loss: 1.1998 | Train F1: 0.96, Val F1: 0.48
Epoch: 074 | Train loss: 0.9253, Val loss: 1.2022 | Train F1: 0.99, Val F1: 0.51
Epoch: 075 | Train loss: 0.9238, Val loss: 1.2048 | Train F1: 0.99, Val F1: 0.50
Epoch: 076 | Train loss: 0.9224, Val loss: 1.2075 | Train F1: 0.99, Val F1: 0.56
Epoch: 077 | Train loss: 0.9211, Val loss: 1.2100 | Train F1: 0.99, Val F1: 0.56
Epoch: 078 | Train loss: 0.9200, Val loss: 1.2123 | Train F1: 0.99, Val F1: 0.56
Epoch: 079 | Train loss: 0.9192, Val loss: 1.2143 | Train F1: 0.99, Val F1: 0.56
Epoch: 080 | Train loss: 0.9186, Val loss: 1.2158 | Train F1: 0.99, Val F1: 0.56
Epoch: 081 | Train loss: 0.9182, Val loss: 1.2170 | Train F1: 0.99, Val F1: 0.54
Epoch: 082 | Train loss: 0.9178, Val loss: 1.2174 | Train F1: 0.99, Val F1: 0.54
Epoch: 083 | Train loss: 0.9173, Val loss: 1.2170 | Train F1: 0.99, Val F1: 0.55
Epoch: 084 | Train loss: 0.9169, Val loss: 1.2159 | Train F1: 0.99, Val F1: 0.55
Epoch: 085 | Train loss: 0.9166, Val loss: 1.2143 | Train F1: 1.00, Val F1: 0.55
Epoch: 086 | Train loss: 0.9162, Val loss: 1.2124 | Train F1: 1.00, Val F1: 0.55
Epoch: 087 | Train loss: 0.9158, Val loss: 1.2102 | Train F1: 1.00, Val F1: 0.57
Epoch: 088 | Train loss: 0.9154, Val loss: 1.2078 | Train F1: 1.00, Val F1: 0.57
Epoch: 089 | Train loss: 0.9151, Val loss: 1.2057 | Train F1: 1.00, Val F1: 0.57
Epoch: 090 | Train loss: 0.9148, Val loss: 1.2036 | Train F1: 1.00, Val F1: 0.57
Epoch: 091 | Train loss: 0.9145, Val loss: 1.2017 | Train F1: 1.00, Val F1: 0.57
Epoch: 092 | Train loss: 0.9142, Val loss: 1.1999 | Train F1: 1.00, Val F1: 0.57
Epoch: 093 | Train loss: 0.9139, Val loss: 1.1982 | Train F1: 1.00, Val F1: 0.57
Epoch: 094 | Train loss: 0.9136, Val loss: 1.1968 | Train F1: 1.00, Val F1: 0.57
Epoch: 095 | Train loss: 0.9134, Val loss: 1.1956 | Train F1: 1.00, Val F1: 0.57
Epoch: 096 | Train loss: 0.9132, Val loss: 1.1945 | Train F1: 1.00, Val F1: 0.57
Epoch: 097 | Train loss: 0.9129, Val loss: 1.1935 | Train F1: 1.00, Val F1: 0.57
Epoch: 098 | Train loss: 0.9127, Val loss: 1.1927 | Train F1: 1.00, Val F1: 0.57
Epoch: 099 | Train loss: 0.9126, Val loss: 1.1920 | Train F1: 1.00, Val F1: 0.57
Epoch: 100 | Train loss: 0.9124, Val loss: 1.1913 | Train F1: 1.00, Val F1: 0.57
Epoch: 101 | Train loss: 0.9122, Val loss: 1.1906 | Train F1: 1.00, Val F1: 0.57
Epoch: 102 | Train loss: 0.9121, Val loss: 1.1901 | Train F1: 1.00, Val F1: 0.57
Epoch: 103 | Train loss: 0.9119, Val loss: 1.1895 | Train F1: 1.00, Val F1: 0.57
Epoch: 104 | Train loss: 0.9118, Val loss: 1.1891 | Train F1: 1.00, Val F1: 0.57
Epoch: 105 | Train loss: 0.9116, Val loss: 1.1887 | Train F1: 1.00, Val F1: 0.57
Epoch: 106 | Train loss: 0.9115, Val loss: 1.1883 | Train F1: 1.00, Val F1: 0.57
Epoch: 107 | Train loss: 0.9114, Val loss: 1.1879 | Train F1: 1.00, Val F1: 0.57
Epoch: 108 | Train loss: 0.9113, Val loss: 1.1876 | Train F1: 1.00, Val F1: 0.56
Epoch: 109 | Train loss: 0.9112, Val loss: 1.1873 | Train F1: 1.00, Val F1: 0.56
Epoch: 110 | Train loss: 0.9111, Val loss: 1.1870 | Train F1: 1.00, Val F1: 0.56
Epoch: 111 | Train loss: 0.9110, Val loss: 1.1867 | Train F1: 1.00, Val F1: 0.56
Epoch: 112 | Train loss: 0.9109, Val loss: 1.1864 | Train F1: 1.00, Val F1: 0.56
Epoch: 113 | Train loss: 0.9108, Val loss: 1.1861 | Train F1: 1.00, Val F1: 0.56
Epoch: 114 | Train loss: 0.9107, Val loss: 1.1858 | Train F1: 1.00, Val F1: 0.56
Epoch: 115 | Train loss: 0.9106, Val loss: 1.1855 | Train F1: 1.00, Val F1: 0.56
Epoch: 116 | Train loss: 0.9105, Val loss: 1.1852 | Train F1: 1.00, Val F1: 0.56
Epoch: 117 | Train loss: 0.9104, Val loss: 1.1849 | Train F1: 1.00, Val F1: 0.56
Epoch: 118 | Train loss: 0.9103, Val loss: 1.1846 | Train F1: 1.00, Val F1: 0.56
Epoch: 119 | Train loss: 0.9102, Val loss: 1.1844 | Train F1: 1.00, Val F1: 0.56
Epoch: 120 | Train loss: 0.9102, Val loss: 1.1841 | Train F1: 1.00, Val F1: 0.56
Epoch: 121 | Train loss: 0.9101, Val loss: 1.1838 | Train F1: 1.00, Val F1: 0.56
Epoch: 122 | Train loss: 0.9100, Val loss: 1.1835 | Train F1: 1.00, Val F1: 0.56
Epoch: 123 | Train loss: 0.9099, Val loss: 1.1832 | Train F1: 1.00, Val F1: 0.56
Epoch: 124 | Train loss: 0.9099, Val loss: 1.1829 | Train F1: 1.00, Val F1: 0.56
Epoch: 125 | Train loss: 0.9098, Val loss: 1.1827 | Train F1: 1.00, Val F1: 0.56
Epoch: 126 | Train loss: 0.9097, Val loss: 1.1824 | Train F1: 1.00, Val F1: 0.56
Epoch: 127 | Train loss: 0.9096, Val loss: 1.1822 | Train F1: 1.00, Val F1: 0.56
Epoch: 128 | Train loss: 0.9096, Val loss: 1.1820 | Train F1: 1.00, Val F1: 0.56
Epoch: 129 | Train loss: 0.9095, Val loss: 1.1817 | Train F1: 1.00, Val F1: 0.56
Epoch: 130 | Train loss: 0.9094, Val loss: 1.1815 | Train F1: 1.00, Val F1: 0.56
Epoch: 131 | Train loss: 0.9094, Val loss: 1.1814 | Train F1: 1.00, Val F1: 0.56
Epoch: 132 | Train loss: 0.9093, Val loss: 1.1812 | Train F1: 1.00, Val F1: 0.56
Epoch: 133 | Train loss: 0.9092, Val loss: 1.1810 | Train F1: 1.00, Val F1: 0.56
Epoch: 134 | Train loss: 0.9092, Val loss: 1.1808 | Train F1: 1.00, Val F1: 0.56
Epoch: 135 | Train loss: 0.9091, Val loss: 1.1806 | Train F1: 1.00, Val F1: 0.56
Epoch: 136 | Train loss: 0.9091, Val loss: 1.1805 | Train F1: 1.00, Val F1: 0.56
Epoch: 137 | Train loss: 0.9090, Val loss: 1.1803 | Train F1: 1.00, Val F1: 0.56
Epoch: 138 | Train loss: 0.9089, Val loss: 1.1801 | Train F1: 1.00, Val F1: 0.56
Epoch: 139 | Train loss: 0.9089, Val loss: 1.1800 | Train F1: 1.00, Val F1: 0.56
Epoch: 140 | Train loss: 0.9088, Val loss: 1.1798 | Train F1: 1.00, Val F1: 0.56
Epoch: 141 | Train loss: 0.9088, Val loss: 1.1797 | Train F1: 1.00, Val F1: 0.56
Epoch: 142 | Train loss: 0.9087, Val loss: 1.1795 | Train F1: 1.00, Val F1: 0.56
Epoch: 143 | Train loss: 0.9087, Val loss: 1.1793 | Train F1: 1.00, Val F1: 0.56
Epoch: 144 | Train loss: 0.9086, Val loss: 1.1792 | Train F1: 1.00, Val F1: 0.56
Epoch: 145 | Train loss: 0.9086, Val loss: 1.1790 | Train F1: 1.00, Val F1: 0.56
Best model:
Train loss: 0.9106, Val loss: 1.1855, Test loss: 1.2341
Train F1: 1.00, Val F1: 0.56, Test F1: 0.48

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/n_layers', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5796, Val loss: 1.5930 | Train F1: 0.37, Val F1: 0.23
Epoch: 001 | Train loss: 1.5381, Val loss: 1.5642 | Train F1: 0.43, Val F1: 0.26
Epoch: 002 | Train loss: 1.4884, Val loss: 1.5283 | Train F1: 0.40, Val F1: 0.36
Epoch: 003 | Train loss: 1.4341, Val loss: 1.4873 | Train F1: 0.40, Val F1: 0.34
Epoch: 004 | Train loss: 1.3829, Val loss: 1.4461 | Train F1: 0.40, Val F1: 0.35
Epoch: 005 | Train loss: 1.3400, Val loss: 1.4100 | Train F1: 0.40, Val F1: 0.36
Epoch: 006 | Train loss: 1.3047, Val loss: 1.3812 | Train F1: 0.41, Val F1: 0.37
Epoch: 007 | Train loss: 1.2743, Val loss: 1.3593 | Train F1: 0.41, Val F1: 0.37
Epoch: 008 | Train loss: 1.2464, Val loss: 1.3443 | Train F1: 0.42, Val F1: 0.36
Epoch: 009 | Train loss: 1.2196, Val loss: 1.3344 | Train F1: 0.51, Val F1: 0.36
Epoch: 010 | Train loss: 1.1924, Val loss: 1.3258 | Train F1: 0.55, Val F1: 0.36
Epoch: 011 | Train loss: 1.1668, Val loss: 1.3179 | Train F1: 0.55, Val F1: 0.36
Epoch: 012 | Train loss: 1.1447, Val loss: 1.3109 | Train F1: 0.55, Val F1: 0.35
Epoch: 013 | Train loss: 1.1245, Val loss: 1.3034 | Train F1: 0.57, Val F1: 0.35
Epoch: 014 | Train loss: 1.1033, Val loss: 1.2940 | Train F1: 0.59, Val F1: 0.38
Epoch: 015 | Train loss: 1.0836, Val loss: 1.2833 | Train F1: 0.60, Val F1: 0.38
Epoch: 016 | Train loss: 1.0696, Val loss: 1.2738 | Train F1: 0.61, Val F1: 0.38
Epoch: 017 | Train loss: 1.0606, Val loss: 1.2682 | Train F1: 0.61, Val F1: 0.40
Epoch: 018 | Train loss: 1.0549, Val loss: 1.2661 | Train F1: 0.61, Val F1: 0.41
Epoch: 019 | Train loss: 1.0499, Val loss: 1.2643 | Train F1: 0.61, Val F1: 0.42
Epoch: 020 | Train loss: 1.0442, Val loss: 1.2609 | Train F1: 0.62, Val F1: 0.43
Epoch: 021 | Train loss: 1.0371, Val loss: 1.2552 | Train F1: 0.65, Val F1: 0.43
Epoch: 022 | Train loss: 1.0289, Val loss: 1.2477 | Train F1: 0.65, Val F1: 0.49
Epoch: 023 | Train loss: 1.0204, Val loss: 1.2417 | Train F1: 0.67, Val F1: 0.50
Epoch: 024 | Train loss: 1.0132, Val loss: 1.2376 | Train F1: 0.76, Val F1: 0.50
Epoch: 025 | Train loss: 1.0083, Val loss: 1.2358 | Train F1: 0.67, Val F1: 0.48
Epoch: 026 | Train loss: 1.0050, Val loss: 1.2359 | Train F1: 0.67, Val F1: 0.46
Epoch: 027 | Train loss: 1.0017, Val loss: 1.2366 | Train F1: 0.67, Val F1: 0.45
Epoch: 028 | Train loss: 0.9978, Val loss: 1.2363 | Train F1: 0.69, Val F1: 0.45
Epoch: 029 | Train loss: 0.9932, Val loss: 1.2351 | Train F1: 0.78, Val F1: 0.45
Epoch: 030 | Train loss: 0.9875, Val loss: 1.2326 | Train F1: 0.78, Val F1: 0.45
Epoch: 031 | Train loss: 0.9814, Val loss: 1.2293 | Train F1: 0.80, Val F1: 0.45
Epoch: 032 | Train loss: 0.9756, Val loss: 1.2260 | Train F1: 0.82, Val F1: 0.46
Epoch: 033 | Train loss: 0.9706, Val loss: 1.2242 | Train F1: 0.88, Val F1: 0.46
Epoch: 034 | Train loss: 0.9662, Val loss: 1.2237 | Train F1: 0.88, Val F1: 0.46
Epoch: 035 | Train loss: 0.9624, Val loss: 1.2240 | Train F1: 0.89, Val F1: 0.48
Epoch: 036 | Train loss: 0.9592, Val loss: 1.2254 | Train F1: 0.89, Val F1: 0.48
Epoch: 037 | Train loss: 0.9569, Val loss: 1.2267 | Train F1: 0.89, Val F1: 0.47
Epoch: 038 | Train loss: 0.9549, Val loss: 1.2274 | Train F1: 0.89, Val F1: 0.46
Epoch: 039 | Train loss: 0.9529, Val loss: 1.2267 | Train F1: 0.89, Val F1: 0.53
Epoch: 040 | Train loss: 0.9506, Val loss: 1.2242 | Train F1: 0.89, Val F1: 0.53
Epoch: 041 | Train loss: 0.9481, Val loss: 1.2199 | Train F1: 0.89, Val F1: 0.49
Epoch: 042 | Train loss: 0.9456, Val loss: 1.2155 | Train F1: 0.89, Val F1: 0.49
Epoch: 043 | Train loss: 0.9435, Val loss: 1.2118 | Train F1: 0.93, Val F1: 0.49
Epoch: 044 | Train loss: 0.9415, Val loss: 1.2074 | Train F1: 0.93, Val F1: 0.48
Epoch: 045 | Train loss: 0.9396, Val loss: 1.2042 | Train F1: 0.93, Val F1: 0.47
Epoch: 046 | Train loss: 0.9378, Val loss: 1.2025 | Train F1: 0.95, Val F1: 0.47
Epoch: 047 | Train loss: 0.9361, Val loss: 1.2013 | Train F1: 0.95, Val F1: 0.49
Epoch: 048 | Train loss: 0.9345, Val loss: 1.2005 | Train F1: 0.95, Val F1: 0.47
Epoch: 049 | Train loss: 0.9332, Val loss: 1.1993 | Train F1: 0.94, Val F1: 0.47
Epoch: 050 | Train loss: 0.9320, Val loss: 1.1977 | Train F1: 0.94, Val F1: 0.48
Epoch: 051 | Train loss: 0.9309, Val loss: 1.1973 | Train F1: 0.94, Val F1: 0.48
Epoch: 052 | Train loss: 0.9298, Val loss: 1.1969 | Train F1: 0.94, Val F1: 0.48
Epoch: 053 | Train loss: 0.9287, Val loss: 1.1956 | Train F1: 0.94, Val F1: 0.48
Epoch: 054 | Train loss: 0.9277, Val loss: 1.1944 | Train F1: 0.94, Val F1: 0.48
Epoch: 055 | Train loss: 0.9266, Val loss: 1.1931 | Train F1: 0.94, Val F1: 0.48
Epoch: 056 | Train loss: 0.9255, Val loss: 1.1922 | Train F1: 0.94, Val F1: 0.48
Epoch: 057 | Train loss: 0.9244, Val loss: 1.1914 | Train F1: 0.98, Val F1: 0.46
Epoch: 058 | Train loss: 0.9234, Val loss: 1.1906 | Train F1: 0.98, Val F1: 0.46
Epoch: 059 | Train loss: 0.9225, Val loss: 1.1903 | Train F1: 0.98, Val F1: 0.46
Epoch: 060 | Train loss: 0.9217, Val loss: 1.1903 | Train F1: 0.98, Val F1: 0.51
Epoch: 061 | Train loss: 0.9211, Val loss: 1.1904 | Train F1: 0.98, Val F1: 0.51
Epoch: 062 | Train loss: 0.9205, Val loss: 1.1905 | Train F1: 0.99, Val F1: 0.53
Epoch: 063 | Train loss: 0.9200, Val loss: 1.1903 | Train F1: 0.99, Val F1: 0.56
Epoch: 064 | Train loss: 0.9196, Val loss: 1.1902 | Train F1: 0.99, Val F1: 0.57
Epoch: 065 | Train loss: 0.9192, Val loss: 1.1901 | Train F1: 0.99, Val F1: 0.57
Epoch: 066 | Train loss: 0.9188, Val loss: 1.1897 | Train F1: 0.99, Val F1: 0.57
Epoch: 067 | Train loss: 0.9184, Val loss: 1.1893 | Train F1: 0.99, Val F1: 0.57
Epoch: 068 | Train loss: 0.9180, Val loss: 1.1889 | Train F1: 0.99, Val F1: 0.57
Epoch: 069 | Train loss: 0.9176, Val loss: 1.1886 | Train F1: 0.99, Val F1: 0.57
Epoch: 070 | Train loss: 0.9173, Val loss: 1.1883 | Train F1: 0.99, Val F1: 0.57
Epoch: 071 | Train loss: 0.9169, Val loss: 1.1882 | Train F1: 0.99, Val F1: 0.58
Epoch: 072 | Train loss: 0.9166, Val loss: 1.1882 | Train F1: 0.99, Val F1: 0.58
Epoch: 073 | Train loss: 0.9162, Val loss: 1.1882 | Train F1: 0.99, Val F1: 0.58
Epoch: 074 | Train loss: 0.9158, Val loss: 1.1883 | Train F1: 0.99, Val F1: 0.54
Epoch: 075 | Train loss: 0.9155, Val loss: 1.1886 | Train F1: 0.99, Val F1: 0.54
Epoch: 076 | Train loss: 0.9151, Val loss: 1.1890 | Train F1: 1.00, Val F1: 0.54
Epoch: 077 | Train loss: 0.9148, Val loss: 1.1894 | Train F1: 1.00, Val F1: 0.53
Epoch: 078 | Train loss: 0.9144, Val loss: 1.1899 | Train F1: 1.00, Val F1: 0.53
Epoch: 079 | Train loss: 0.9141, Val loss: 1.1903 | Train F1: 1.00, Val F1: 0.56
Epoch: 080 | Train loss: 0.9138, Val loss: 1.1906 | Train F1: 1.00, Val F1: 0.56
Epoch: 081 | Train loss: 0.9136, Val loss: 1.1910 | Train F1: 1.00, Val F1: 0.56
Epoch: 082 | Train loss: 0.9133, Val loss: 1.1913 | Train F1: 1.00, Val F1: 0.56
Epoch: 083 | Train loss: 0.9130, Val loss: 1.1916 | Train F1: 1.00, Val F1: 0.56
Epoch: 084 | Train loss: 0.9128, Val loss: 1.1918 | Train F1: 1.00, Val F1: 0.56
Epoch: 085 | Train loss: 0.9126, Val loss: 1.1919 | Train F1: 1.00, Val F1: 0.56
Best model:
Train loss: 0.9266, Val loss: 1.1931, Test loss: 1.2179
Train F1: 0.94, Val F1: 0.48, Test F1: 0.52

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/n_layers', lr=0.001, max_epochs=1000, n_layers=4, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5764, Val loss: 1.5823 | Train F1: 0.16, Val F1: 0.14
Epoch: 001 | Train loss: 1.5477, Val loss: 1.5605 | Train F1: 0.16, Val F1: 0.13
Epoch: 002 | Train loss: 1.5120, Val loss: 1.5322 | Train F1: 0.20, Val F1: 0.16
Epoch: 003 | Train loss: 1.4733, Val loss: 1.4996 | Train F1: 0.17, Val F1: 0.17
Epoch: 004 | Train loss: 1.4387, Val loss: 1.4676 | Train F1: 0.18, Val F1: 0.19
Epoch: 005 | Train loss: 1.4066, Val loss: 1.4405 | Train F1: 0.21, Val F1: 0.21
Epoch: 006 | Train loss: 1.3688, Val loss: 1.4158 | Train F1: 0.26, Val F1: 0.24
Epoch: 007 | Train loss: 1.3287, Val loss: 1.3916 | Train F1: 0.29, Val F1: 0.26
Epoch: 008 | Train loss: 1.3015, Val loss: 1.3771 | Train F1: 0.30, Val F1: 0.26
Epoch: 009 | Train loss: 1.2810, Val loss: 1.3667 | Train F1: 0.30, Val F1: 0.25
Epoch: 010 | Train loss: 1.2533, Val loss: 1.3469 | Train F1: 0.30, Val F1: 0.26
Epoch: 011 | Train loss: 1.2305, Val loss: 1.3326 | Train F1: 0.39, Val F1: 0.26
Epoch: 012 | Train loss: 1.2131, Val loss: 1.3235 | Train F1: 0.39, Val F1: 0.26
Epoch: 013 | Train loss: 1.1996, Val loss: 1.3167 | Train F1: 0.42, Val F1: 0.26
Epoch: 014 | Train loss: 1.1846, Val loss: 1.3114 | Train F1: 0.46, Val F1: 0.27
Epoch: 015 | Train loss: 1.1693, Val loss: 1.3081 | Train F1: 0.50, Val F1: 0.27
Epoch: 016 | Train loss: 1.1573, Val loss: 1.3078 | Train F1: 0.51, Val F1: 0.26
Epoch: 017 | Train loss: 1.1482, Val loss: 1.3070 | Train F1: 0.55, Val F1: 0.32
Epoch: 018 | Train loss: 1.1381, Val loss: 1.3026 | Train F1: 0.54, Val F1: 0.35
Epoch: 019 | Train loss: 1.1286, Val loss: 1.2968 | Train F1: 0.54, Val F1: 0.43
Epoch: 020 | Train loss: 1.1202, Val loss: 1.2917 | Train F1: 0.54, Val F1: 0.47
Epoch: 021 | Train loss: 1.1111, Val loss: 1.2876 | Train F1: 0.58, Val F1: 0.46
Epoch: 022 | Train loss: 1.1020, Val loss: 1.2842 | Train F1: 0.60, Val F1: 0.46
Epoch: 023 | Train loss: 1.0940, Val loss: 1.2817 | Train F1: 0.60, Val F1: 0.46
Epoch: 024 | Train loss: 1.0855, Val loss: 1.2793 | Train F1: 0.60, Val F1: 0.46
Epoch: 025 | Train loss: 1.0746, Val loss: 1.2774 | Train F1: 0.64, Val F1: 0.49
Epoch: 026 | Train loss: 1.0645, Val loss: 1.2782 | Train F1: 0.64, Val F1: 0.43
Epoch: 027 | Train loss: 1.0548, Val loss: 1.2858 | Train F1: 0.65, Val F1: 0.40
Epoch: 028 | Train loss: 1.0514, Val loss: 1.2940 | Train F1: 0.67, Val F1: 0.38
Epoch: 029 | Train loss: 1.0478, Val loss: 1.2969 | Train F1: 0.67, Val F1: 0.38
Epoch: 030 | Train loss: 1.0423, Val loss: 1.2938 | Train F1: 0.67, Val F1: 0.39
Epoch: 031 | Train loss: 1.0370, Val loss: 1.2873 | Train F1: 0.67, Val F1: 0.39
Epoch: 032 | Train loss: 1.0325, Val loss: 1.2806 | Train F1: 0.76, Val F1: 0.39
Epoch: 033 | Train loss: 1.0284, Val loss: 1.2743 | Train F1: 0.76, Val F1: 0.39
Epoch: 034 | Train loss: 1.0247, Val loss: 1.2679 | Train F1: 0.75, Val F1: 0.40
Epoch: 035 | Train loss: 1.0198, Val loss: 1.2622 | Train F1: 0.75, Val F1: 0.40
Epoch: 036 | Train loss: 1.0119, Val loss: 1.2572 | Train F1: 0.76, Val F1: 0.43
Epoch: 037 | Train loss: 0.9997, Val loss: 1.2525 | Train F1: 0.76, Val F1: 0.43
Epoch: 038 | Train loss: 0.9839, Val loss: 1.2462 | Train F1: 0.86, Val F1: 0.45
Epoch: 039 | Train loss: 0.9745, Val loss: 1.2387 | Train F1: 0.86, Val F1: 0.45
Epoch: 040 | Train loss: 0.9683, Val loss: 1.2335 | Train F1: 0.86, Val F1: 0.47
Epoch: 041 | Train loss: 0.9656, Val loss: 1.2349 | Train F1: 0.86, Val F1: 0.42
Epoch: 042 | Train loss: 0.9638, Val loss: 1.2373 | Train F1: 0.86, Val F1: 0.47
Epoch: 043 | Train loss: 0.9625, Val loss: 1.2411 | Train F1: 0.87, Val F1: 0.45
Epoch: 044 | Train loss: 0.9608, Val loss: 1.2438 | Train F1: 0.87, Val F1: 0.45
Epoch: 045 | Train loss: 0.9585, Val loss: 1.2411 | Train F1: 0.87, Val F1: 0.45
Epoch: 046 | Train loss: 0.9562, Val loss: 1.2323 | Train F1: 0.87, Val F1: 0.46
Epoch: 047 | Train loss: 0.9542, Val loss: 1.2253 | Train F1: 0.87, Val F1: 0.47
Epoch: 048 | Train loss: 0.9519, Val loss: 1.2221 | Train F1: 0.88, Val F1: 0.47
Epoch: 049 | Train loss: 0.9494, Val loss: 1.2247 | Train F1: 0.88, Val F1: 0.47
Epoch: 050 | Train loss: 0.9474, Val loss: 1.2318 | Train F1: 0.89, Val F1: 0.47
Epoch: 051 | Train loss: 0.9454, Val loss: 1.2360 | Train F1: 0.89, Val F1: 0.42
Epoch: 052 | Train loss: 0.9430, Val loss: 1.2366 | Train F1: 0.94, Val F1: 0.42
Epoch: 053 | Train loss: 0.9406, Val loss: 1.2329 | Train F1: 0.94, Val F1: 0.43
Epoch: 054 | Train loss: 0.9386, Val loss: 1.2278 | Train F1: 0.94, Val F1: 0.42
Epoch: 055 | Train loss: 0.9361, Val loss: 1.2260 | Train F1: 0.95, Val F1: 0.42
Epoch: 056 | Train loss: 0.9333, Val loss: 1.2275 | Train F1: 0.95, Val F1: 0.42
Epoch: 057 | Train loss: 0.9312, Val loss: 1.2309 | Train F1: 0.95, Val F1: 0.41
Epoch: 058 | Train loss: 0.9297, Val loss: 1.2366 | Train F1: 0.95, Val F1: 0.41
Epoch: 059 | Train loss: 0.9284, Val loss: 1.2443 | Train F1: 0.95, Val F1: 0.41
Epoch: 060 | Train loss: 0.9274, Val loss: 1.2533 | Train F1: 0.94, Val F1: 0.40
Epoch: 061 | Train loss: 0.9268, Val loss: 1.2610 | Train F1: 0.94, Val F1: 0.40
Epoch: 062 | Train loss: 0.9261, Val loss: 1.2658 | Train F1: 0.94, Val F1: 0.39
Epoch: 063 | Train loss: 0.9252, Val loss: 1.2659 | Train F1: 0.94, Val F1: 0.39
Epoch: 064 | Train loss: 0.9241, Val loss: 1.2630 | Train F1: 0.98, Val F1: 0.39
Epoch: 065 | Train loss: 0.9231, Val loss: 1.2565 | Train F1: 0.98, Val F1: 0.39
Epoch: 066 | Train loss: 0.9222, Val loss: 1.2505 | Train F1: 0.98, Val F1: 0.40
Epoch: 067 | Train loss: 0.9214, Val loss: 1.2446 | Train F1: 0.98, Val F1: 0.41
Epoch: 068 | Train loss: 0.9208, Val loss: 1.2392 | Train F1: 0.98, Val F1: 0.41
Epoch: 069 | Train loss: 0.9203, Val loss: 1.2349 | Train F1: 0.98, Val F1: 0.41
Epoch: 070 | Train loss: 0.9197, Val loss: 1.2309 | Train F1: 0.99, Val F1: 0.45
Epoch: 071 | Train loss: 0.9189, Val loss: 1.2294 | Train F1: 0.99, Val F1: 0.45
Epoch: 072 | Train loss: 0.9183, Val loss: 1.2284 | Train F1: 0.99, Val F1: 0.48
Epoch: 073 | Train loss: 0.9176, Val loss: 1.2281 | Train F1: 0.99, Val F1: 0.48
Epoch: 074 | Train loss: 0.9171, Val loss: 1.2282 | Train F1: 0.99, Val F1: 0.48
Epoch: 075 | Train loss: 0.9166, Val loss: 1.2284 | Train F1: 0.99, Val F1: 0.48
Epoch: 076 | Train loss: 0.9161, Val loss: 1.2283 | Train F1: 0.99, Val F1: 0.48
Epoch: 077 | Train loss: 0.9156, Val loss: 1.2282 | Train F1: 0.99, Val F1: 0.49
Epoch: 078 | Train loss: 0.9151, Val loss: 1.2282 | Train F1: 0.99, Val F1: 0.49
Epoch: 079 | Train loss: 0.9146, Val loss: 1.2278 | Train F1: 1.00, Val F1: 0.48
Epoch: 080 | Train loss: 0.9142, Val loss: 1.2273 | Train F1: 1.00, Val F1: 0.48
Epoch: 081 | Train loss: 0.9138, Val loss: 1.2265 | Train F1: 1.00, Val F1: 0.48
Epoch: 082 | Train loss: 0.9135, Val loss: 1.2257 | Train F1: 1.00, Val F1: 0.48
Epoch: 083 | Train loss: 0.9132, Val loss: 1.2247 | Train F1: 1.00, Val F1: 0.48
Epoch: 084 | Train loss: 0.9130, Val loss: 1.2235 | Train F1: 1.00, Val F1: 0.48
Epoch: 085 | Train loss: 0.9127, Val loss: 1.2223 | Train F1: 1.00, Val F1: 0.49
Epoch: 086 | Train loss: 0.9125, Val loss: 1.2211 | Train F1: 1.00, Val F1: 0.49
Epoch: 087 | Train loss: 0.9123, Val loss: 1.2198 | Train F1: 1.00, Val F1: 0.49
Epoch: 088 | Train loss: 0.9120, Val loss: 1.2185 | Train F1: 1.00, Val F1: 0.49
Epoch: 089 | Train loss: 0.9118, Val loss: 1.2170 | Train F1: 1.00, Val F1: 0.49
Epoch: 090 | Train loss: 0.9116, Val loss: 1.2155 | Train F1: 1.00, Val F1: 0.53
Epoch: 091 | Train loss: 0.9113, Val loss: 1.2140 | Train F1: 1.00, Val F1: 0.53
Epoch: 092 | Train loss: 0.9110, Val loss: 1.2127 | Train F1: 1.00, Val F1: 0.53
Epoch: 093 | Train loss: 0.9108, Val loss: 1.2114 | Train F1: 1.00, Val F1: 0.53
Epoch: 094 | Train loss: 0.9106, Val loss: 1.2103 | Train F1: 1.00, Val F1: 0.54
Epoch: 095 | Train loss: 0.9103, Val loss: 1.2093 | Train F1: 1.00, Val F1: 0.54
Epoch: 096 | Train loss: 0.9101, Val loss: 1.2083 | Train F1: 1.00, Val F1: 0.58
Epoch: 097 | Train loss: 0.9100, Val loss: 1.2074 | Train F1: 1.00, Val F1: 0.58
Epoch: 098 | Train loss: 0.9098, Val loss: 1.2067 | Train F1: 1.00, Val F1: 0.56
Epoch: 099 | Train loss: 0.9096, Val loss: 1.2060 | Train F1: 1.00, Val F1: 0.56
Epoch: 100 | Train loss: 0.9095, Val loss: 1.2053 | Train F1: 1.00, Val F1: 0.56
Epoch: 101 | Train loss: 0.9093, Val loss: 1.2047 | Train F1: 1.00, Val F1: 0.56
Epoch: 102 | Train loss: 0.9092, Val loss: 1.2041 | Train F1: 1.00, Val F1: 0.55
Epoch: 103 | Train loss: 0.9091, Val loss: 1.2036 | Train F1: 1.00, Val F1: 0.55
Epoch: 104 | Train loss: 0.9089, Val loss: 1.2031 | Train F1: 1.00, Val F1: 0.55
Epoch: 105 | Train loss: 0.9088, Val loss: 1.2026 | Train F1: 1.00, Val F1: 0.55
Epoch: 106 | Train loss: 0.9087, Val loss: 1.2020 | Train F1: 1.00, Val F1: 0.55
Epoch: 107 | Train loss: 0.9086, Val loss: 1.2014 | Train F1: 1.00, Val F1: 0.56
Epoch: 108 | Train loss: 0.9085, Val loss: 1.2009 | Train F1: 1.00, Val F1: 0.56
Epoch: 109 | Train loss: 0.9084, Val loss: 1.2005 | Train F1: 1.00, Val F1: 0.56
Epoch: 110 | Train loss: 0.9083, Val loss: 1.2001 | Train F1: 1.00, Val F1: 0.56
Epoch: 111 | Train loss: 0.9082, Val loss: 1.1996 | Train F1: 1.00, Val F1: 0.56
Epoch: 112 | Train loss: 0.9081, Val loss: 1.1991 | Train F1: 1.00, Val F1: 0.56
Epoch: 113 | Train loss: 0.9080, Val loss: 1.1986 | Train F1: 1.00, Val F1: 0.56
Epoch: 114 | Train loss: 0.9079, Val loss: 1.1981 | Train F1: 1.00, Val F1: 0.56
Epoch: 115 | Train loss: 0.9078, Val loss: 1.1977 | Train F1: 1.00, Val F1: 0.56
Epoch: 116 | Train loss: 0.9077, Val loss: 1.1974 | Train F1: 1.00, Val F1: 0.56
Epoch: 117 | Train loss: 0.9076, Val loss: 1.1971 | Train F1: 1.00, Val F1: 0.56
Epoch: 118 | Train loss: 0.9075, Val loss: 1.1969 | Train F1: 1.00, Val F1: 0.56
Epoch: 119 | Train loss: 0.9075, Val loss: 1.1967 | Train F1: 1.00, Val F1: 0.56
Epoch: 120 | Train loss: 0.9074, Val loss: 1.1966 | Train F1: 1.00, Val F1: 0.56
Epoch: 121 | Train loss: 0.9073, Val loss: 1.1960 | Train F1: 1.00, Val F1: 0.56
Epoch: 122 | Train loss: 0.9072, Val loss: 1.1955 | Train F1: 1.00, Val F1: 0.56
Epoch: 123 | Train loss: 0.9071, Val loss: 1.1949 | Train F1: 1.00, Val F1: 0.56
Epoch: 124 | Train loss: 0.9071, Val loss: 1.1943 | Train F1: 1.00, Val F1: 0.56
Epoch: 125 | Train loss: 0.9070, Val loss: 1.1938 | Train F1: 1.00, Val F1: 0.57
Epoch: 126 | Train loss: 0.9069, Val loss: 1.1932 | Train F1: 1.00, Val F1: 0.57
Epoch: 127 | Train loss: 0.9069, Val loss: 1.1927 | Train F1: 1.00, Val F1: 0.57
Epoch: 128 | Train loss: 0.9068, Val loss: 1.1923 | Train F1: 1.00, Val F1: 0.57
Epoch: 129 | Train loss: 0.9067, Val loss: 1.1919 | Train F1: 1.00, Val F1: 0.57
Epoch: 130 | Train loss: 0.9067, Val loss: 1.1924 | Train F1: 1.00, Val F1: 0.58
Epoch: 131 | Train loss: 0.9066, Val loss: 1.1929 | Train F1: 1.00, Val F1: 0.58
Epoch: 132 | Train loss: 0.9066, Val loss: 1.1936 | Train F1: 1.00, Val F1: 0.58
Epoch: 133 | Train loss: 0.9065, Val loss: 1.1942 | Train F1: 1.00, Val F1: 0.55
Epoch: 134 | Train loss: 0.9064, Val loss: 1.1949 | Train F1: 1.00, Val F1: 0.55
Epoch: 135 | Train loss: 0.9064, Val loss: 1.1955 | Train F1: 1.00, Val F1: 0.55
Epoch: 136 | Train loss: 0.9063, Val loss: 1.1961 | Train F1: 1.00, Val F1: 0.55
Epoch: 137 | Train loss: 0.9063, Val loss: 1.1966 | Train F1: 1.00, Val F1: 0.55
Epoch: 138 | Train loss: 0.9062, Val loss: 1.1971 | Train F1: 1.00, Val F1: 0.55
Epoch: 139 | Train loss: 0.9062, Val loss: 1.1975 | Train F1: 1.00, Val F1: 0.55
Epoch: 140 | Train loss: 0.9061, Val loss: 1.1979 | Train F1: 1.00, Val F1: 0.55
Epoch: 141 | Train loss: 0.9061, Val loss: 1.1982 | Train F1: 1.00, Val F1: 0.55
Epoch: 142 | Train loss: 0.9060, Val loss: 1.1984 | Train F1: 1.00, Val F1: 0.55
Epoch: 143 | Train loss: 0.9060, Val loss: 1.1986 | Train F1: 1.00, Val F1: 0.55
Epoch: 144 | Train loss: 0.9059, Val loss: 1.1986 | Train F1: 1.00, Val F1: 0.55
Epoch: 145 | Train loss: 0.9059, Val loss: 1.1987 | Train F1: 1.00, Val F1: 0.55
Epoch: 146 | Train loss: 0.9059, Val loss: 1.1987 | Train F1: 1.00, Val F1: 0.55
Epoch: 147 | Train loss: 0.9058, Val loss: 1.1987 | Train F1: 1.00, Val F1: 0.55
Epoch: 148 | Train loss: 0.9058, Val loss: 1.1986 | Train F1: 1.00, Val F1: 0.54
Epoch: 149 | Train loss: 0.9057, Val loss: 1.1986 | Train F1: 1.00, Val F1: 0.54
Best model:
Train loss: 0.9075, Val loss: 1.1967, Test loss: 1.2595
Train F1: 1.00, Val F1: 0.56, Test F1: 0.48

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/n_layers', lr=0.001, max_epochs=1000, n_layers=5, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5947, Val loss: 1.6002 | Train F1: 0.36, Val F1: 0.22
Epoch: 001 | Train loss: 1.5685, Val loss: 1.5800 | Train F1: 0.28, Val F1: 0.23
Epoch: 002 | Train loss: 1.5319, Val loss: 1.5512 | Train F1: 0.29, Val F1: 0.20
Epoch: 003 | Train loss: 1.4870, Val loss: 1.5150 | Train F1: 0.24, Val F1: 0.20
Epoch: 004 | Train loss: 1.4432, Val loss: 1.4778 | Train F1: 0.22, Val F1: 0.20
Epoch: 005 | Train loss: 1.4067, Val loss: 1.4446 | Train F1: 0.22, Val F1: 0.21
Epoch: 006 | Train loss: 1.3762, Val loss: 1.4183 | Train F1: 0.24, Val F1: 0.22
Epoch: 007 | Train loss: 1.3515, Val loss: 1.3995 | Train F1: 0.28, Val F1: 0.24
Epoch: 008 | Train loss: 1.3304, Val loss: 1.3849 | Train F1: 0.29, Val F1: 0.26
Epoch: 009 | Train loss: 1.3115, Val loss: 1.3734 | Train F1: 0.29, Val F1: 0.26
Epoch: 010 | Train loss: 1.2997, Val loss: 1.3685 | Train F1: 0.30, Val F1: 0.25
Epoch: 011 | Train loss: 1.2842, Val loss: 1.3608 | Train F1: 0.30, Val F1: 0.26
Epoch: 012 | Train loss: 1.2655, Val loss: 1.3459 | Train F1: 0.30, Val F1: 0.28
Epoch: 013 | Train loss: 1.2511, Val loss: 1.3366 | Train F1: 0.31, Val F1: 0.28
Epoch: 014 | Train loss: 1.2395, Val loss: 1.3324 | Train F1: 0.31, Val F1: 0.27
Epoch: 015 | Train loss: 1.2293, Val loss: 1.3305 | Train F1: 0.31, Val F1: 0.26
Epoch: 016 | Train loss: 1.2203, Val loss: 1.3233 | Train F1: 0.33, Val F1: 0.25
Epoch: 017 | Train loss: 1.2122, Val loss: 1.3192 | Train F1: 0.37, Val F1: 0.25
Epoch: 018 | Train loss: 1.2029, Val loss: 1.3181 | Train F1: 0.41, Val F1: 0.26
Epoch: 019 | Train loss: 1.1916, Val loss: 1.3206 | Train F1: 0.43, Val F1: 0.26
Epoch: 020 | Train loss: 1.1788, Val loss: 1.3262 | Train F1: 0.43, Val F1: 0.26
Epoch: 021 | Train loss: 1.1703, Val loss: 1.3304 | Train F1: 0.43, Val F1: 0.31
Epoch: 022 | Train loss: 1.1612, Val loss: 1.3300 | Train F1: 0.44, Val F1: 0.31
Epoch: 023 | Train loss: 1.1495, Val loss: 1.3243 | Train F1: 0.45, Val F1: 0.31
Epoch: 024 | Train loss: 1.1294, Val loss: 1.3090 | Train F1: 0.56, Val F1: 0.31
Epoch: 025 | Train loss: 1.1165, Val loss: 1.2928 | Train F1: 0.56, Val F1: 0.47
Epoch: 026 | Train loss: 1.1110, Val loss: 1.2921 | Train F1: 0.56, Val F1: 0.46
Epoch: 027 | Train loss: 1.1041, Val loss: 1.3083 | Train F1: 0.57, Val F1: 0.38
Epoch: 028 | Train loss: 1.0960, Val loss: 1.3215 | Train F1: 0.57, Val F1: 0.36
Epoch: 029 | Train loss: 1.0846, Val loss: 1.3246 | Train F1: 0.58, Val F1: 0.36
Epoch: 030 | Train loss: 1.0688, Val loss: 1.3210 | Train F1: 0.59, Val F1: 0.38
Epoch: 031 | Train loss: 1.0482, Val loss: 1.3123 | Train F1: 0.64, Val F1: 0.41
Epoch: 032 | Train loss: 1.0386, Val loss: 1.3169 | Train F1: 0.66, Val F1: 0.41
Epoch: 033 | Train loss: 1.0313, Val loss: 1.3178 | Train F1: 0.65, Val F1: 0.40
Epoch: 034 | Train loss: 1.0218, Val loss: 1.3079 | Train F1: 0.67, Val F1: 0.42
Epoch: 035 | Train loss: 1.0133, Val loss: 1.2839 | Train F1: 0.67, Val F1: 0.42
Epoch: 036 | Train loss: 1.0113, Val loss: 1.2665 | Train F1: 0.67, Val F1: 0.44
Epoch: 037 | Train loss: 1.0100, Val loss: 1.2575 | Train F1: 0.67, Val F1: 0.43
Epoch: 038 | Train loss: 1.0084, Val loss: 1.2518 | Train F1: 0.67, Val F1: 0.44
Epoch: 039 | Train loss: 1.0068, Val loss: 1.2477 | Train F1: 0.67, Val F1: 0.44
Epoch: 040 | Train loss: 1.0060, Val loss: 1.2443 | Train F1: 0.68, Val F1: 0.44
Epoch: 041 | Train loss: 1.0044, Val loss: 1.2425 | Train F1: 0.70, Val F1: 0.43
Epoch: 042 | Train loss: 1.0019, Val loss: 1.2420 | Train F1: 0.70, Val F1: 0.43
Epoch: 043 | Train loss: 0.9992, Val loss: 1.2425 | Train F1: 0.70, Val F1: 0.42
Epoch: 044 | Train loss: 0.9934, Val loss: 1.2447 | Train F1: 0.70, Val F1: 0.42
Epoch: 045 | Train loss: 0.9867, Val loss: 1.2491 | Train F1: 0.79, Val F1: 0.39
Epoch: 046 | Train loss: 0.9838, Val loss: 1.2500 | Train F1: 0.85, Val F1: 0.39
Epoch: 047 | Train loss: 0.9797, Val loss: 1.2499 | Train F1: 0.85, Val F1: 0.41
Epoch: 048 | Train loss: 0.9742, Val loss: 1.2500 | Train F1: 0.83, Val F1: 0.41
Epoch: 049 | Train loss: 0.9701, Val loss: 1.2533 | Train F1: 0.85, Val F1: 0.38
Epoch: 050 | Train loss: 0.9666, Val loss: 1.2584 | Train F1: 0.87, Val F1: 0.39
Epoch: 051 | Train loss: 0.9643, Val loss: 1.2662 | Train F1: 0.87, Val F1: 0.39
Epoch: 052 | Train loss: 0.9630, Val loss: 1.2726 | Train F1: 0.87, Val F1: 0.38
Epoch: 053 | Train loss: 0.9624, Val loss: 1.2757 | Train F1: 0.87, Val F1: 0.38
Epoch: 054 | Train loss: 0.9621, Val loss: 1.2782 | Train F1: 0.86, Val F1: 0.39
Epoch: 055 | Train loss: 0.9614, Val loss: 1.2778 | Train F1: 0.87, Val F1: 0.37
Epoch: 056 | Train loss: 0.9613, Val loss: 1.2768 | Train F1: 0.87, Val F1: 0.37
Epoch: 057 | Train loss: 0.9609, Val loss: 1.2823 | Train F1: 0.87, Val F1: 0.35
Epoch: 058 | Train loss: 0.9594, Val loss: 1.2877 | Train F1: 0.87, Val F1: 0.36
Epoch: 059 | Train loss: 0.9551, Val loss: 1.2931 | Train F1: 0.89, Val F1: 0.36
Epoch: 060 | Train loss: 0.9510, Val loss: 1.2981 | Train F1: 0.89, Val F1: 0.36
Epoch: 061 | Train loss: 0.9483, Val loss: 1.3014 | Train F1: 0.88, Val F1: 0.38
Epoch: 062 | Train loss: 0.9457, Val loss: 1.3064 | Train F1: 0.90, Val F1: 0.38
Epoch: 063 | Train loss: 0.9445, Val loss: 1.3121 | Train F1: 0.90, Val F1: 0.37
Epoch: 064 | Train loss: 0.9430, Val loss: 1.3112 | Train F1: 0.90, Val F1: 0.37
Epoch: 065 | Train loss: 0.9416, Val loss: 1.3002 | Train F1: 0.91, Val F1: 0.37
Epoch: 066 | Train loss: 0.9417, Val loss: 1.2853 | Train F1: 0.92, Val F1: 0.38
Epoch: 067 | Train loss: 0.9414, Val loss: 1.2775 | Train F1: 0.92, Val F1: 0.40
Epoch: 068 | Train loss: 0.9405, Val loss: 1.2721 | Train F1: 0.92, Val F1: 0.40
Epoch: 069 | Train loss: 0.9392, Val loss: 1.2683 | Train F1: 0.95, Val F1: 0.41
Epoch: 070 | Train loss: 0.9377, Val loss: 1.2654 | Train F1: 0.95, Val F1: 0.41
Epoch: 071 | Train loss: 0.9360, Val loss: 1.2640 | Train F1: 0.95, Val F1: 0.41
Epoch: 072 | Train loss: 0.9347, Val loss: 1.2661 | Train F1: 0.95, Val F1: 0.41
Epoch: 073 | Train loss: 0.9338, Val loss: 1.2701 | Train F1: 0.95, Val F1: 0.42
Epoch: 074 | Train loss: 0.9331, Val loss: 1.2745 | Train F1: 0.97, Val F1: 0.42
Epoch: 075 | Train loss: 0.9324, Val loss: 1.2782 | Train F1: 0.97, Val F1: 0.40
Epoch: 076 | Train loss: 0.9319, Val loss: 1.2797 | Train F1: 0.97, Val F1: 0.40
Epoch: 077 | Train loss: 0.9314, Val loss: 1.2802 | Train F1: 0.97, Val F1: 0.38
Epoch: 078 | Train loss: 0.9312, Val loss: 1.2763 | Train F1: 0.97, Val F1: 0.38
Epoch: 079 | Train loss: 0.9310, Val loss: 1.2698 | Train F1: 0.97, Val F1: 0.39
Epoch: 080 | Train loss: 0.9308, Val loss: 1.2643 | Train F1: 0.97, Val F1: 0.41
Epoch: 081 | Train loss: 0.9306, Val loss: 1.2599 | Train F1: 0.97, Val F1: 0.41
Epoch: 082 | Train loss: 0.9304, Val loss: 1.2567 | Train F1: 0.97, Val F1: 0.41
Epoch: 083 | Train loss: 0.9300, Val loss: 1.2566 | Train F1: 0.97, Val F1: 0.41
Epoch: 084 | Train loss: 0.9296, Val loss: 1.2570 | Train F1: 0.97, Val F1: 0.40
Epoch: 085 | Train loss: 0.9289, Val loss: 1.2590 | Train F1: 0.97, Val F1: 0.40
Epoch: 086 | Train loss: 0.9283, Val loss: 1.2618 | Train F1: 0.97, Val F1: 0.40
Epoch: 087 | Train loss: 0.9278, Val loss: 1.2648 | Train F1: 0.95, Val F1: 0.40
Epoch: 088 | Train loss: 0.9273, Val loss: 1.2632 | Train F1: 0.95, Val F1: 0.40
Epoch: 089 | Train loss: 0.9269, Val loss: 1.2616 | Train F1: 0.95, Val F1: 0.40
Epoch: 090 | Train loss: 0.9265, Val loss: 1.2591 | Train F1: 0.95, Val F1: 0.42
Epoch: 091 | Train loss: 0.9261, Val loss: 1.2565 | Train F1: 0.95, Val F1: 0.42
Epoch: 092 | Train loss: 0.9258, Val loss: 1.2540 | Train F1: 0.95, Val F1: 0.42
Epoch: 093 | Train loss: 0.9255, Val loss: 1.2523 | Train F1: 0.95, Val F1: 0.42
Epoch: 094 | Train loss: 0.9252, Val loss: 1.2510 | Train F1: 0.95, Val F1: 0.43
Epoch: 095 | Train loss: 0.9250, Val loss: 1.2497 | Train F1: 0.95, Val F1: 0.43
Epoch: 096 | Train loss: 0.9248, Val loss: 1.2489 | Train F1: 0.95, Val F1: 0.43
Epoch: 097 | Train loss: 0.9247, Val loss: 1.2492 | Train F1: 0.95, Val F1: 0.43
Epoch: 098 | Train loss: 0.9247, Val loss: 1.2494 | Train F1: 0.95, Val F1: 0.43
Epoch: 099 | Train loss: 0.9245, Val loss: 1.2492 | Train F1: 0.97, Val F1: 0.43
Epoch: 100 | Train loss: 0.9243, Val loss: 1.2490 | Train F1: 0.97, Val F1: 0.43
Epoch: 101 | Train loss: 0.9240, Val loss: 1.2482 | Train F1: 0.97, Val F1: 0.43
Epoch: 102 | Train loss: 0.9238, Val loss: 1.2478 | Train F1: 0.97, Val F1: 0.43
Epoch: 103 | Train loss: 0.9236, Val loss: 1.2497 | Train F1: 0.95, Val F1: 0.43
Epoch: 104 | Train loss: 0.9235, Val loss: 1.2509 | Train F1: 0.95, Val F1: 0.43
Epoch: 105 | Train loss: 0.9234, Val loss: 1.2534 | Train F1: 0.95, Val F1: 0.43
Epoch: 106 | Train loss: 0.9233, Val loss: 1.2542 | Train F1: 0.95, Val F1: 0.43
Epoch: 107 | Train loss: 0.9232, Val loss: 1.2546 | Train F1: 0.95, Val F1: 0.43
Epoch: 108 | Train loss: 0.9230, Val loss: 1.2548 | Train F1: 0.95, Val F1: 0.43
Epoch: 109 | Train loss: 0.9228, Val loss: 1.2548 | Train F1: 0.96, Val F1: 0.43
Epoch: 110 | Train loss: 0.9226, Val loss: 1.2550 | Train F1: 0.97, Val F1: 0.43
Epoch: 111 | Train loss: 0.9223, Val loss: 1.2548 | Train F1: 0.97, Val F1: 0.43
Epoch: 112 | Train loss: 0.9221, Val loss: 1.2548 | Train F1: 0.97, Val F1: 0.43
Epoch: 113 | Train loss: 0.9219, Val loss: 1.2543 | Train F1: 0.97, Val F1: 0.43
Epoch: 114 | Train loss: 0.9218, Val loss: 1.2533 | Train F1: 0.97, Val F1: 0.43
Epoch: 115 | Train loss: 0.9216, Val loss: 1.2512 | Train F1: 0.99, Val F1: 0.42
Epoch: 116 | Train loss: 0.9215, Val loss: 1.2496 | Train F1: 0.99, Val F1: 0.42
Epoch: 117 | Train loss: 0.9215, Val loss: 1.2478 | Train F1: 0.99, Val F1: 0.42
Epoch: 118 | Train loss: 0.9214, Val loss: 1.2437 | Train F1: 0.99, Val F1: 0.41
Epoch: 119 | Train loss: 0.9213, Val loss: 1.2411 | Train F1: 0.99, Val F1: 0.41
Epoch: 120 | Train loss: 0.9211, Val loss: 1.2402 | Train F1: 0.99, Val F1: 0.41
Epoch: 121 | Train loss: 0.9209, Val loss: 1.2410 | Train F1: 0.99, Val F1: 0.41
Epoch: 122 | Train loss: 0.9207, Val loss: 1.2422 | Train F1: 0.99, Val F1: 0.39
Epoch: 123 | Train loss: 0.9204, Val loss: 1.2438 | Train F1: 0.99, Val F1: 0.39
Epoch: 124 | Train loss: 0.9201, Val loss: 1.2462 | Train F1: 0.99, Val F1: 0.39
Epoch: 125 | Train loss: 0.9197, Val loss: 1.2507 | Train F1: 0.99, Val F1: 0.38
Best model:
Train loss: 0.9250, Val loss: 1.2497, Test loss: 1.2648
Train F1: 0.95, Val F1: 0.43, Test F1: 0.45

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='n_layers', log_path='log/graphsage/wisconsin/n_layers', method='graphsage', path='data/graphsage', plot_path='plots/graphsage/wisconsin/n_layers')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=32, hops=3, log_path='log/nagphormer/cora/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=32, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=32, out_features=32, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=32, out_features=64, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=64, out_features=32, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=32, out_features=16, bias=True)
  (attn_layer): Linear(in_features=64, out_features=1, bias=True)
  (Linear1): Linear(in_features=16, out_features=7, bias=True)
)
total params: 55305
Epoch: 0001 | loss_train: 1.9463 loss_val: 1.9447 | acc_train: 0.1000 acc_val: 0.1600 | f1_train: 0.0825 f1_val: 0.1171
Epoch: 0002 | loss_train: 1.9461 loss_val: 1.9445 | acc_train: 0.1000 acc_val: 0.1700 | f1_train: 0.0828 f1_val: 0.1247
Epoch: 0003 | loss_train: 1.9458 loss_val: 1.9442 | acc_train: 0.1143 acc_val: 0.1820 | f1_train: 0.0975 f1_val: 0.1388
Epoch: 0004 | loss_train: 1.9451 loss_val: 1.9438 | acc_train: 0.1429 acc_val: 0.1940 | f1_train: 0.1224 f1_val: 0.1458
Epoch: 0005 | loss_train: 1.9443 loss_val: 1.9432 | acc_train: 0.1643 acc_val: 0.2240 | f1_train: 0.1398 f1_val: 0.1742
Epoch: 0006 | loss_train: 1.9434 loss_val: 1.9426 | acc_train: 0.2000 acc_val: 0.2500 | f1_train: 0.1709 f1_val: 0.1967
Epoch: 0007 | loss_train: 1.9423 loss_val: 1.9419 | acc_train: 0.2714 acc_val: 0.2800 | f1_train: 0.2365 f1_val: 0.2224
Epoch: 0008 | loss_train: 1.9409 loss_val: 1.9410 | acc_train: 0.3143 acc_val: 0.2940 | f1_train: 0.2737 f1_val: 0.2342
Epoch: 0009 | loss_train: 1.9394 loss_val: 1.9401 | acc_train: 0.3643 acc_val: 0.3240 | f1_train: 0.3158 f1_val: 0.2595
Epoch: 0010 | loss_train: 1.9378 loss_val: 1.9390 | acc_train: 0.3929 acc_val: 0.3500 | f1_train: 0.3395 f1_val: 0.2762
Epoch: 0011 | loss_train: 1.9359 loss_val: 1.9378 | acc_train: 0.4643 acc_val: 0.3860 | f1_train: 0.4057 f1_val: 0.3152
Epoch: 0012 | loss_train: 1.9338 loss_val: 1.9363 | acc_train: 0.5286 acc_val: 0.4060 | f1_train: 0.4760 f1_val: 0.3326
Epoch: 0013 | loss_train: 1.9315 loss_val: 1.9347 | acc_train: 0.5643 acc_val: 0.4320 | f1_train: 0.5153 f1_val: 0.3531
Epoch: 0014 | loss_train: 1.9289 loss_val: 1.9328 | acc_train: 0.5643 acc_val: 0.4600 | f1_train: 0.5146 f1_val: 0.3923
Epoch: 0015 | loss_train: 1.9261 loss_val: 1.9307 | acc_train: 0.5929 acc_val: 0.4920 | f1_train: 0.5509 f1_val: 0.4284
Epoch: 0016 | loss_train: 1.9229 loss_val: 1.9284 | acc_train: 0.6286 acc_val: 0.5100 | f1_train: 0.5993 f1_val: 0.4497
Epoch: 0017 | loss_train: 1.9194 loss_val: 1.9258 | acc_train: 0.7000 acc_val: 0.5260 | f1_train: 0.6893 f1_val: 0.4769
Epoch: 0018 | loss_train: 1.9155 loss_val: 1.9229 | acc_train: 0.7286 acc_val: 0.5380 | f1_train: 0.7156 f1_val: 0.4907
Epoch: 0019 | loss_train: 1.9113 loss_val: 1.9196 | acc_train: 0.7286 acc_val: 0.5580 | f1_train: 0.7156 f1_val: 0.5139
Epoch: 0020 | loss_train: 1.9066 loss_val: 1.9160 | acc_train: 0.7357 acc_val: 0.5740 | f1_train: 0.7231 f1_val: 0.5382
Epoch: 0021 | loss_train: 1.9014 loss_val: 1.9120 | acc_train: 0.7643 acc_val: 0.5820 | f1_train: 0.7587 f1_val: 0.5430
Epoch: 0022 | loss_train: 1.8957 loss_val: 1.9077 | acc_train: 0.7500 acc_val: 0.5860 | f1_train: 0.7414 f1_val: 0.5477
Epoch: 0023 | loss_train: 1.8895 loss_val: 1.9028 | acc_train: 0.7643 acc_val: 0.6040 | f1_train: 0.7532 f1_val: 0.5620
Epoch: 0024 | loss_train: 1.8825 loss_val: 1.8975 | acc_train: 0.7714 acc_val: 0.6100 | f1_train: 0.7535 f1_val: 0.5698
Epoch: 0025 | loss_train: 1.8750 loss_val: 1.8916 | acc_train: 0.7857 acc_val: 0.6040 | f1_train: 0.7659 f1_val: 0.5656
Epoch: 0026 | loss_train: 1.8667 loss_val: 1.8849 | acc_train: 0.8000 acc_val: 0.6040 | f1_train: 0.7850 f1_val: 0.5633
Epoch: 0027 | loss_train: 1.8574 loss_val: 1.8775 | acc_train: 0.8071 acc_val: 0.6060 | f1_train: 0.7923 f1_val: 0.5686
Epoch: 0028 | loss_train: 1.8478 loss_val: 1.8691 | acc_train: 0.8000 acc_val: 0.6160 | f1_train: 0.7832 f1_val: 0.5803
Epoch: 0029 | loss_train: 1.8373 loss_val: 1.8603 | acc_train: 0.7929 acc_val: 0.6180 | f1_train: 0.7753 f1_val: 0.5874
Epoch: 0030 | loss_train: 1.8259 loss_val: 1.8510 | acc_train: 0.8000 acc_val: 0.6240 | f1_train: 0.7845 f1_val: 0.5957
Epoch: 0031 | loss_train: 1.8136 loss_val: 1.8415 | acc_train: 0.8214 acc_val: 0.6260 | f1_train: 0.8043 f1_val: 0.5969
Epoch: 0032 | loss_train: 1.8004 loss_val: 1.8312 | acc_train: 0.8357 acc_val: 0.6200 | f1_train: 0.8200 f1_val: 0.5865
Epoch: 0033 | loss_train: 1.7861 loss_val: 1.8197 | acc_train: 0.8357 acc_val: 0.6200 | f1_train: 0.8200 f1_val: 0.5883
Epoch: 0034 | loss_train: 1.7710 loss_val: 1.8073 | acc_train: 0.8357 acc_val: 0.6140 | f1_train: 0.8200 f1_val: 0.5778
Epoch: 0035 | loss_train: 1.7544 loss_val: 1.7940 | acc_train: 0.8357 acc_val: 0.6080 | f1_train: 0.8200 f1_val: 0.5740
Epoch: 0036 | loss_train: 1.7369 loss_val: 1.7804 | acc_train: 0.8214 acc_val: 0.6040 | f1_train: 0.8020 f1_val: 0.5658
Epoch: 0037 | loss_train: 1.7178 loss_val: 1.7658 | acc_train: 0.8286 acc_val: 0.6000 | f1_train: 0.8112 f1_val: 0.5596
Epoch: 0038 | loss_train: 1.6974 loss_val: 1.7494 | acc_train: 0.8071 acc_val: 0.5960 | f1_train: 0.7822 f1_val: 0.5495
Epoch: 0039 | loss_train: 1.6759 loss_val: 1.7324 | acc_train: 0.8000 acc_val: 0.5920 | f1_train: 0.7721 f1_val: 0.5437
Epoch: 0040 | loss_train: 1.6524 loss_val: 1.7157 | acc_train: 0.7929 acc_val: 0.5900 | f1_train: 0.7601 f1_val: 0.5409
Epoch: 0041 | loss_train: 1.6276 loss_val: 1.6960 | acc_train: 0.7929 acc_val: 0.5860 | f1_train: 0.7486 f1_val: 0.5354
Epoch: 0042 | loss_train: 1.6012 loss_val: 1.6746 | acc_train: 0.8000 acc_val: 0.5900 | f1_train: 0.7570 f1_val: 0.5334
Epoch: 0043 | loss_train: 1.5739 loss_val: 1.6567 | acc_train: 0.7786 acc_val: 0.5920 | f1_train: 0.7245 f1_val: 0.5426
Epoch: 0044 | loss_train: 1.5437 loss_val: 1.6333 | acc_train: 0.8000 acc_val: 0.5960 | f1_train: 0.7500 f1_val: 0.5450
Epoch: 0045 | loss_train: 1.5124 loss_val: 1.6070 | acc_train: 0.7857 acc_val: 0.5820 | f1_train: 0.7333 f1_val: 0.5211
Epoch: 0046 | loss_train: 1.4800 loss_val: 1.5861 | acc_train: 0.7857 acc_val: 0.5920 | f1_train: 0.7333 f1_val: 0.5412
Epoch: 0047 | loss_train: 1.4444 loss_val: 1.5648 | acc_train: 0.7857 acc_val: 0.5920 | f1_train: 0.7333 f1_val: 0.5451
Epoch: 0048 | loss_train: 1.4102 loss_val: 1.5327 | acc_train: 0.8000 acc_val: 0.5900 | f1_train: 0.7500 f1_val: 0.5361
Epoch: 0049 | loss_train: 1.3711 loss_val: 1.5072 | acc_train: 0.7571 acc_val: 0.5820 | f1_train: 0.6956 f1_val: 0.5287
Epoch: 0050 | loss_train: 1.3320 loss_val: 1.4893 | acc_train: 0.8000 acc_val: 0.6000 | f1_train: 0.7500 f1_val: 0.5514
Epoch: 0051 | loss_train: 1.2911 loss_val: 1.4626 | acc_train: 0.8286 acc_val: 0.5980 | f1_train: 0.7807 f1_val: 0.5579
Epoch: 0052 | loss_train: 1.2489 loss_val: 1.4313 | acc_train: 0.8214 acc_val: 0.5940 | f1_train: 0.7732 f1_val: 0.5553
Epoch: 0053 | loss_train: 1.2063 loss_val: 1.4083 | acc_train: 0.8214 acc_val: 0.6020 | f1_train: 0.7732 f1_val: 0.5665
Epoch: 0054 | loss_train: 1.1629 loss_val: 1.3885 | acc_train: 0.8429 acc_val: 0.6040 | f1_train: 0.7952 f1_val: 0.5688
Epoch: 0055 | loss_train: 1.1182 loss_val: 1.3602 | acc_train: 0.8500 acc_val: 0.6100 | f1_train: 0.8024 f1_val: 0.5756
Epoch: 0056 | loss_train: 1.0733 loss_val: 1.3379 | acc_train: 0.8571 acc_val: 0.6180 | f1_train: 0.8095 f1_val: 0.5902
Epoch: 0057 | loss_train: 1.0283 loss_val: 1.3164 | acc_train: 0.8571 acc_val: 0.6380 | f1_train: 0.8095 f1_val: 0.6116
Epoch: 0058 | loss_train: 0.9833 loss_val: 1.2936 | acc_train: 0.8643 acc_val: 0.6560 | f1_train: 0.8247 f1_val: 0.6392
Epoch: 0059 | loss_train: 0.9375 loss_val: 1.2815 | acc_train: 0.9500 acc_val: 0.6480 | f1_train: 0.9484 f1_val: 0.6338
Epoch: 0060 | loss_train: 0.8936 loss_val: 1.2589 | acc_train: 0.9786 acc_val: 0.6480 | f1_train: 0.9785 f1_val: 0.6294
Epoch: 0061 | loss_train: 0.8514 loss_val: 1.2427 | acc_train: 0.9857 acc_val: 0.6520 | f1_train: 0.9857 f1_val: 0.6372
Epoch: 0062 | loss_train: 0.8048 loss_val: 1.2296 | acc_train: 0.9929 acc_val: 0.6520 | f1_train: 0.9929 f1_val: 0.6399
Epoch: 0063 | loss_train: 0.7635 loss_val: 1.2066 | acc_train: 1.0000 acc_val: 0.6600 | f1_train: 1.0000 f1_val: 0.6461
Epoch: 0064 | loss_train: 0.7218 loss_val: 1.1896 | acc_train: 1.0000 acc_val: 0.6760 | f1_train: 1.0000 f1_val: 0.6688
Epoch: 0065 | loss_train: 0.6775 loss_val: 1.1914 | acc_train: 1.0000 acc_val: 0.6580 | f1_train: 1.0000 f1_val: 0.6510
Epoch: 0066 | loss_train: 0.6376 loss_val: 1.1709 | acc_train: 1.0000 acc_val: 0.6700 | f1_train: 1.0000 f1_val: 0.6614
Epoch: 0067 | loss_train: 0.5983 loss_val: 1.1620 | acc_train: 1.0000 acc_val: 0.6820 | f1_train: 1.0000 f1_val: 0.6709
Epoch: 0068 | loss_train: 0.5598 loss_val: 1.1712 | acc_train: 1.0000 acc_val: 0.6660 | f1_train: 1.0000 f1_val: 0.6589
Epoch: 0069 | loss_train: 0.5207 loss_val: 1.1665 | acc_train: 1.0000 acc_val: 0.6460 | f1_train: 1.0000 f1_val: 0.6374
Epoch: 0070 | loss_train: 0.4843 loss_val: 1.1332 | acc_train: 1.0000 acc_val: 0.6620 | f1_train: 1.0000 f1_val: 0.6546
Epoch: 0071 | loss_train: 0.4491 loss_val: 1.1398 | acc_train: 1.0000 acc_val: 0.6660 | f1_train: 1.0000 f1_val: 0.6593
Epoch: 0072 | loss_train: 0.4139 loss_val: 1.1517 | acc_train: 1.0000 acc_val: 0.6600 | f1_train: 1.0000 f1_val: 0.6532
Epoch: 0073 | loss_train: 0.3824 loss_val: 1.1309 | acc_train: 1.0000 acc_val: 0.6760 | f1_train: 1.0000 f1_val: 0.6685
Epoch: 0074 | loss_train: 0.3496 loss_val: 1.1300 | acc_train: 1.0000 acc_val: 0.6680 | f1_train: 1.0000 f1_val: 0.6588
Epoch: 0075 | loss_train: 0.3217 loss_val: 1.1424 | acc_train: 1.0000 acc_val: 0.6600 | f1_train: 1.0000 f1_val: 0.6547
Epoch: 0076 | loss_train: 0.2898 loss_val: 1.1478 | acc_train: 1.0000 acc_val: 0.6660 | f1_train: 1.0000 f1_val: 0.6609
Epoch: 0077 | loss_train: 0.2635 loss_val: 1.1334 | acc_train: 1.0000 acc_val: 0.6720 | f1_train: 1.0000 f1_val: 0.6654
Epoch: 0078 | loss_train: 0.2372 loss_val: 1.1295 | acc_train: 1.0000 acc_val: 0.6720 | f1_train: 1.0000 f1_val: 0.6637
Epoch: 0079 | loss_train: 0.2136 loss_val: 1.1360 | acc_train: 1.0000 acc_val: 0.6720 | f1_train: 1.0000 f1_val: 0.6641
Epoch: 0080 | loss_train: 0.1911 loss_val: 1.1483 | acc_train: 1.0000 acc_val: 0.6660 | f1_train: 1.0000 f1_val: 0.6583
Epoch: 0081 | loss_train: 0.1711 loss_val: 1.1649 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6563
Epoch: 0082 | loss_train: 0.1519 loss_val: 1.1756 | acc_train: 1.0000 acc_val: 0.6600 | f1_train: 1.0000 f1_val: 0.6510
Epoch: 0083 | loss_train: 0.1355 loss_val: 1.1701 | acc_train: 1.0000 acc_val: 0.6600 | f1_train: 1.0000 f1_val: 0.6508
Epoch: 0084 | loss_train: 0.1199 loss_val: 1.1697 | acc_train: 1.0000 acc_val: 0.6660 | f1_train: 1.0000 f1_val: 0.6573
Epoch: 0085 | loss_train: 0.1063 loss_val: 1.1876 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6554
Epoch: 0086 | loss_train: 0.0935 loss_val: 1.2037 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6545
Epoch: 0087 | loss_train: 0.0829 loss_val: 1.2127 | acc_train: 1.0000 acc_val: 0.6620 | f1_train: 1.0000 f1_val: 0.6527
Epoch: 0088 | loss_train: 0.0733 loss_val: 1.2143 | acc_train: 1.0000 acc_val: 0.6620 | f1_train: 1.0000 f1_val: 0.6532
Epoch: 0089 | loss_train: 0.0648 loss_val: 1.2194 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6554
Epoch: 0090 | loss_train: 0.0576 loss_val: 1.2302 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6532
Epoch: 0091 | loss_train: 0.0511 loss_val: 1.2440 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6530
Epoch: 0092 | loss_train: 0.0455 loss_val: 1.2595 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6507
Epoch: 0093 | loss_train: 0.0408 loss_val: 1.2793 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6515
Epoch: 0094 | loss_train: 0.0365 loss_val: 1.2949 | acc_train: 1.0000 acc_val: 0.6560 | f1_train: 1.0000 f1_val: 0.6433
Epoch: 0095 | loss_train: 0.0329 loss_val: 1.3056 | acc_train: 1.0000 acc_val: 0.6540 | f1_train: 1.0000 f1_val: 0.6415
Epoch: 0096 | loss_train: 0.0297 loss_val: 1.3116 | acc_train: 1.0000 acc_val: 0.6540 | f1_train: 1.0000 f1_val: 0.6412
Epoch: 0097 | loss_train: 0.0268 loss_val: 1.3184 | acc_train: 1.0000 acc_val: 0.6540 | f1_train: 1.0000 f1_val: 0.6414
Epoch: 0098 | loss_train: 0.0246 loss_val: 1.3298 | acc_train: 1.0000 acc_val: 0.6540 | f1_train: 1.0000 f1_val: 0.6415
Epoch: 0099 | loss_train: 0.0223 loss_val: 1.3439 | acc_train: 1.0000 acc_val: 0.6540 | f1_train: 1.0000 f1_val: 0.6413
Epoch: 0100 | loss_train: 0.0204 loss_val: 1.3589 | acc_train: 1.0000 acc_val: 0.6540 | f1_train: 1.0000 f1_val: 0.6406
Epoch: 0101 | loss_train: 0.0188 loss_val: 1.3723 | acc_train: 1.0000 acc_val: 0.6520 | f1_train: 1.0000 f1_val: 0.6397
Epoch: 0102 | loss_train: 0.0172 loss_val: 1.3814 | acc_train: 1.0000 acc_val: 0.6540 | f1_train: 1.0000 f1_val: 0.6413
Epoch: 0103 | loss_train: 0.0160 loss_val: 1.3900 | acc_train: 1.0000 acc_val: 0.6560 | f1_train: 1.0000 f1_val: 0.6440
Epoch: 0104 | loss_train: 0.0149 loss_val: 1.3976 | acc_train: 1.0000 acc_val: 0.6580 | f1_train: 1.0000 f1_val: 0.6457
Epoch: 0105 | loss_train: 0.0138 loss_val: 1.4038 | acc_train: 1.0000 acc_val: 0.6600 | f1_train: 1.0000 f1_val: 0.6476
Epoch: 0106 | loss_train: 0.0130 loss_val: 1.4098 | acc_train: 1.0000 acc_val: 0.6600 | f1_train: 1.0000 f1_val: 0.6476
Epoch: 0107 | loss_train: 0.0121 loss_val: 1.4152 | acc_train: 1.0000 acc_val: 0.6620 | f1_train: 1.0000 f1_val: 0.6491
Epoch: 0108 | loss_train: 0.0113 loss_val: 1.4234 | acc_train: 1.0000 acc_val: 0.6620 | f1_train: 1.0000 f1_val: 0.6493
Optimization Finished!
Train cost: 11.7224s
Loading 67th epoch
Test set results: loss= 1.1657 accuracy= 0.7000

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=64, hops=3, log_path='log/nagphormer/cora/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=64, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=64, out_features=64, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=64, out_features=128, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=128, out_features=64, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=64, out_features=32, bias=True)
  (attn_layer): Linear(in_features=128, out_features=1, bias=True)
  (Linear1): Linear(in_features=32, out_features=7, bias=True)
)
total params: 128009
Epoch: 0001 | loss_train: 1.9456 loss_val: 1.9498 | acc_train: 0.1071 acc_val: 0.1140 | f1_train: 0.0873 f1_val: 0.0804
Epoch: 0002 | loss_train: 1.9444 loss_val: 1.9492 | acc_train: 0.1214 acc_val: 0.1240 | f1_train: 0.0980 f1_val: 0.0869
Epoch: 0003 | loss_train: 1.9432 loss_val: 1.9482 | acc_train: 0.1214 acc_val: 0.1240 | f1_train: 0.0985 f1_val: 0.0866
Epoch: 0004 | loss_train: 1.9406 loss_val: 1.9469 | acc_train: 0.1429 acc_val: 0.1320 | f1_train: 0.1151 f1_val: 0.0931
Epoch: 0005 | loss_train: 1.9378 loss_val: 1.9454 | acc_train: 0.1571 acc_val: 0.1460 | f1_train: 0.1250 f1_val: 0.1085
Epoch: 0006 | loss_train: 1.9344 loss_val: 1.9436 | acc_train: 0.1857 acc_val: 0.1580 | f1_train: 0.1588 f1_val: 0.1226
Epoch: 0007 | loss_train: 1.9301 loss_val: 1.9415 | acc_train: 0.2429 acc_val: 0.1760 | f1_train: 0.2142 f1_val: 0.1444
Epoch: 0008 | loss_train: 1.9251 loss_val: 1.9391 | acc_train: 0.3500 acc_val: 0.2120 | f1_train: 0.3009 f1_val: 0.1827
Epoch: 0009 | loss_train: 1.9197 loss_val: 1.9363 | acc_train: 0.4071 acc_val: 0.2700 | f1_train: 0.3526 f1_val: 0.2466
Epoch: 0010 | loss_train: 1.9133 loss_val: 1.9332 | acc_train: 0.4714 acc_val: 0.2960 | f1_train: 0.4114 f1_val: 0.2766
Epoch: 0011 | loss_train: 1.9064 loss_val: 1.9297 | acc_train: 0.5786 acc_val: 0.3100 | f1_train: 0.5160 f1_val: 0.2979
Epoch: 0012 | loss_train: 1.8984 loss_val: 1.9257 | acc_train: 0.6500 acc_val: 0.3460 | f1_train: 0.6022 f1_val: 0.3395
Epoch: 0013 | loss_train: 1.8894 loss_val: 1.9213 | acc_train: 0.7286 acc_val: 0.3840 | f1_train: 0.7129 f1_val: 0.3846
Epoch: 0014 | loss_train: 1.8794 loss_val: 1.9164 | acc_train: 0.7571 acc_val: 0.4180 | f1_train: 0.7441 f1_val: 0.4277
Epoch: 0015 | loss_train: 1.8687 loss_val: 1.9110 | acc_train: 0.7571 acc_val: 0.4380 | f1_train: 0.7467 f1_val: 0.4453
Epoch: 0016 | loss_train: 1.8569 loss_val: 1.9048 | acc_train: 0.7786 acc_val: 0.4580 | f1_train: 0.7690 f1_val: 0.4663
Epoch: 0017 | loss_train: 1.8444 loss_val: 1.8978 | acc_train: 0.8000 acc_val: 0.4880 | f1_train: 0.7929 f1_val: 0.4940
Epoch: 0018 | loss_train: 1.8304 loss_val: 1.8903 | acc_train: 0.8357 acc_val: 0.5080 | f1_train: 0.8309 f1_val: 0.5170
Epoch: 0019 | loss_train: 1.8157 loss_val: 1.8820 | acc_train: 0.8571 acc_val: 0.5200 | f1_train: 0.8547 f1_val: 0.5359
Epoch: 0020 | loss_train: 1.7998 loss_val: 1.8730 | acc_train: 0.8786 acc_val: 0.5300 | f1_train: 0.8751 f1_val: 0.5485
Epoch: 0021 | loss_train: 1.7828 loss_val: 1.8634 | acc_train: 0.8786 acc_val: 0.5440 | f1_train: 0.8748 f1_val: 0.5623
Epoch: 0022 | loss_train: 1.7641 loss_val: 1.8530 | acc_train: 0.9000 acc_val: 0.5560 | f1_train: 0.8971 f1_val: 0.5763
Epoch: 0023 | loss_train: 1.7442 loss_val: 1.8417 | acc_train: 0.9071 acc_val: 0.5780 | f1_train: 0.9051 f1_val: 0.5950
Epoch: 0024 | loss_train: 1.7229 loss_val: 1.8293 | acc_train: 0.9286 acc_val: 0.5840 | f1_train: 0.9265 f1_val: 0.6023
Epoch: 0025 | loss_train: 1.6989 loss_val: 1.8157 | acc_train: 0.9357 acc_val: 0.6080 | f1_train: 0.9340 f1_val: 0.6212
Epoch: 0026 | loss_train: 1.6733 loss_val: 1.8010 | acc_train: 0.9500 acc_val: 0.6180 | f1_train: 0.9486 f1_val: 0.6322
Epoch: 0027 | loss_train: 1.6452 loss_val: 1.7852 | acc_train: 0.9571 acc_val: 0.6160 | f1_train: 0.9563 f1_val: 0.6290
Epoch: 0028 | loss_train: 1.6153 loss_val: 1.7689 | acc_train: 0.9714 acc_val: 0.6180 | f1_train: 0.9714 f1_val: 0.6298
Epoch: 0029 | loss_train: 1.5831 loss_val: 1.7517 | acc_train: 0.9714 acc_val: 0.6280 | f1_train: 0.9714 f1_val: 0.6367
Epoch: 0030 | loss_train: 1.5486 loss_val: 1.7333 | acc_train: 0.9714 acc_val: 0.6440 | f1_train: 0.9714 f1_val: 0.6515
Epoch: 0031 | loss_train: 1.5114 loss_val: 1.7134 | acc_train: 0.9786 acc_val: 0.6520 | f1_train: 0.9785 f1_val: 0.6606
Epoch: 0032 | loss_train: 1.4719 loss_val: 1.6906 | acc_train: 0.9786 acc_val: 0.6540 | f1_train: 0.9785 f1_val: 0.6628
Epoch: 0033 | loss_train: 1.4298 loss_val: 1.6672 | acc_train: 0.9786 acc_val: 0.6560 | f1_train: 0.9785 f1_val: 0.6630
Epoch: 0034 | loss_train: 1.3852 loss_val: 1.6450 | acc_train: 0.9857 acc_val: 0.6500 | f1_train: 0.9857 f1_val: 0.6550
Epoch: 0035 | loss_train: 1.3375 loss_val: 1.6203 | acc_train: 0.9929 acc_val: 0.6620 | f1_train: 0.9929 f1_val: 0.6692
Epoch: 0036 | loss_train: 1.2879 loss_val: 1.5893 | acc_train: 1.0000 acc_val: 0.6700 | f1_train: 1.0000 f1_val: 0.6732
Epoch: 0037 | loss_train: 1.2347 loss_val: 1.5621 | acc_train: 1.0000 acc_val: 0.6640 | f1_train: 1.0000 f1_val: 0.6652
Epoch: 0038 | loss_train: 1.1788 loss_val: 1.5372 | acc_train: 1.0000 acc_val: 0.6720 | f1_train: 1.0000 f1_val: 0.6769
Epoch: 0039 | loss_train: 1.1212 loss_val: 1.4948 | acc_train: 1.0000 acc_val: 0.6820 | f1_train: 1.0000 f1_val: 0.6837
Epoch: 0040 | loss_train: 1.0603 loss_val: 1.4650 | acc_train: 1.0000 acc_val: 0.6840 | f1_train: 1.0000 f1_val: 0.6857
Epoch: 0041 | loss_train: 0.9974 loss_val: 1.4451 | acc_train: 1.0000 acc_val: 0.6820 | f1_train: 1.0000 f1_val: 0.6866
Epoch: 0042 | loss_train: 0.9348 loss_val: 1.3976 | acc_train: 1.0000 acc_val: 0.7020 | f1_train: 1.0000 f1_val: 0.7046
Epoch: 0043 | loss_train: 0.8688 loss_val: 1.3638 | acc_train: 1.0000 acc_val: 0.7100 | f1_train: 1.0000 f1_val: 0.7131
Epoch: 0044 | loss_train: 0.8006 loss_val: 1.3464 | acc_train: 1.0000 acc_val: 0.7040 | f1_train: 1.0000 f1_val: 0.7079
Epoch: 0045 | loss_train: 0.7352 loss_val: 1.2942 | acc_train: 1.0000 acc_val: 0.7180 | f1_train: 1.0000 f1_val: 0.7205
Epoch: 0046 | loss_train: 0.6692 loss_val: 1.2707 | acc_train: 1.0000 acc_val: 0.7160 | f1_train: 1.0000 f1_val: 0.7184
Epoch: 0047 | loss_train: 0.6014 loss_val: 1.2465 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7234
Epoch: 0048 | loss_train: 0.5392 loss_val: 1.2094 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7230
Epoch: 0049 | loss_train: 0.4772 loss_val: 1.1877 | acc_train: 1.0000 acc_val: 0.7160 | f1_train: 1.0000 f1_val: 0.7125
Epoch: 0050 | loss_train: 0.4195 loss_val: 1.1612 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7088
Epoch: 0051 | loss_train: 0.3646 loss_val: 1.1581 | acc_train: 1.0000 acc_val: 0.7080 | f1_train: 1.0000 f1_val: 0.7051
Epoch: 0052 | loss_train: 0.3147 loss_val: 1.1313 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7105
Epoch: 0053 | loss_train: 0.2686 loss_val: 1.1021 | acc_train: 1.0000 acc_val: 0.7140 | f1_train: 1.0000 f1_val: 0.7080
Epoch: 0054 | loss_train: 0.2275 loss_val: 1.1029 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7154
Epoch: 0055 | loss_train: 0.1925 loss_val: 1.1009 | acc_train: 1.0000 acc_val: 0.7180 | f1_train: 1.0000 f1_val: 0.7133
Epoch: 0056 | loss_train: 0.1605 loss_val: 1.0901 | acc_train: 1.0000 acc_val: 0.7180 | f1_train: 1.0000 f1_val: 0.7118
Epoch: 0057 | loss_train: 0.1343 loss_val: 1.0778 | acc_train: 1.0000 acc_val: 0.7160 | f1_train: 1.0000 f1_val: 0.7094
Epoch: 0058 | loss_train: 0.1115 loss_val: 1.0819 | acc_train: 1.0000 acc_val: 0.7160 | f1_train: 1.0000 f1_val: 0.7076
Epoch: 0059 | loss_train: 0.0924 loss_val: 1.0942 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7150
Epoch: 0060 | loss_train: 0.0768 loss_val: 1.1058 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7185
Epoch: 0061 | loss_train: 0.0638 loss_val: 1.1116 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7199
Epoch: 0062 | loss_train: 0.0532 loss_val: 1.1095 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7206
Epoch: 0063 | loss_train: 0.0446 loss_val: 1.1155 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7150
Epoch: 0064 | loss_train: 0.0375 loss_val: 1.1345 | acc_train: 1.0000 acc_val: 0.7240 | f1_train: 1.0000 f1_val: 0.7166
Epoch: 0065 | loss_train: 0.0319 loss_val: 1.1618 | acc_train: 1.0000 acc_val: 0.7220 | f1_train: 1.0000 f1_val: 0.7152
Epoch: 0066 | loss_train: 0.0269 loss_val: 1.1888 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7213
Epoch: 0067 | loss_train: 0.0231 loss_val: 1.2093 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7204
Epoch: 0068 | loss_train: 0.0198 loss_val: 1.2197 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7245
Epoch: 0069 | loss_train: 0.0172 loss_val: 1.2230 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7271
Epoch: 0070 | loss_train: 0.0149 loss_val: 1.2260 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7266
Epoch: 0071 | loss_train: 0.0131 loss_val: 1.2362 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7233
Epoch: 0072 | loss_train: 0.0115 loss_val: 1.2553 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7259
Epoch: 0073 | loss_train: 0.0103 loss_val: 1.2810 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0074 | loss_train: 0.0090 loss_val: 1.3085 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0075 | loss_train: 0.0081 loss_val: 1.3307 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7213
Epoch: 0076 | loss_train: 0.0073 loss_val: 1.3467 | acc_train: 1.0000 acc_val: 0.7280 | f1_train: 1.0000 f1_val: 0.7221
Epoch: 0077 | loss_train: 0.0067 loss_val: 1.3580 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7186
Epoch: 0078 | loss_train: 0.0061 loss_val: 1.3663 | acc_train: 1.0000 acc_val: 0.7260 | f1_train: 1.0000 f1_val: 0.7186
Epoch: 0079 | loss_train: 0.0055 loss_val: 1.3737 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7216
Epoch: 0080 | loss_train: 0.0051 loss_val: 1.3815 | acc_train: 1.0000 acc_val: 0.7300 | f1_train: 1.0000 f1_val: 0.7219
Epoch: 0081 | loss_train: 0.0047 loss_val: 1.3893 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7249
Epoch: 0082 | loss_train: 0.0044 loss_val: 1.3980 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7339
Epoch: 0083 | loss_train: 0.0041 loss_val: 1.4074 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7333
Epoch: 0084 | loss_train: 0.0038 loss_val: 1.4178 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7335
Epoch: 0085 | loss_train: 0.0036 loss_val: 1.4286 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7338
Epoch: 0086 | loss_train: 0.0034 loss_val: 1.4388 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7298
Epoch: 0087 | loss_train: 0.0032 loss_val: 1.4487 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7305
Epoch: 0088 | loss_train: 0.0030 loss_val: 1.4573 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7302
Epoch: 0089 | loss_train: 0.0029 loss_val: 1.4638 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7260
Epoch: 0090 | loss_train: 0.0027 loss_val: 1.4688 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7260
Epoch: 0091 | loss_train: 0.0026 loss_val: 1.4731 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7279
Epoch: 0092 | loss_train: 0.0025 loss_val: 1.4774 | acc_train: 1.0000 acc_val: 0.7320 | f1_train: 1.0000 f1_val: 0.7268
Epoch: 0093 | loss_train: 0.0024 loss_val: 1.4811 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7282
Epoch: 0094 | loss_train: 0.0023 loss_val: 1.4849 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7282
Epoch: 0095 | loss_train: 0.0022 loss_val: 1.4890 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7280
Epoch: 0096 | loss_train: 0.0022 loss_val: 1.4921 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7315
Epoch: 0097 | loss_train: 0.0021 loss_val: 1.4947 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7340
Epoch: 0098 | loss_train: 0.0020 loss_val: 1.4982 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7342
Epoch: 0099 | loss_train: 0.0020 loss_val: 1.5017 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7342
Epoch: 0100 | loss_train: 0.0019 loss_val: 1.5056 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7342
Epoch: 0101 | loss_train: 0.0019 loss_val: 1.5101 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7342
Epoch: 0102 | loss_train: 0.0018 loss_val: 1.5152 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7342
Epoch: 0103 | loss_train: 0.0018 loss_val: 1.5200 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7337
Epoch: 0104 | loss_train: 0.0017 loss_val: 1.5247 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7315
Epoch: 0105 | loss_train: 0.0017 loss_val: 1.5291 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7315
Epoch: 0106 | loss_train: 0.0016 loss_val: 1.5331 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7301
Epoch: 0107 | loss_train: 0.0016 loss_val: 1.5365 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0108 | loss_train: 0.0016 loss_val: 1.5394 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0109 | loss_train: 0.0016 loss_val: 1.5413 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0110 | loss_train: 0.0015 loss_val: 1.5425 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0111 | loss_train: 0.0015 loss_val: 1.5429 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7308
Epoch: 0112 | loss_train: 0.0015 loss_val: 1.5428 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0113 | loss_train: 0.0014 loss_val: 1.5430 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0114 | loss_train: 0.0014 loss_val: 1.5433 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0115 | loss_train: 0.0014 loss_val: 1.5440 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0116 | loss_train: 0.0014 loss_val: 1.5456 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0117 | loss_train: 0.0014 loss_val: 1.5483 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0118 | loss_train: 0.0013 loss_val: 1.5510 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0119 | loss_train: 0.0013 loss_val: 1.5543 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0120 | loss_train: 0.0013 loss_val: 1.5569 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7345
Epoch: 0121 | loss_train: 0.0013 loss_val: 1.5599 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7328
Epoch: 0122 | loss_train: 0.0013 loss_val: 1.5623 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7324
Epoch: 0123 | loss_train: 0.0012 loss_val: 1.5639 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7311
Epoch: 0124 | loss_train: 0.0012 loss_val: 1.5653 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7311
Epoch: 0125 | loss_train: 0.0012 loss_val: 1.5670 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7311
Epoch: 0126 | loss_train: 0.0012 loss_val: 1.5684 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7324
Epoch: 0127 | loss_train: 0.0012 loss_val: 1.5690 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7328
Epoch: 0128 | loss_train: 0.0012 loss_val: 1.5698 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7328
Epoch: 0129 | loss_train: 0.0011 loss_val: 1.5699 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7367
Epoch: 0130 | loss_train: 0.0011 loss_val: 1.5712 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7367
Epoch: 0131 | loss_train: 0.0011 loss_val: 1.5730 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7367
Epoch: 0132 | loss_train: 0.0011 loss_val: 1.5746 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7366
Epoch: 0133 | loss_train: 0.0011 loss_val: 1.5756 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7366
Epoch: 0134 | loss_train: 0.0011 loss_val: 1.5768 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7344
Epoch: 0135 | loss_train: 0.0011 loss_val: 1.5782 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7344
Epoch: 0136 | loss_train: 0.0010 loss_val: 1.5803 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7344
Epoch: 0137 | loss_train: 0.0010 loss_val: 1.5821 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7344
Epoch: 0138 | loss_train: 0.0010 loss_val: 1.5843 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7344
Epoch: 0139 | loss_train: 0.0010 loss_val: 1.5865 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7330
Epoch: 0140 | loss_train: 0.0010 loss_val: 1.5882 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7330
Epoch: 0141 | loss_train: 0.0010 loss_val: 1.5900 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0142 | loss_train: 0.0010 loss_val: 1.5918 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0143 | loss_train: 0.0009 loss_val: 1.5938 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0144 | loss_train: 0.0009 loss_val: 1.5951 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0145 | loss_train: 0.0009 loss_val: 1.5964 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0146 | loss_train: 0.0009 loss_val: 1.5976 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0147 | loss_train: 0.0009 loss_val: 1.5988 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0148 | loss_train: 0.0009 loss_val: 1.5998 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7323
Epoch: 0149 | loss_train: 0.0009 loss_val: 1.6004 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7323
Epoch: 0150 | loss_train: 0.0009 loss_val: 1.6012 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7323
Epoch: 0151 | loss_train: 0.0009 loss_val: 1.6010 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0152 | loss_train: 0.0009 loss_val: 1.6014 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0153 | loss_train: 0.0008 loss_val: 1.6018 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0154 | loss_train: 0.0008 loss_val: 1.6019 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7309
Epoch: 0155 | loss_train: 0.0008 loss_val: 1.6031 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0156 | loss_train: 0.0008 loss_val: 1.6046 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0157 | loss_train: 0.0008 loss_val: 1.6063 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0158 | loss_train: 0.0008 loss_val: 1.6082 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0159 | loss_train: 0.0008 loss_val: 1.6101 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0160 | loss_train: 0.0008 loss_val: 1.6121 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0161 | loss_train: 0.0008 loss_val: 1.6144 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7341
Epoch: 0162 | loss_train: 0.0008 loss_val: 1.6164 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7341
Epoch: 0163 | loss_train: 0.0007 loss_val: 1.6178 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7341
Optimization Finished!
Train cost: 25.3444s
Loading 129th epoch
Test set results: loss= 1.5079 accuracy= 0.7460

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/cora/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=7, bias=True)
)
total params: 325641
Epoch: 0001 | loss_train: 1.9508 loss_val: 1.9448 | acc_train: 0.1786 acc_val: 0.1340 | f1_train: 0.1319 f1_val: 0.0945
Epoch: 0002 | loss_train: 1.9476 loss_val: 1.9427 | acc_train: 0.1857 acc_val: 0.1540 | f1_train: 0.1452 f1_val: 0.1089
Epoch: 0003 | loss_train: 1.9425 loss_val: 1.9396 | acc_train: 0.2143 acc_val: 0.1820 | f1_train: 0.1722 f1_val: 0.1313
Epoch: 0004 | loss_train: 1.9358 loss_val: 1.9355 | acc_train: 0.2786 acc_val: 0.2180 | f1_train: 0.2360 f1_val: 0.1598
Epoch: 0005 | loss_train: 1.9272 loss_val: 1.9302 | acc_train: 0.3357 acc_val: 0.2600 | f1_train: 0.2929 f1_val: 0.1925
Epoch: 0006 | loss_train: 1.9152 loss_val: 1.9238 | acc_train: 0.4357 acc_val: 0.3060 | f1_train: 0.3902 f1_val: 0.2301
Epoch: 0007 | loss_train: 1.9021 loss_val: 1.9159 | acc_train: 0.5000 acc_val: 0.3340 | f1_train: 0.4641 f1_val: 0.2513
Epoch: 0008 | loss_train: 1.8865 loss_val: 1.9066 | acc_train: 0.5429 acc_val: 0.3580 | f1_train: 0.5052 f1_val: 0.2688
Epoch: 0009 | loss_train: 1.8693 loss_val: 1.8954 | acc_train: 0.5714 acc_val: 0.3900 | f1_train: 0.5385 f1_val: 0.3172
Epoch: 0010 | loss_train: 1.8488 loss_val: 1.8823 | acc_train: 0.6571 acc_val: 0.4140 | f1_train: 0.6605 f1_val: 0.3626
Epoch: 0011 | loss_train: 1.8250 loss_val: 1.8668 | acc_train: 0.7071 acc_val: 0.4240 | f1_train: 0.7109 f1_val: 0.3866
Epoch: 0012 | loss_train: 1.7985 loss_val: 1.8489 | acc_train: 0.7357 acc_val: 0.4600 | f1_train: 0.7395 f1_val: 0.4502
Epoch: 0013 | loss_train: 1.7697 loss_val: 1.8281 | acc_train: 0.7500 acc_val: 0.4840 | f1_train: 0.7554 f1_val: 0.4901
Epoch: 0014 | loss_train: 1.7358 loss_val: 1.8044 | acc_train: 0.8000 acc_val: 0.5060 | f1_train: 0.8106 f1_val: 0.5200
Epoch: 0015 | loss_train: 1.6987 loss_val: 1.7778 | acc_train: 0.8500 acc_val: 0.5280 | f1_train: 0.8486 f1_val: 0.5433
Epoch: 0016 | loss_train: 1.6577 loss_val: 1.7480 | acc_train: 0.8500 acc_val: 0.5480 | f1_train: 0.8489 f1_val: 0.5734
Epoch: 0017 | loss_train: 1.6134 loss_val: 1.7155 | acc_train: 0.8714 acc_val: 0.5660 | f1_train: 0.8650 f1_val: 0.5833
Epoch: 0018 | loss_train: 1.5663 loss_val: 1.6804 | acc_train: 0.8929 acc_val: 0.6140 | f1_train: 0.8877 f1_val: 0.6291
Epoch: 0019 | loss_train: 1.5155 loss_val: 1.6436 | acc_train: 0.9143 acc_val: 0.6360 | f1_train: 0.9110 f1_val: 0.6444
Epoch: 0020 | loss_train: 1.4606 loss_val: 1.6052 | acc_train: 0.9286 acc_val: 0.6540 | f1_train: 0.9242 f1_val: 0.6594
Epoch: 0021 | loss_train: 1.4026 loss_val: 1.5652 | acc_train: 0.9429 acc_val: 0.6740 | f1_train: 0.9407 f1_val: 0.6747
Epoch: 0022 | loss_train: 1.3409 loss_val: 1.5240 | acc_train: 0.9571 acc_val: 0.6920 | f1_train: 0.9566 f1_val: 0.6908
Epoch: 0023 | loss_train: 1.2749 loss_val: 1.4816 | acc_train: 0.9714 acc_val: 0.7120 | f1_train: 0.9715 f1_val: 0.7090
Epoch: 0024 | loss_train: 1.2065 loss_val: 1.4362 | acc_train: 0.9786 acc_val: 0.7260 | f1_train: 0.9785 f1_val: 0.7171
Epoch: 0025 | loss_train: 1.1362 loss_val: 1.3880 | acc_train: 0.9857 acc_val: 0.7380 | f1_train: 0.9859 f1_val: 0.7278
Epoch: 0026 | loss_train: 1.0621 loss_val: 1.3364 | acc_train: 0.9929 acc_val: 0.7460 | f1_train: 0.9929 f1_val: 0.7360
Epoch: 0027 | loss_train: 0.9859 loss_val: 1.2861 | acc_train: 0.9929 acc_val: 0.7460 | f1_train: 0.9929 f1_val: 0.7378
Epoch: 0028 | loss_train: 0.9076 loss_val: 1.2405 | acc_train: 0.9929 acc_val: 0.7380 | f1_train: 0.9929 f1_val: 0.7293
Epoch: 0029 | loss_train: 0.8266 loss_val: 1.1959 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7365
Epoch: 0030 | loss_train: 0.7456 loss_val: 1.1457 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7263
Epoch: 0031 | loss_train: 0.6649 loss_val: 1.1006 | acc_train: 1.0000 acc_val: 0.7380 | f1_train: 1.0000 f1_val: 0.7314
Epoch: 0032 | loss_train: 0.5851 loss_val: 1.0592 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7370
Epoch: 0033 | loss_train: 0.5080 loss_val: 1.0203 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7358
Epoch: 0034 | loss_train: 0.4338 loss_val: 0.9814 | acc_train: 1.0000 acc_val: 0.7340 | f1_train: 1.0000 f1_val: 0.7275
Epoch: 0035 | loss_train: 0.3646 loss_val: 0.9473 | acc_train: 1.0000 acc_val: 0.7360 | f1_train: 1.0000 f1_val: 0.7265
Epoch: 0036 | loss_train: 0.3035 loss_val: 0.9198 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7399
Epoch: 0037 | loss_train: 0.2475 loss_val: 0.9030 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7435
Epoch: 0038 | loss_train: 0.1992 loss_val: 0.8909 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7434
Epoch: 0039 | loss_train: 0.1576 loss_val: 0.8845 | acc_train: 1.0000 acc_val: 0.7400 | f1_train: 1.0000 f1_val: 0.7338
Epoch: 0040 | loss_train: 0.1236 loss_val: 0.8791 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7364
Epoch: 0041 | loss_train: 0.0959 loss_val: 0.8765 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7363
Epoch: 0042 | loss_train: 0.0739 loss_val: 0.8769 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7340
Epoch: 0043 | loss_train: 0.0565 loss_val: 0.8818 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7398
Epoch: 0044 | loss_train: 0.0434 loss_val: 0.8932 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7411
Epoch: 0045 | loss_train: 0.0334 loss_val: 0.9111 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7321
Epoch: 0046 | loss_train: 0.0259 loss_val: 0.9339 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7301
Epoch: 0047 | loss_train: 0.0200 loss_val: 0.9599 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7336
Epoch: 0048 | loss_train: 0.0157 loss_val: 0.9871 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7336
Epoch: 0049 | loss_train: 0.0124 loss_val: 1.0147 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7392
Epoch: 0050 | loss_train: 0.0100 loss_val: 1.0412 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7392
Epoch: 0051 | loss_train: 0.0081 loss_val: 1.0663 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7387
Epoch: 0052 | loss_train: 0.0066 loss_val: 1.0893 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7349
Epoch: 0053 | loss_train: 0.0054 loss_val: 1.1102 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7346
Epoch: 0054 | loss_train: 0.0045 loss_val: 1.1291 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7359
Epoch: 0055 | loss_train: 0.0039 loss_val: 1.1461 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7372
Epoch: 0056 | loss_train: 0.0032 loss_val: 1.1616 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7340
Epoch: 0057 | loss_train: 0.0028 loss_val: 1.1757 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7340
Epoch: 0058 | loss_train: 0.0024 loss_val: 1.1887 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7334
Epoch: 0059 | loss_train: 0.0021 loss_val: 1.2009 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7334
Epoch: 0060 | loss_train: 0.0019 loss_val: 1.2124 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7319
Epoch: 0061 | loss_train: 0.0016 loss_val: 1.2236 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7319
Epoch: 0062 | loss_train: 0.0015 loss_val: 1.2346 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7331
Epoch: 0063 | loss_train: 0.0013 loss_val: 1.2454 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7316
Epoch: 0064 | loss_train: 0.0012 loss_val: 1.2561 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7296
Epoch: 0065 | loss_train: 0.0011 loss_val: 1.2667 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7297
Epoch: 0066 | loss_train: 0.0010 loss_val: 1.2771 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7297
Epoch: 0067 | loss_train: 0.0009 loss_val: 1.2875 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7317
Epoch: 0068 | loss_train: 0.0009 loss_val: 1.2977 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7320
Epoch: 0069 | loss_train: 0.0008 loss_val: 1.3077 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7320
Epoch: 0070 | loss_train: 0.0007 loss_val: 1.3174 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0071 | loss_train: 0.0007 loss_val: 1.3269 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7322
Epoch: 0072 | loss_train: 0.0007 loss_val: 1.3360 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7371
Epoch: 0073 | loss_train: 0.0006 loss_val: 1.3446 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7348
Epoch: 0074 | loss_train: 0.0006 loss_val: 1.3527 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7348
Optimization Finished!
Train cost: 9.9620s
Loading 44th epoch
Test set results: loss= 0.8028 accuracy= 0.7670

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=256, hops=3, log_path='log/nagphormer/cora/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=256, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=256, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=512, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=256, out_features=128, bias=True)
  (attn_layer): Linear(in_features=512, out_features=1, bias=True)
  (Linear1): Linear(in_features=128, out_features=7, bias=True)
)
total params: 929801
Epoch: 0001 | loss_train: 1.9464 loss_val: 1.9480 | acc_train: 0.1357 acc_val: 0.1380 | f1_train: 0.0888 f1_val: 0.0976
Epoch: 0002 | loss_train: 1.9362 loss_val: 1.9365 | acc_train: 0.1286 acc_val: 0.1680 | f1_train: 0.0797 f1_val: 0.1138
Epoch: 0003 | loss_train: 1.9167 loss_val: 1.9195 | acc_train: 0.2429 acc_val: 0.2240 | f1_train: 0.1902 f1_val: 0.1769
Epoch: 0004 | loss_train: 1.8873 loss_val: 1.8969 | acc_train: 0.3500 acc_val: 0.3140 | f1_train: 0.2788 f1_val: 0.2925
Epoch: 0005 | loss_train: 1.8450 loss_val: 1.8688 | acc_train: 0.5143 acc_val: 0.4140 | f1_train: 0.4805 f1_val: 0.4102
Epoch: 0006 | loss_train: 1.7974 loss_val: 1.8348 | acc_train: 0.7071 acc_val: 0.4900 | f1_train: 0.6875 f1_val: 0.4894
Epoch: 0007 | loss_train: 1.7444 loss_val: 1.7946 | acc_train: 0.7857 acc_val: 0.5620 | f1_train: 0.7758 f1_val: 0.5682
Epoch: 0008 | loss_train: 1.6746 loss_val: 1.7476 | acc_train: 0.8500 acc_val: 0.6220 | f1_train: 0.8477 f1_val: 0.6295
Epoch: 0009 | loss_train: 1.5997 loss_val: 1.6931 | acc_train: 0.8786 acc_val: 0.6460 | f1_train: 0.8786 f1_val: 0.6523
Epoch: 0010 | loss_train: 1.5169 loss_val: 1.6306 | acc_train: 0.9357 acc_val: 0.6700 | f1_train: 0.9354 f1_val: 0.6728
Epoch: 0011 | loss_train: 1.4248 loss_val: 1.5608 | acc_train: 0.9429 acc_val: 0.7020 | f1_train: 0.9425 f1_val: 0.6997
Epoch: 0012 | loss_train: 1.3247 loss_val: 1.4852 | acc_train: 0.9357 acc_val: 0.7260 | f1_train: 0.9348 f1_val: 0.7209
Epoch: 0013 | loss_train: 1.2238 loss_val: 1.4049 | acc_train: 0.9429 acc_val: 0.7520 | f1_train: 0.9425 f1_val: 0.7443
Epoch: 0014 | loss_train: 1.1183 loss_val: 1.3229 | acc_train: 0.9571 acc_val: 0.7580 | f1_train: 0.9567 f1_val: 0.7499
Epoch: 0015 | loss_train: 1.0059 loss_val: 1.2418 | acc_train: 0.9786 acc_val: 0.7640 | f1_train: 0.9784 f1_val: 0.7584
Epoch: 0016 | loss_train: 0.8982 loss_val: 1.1637 | acc_train: 0.9786 acc_val: 0.7680 | f1_train: 0.9784 f1_val: 0.7652
Epoch: 0017 | loss_train: 0.7884 loss_val: 1.0886 | acc_train: 0.9786 acc_val: 0.7720 | f1_train: 0.9784 f1_val: 0.7693
Epoch: 0018 | loss_train: 0.6817 loss_val: 1.0183 | acc_train: 0.9929 acc_val: 0.7740 | f1_train: 0.9929 f1_val: 0.7702
Epoch: 0019 | loss_train: 0.5788 loss_val: 0.9541 | acc_train: 0.9929 acc_val: 0.7740 | f1_train: 0.9929 f1_val: 0.7727
Epoch: 0020 | loss_train: 0.4831 loss_val: 0.8970 | acc_train: 0.9929 acc_val: 0.7660 | f1_train: 0.9929 f1_val: 0.7615
Epoch: 0021 | loss_train: 0.3942 loss_val: 0.8510 | acc_train: 0.9929 acc_val: 0.7680 | f1_train: 0.9929 f1_val: 0.7669
Epoch: 0022 | loss_train: 0.3140 loss_val: 0.8190 | acc_train: 1.0000 acc_val: 0.7600 | f1_train: 1.0000 f1_val: 0.7580
Epoch: 0023 | loss_train: 0.2434 loss_val: 0.7983 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7491
Epoch: 0024 | loss_train: 0.1860 loss_val: 0.7835 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7535
Epoch: 0025 | loss_train: 0.1389 loss_val: 0.7739 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7582
Epoch: 0026 | loss_train: 0.1022 loss_val: 0.7712 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7632
Epoch: 0027 | loss_train: 0.0744 loss_val: 0.7763 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7606
Epoch: 0028 | loss_train: 0.0535 loss_val: 0.7872 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7647
Epoch: 0029 | loss_train: 0.0385 loss_val: 0.8039 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7663
Epoch: 0030 | loss_train: 0.0279 loss_val: 0.8251 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7597
Epoch: 0031 | loss_train: 0.0199 loss_val: 0.8501 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7659
Epoch: 0032 | loss_train: 0.0143 loss_val: 0.8785 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7672
Epoch: 0033 | loss_train: 0.0105 loss_val: 0.9092 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7696
Epoch: 0034 | loss_train: 0.0077 loss_val: 0.9413 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7686
Epoch: 0035 | loss_train: 0.0057 loss_val: 0.9747 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7702
Epoch: 0036 | loss_train: 0.0044 loss_val: 1.0085 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7702
Epoch: 0037 | loss_train: 0.0033 loss_val: 1.0423 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7696
Epoch: 0038 | loss_train: 0.0026 loss_val: 1.0757 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7709
Epoch: 0039 | loss_train: 0.0020 loss_val: 1.1084 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7709
Epoch: 0040 | loss_train: 0.0016 loss_val: 1.1401 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7709
Epoch: 0041 | loss_train: 0.0013 loss_val: 1.1708 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7677
Epoch: 0042 | loss_train: 0.0011 loss_val: 1.2002 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7646
Epoch: 0043 | loss_train: 0.0009 loss_val: 1.2284 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7642
Epoch: 0044 | loss_train: 0.0007 loss_val: 1.2554 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7655
Epoch: 0045 | loss_train: 0.0006 loss_val: 1.2810 | acc_train: 1.0000 acc_val: 0.7740 | f1_train: 1.0000 f1_val: 0.7678
Epoch: 0046 | loss_train: 0.0005 loss_val: 1.3053 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7666
Epoch: 0047 | loss_train: 0.0004 loss_val: 1.3282 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0048 | loss_train: 0.0004 loss_val: 1.3499 | acc_train: 1.0000 acc_val: 0.7720 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0049 | loss_train: 0.0003 loss_val: 1.3703 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7659
Epoch: 0050 | loss_train: 0.0003 loss_val: 1.3896 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7659
Epoch: 0051 | loss_train: 0.0003 loss_val: 1.4076 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7659
Epoch: 0052 | loss_train: 0.0002 loss_val: 1.4244 | acc_train: 1.0000 acc_val: 0.7680 | f1_train: 1.0000 f1_val: 0.7644
Epoch: 0053 | loss_train: 0.0002 loss_val: 1.4401 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0054 | loss_train: 0.0002 loss_val: 1.4550 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0055 | loss_train: 0.0002 loss_val: 1.4687 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0056 | loss_train: 0.0002 loss_val: 1.4814 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0057 | loss_train: 0.0001 loss_val: 1.4934 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0058 | loss_train: 0.0001 loss_val: 1.5044 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0059 | loss_train: 0.0001 loss_val: 1.5147 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0060 | loss_train: 0.0001 loss_val: 1.5242 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0061 | loss_train: 0.0001 loss_val: 1.5331 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0062 | loss_train: 0.0001 loss_val: 1.5413 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0063 | loss_train: 0.0001 loss_val: 1.5488 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0064 | loss_train: 0.0001 loss_val: 1.5558 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0065 | loss_train: 0.0001 loss_val: 1.5623 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0066 | loss_train: 0.0001 loss_val: 1.5682 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0067 | loss_train: 0.0001 loss_val: 1.5738 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0068 | loss_train: 0.0001 loss_val: 1.5790 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0069 | loss_train: 0.0001 loss_val: 1.5837 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Epoch: 0070 | loss_train: 0.0001 loss_val: 1.5881 | acc_train: 1.0000 acc_val: 0.7640 | f1_train: 1.0000 f1_val: 0.7611
Optimization Finished!
Train cost: 10.6610s
Loading 33th epoch
Test set results: loss= 0.8594 accuracy= 0.7790

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='cora', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/cora/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1436, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=7, bias=True)
)
total params: 2973705
Epoch: 0001 | loss_train: 1.9644 loss_val: 1.8685 | acc_train: 0.1500 acc_val: 0.3380 | f1_train: 0.0961 f1_val: 0.1805
Epoch: 0002 | loss_train: 1.9218 loss_val: 1.8201 | acc_train: 0.2429 acc_val: 0.4180 | f1_train: 0.1645 f1_val: 0.2550
Epoch: 0003 | loss_train: 1.8483 loss_val: 1.7494 | acc_train: 0.3143 acc_val: 0.5420 | f1_train: 0.2601 f1_val: 0.4715
Epoch: 0004 | loss_train: 1.7314 loss_val: 1.6593 | acc_train: 0.6143 acc_val: 0.6300 | f1_train: 0.6107 f1_val: 0.5859
Epoch: 0005 | loss_train: 1.5866 loss_val: 1.5494 | acc_train: 0.7643 acc_val: 0.6940 | f1_train: 0.7638 f1_val: 0.6637
Epoch: 0006 | loss_train: 1.4234 loss_val: 1.4177 | acc_train: 0.8429 acc_val: 0.7260 | f1_train: 0.8451 f1_val: 0.7061
Epoch: 0007 | loss_train: 1.2424 loss_val: 1.2689 | acc_train: 0.8929 acc_val: 0.7400 | f1_train: 0.8946 f1_val: 0.7335
Epoch: 0008 | loss_train: 1.0407 loss_val: 1.1175 | acc_train: 0.9214 acc_val: 0.7560 | f1_train: 0.9217 f1_val: 0.7481
Epoch: 0009 | loss_train: 0.8483 loss_val: 0.9828 | acc_train: 0.9500 acc_val: 0.7660 | f1_train: 0.9508 f1_val: 0.7615
Epoch: 0010 | loss_train: 0.6650 loss_val: 0.8742 | acc_train: 0.9429 acc_val: 0.7660 | f1_train: 0.9426 f1_val: 0.7614
Epoch: 0011 | loss_train: 0.5059 loss_val: 0.7956 | acc_train: 0.9714 acc_val: 0.7640 | f1_train: 0.9712 f1_val: 0.7592
Epoch: 0012 | loss_train: 0.3727 loss_val: 0.7420 | acc_train: 0.9786 acc_val: 0.7620 | f1_train: 0.9784 f1_val: 0.7596
Epoch: 0013 | loss_train: 0.2600 loss_val: 0.7074 | acc_train: 0.9857 acc_val: 0.7720 | f1_train: 0.9855 f1_val: 0.7715
Epoch: 0014 | loss_train: 0.1732 loss_val: 0.6932 | acc_train: 0.9929 acc_val: 0.7720 | f1_train: 0.9929 f1_val: 0.7727
Epoch: 0015 | loss_train: 0.1114 loss_val: 0.7023 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7773
Epoch: 0016 | loss_train: 0.0693 loss_val: 0.7350 | acc_train: 1.0000 acc_val: 0.7760 | f1_train: 1.0000 f1_val: 0.7769
Epoch: 0017 | loss_train: 0.0415 loss_val: 0.7858 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7674
Epoch: 0018 | loss_train: 0.0253 loss_val: 0.8479 | acc_train: 1.0000 acc_val: 0.7700 | f1_train: 1.0000 f1_val: 0.7655
Epoch: 0019 | loss_train: 0.0163 loss_val: 0.9169 | acc_train: 1.0000 acc_val: 0.7660 | f1_train: 1.0000 f1_val: 0.7605
Epoch: 0020 | loss_train: 0.0101 loss_val: 0.9886 | acc_train: 1.0000 acc_val: 0.7580 | f1_train: 1.0000 f1_val: 0.7511
Epoch: 0021 | loss_train: 0.0070 loss_val: 1.0602 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0022 | loss_train: 0.0047 loss_val: 1.1301 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7422
Epoch: 0023 | loss_train: 0.0033 loss_val: 1.1974 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7394
Epoch: 0024 | loss_train: 0.0024 loss_val: 1.2620 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7351
Epoch: 0025 | loss_train: 0.0017 loss_val: 1.3238 | acc_train: 1.0000 acc_val: 0.7420 | f1_train: 1.0000 f1_val: 0.7355
Epoch: 0026 | loss_train: 0.0013 loss_val: 1.3828 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7391
Epoch: 0027 | loss_train: 0.0010 loss_val: 1.4388 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7396
Epoch: 0028 | loss_train: 0.0008 loss_val: 1.4920 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7426
Epoch: 0029 | loss_train: 0.0006 loss_val: 1.5427 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7427
Epoch: 0030 | loss_train: 0.0005 loss_val: 1.5906 | acc_train: 1.0000 acc_val: 0.7440 | f1_train: 1.0000 f1_val: 0.7439
Epoch: 0031 | loss_train: 0.0004 loss_val: 1.6363 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7467
Epoch: 0032 | loss_train: 0.0003 loss_val: 1.6800 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7431
Epoch: 0033 | loss_train: 0.0002 loss_val: 1.7215 | acc_train: 1.0000 acc_val: 0.7480 | f1_train: 1.0000 f1_val: 0.7456
Epoch: 0034 | loss_train: 0.0002 loss_val: 1.7610 | acc_train: 1.0000 acc_val: 0.7460 | f1_train: 1.0000 f1_val: 0.7452
Epoch: 0035 | loss_train: 0.0002 loss_val: 1.7988 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7492
Epoch: 0036 | loss_train: 0.0001 loss_val: 1.8345 | acc_train: 1.0000 acc_val: 0.7520 | f1_train: 1.0000 f1_val: 0.7510
Epoch: 0037 | loss_train: 0.0001 loss_val: 1.8685 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0038 | loss_train: 0.0001 loss_val: 1.9008 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7531
Epoch: 0039 | loss_train: 0.0001 loss_val: 1.9313 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0040 | loss_train: 0.0001 loss_val: 1.9601 | acc_train: 1.0000 acc_val: 0.7560 | f1_train: 1.0000 f1_val: 0.7550
Epoch: 0041 | loss_train: 0.0001 loss_val: 1.9873 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7542
Epoch: 0042 | loss_train: 0.0001 loss_val: 2.0127 | acc_train: 1.0000 acc_val: 0.7540 | f1_train: 1.0000 f1_val: 0.7551
Epoch: 0043 | loss_train: 0.0001 loss_val: 2.0368 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7504
Epoch: 0044 | loss_train: 0.0001 loss_val: 2.0593 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7502
Epoch: 0045 | loss_train: 0.0000 loss_val: 2.0804 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7507
Epoch: 0046 | loss_train: 0.0000 loss_val: 2.0999 | acc_train: 1.0000 acc_val: 0.7500 | f1_train: 1.0000 f1_val: 0.7506
Optimization Finished!
Train cost: 11.0661s
Loading 15th epoch
Test set results: loss= 0.6384 accuracy= 0.7910

>>> run.py: Namespace(dataset='cora', device=1, experiment='hidden_dim', log_path='log/nagphormer/cora/hidden_dim', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/cora/hidden_dim')

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=32, hops=3, log_path='log/nagphormer/wisconsin/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=32, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=32, out_features=32, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=32, out_features=64, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=64, out_features=32, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=32, out_features=16, bias=True)
  (attn_layer): Linear(in_features=64, out_features=1, bias=True)
  (Linear1): Linear(in_features=16, out_features=5, bias=True)
)
total params: 63911
Epoch: 0001 | loss_train: 1.6070 loss_val: 1.6077 | acc_train: 0.2917 acc_val: 0.2625 | f1_train: 0.1942 f1_val: 0.1685
Epoch: 0002 | loss_train: 1.6065 loss_val: 1.6069 | acc_train: 0.3167 acc_val: 0.2500 | f1_train: 0.2085 f1_val: 0.1469
Epoch: 0003 | loss_train: 1.6054 loss_val: 1.6056 | acc_train: 0.3667 acc_val: 0.3250 | f1_train: 0.2319 f1_val: 0.1636
Epoch: 0004 | loss_train: 1.6044 loss_val: 1.6039 | acc_train: 0.4000 acc_val: 0.3875 | f1_train: 0.2434 f1_val: 0.1895
Epoch: 0005 | loss_train: 1.6027 loss_val: 1.6021 | acc_train: 0.4583 acc_val: 0.5000 | f1_train: 0.2723 f1_val: 0.2316
Epoch: 0006 | loss_train: 1.6009 loss_val: 1.6002 | acc_train: 0.5167 acc_val: 0.5125 | f1_train: 0.2767 f1_val: 0.2324
Epoch: 0007 | loss_train: 1.5988 loss_val: 1.5982 | acc_train: 0.5500 acc_val: 0.5375 | f1_train: 0.2908 f1_val: 0.2429
Epoch: 0008 | loss_train: 1.5965 loss_val: 1.5961 | acc_train: 0.5750 acc_val: 0.5250 | f1_train: 0.2858 f1_val: 0.2377
Epoch: 0009 | loss_train: 1.5939 loss_val: 1.5939 | acc_train: 0.5750 acc_val: 0.5375 | f1_train: 0.2663 f1_val: 0.2432
Epoch: 0010 | loss_train: 1.5913 loss_val: 1.5916 | acc_train: 0.5833 acc_val: 0.5375 | f1_train: 0.2703 f1_val: 0.2432
Epoch: 0011 | loss_train: 1.5886 loss_val: 1.5892 | acc_train: 0.5833 acc_val: 0.5375 | f1_train: 0.2703 f1_val: 0.2432
Epoch: 0012 | loss_train: 1.5857 loss_val: 1.5866 | acc_train: 0.5750 acc_val: 0.5375 | f1_train: 0.2667 f1_val: 0.2432
Epoch: 0013 | loss_train: 1.5827 loss_val: 1.5840 | acc_train: 0.5750 acc_val: 0.5375 | f1_train: 0.2667 f1_val: 0.2432
Epoch: 0014 | loss_train: 1.5794 loss_val: 1.5811 | acc_train: 0.5750 acc_val: 0.5375 | f1_train: 0.2667 f1_val: 0.2429
Epoch: 0015 | loss_train: 1.5757 loss_val: 1.5780 | acc_train: 0.6000 acc_val: 0.5500 | f1_train: 0.2780 f1_val: 0.2478
Epoch: 0016 | loss_train: 1.5720 loss_val: 1.5746 | acc_train: 0.6167 acc_val: 0.5500 | f1_train: 0.2856 f1_val: 0.2478
Epoch: 0017 | loss_train: 1.5680 loss_val: 1.5708 | acc_train: 0.6250 acc_val: 0.5625 | f1_train: 0.2893 f1_val: 0.2540
Epoch: 0018 | loss_train: 1.5636 loss_val: 1.5667 | acc_train: 0.6417 acc_val: 0.5625 | f1_train: 0.2967 f1_val: 0.2540
Epoch: 0019 | loss_train: 1.5587 loss_val: 1.5622 | acc_train: 0.6417 acc_val: 0.5625 | f1_train: 0.2967 f1_val: 0.2538
Epoch: 0020 | loss_train: 1.5532 loss_val: 1.5573 | acc_train: 0.6417 acc_val: 0.5625 | f1_train: 0.2967 f1_val: 0.2538
Epoch: 0021 | loss_train: 1.5472 loss_val: 1.5519 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3002 f1_val: 0.2538
Epoch: 0022 | loss_train: 1.5408 loss_val: 1.5460 | acc_train: 0.6500 acc_val: 0.5875 | f1_train: 0.3002 f1_val: 0.2646
Epoch: 0023 | loss_train: 1.5337 loss_val: 1.5394 | acc_train: 0.6583 acc_val: 0.5875 | f1_train: 0.3037 f1_val: 0.2646
Epoch: 0024 | loss_train: 1.5257 loss_val: 1.5322 | acc_train: 0.6583 acc_val: 0.6000 | f1_train: 0.3036 f1_val: 0.2699
Epoch: 0025 | loss_train: 1.5169 loss_val: 1.5242 | acc_train: 0.6667 acc_val: 0.6000 | f1_train: 0.3073 f1_val: 0.2708
Epoch: 0026 | loss_train: 1.5073 loss_val: 1.5159 | acc_train: 0.6833 acc_val: 0.5875 | f1_train: 0.3146 f1_val: 0.2646
Epoch: 0027 | loss_train: 1.4969 loss_val: 1.5056 | acc_train: 0.7000 acc_val: 0.5875 | f1_train: 0.3535 f1_val: 0.2649
Epoch: 0028 | loss_train: 1.4854 loss_val: 1.4946 | acc_train: 0.7083 acc_val: 0.6000 | f1_train: 0.3572 f1_val: 0.2708
Epoch: 0029 | loss_train: 1.4731 loss_val: 1.4837 | acc_train: 0.7000 acc_val: 0.6000 | f1_train: 0.3224 f1_val: 0.2708
Epoch: 0030 | loss_train: 1.4598 loss_val: 1.4702 | acc_train: 0.7083 acc_val: 0.6250 | f1_train: 0.3260 f1_val: 0.2816
Epoch: 0031 | loss_train: 1.4446 loss_val: 1.4566 | acc_train: 0.7083 acc_val: 0.6250 | f1_train: 0.3256 f1_val: 0.2816
Epoch: 0032 | loss_train: 1.4285 loss_val: 1.4443 | acc_train: 0.7083 acc_val: 0.6000 | f1_train: 0.3256 f1_val: 0.2708
Epoch: 0033 | loss_train: 1.4117 loss_val: 1.4272 | acc_train: 0.7167 acc_val: 0.6500 | f1_train: 0.3298 f1_val: 0.2925
Epoch: 0034 | loss_train: 1.3931 loss_val: 1.4097 | acc_train: 0.7250 acc_val: 0.6500 | f1_train: 0.3708 f1_val: 0.2925
Epoch: 0035 | loss_train: 1.3716 loss_val: 1.3922 | acc_train: 0.7250 acc_val: 0.6375 | f1_train: 0.3708 f1_val: 0.2870
Epoch: 0036 | loss_train: 1.3491 loss_val: 1.3726 | acc_train: 0.7417 acc_val: 0.6625 | f1_train: 0.3934 f1_val: 0.2979
Epoch: 0037 | loss_train: 1.3245 loss_val: 1.3560 | acc_train: 0.7500 acc_val: 0.6375 | f1_train: 0.4235 f1_val: 0.2896
Epoch: 0038 | loss_train: 1.2990 loss_val: 1.3405 | acc_train: 0.7750 acc_val: 0.6125 | f1_train: 0.5752 f1_val: 0.2803
Epoch: 0039 | loss_train: 1.2718 loss_val: 1.3192 | acc_train: 0.7917 acc_val: 0.6375 | f1_train: 0.6375 f1_val: 0.2922
Epoch: 0040 | loss_train: 1.2422 loss_val: 1.3012 | acc_train: 0.8000 acc_val: 0.6250 | f1_train: 0.6572 f1_val: 0.2883
Epoch: 0041 | loss_train: 1.2119 loss_val: 1.2823 | acc_train: 0.8083 acc_val: 0.6250 | f1_train: 0.7389 f1_val: 0.2890
Epoch: 0042 | loss_train: 1.1803 loss_val: 1.2590 | acc_train: 0.8083 acc_val: 0.6125 | f1_train: 0.7352 f1_val: 0.2843
Epoch: 0043 | loss_train: 1.1469 loss_val: 1.2403 | acc_train: 0.8083 acc_val: 0.5875 | f1_train: 0.7083 f1_val: 0.2789
Epoch: 0044 | loss_train: 1.1131 loss_val: 1.2293 | acc_train: 0.8083 acc_val: 0.6250 | f1_train: 0.7083 f1_val: 0.3378
Epoch: 0045 | loss_train: 1.0781 loss_val: 1.2145 | acc_train: 0.8250 acc_val: 0.6125 | f1_train: 0.6027 f1_val: 0.3288
Epoch: 0046 | loss_train: 1.0454 loss_val: 1.1905 | acc_train: 0.8167 acc_val: 0.6250 | f1_train: 0.5797 f1_val: 0.3621
Epoch: 0047 | loss_train: 1.0086 loss_val: 1.1728 | acc_train: 0.8333 acc_val: 0.6000 | f1_train: 0.6369 f1_val: 0.3221
Epoch: 0048 | loss_train: 0.9734 loss_val: 1.1663 | acc_train: 0.8417 acc_val: 0.6125 | f1_train: 0.6415 f1_val: 0.3594
Epoch: 0049 | loss_train: 0.9388 loss_val: 1.1427 | acc_train: 0.8667 acc_val: 0.6250 | f1_train: 0.6720 f1_val: 0.3905
Epoch: 0050 | loss_train: 0.9008 loss_val: 1.1327 | acc_train: 0.8750 acc_val: 0.6000 | f1_train: 0.6814 f1_val: 0.3742
Epoch: 0051 | loss_train: 0.8659 loss_val: 1.1251 | acc_train: 0.8750 acc_val: 0.6000 | f1_train: 0.6814 f1_val: 0.3742
Epoch: 0052 | loss_train: 0.8297 loss_val: 1.1136 | acc_train: 0.8750 acc_val: 0.6000 | f1_train: 0.6746 f1_val: 0.3750
Epoch: 0053 | loss_train: 0.7963 loss_val: 1.1107 | acc_train: 0.8833 acc_val: 0.5750 | f1_train: 0.6840 f1_val: 0.3616
Epoch: 0054 | loss_train: 0.7614 loss_val: 1.0973 | acc_train: 0.8917 acc_val: 0.5875 | f1_train: 0.6886 f1_val: 0.3682
Epoch: 0055 | loss_train: 0.7280 loss_val: 1.0961 | acc_train: 0.8917 acc_val: 0.5875 | f1_train: 0.6886 f1_val: 0.3682
Epoch: 0056 | loss_train: 0.6931 loss_val: 1.0910 | acc_train: 0.9167 acc_val: 0.5875 | f1_train: 0.7170 f1_val: 0.3682
Epoch: 0057 | loss_train: 0.6588 loss_val: 1.0760 | acc_train: 0.9250 acc_val: 0.5750 | f1_train: 0.7330 f1_val: 0.3616
Epoch: 0058 | loss_train: 0.6288 loss_val: 1.1037 | acc_train: 0.9167 acc_val: 0.5625 | f1_train: 0.7196 f1_val: 0.3729
Epoch: 0059 | loss_train: 0.5993 loss_val: 1.0776 | acc_train: 0.9333 acc_val: 0.5750 | f1_train: 0.7429 f1_val: 0.3610
Epoch: 0060 | loss_train: 0.5646 loss_val: 1.0752 | acc_train: 0.9333 acc_val: 0.6000 | f1_train: 0.7429 f1_val: 0.3736
Epoch: 0061 | loss_train: 0.5341 loss_val: 1.1375 | acc_train: 0.9167 acc_val: 0.5625 | f1_train: 0.7170 f1_val: 0.3675
Epoch: 0062 | loss_train: 0.5105 loss_val: 1.1001 | acc_train: 0.9583 acc_val: 0.5750 | f1_train: 0.7648 f1_val: 0.3623
Epoch: 0063 | loss_train: 0.4748 loss_val: 1.0806 | acc_train: 0.9417 acc_val: 0.5750 | f1_train: 0.7475 f1_val: 0.3623
Epoch: 0064 | loss_train: 0.4495 loss_val: 1.0910 | acc_train: 0.9417 acc_val: 0.5500 | f1_train: 0.7500 f1_val: 0.3604
Epoch: 0065 | loss_train: 0.4246 loss_val: 1.1202 | acc_train: 0.9583 acc_val: 0.5500 | f1_train: 0.7648 f1_val: 0.3592
Epoch: 0066 | loss_train: 0.3974 loss_val: 1.1231 | acc_train: 0.9583 acc_val: 0.5500 | f1_train: 0.7648 f1_val: 0.3432
Epoch: 0067 | loss_train: 0.3723 loss_val: 1.1480 | acc_train: 0.9667 acc_val: 0.5500 | f1_train: 0.7789 f1_val: 0.3432
Epoch: 0068 | loss_train: 0.3503 loss_val: 1.1765 | acc_train: 0.9583 acc_val: 0.5375 | f1_train: 0.7704 f1_val: 0.3520
Epoch: 0069 | loss_train: 0.3276 loss_val: 1.1973 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3511
Epoch: 0070 | loss_train: 0.3089 loss_val: 1.2155 | acc_train: 0.9667 acc_val: 0.5625 | f1_train: 0.7789 f1_val: 0.3660
Epoch: 0071 | loss_train: 0.2854 loss_val: 1.2240 | acc_train: 0.9667 acc_val: 0.5500 | f1_train: 0.7789 f1_val: 0.3451
Epoch: 0072 | loss_train: 0.2675 loss_val: 1.2275 | acc_train: 0.9667 acc_val: 0.5625 | f1_train: 0.7789 f1_val: 0.3666
Epoch: 0073 | loss_train: 0.2478 loss_val: 1.2707 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3488
Epoch: 0074 | loss_train: 0.2321 loss_val: 1.3135 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3488
Epoch: 0075 | loss_train: 0.2188 loss_val: 1.3071 | acc_train: 0.9667 acc_val: 0.5625 | f1_train: 0.7789 f1_val: 0.3635
Epoch: 0076 | loss_train: 0.2011 loss_val: 1.3344 | acc_train: 0.9667 acc_val: 0.5625 | f1_train: 0.7789 f1_val: 0.3643
Epoch: 0077 | loss_train: 0.1879 loss_val: 1.4021 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3362
Epoch: 0078 | loss_train: 0.1747 loss_val: 1.4275 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3488
Epoch: 0079 | loss_train: 0.1616 loss_val: 1.4341 | acc_train: 0.9667 acc_val: 0.5500 | f1_train: 0.7789 f1_val: 0.3709
Epoch: 0080 | loss_train: 0.1497 loss_val: 1.4484 | acc_train: 0.9667 acc_val: 0.5500 | f1_train: 0.7789 f1_val: 0.3557
Epoch: 0081 | loss_train: 0.1396 loss_val: 1.4703 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3493
Epoch: 0082 | loss_train: 0.1297 loss_val: 1.5284 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3636
Epoch: 0083 | loss_train: 0.1204 loss_val: 1.5604 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3636
Epoch: 0084 | loss_train: 0.1127 loss_val: 1.5713 | acc_train: 0.9667 acc_val: 0.5250 | f1_train: 0.7789 f1_val: 0.3286
Epoch: 0085 | loss_train: 0.1046 loss_val: 1.6039 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3350
Epoch: 0086 | loss_train: 0.1001 loss_val: 1.5872 | acc_train: 0.9667 acc_val: 0.5625 | f1_train: 0.7789 f1_val: 0.3778
Epoch: 0087 | loss_train: 0.1050 loss_val: 1.6612 | acc_train: 0.9667 acc_val: 0.5500 | f1_train: 0.7789 f1_val: 0.3711
Epoch: 0088 | loss_train: 0.0951 loss_val: 1.7193 | acc_train: 0.9667 acc_val: 0.5250 | f1_train: 0.7789 f1_val: 0.3579
Epoch: 0089 | loss_train: 0.0849 loss_val: 1.7717 | acc_train: 0.9667 acc_val: 0.5250 | f1_train: 0.7789 f1_val: 0.3441
Epoch: 0090 | loss_train: 0.0845 loss_val: 1.7998 | acc_train: 0.9667 acc_val: 0.5375 | f1_train: 0.7789 f1_val: 0.3640
Optimization Finished!
Train cost: 10.7323s
Loading 36th epoch
Test set results: loss= 1.4030 accuracy= 0.5294

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=64, hops=3, log_path='log/nagphormer/wisconsin/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=64, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=64, out_features=64, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=64, out_features=128, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=128, out_features=64, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=64, out_features=32, bias=True)
  (attn_layer): Linear(in_features=128, out_features=1, bias=True)
  (Linear1): Linear(in_features=32, out_features=5, bias=True)
)
total params: 145223
Epoch: 0001 | loss_train: 1.6105 loss_val: 1.6056 | acc_train: 0.1333 acc_val: 0.2125 | f1_train: 0.0939 f1_val: 0.1687
Epoch: 0002 | loss_train: 1.6089 loss_val: 1.6032 | acc_train: 0.1250 acc_val: 0.2250 | f1_train: 0.0812 f1_val: 0.1742
Epoch: 0003 | loss_train: 1.6064 loss_val: 1.5997 | acc_train: 0.1917 acc_val: 0.3500 | f1_train: 0.1264 f1_val: 0.2113
Epoch: 0004 | loss_train: 1.6023 loss_val: 1.5955 | acc_train: 0.3167 acc_val: 0.4875 | f1_train: 0.2060 f1_val: 0.2711
Epoch: 0005 | loss_train: 1.5976 loss_val: 1.5906 | acc_train: 0.4250 acc_val: 0.5250 | f1_train: 0.2441 f1_val: 0.2410
Epoch: 0006 | loss_train: 1.5925 loss_val: 1.5853 | acc_train: 0.4833 acc_val: 0.5375 | f1_train: 0.2639 f1_val: 0.2419
Epoch: 0007 | loss_train: 1.5864 loss_val: 1.5795 | acc_train: 0.5417 acc_val: 0.5250 | f1_train: 0.3235 f1_val: 0.2360
Epoch: 0008 | loss_train: 1.5803 loss_val: 1.5734 | acc_train: 0.5500 acc_val: 0.5500 | f1_train: 0.2738 f1_val: 0.2499
Epoch: 0009 | loss_train: 1.5738 loss_val: 1.5667 | acc_train: 0.5500 acc_val: 0.5625 | f1_train: 0.2529 f1_val: 0.2566
Epoch: 0010 | loss_train: 1.5671 loss_val: 1.5594 | acc_train: 0.5667 acc_val: 0.5625 | f1_train: 0.2607 f1_val: 0.2566
Epoch: 0011 | loss_train: 1.5590 loss_val: 1.5512 | acc_train: 0.5500 acc_val: 0.5500 | f1_train: 0.2531 f1_val: 0.2521
Epoch: 0012 | loss_train: 1.5507 loss_val: 1.5422 | acc_train: 0.5583 acc_val: 0.5625 | f1_train: 0.2565 f1_val: 0.2578
Epoch: 0013 | loss_train: 1.5413 loss_val: 1.5324 | acc_train: 0.5583 acc_val: 0.5750 | f1_train: 0.2557 f1_val: 0.2635
Epoch: 0014 | loss_train: 1.5308 loss_val: 1.5218 | acc_train: 0.5667 acc_val: 0.6000 | f1_train: 0.2602 f1_val: 0.2750
Epoch: 0015 | loss_train: 1.5198 loss_val: 1.5104 | acc_train: 0.5583 acc_val: 0.6250 | f1_train: 0.2538 f1_val: 0.2868
Epoch: 0016 | loss_train: 1.5081 loss_val: 1.4984 | acc_train: 0.5750 acc_val: 0.6500 | f1_train: 0.2612 f1_val: 0.2965
Epoch: 0017 | loss_train: 1.4960 loss_val: 1.4855 | acc_train: 0.5500 acc_val: 0.6375 | f1_train: 0.2443 f1_val: 0.2887
Epoch: 0018 | loss_train: 1.4826 loss_val: 1.4719 | acc_train: 0.5583 acc_val: 0.6375 | f1_train: 0.2486 f1_val: 0.2867
Epoch: 0019 | loss_train: 1.4692 loss_val: 1.4577 | acc_train: 0.5583 acc_val: 0.6375 | f1_train: 0.2486 f1_val: 0.2867
Epoch: 0020 | loss_train: 1.4541 loss_val: 1.4425 | acc_train: 0.5500 acc_val: 0.6500 | f1_train: 0.2410 f1_val: 0.2880
Epoch: 0021 | loss_train: 1.4381 loss_val: 1.4261 | acc_train: 0.5500 acc_val: 0.6500 | f1_train: 0.2402 f1_val: 0.2880
Epoch: 0022 | loss_train: 1.4205 loss_val: 1.4084 | acc_train: 0.5250 acc_val: 0.6625 | f1_train: 0.2218 f1_val: 0.2944
Epoch: 0023 | loss_train: 1.4004 loss_val: 1.3889 | acc_train: 0.5167 acc_val: 0.6750 | f1_train: 0.2147 f1_val: 0.3006
Epoch: 0024 | loss_train: 1.3782 loss_val: 1.3675 | acc_train: 0.5250 acc_val: 0.6750 | f1_train: 0.2490 f1_val: 0.3027
Epoch: 0025 | loss_train: 1.3538 loss_val: 1.3443 | acc_train: 0.5500 acc_val: 0.6750 | f1_train: 0.2913 f1_val: 0.3027
Epoch: 0026 | loss_train: 1.3258 loss_val: 1.3192 | acc_train: 0.5833 acc_val: 0.6625 | f1_train: 0.3311 f1_val: 0.2926
Epoch: 0027 | loss_train: 1.2948 loss_val: 1.2948 | acc_train: 0.6500 acc_val: 0.6625 | f1_train: 0.3989 f1_val: 0.2980
Epoch: 0028 | loss_train: 1.2588 loss_val: 1.2781 | acc_train: 0.6833 acc_val: 0.6500 | f1_train: 0.4490 f1_val: 0.2940
Epoch: 0029 | loss_train: 1.2289 loss_val: 1.2366 | acc_train: 0.7583 acc_val: 0.6625 | f1_train: 0.5854 f1_val: 0.3012
Epoch: 0030 | loss_train: 1.1861 loss_val: 1.2039 | acc_train: 0.7500 acc_val: 0.6500 | f1_train: 0.5182 f1_val: 0.2921
Epoch: 0031 | loss_train: 1.1476 loss_val: 1.1733 | acc_train: 0.6917 acc_val: 0.6625 | f1_train: 0.4702 f1_val: 0.2979
Epoch: 0032 | loss_train: 1.1068 loss_val: 1.1485 | acc_train: 0.7167 acc_val: 0.6500 | f1_train: 0.4922 f1_val: 0.2927
Epoch: 0033 | loss_train: 1.0639 loss_val: 1.1387 | acc_train: 0.7667 acc_val: 0.6375 | f1_train: 0.5386 f1_val: 0.2975
Epoch: 0034 | loss_train: 1.0190 loss_val: 1.1249 | acc_train: 0.8000 acc_val: 0.6375 | f1_train: 0.6341 f1_val: 0.3044
Epoch: 0035 | loss_train: 0.9720 loss_val: 1.1133 | acc_train: 0.8250 acc_val: 0.6375 | f1_train: 0.7074 f1_val: 0.3914
Epoch: 0036 | loss_train: 0.9255 loss_val: 1.0929 | acc_train: 0.8500 acc_val: 0.6250 | f1_train: 0.7676 f1_val: 0.3847
Epoch: 0037 | loss_train: 0.8771 loss_val: 1.0586 | acc_train: 0.8500 acc_val: 0.6500 | f1_train: 0.7645 f1_val: 0.3954
Epoch: 0038 | loss_train: 0.8316 loss_val: 1.0373 | acc_train: 0.8583 acc_val: 0.6500 | f1_train: 0.7722 f1_val: 0.3936
Epoch: 0039 | loss_train: 0.7852 loss_val: 1.0322 | acc_train: 0.8417 acc_val: 0.5750 | f1_train: 0.7329 f1_val: 0.3589
Epoch: 0040 | loss_train: 0.7392 loss_val: 1.0297 | acc_train: 0.8667 acc_val: 0.5750 | f1_train: 0.7507 f1_val: 0.3610
Epoch: 0041 | loss_train: 0.6947 loss_val: 1.0283 | acc_train: 0.9083 acc_val: 0.5750 | f1_train: 0.8331 f1_val: 0.4016
Epoch: 0042 | loss_train: 0.6520 loss_val: 1.0327 | acc_train: 0.9167 acc_val: 0.5500 | f1_train: 0.8425 f1_val: 0.3246
Epoch: 0043 | loss_train: 0.6095 loss_val: 1.0209 | acc_train: 0.9167 acc_val: 0.6000 | f1_train: 0.8453 f1_val: 0.4132
Epoch: 0044 | loss_train: 0.5688 loss_val: 1.0051 | acc_train: 0.9250 acc_val: 0.6000 | f1_train: 0.8665 f1_val: 0.3861
Epoch: 0045 | loss_train: 0.5297 loss_val: 1.0089 | acc_train: 0.9333 acc_val: 0.6000 | f1_train: 0.8833 f1_val: 0.3859
Epoch: 0046 | loss_train: 0.4938 loss_val: 1.0309 | acc_train: 0.9167 acc_val: 0.5750 | f1_train: 0.7123 f1_val: 0.3744
Epoch: 0047 | loss_train: 0.4588 loss_val: 1.0424 | acc_train: 0.9417 acc_val: 0.6000 | f1_train: 0.8894 f1_val: 0.3859
Epoch: 0048 | loss_train: 0.4261 loss_val: 1.0294 | acc_train: 0.9500 acc_val: 0.6125 | f1_train: 0.9073 f1_val: 0.4169
Epoch: 0049 | loss_train: 0.3940 loss_val: 1.0191 | acc_train: 0.9583 acc_val: 0.6125 | f1_train: 0.9242 f1_val: 0.4175
Epoch: 0050 | loss_train: 0.3653 loss_val: 1.0318 | acc_train: 0.9500 acc_val: 0.6125 | f1_train: 0.9107 f1_val: 0.4168
Epoch: 0051 | loss_train: 0.3381 loss_val: 1.0623 | acc_train: 0.9583 acc_val: 0.6125 | f1_train: 0.9242 f1_val: 0.4463
Epoch: 0052 | loss_train: 0.3125 loss_val: 1.0824 | acc_train: 0.9750 acc_val: 0.6125 | f1_train: 0.9672 f1_val: 0.3973
Epoch: 0053 | loss_train: 0.2885 loss_val: 1.0834 | acc_train: 0.9833 acc_val: 0.6250 | f1_train: 0.9761 f1_val: 0.4051
Epoch: 0054 | loss_train: 0.2654 loss_val: 1.0916 | acc_train: 0.9917 acc_val: 0.6250 | f1_train: 0.9848 f1_val: 0.4021
Epoch: 0055 | loss_train: 0.2435 loss_val: 1.1134 | acc_train: 0.9917 acc_val: 0.6250 | f1_train: 0.9848 f1_val: 0.3997
Epoch: 0056 | loss_train: 0.2220 loss_val: 1.1428 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.3984
Epoch: 0057 | loss_train: 0.2030 loss_val: 1.1836 | acc_train: 0.9917 acc_val: 0.6125 | f1_train: 0.9627 f1_val: 0.3914
Epoch: 0058 | loss_train: 0.1843 loss_val: 1.2297 | acc_train: 0.9917 acc_val: 0.6125 | f1_train: 0.9627 f1_val: 0.3933
Epoch: 0059 | loss_train: 0.1690 loss_val: 1.2746 | acc_train: 0.9917 acc_val: 0.6125 | f1_train: 0.9627 f1_val: 0.3914
Epoch: 0060 | loss_train: 0.1530 loss_val: 1.3185 | acc_train: 0.9667 acc_val: 0.5875 | f1_train: 0.7692 f1_val: 0.3784
Epoch: 0061 | loss_train: 0.1389 loss_val: 1.3638 | acc_train: 0.9667 acc_val: 0.5750 | f1_train: 0.7692 f1_val: 0.3707
Epoch: 0062 | loss_train: 0.1260 loss_val: 1.4073 | acc_train: 0.9667 acc_val: 0.5750 | f1_train: 0.7692 f1_val: 0.3707
Epoch: 0063 | loss_train: 0.1137 loss_val: 1.4563 | acc_train: 0.9667 acc_val: 0.5750 | f1_train: 0.7692 f1_val: 0.3707
Epoch: 0064 | loss_train: 0.1028 loss_val: 1.5059 | acc_train: 0.9750 acc_val: 0.5750 | f1_train: 0.8560 f1_val: 0.3728
Epoch: 0065 | loss_train: 0.0929 loss_val: 1.5493 | acc_train: 0.9750 acc_val: 0.5875 | f1_train: 0.8560 f1_val: 0.3974
Epoch: 0066 | loss_train: 0.0834 loss_val: 1.5805 | acc_train: 0.9833 acc_val: 0.5875 | f1_train: 0.9167 f1_val: 0.3974
Epoch: 0067 | loss_train: 0.0753 loss_val: 1.6306 | acc_train: 0.9833 acc_val: 0.6000 | f1_train: 0.9167 f1_val: 0.4193
Epoch: 0068 | loss_train: 0.0653 loss_val: 1.6904 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4815
Epoch: 0069 | loss_train: 0.0574 loss_val: 1.7480 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4608
Epoch: 0070 | loss_train: 0.0508 loss_val: 1.7989 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4608
Epoch: 0071 | loss_train: 0.0450 loss_val: 1.8415 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4608
Epoch: 0072 | loss_train: 0.0397 loss_val: 1.8887 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4608
Epoch: 0073 | loss_train: 0.0351 loss_val: 1.9394 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4608
Epoch: 0074 | loss_train: 0.0306 loss_val: 1.9874 | acc_train: 1.0000 acc_val: 0.5750 | f1_train: 1.0000 f1_val: 0.4618
Optimization Finished!
Train cost: 7.3733s
Loading 25th epoch
Test set results: loss= 1.3739 accuracy= 0.5686

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=128, hops=3, log_path='log/nagphormer/wisconsin/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=128, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=128, out_features=128, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=128, out_features=256, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=256, out_features=128, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=128, out_features=64, bias=True)
  (attn_layer): Linear(in_features=256, out_features=1, bias=True)
  (Linear1): Linear(in_features=64, out_features=5, bias=True)
)
total params: 360071
Epoch: 0001 | loss_train: 1.5985 loss_val: 1.5879 | acc_train: 0.3500 acc_val: 0.5500 | f1_train: 0.2661 f1_val: 0.3657
Epoch: 0002 | loss_train: 1.5932 loss_val: 1.5783 | acc_train: 0.3750 acc_val: 0.6000 | f1_train: 0.2396 f1_val: 0.3954
Epoch: 0003 | loss_train: 1.5837 loss_val: 1.5653 | acc_train: 0.5250 acc_val: 0.6750 | f1_train: 0.3078 f1_val: 0.4503
Epoch: 0004 | loss_train: 1.5697 loss_val: 1.5495 | acc_train: 0.5917 acc_val: 0.6500 | f1_train: 0.3369 f1_val: 0.4083
Epoch: 0005 | loss_train: 1.5534 loss_val: 1.5317 | acc_train: 0.5750 acc_val: 0.6500 | f1_train: 0.2878 f1_val: 0.3926
Epoch: 0006 | loss_train: 1.5348 loss_val: 1.5122 | acc_train: 0.5917 acc_val: 0.6125 | f1_train: 0.2922 f1_val: 0.3572
Epoch: 0007 | loss_train: 1.5129 loss_val: 1.4918 | acc_train: 0.5833 acc_val: 0.5750 | f1_train: 0.2688 f1_val: 0.3054
Epoch: 0008 | loss_train: 1.4909 loss_val: 1.4710 | acc_train: 0.5917 acc_val: 0.5500 | f1_train: 0.2728 f1_val: 0.2480
Epoch: 0009 | loss_train: 1.4679 loss_val: 1.4499 | acc_train: 0.6083 acc_val: 0.5500 | f1_train: 0.2809 f1_val: 0.2480
Epoch: 0010 | loss_train: 1.4442 loss_val: 1.4282 | acc_train: 0.6083 acc_val: 0.5500 | f1_train: 0.2807 f1_val: 0.2480
Epoch: 0011 | loss_train: 1.4181 loss_val: 1.4058 | acc_train: 0.6083 acc_val: 0.5500 | f1_train: 0.2809 f1_val: 0.2480
Epoch: 0012 | loss_train: 1.3920 loss_val: 1.3831 | acc_train: 0.6250 acc_val: 0.5500 | f1_train: 0.2885 f1_val: 0.2480
Epoch: 0013 | loss_train: 1.3645 loss_val: 1.3596 | acc_train: 0.6250 acc_val: 0.5500 | f1_train: 0.2885 f1_val: 0.2480
Epoch: 0014 | loss_train: 1.3343 loss_val: 1.3349 | acc_train: 0.6250 acc_val: 0.5625 | f1_train: 0.2885 f1_val: 0.2546
Epoch: 0015 | loss_train: 1.3026 loss_val: 1.3086 | acc_train: 0.6167 acc_val: 0.5625 | f1_train: 0.2849 f1_val: 0.2546
Epoch: 0016 | loss_train: 1.2667 loss_val: 1.2803 | acc_train: 0.6167 acc_val: 0.5625 | f1_train: 0.2849 f1_val: 0.2551
Epoch: 0017 | loss_train: 1.2260 loss_val: 1.2504 | acc_train: 0.6333 acc_val: 0.5750 | f1_train: 0.3122 f1_val: 0.2633
Epoch: 0018 | loss_train: 1.1819 loss_val: 1.2195 | acc_train: 0.6583 acc_val: 0.5750 | f1_train: 0.3688 f1_val: 0.2633
Epoch: 0019 | loss_train: 1.1364 loss_val: 1.1867 | acc_train: 0.6833 acc_val: 0.5875 | f1_train: 0.4778 f1_val: 0.3046
Epoch: 0020 | loss_train: 1.0888 loss_val: 1.1463 | acc_train: 0.7250 acc_val: 0.5875 | f1_train: 0.4826 f1_val: 0.3053
Epoch: 0021 | loss_train: 1.0376 loss_val: 1.1028 | acc_train: 0.7500 acc_val: 0.5875 | f1_train: 0.5142 f1_val: 0.3053
Epoch: 0022 | loss_train: 0.9837 loss_val: 1.0612 | acc_train: 0.7583 acc_val: 0.6000 | f1_train: 0.5186 f1_val: 0.3123
Epoch: 0023 | loss_train: 0.9320 loss_val: 1.0222 | acc_train: 0.7750 acc_val: 0.6000 | f1_train: 0.5343 f1_val: 0.3117
Epoch: 0024 | loss_train: 0.8729 loss_val: 0.9871 | acc_train: 0.8083 acc_val: 0.6125 | f1_train: 0.5996 f1_val: 0.3378
Epoch: 0025 | loss_train: 0.8167 loss_val: 0.9651 | acc_train: 0.8250 acc_val: 0.6000 | f1_train: 0.6142 f1_val: 0.3308
Epoch: 0026 | loss_train: 0.7564 loss_val: 0.9304 | acc_train: 0.8667 acc_val: 0.6375 | f1_train: 0.6491 f1_val: 0.3487
Epoch: 0027 | loss_train: 0.6983 loss_val: 0.8954 | acc_train: 0.8750 acc_val: 0.6375 | f1_train: 0.6643 f1_val: 0.3476
Epoch: 0028 | loss_train: 0.6410 loss_val: 0.8795 | acc_train: 0.8833 acc_val: 0.6500 | f1_train: 0.6574 f1_val: 0.3722
Epoch: 0029 | loss_train: 0.5888 loss_val: 0.8747 | acc_train: 0.8917 acc_val: 0.6500 | f1_train: 0.6618 f1_val: 0.3676
Epoch: 0030 | loss_train: 0.5323 loss_val: 0.8836 | acc_train: 0.9000 acc_val: 0.6250 | f1_train: 0.6704 f1_val: 0.3545
Epoch: 0031 | loss_train: 0.4852 loss_val: 0.8804 | acc_train: 0.9083 acc_val: 0.6125 | f1_train: 0.6771 f1_val: 0.3469
Epoch: 0032 | loss_train: 0.4390 loss_val: 0.8862 | acc_train: 0.9250 acc_val: 0.6500 | f1_train: 0.7017 f1_val: 0.3682
Epoch: 0033 | loss_train: 0.3974 loss_val: 0.9189 | acc_train: 0.9250 acc_val: 0.6500 | f1_train: 0.6950 f1_val: 0.4082
Epoch: 0034 | loss_train: 0.3598 loss_val: 0.9741 | acc_train: 0.9250 acc_val: 0.6125 | f1_train: 0.6950 f1_val: 0.3880
Epoch: 0035 | loss_train: 0.3213 loss_val: 1.0155 | acc_train: 0.9250 acc_val: 0.5875 | f1_train: 0.6950 f1_val: 0.3609
Epoch: 0036 | loss_train: 0.2904 loss_val: 1.0381 | acc_train: 0.9417 acc_val: 0.5875 | f1_train: 0.7304 f1_val: 0.3609
Epoch: 0037 | loss_train: 0.2596 loss_val: 1.0604 | acc_train: 0.9500 acc_val: 0.6250 | f1_train: 0.7434 f1_val: 0.3794
Epoch: 0038 | loss_train: 0.2308 loss_val: 1.0929 | acc_train: 0.9500 acc_val: 0.6250 | f1_train: 0.7434 f1_val: 0.3784
Epoch: 0039 | loss_train: 0.2059 loss_val: 1.1522 | acc_train: 0.9500 acc_val: 0.6000 | f1_train: 0.7389 f1_val: 0.3646
Epoch: 0040 | loss_train: 0.1831 loss_val: 1.2401 | acc_train: 0.9583 acc_val: 0.5875 | f1_train: 0.7543 f1_val: 0.3802
Epoch: 0041 | loss_train: 0.1636 loss_val: 1.3252 | acc_train: 0.9583 acc_val: 0.5750 | f1_train: 0.7543 f1_val: 0.3741
Epoch: 0042 | loss_train: 0.1447 loss_val: 1.3645 | acc_train: 0.9667 acc_val: 0.5750 | f1_train: 0.7692 f1_val: 0.3729
Epoch: 0043 | loss_train: 0.1263 loss_val: 1.3903 | acc_train: 0.9667 acc_val: 0.5750 | f1_train: 0.7692 f1_val: 0.3743
Epoch: 0044 | loss_train: 0.1101 loss_val: 1.4297 | acc_train: 0.9667 acc_val: 0.5875 | f1_train: 0.7692 f1_val: 0.3806
Epoch: 0045 | loss_train: 0.0968 loss_val: 1.4838 | acc_train: 0.9667 acc_val: 0.5875 | f1_train: 0.7692 f1_val: 0.3806
Epoch: 0046 | loss_train: 0.0855 loss_val: 1.5563 | acc_train: 0.9833 acc_val: 0.5750 | f1_train: 0.9167 f1_val: 0.3743
Epoch: 0047 | loss_train: 0.0741 loss_val: 1.6350 | acc_train: 0.9917 acc_val: 0.5750 | f1_train: 0.9627 f1_val: 0.3743
Epoch: 0048 | loss_train: 0.0660 loss_val: 1.7032 | acc_train: 0.9917 acc_val: 0.5750 | f1_train: 0.9627 f1_val: 0.3735
Epoch: 0049 | loss_train: 0.0587 loss_val: 1.7589 | acc_train: 0.9917 acc_val: 0.5750 | f1_train: 0.9627 f1_val: 0.3735
Epoch: 0050 | loss_train: 0.0526 loss_val: 1.8016 | acc_train: 0.9917 acc_val: 0.5750 | f1_train: 0.9627 f1_val: 0.3925
Epoch: 0051 | loss_train: 0.0460 loss_val: 1.8414 | acc_train: 0.9917 acc_val: 0.5875 | f1_train: 0.9627 f1_val: 0.4113
Epoch: 0052 | loss_train: 0.0404 loss_val: 1.8895 | acc_train: 0.9917 acc_val: 0.5875 | f1_train: 0.9627 f1_val: 0.4113
Epoch: 0053 | loss_train: 0.0352 loss_val: 1.9375 | acc_train: 0.9917 acc_val: 0.5875 | f1_train: 0.9627 f1_val: 0.4113
Epoch: 0054 | loss_train: 0.0317 loss_val: 1.9924 | acc_train: 1.0000 acc_val: 0.5875 | f1_train: 1.0000 f1_val: 0.4113
Epoch: 0055 | loss_train: 0.0272 loss_val: 2.0457 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4177
Epoch: 0056 | loss_train: 0.0229 loss_val: 2.0943 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4229
Epoch: 0057 | loss_train: 0.0198 loss_val: 2.1377 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4229
Epoch: 0058 | loss_train: 0.0167 loss_val: 2.1776 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4229
Epoch: 0059 | loss_train: 0.0142 loss_val: 2.2134 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4229
Optimization Finished!
Train cost: 9.5277s
Loading 3th epoch
Test set results: loss= 1.5748 accuracy= 0.4706

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=256, hops=3, log_path='log/nagphormer/wisconsin/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=256, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=256, out_features=256, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=256, out_features=512, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=256, out_features=128, bias=True)
  (attn_layer): Linear(in_features=512, out_features=1, bias=True)
  (Linear1): Linear(in_features=128, out_features=5, bias=True)
)
total params: 998663
Epoch: 0001 | loss_train: 1.5833 loss_val: 1.5692 | acc_train: 0.3333 acc_val: 0.3125 | f1_train: 0.2743 f1_val: 0.1912
Epoch: 0002 | loss_train: 1.5691 loss_val: 1.5409 | acc_train: 0.4250 acc_val: 0.4625 | f1_train: 0.2474 f1_val: 0.2843
Epoch: 0003 | loss_train: 1.5431 loss_val: 1.5011 | acc_train: 0.5167 acc_val: 0.5125 | f1_train: 0.2827 f1_val: 0.2696
Epoch: 0004 | loss_train: 1.5042 loss_val: 1.4525 | acc_train: 0.5500 acc_val: 0.5250 | f1_train: 0.2737 f1_val: 0.2363
Epoch: 0005 | loss_train: 1.4545 loss_val: 1.3972 | acc_train: 0.6083 acc_val: 0.5500 | f1_train: 0.3008 f1_val: 0.2470
Epoch: 0006 | loss_train: 1.4000 loss_val: 1.3380 | acc_train: 0.5750 acc_val: 0.5500 | f1_train: 0.2663 f1_val: 0.2474
Epoch: 0007 | loss_train: 1.3411 loss_val: 1.2790 | acc_train: 0.6000 acc_val: 0.5500 | f1_train: 0.2776 f1_val: 0.2492
Epoch: 0008 | loss_train: 1.2825 loss_val: 1.2244 | acc_train: 0.6083 acc_val: 0.5500 | f1_train: 0.2819 f1_val: 0.2492
Epoch: 0009 | loss_train: 1.2204 loss_val: 1.1777 | acc_train: 0.6083 acc_val: 0.5500 | f1_train: 0.2828 f1_val: 0.2485
Epoch: 0010 | loss_train: 1.1621 loss_val: 1.1389 | acc_train: 0.6083 acc_val: 0.5500 | f1_train: 0.2822 f1_val: 0.2485
Epoch: 0011 | loss_train: 1.1014 loss_val: 1.1067 | acc_train: 0.6417 acc_val: 0.5500 | f1_train: 0.2978 f1_val: 0.2479
Epoch: 0012 | loss_train: 1.0420 loss_val: 1.0796 | acc_train: 0.6500 acc_val: 0.5750 | f1_train: 0.3209 f1_val: 0.2600
Epoch: 0013 | loss_train: 0.9696 loss_val: 1.0507 | acc_train: 0.6417 acc_val: 0.5875 | f1_train: 0.3337 f1_val: 0.2679
Epoch: 0014 | loss_train: 0.8898 loss_val: 1.0185 | acc_train: 0.7083 acc_val: 0.5875 | f1_train: 0.4725 f1_val: 0.2723
Epoch: 0015 | loss_train: 0.8108 loss_val: 0.9739 | acc_train: 0.7917 acc_val: 0.6250 | f1_train: 0.5892 f1_val: 0.3670
Epoch: 0016 | loss_train: 0.7332 loss_val: 0.9086 | acc_train: 0.8167 acc_val: 0.6375 | f1_train: 0.6096 f1_val: 0.3672
Epoch: 0017 | loss_train: 0.6530 loss_val: 0.8538 | acc_train: 0.8583 acc_val: 0.6500 | f1_train: 0.6520 f1_val: 0.3759
Epoch: 0018 | loss_train: 0.5892 loss_val: 0.8473 | acc_train: 0.8750 acc_val: 0.6875 | f1_train: 0.6609 f1_val: 0.4350
Epoch: 0019 | loss_train: 0.5268 loss_val: 0.8688 | acc_train: 0.9167 acc_val: 0.6750 | f1_train: 0.8496 f1_val: 0.4376
Epoch: 0020 | loss_train: 0.4609 loss_val: 0.8855 | acc_train: 0.9333 acc_val: 0.6750 | f1_train: 0.8627 f1_val: 0.4378
Epoch: 0021 | loss_train: 0.3983 loss_val: 0.9133 | acc_train: 0.9417 acc_val: 0.6875 | f1_train: 0.8762 f1_val: 0.4562
Epoch: 0022 | loss_train: 0.3436 loss_val: 0.9464 | acc_train: 0.9500 acc_val: 0.6875 | f1_train: 0.8808 f1_val: 0.4454
Epoch: 0023 | loss_train: 0.3016 loss_val: 0.9802 | acc_train: 0.9417 acc_val: 0.6875 | f1_train: 0.8255 f1_val: 0.4454
Epoch: 0024 | loss_train: 0.2588 loss_val: 1.0170 | acc_train: 0.9583 acc_val: 0.6625 | f1_train: 0.8848 f1_val: 0.4120
Epoch: 0025 | loss_train: 0.2226 loss_val: 1.0558 | acc_train: 0.9583 acc_val: 0.6500 | f1_train: 0.8783 f1_val: 0.4032
Epoch: 0026 | loss_train: 0.1887 loss_val: 1.0956 | acc_train: 0.9667 acc_val: 0.6875 | f1_train: 0.9248 f1_val: 0.4727
Epoch: 0027 | loss_train: 0.1579 loss_val: 1.1073 | acc_train: 0.9750 acc_val: 0.6500 | f1_train: 0.9383 f1_val: 0.4327
Epoch: 0028 | loss_train: 0.1290 loss_val: 1.1747 | acc_train: 0.9917 acc_val: 0.6500 | f1_train: 0.9694 f1_val: 0.4312
Epoch: 0029 | loss_train: 0.1027 loss_val: 1.2699 | acc_train: 0.9917 acc_val: 0.6500 | f1_train: 0.9694 f1_val: 0.4312
Epoch: 0030 | loss_train: 0.0821 loss_val: 1.2971 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0031 | loss_train: 0.0661 loss_val: 1.2852 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0032 | loss_train: 0.0507 loss_val: 1.2974 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0033 | loss_train: 0.0401 loss_val: 1.3787 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0034 | loss_train: 0.0301 loss_val: 1.4870 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0035 | loss_train: 0.0219 loss_val: 1.5922 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4272
Epoch: 0036 | loss_train: 0.0165 loss_val: 1.6832 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4195
Epoch: 0037 | loss_train: 0.0123 loss_val: 1.7589 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4181
Epoch: 0038 | loss_train: 0.0090 loss_val: 1.8293 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4181
Epoch: 0039 | loss_train: 0.0068 loss_val: 1.8947 | acc_train: 1.0000 acc_val: 0.6125 | f1_train: 1.0000 f1_val: 0.4102
Epoch: 0040 | loss_train: 0.0064 loss_val: 1.9968 | acc_train: 1.0000 acc_val: 0.6000 | f1_train: 1.0000 f1_val: 0.4040
Epoch: 0041 | loss_train: 0.0044 loss_val: 2.0917 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4400
Epoch: 0042 | loss_train: 0.0038 loss_val: 2.1708 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4339
Epoch: 0043 | loss_train: 0.0043 loss_val: 2.2162 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4301
Epoch: 0044 | loss_train: 0.0030 loss_val: 2.2430 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4301
Epoch: 0045 | loss_train: 0.0020 loss_val: 2.2662 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4301
Epoch: 0046 | loss_train: 0.0016 loss_val: 2.2984 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4304
Epoch: 0047 | loss_train: 0.0013 loss_val: 2.3372 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4367
Epoch: 0048 | loss_train: 0.0021 loss_val: 2.4077 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4304
Epoch: 0049 | loss_train: 0.0008 loss_val: 2.4811 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4304
Epoch: 0050 | loss_train: 0.0008 loss_val: 2.5510 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4382
Epoch: 0051 | loss_train: 0.0009 loss_val: 2.6094 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4464
Epoch: 0052 | loss_train: 0.0011 loss_val: 2.6519 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4464
Epoch: 0053 | loss_train: 0.0010 loss_val: 2.6788 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4464
Epoch: 0054 | loss_train: 0.0010 loss_val: 2.6899 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4382
Epoch: 0055 | loss_train: 0.0007 loss_val: 2.6922 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4301
Epoch: 0056 | loss_train: 0.0005 loss_val: 2.6904 | acc_train: 1.0000 acc_val: 0.6250 | f1_train: 1.0000 f1_val: 0.4304
Optimization Finished!
Train cost: 10.6147s
Loading 18th epoch
Test set results: loss= 1.0464 accuracy= 0.5686

>>> train.py: Namespace(attention_dropout=0.1, batch_size=2000, dataset='wisconsin', device=1, dropout=0.1, end_lr=0.0001, epochs=2000, ffn_dim=64, hidden_dim=512, hops=3, log_path='log/nagphormer/wisconsin/hidden_dim', n_heads=8, n_layers=1, name=None, path='data/nagphormer', patience=30, pe_dim=3, peak_lr=0.01, seed=3407, split=1.0, tot_updates=1000, warmup_updates=400, weight_decay=1e-05)
TransformerModel(
  (att_embeddings_nope): Linear(in_features=1706, out_features=512, bias=True)
  (layers): ModuleList(
    (0): EncoderLayer(
      (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiHeadAttention(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (att_dropout): Dropout(p=0.1, inplace=False)
        (output_layer): Linear(in_features=512, out_features=512, bias=True)
      )
      (self_attention_dropout): Dropout(p=0.1, inplace=False)
      (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (ffn): FeedForwardNetwork(
        (layer1): Linear(in_features=512, out_features=1024, bias=True)
        (gelu): GELU()
        (layer2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ffn_dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (out_proj): Linear(in_features=512, out_features=256, bias=True)
  (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  (Linear1): Linear(in_features=256, out_features=5, bias=True)
)
total params: 3111431
Epoch: 0001 | loss_train: 1.6315 loss_val: 1.5466 | acc_train: 0.1500 acc_val: 0.3000 | f1_train: 0.1211 f1_val: 0.1308
Epoch: 0002 | loss_train: 1.5652 loss_val: 1.4203 | acc_train: 0.2833 acc_val: 0.5000 | f1_train: 0.1653 f1_val: 0.1644
Epoch: 0003 | loss_train: 1.4443 loss_val: 1.2855 | acc_train: 0.4333 acc_val: 0.5125 | f1_train: 0.1550 f1_val: 0.1526
Epoch: 0004 | loss_train: 1.3147 loss_val: 1.1833 | acc_train: 0.4333 acc_val: 0.5750 | f1_train: 0.1390 f1_val: 0.2358
Epoch: 0005 | loss_train: 1.2048 loss_val: 1.1209 | acc_train: 0.4583 acc_val: 0.6125 | f1_train: 0.1674 f1_val: 0.2715
Epoch: 0006 | loss_train: 1.1178 loss_val: 1.0844 | acc_train: 0.5333 acc_val: 0.5625 | f1_train: 0.2372 f1_val: 0.2529
Epoch: 0007 | loss_train: 1.0281 loss_val: 1.0654 | acc_train: 0.5750 acc_val: 0.5500 | f1_train: 0.2665 f1_val: 0.2474
Epoch: 0008 | loss_train: 0.9321 loss_val: 1.0565 | acc_train: 0.6500 acc_val: 0.5625 | f1_train: 0.3003 f1_val: 0.2557
Epoch: 0009 | loss_train: 0.8323 loss_val: 1.0307 | acc_train: 0.6667 acc_val: 0.6125 | f1_train: 0.3606 f1_val: 0.3566
Epoch: 0010 | loss_train: 0.7215 loss_val: 0.9827 | acc_train: 0.7583 acc_val: 0.6000 | f1_train: 0.5366 f1_val: 0.3628
Epoch: 0011 | loss_train: 0.6194 loss_val: 0.9135 | acc_train: 0.8167 acc_val: 0.6500 | f1_train: 0.7483 f1_val: 0.4362
Epoch: 0012 | loss_train: 0.5123 loss_val: 0.8581 | acc_train: 0.8833 acc_val: 0.6500 | f1_train: 0.8614 f1_val: 0.4708
Epoch: 0013 | loss_train: 0.4123 loss_val: 0.8767 | acc_train: 0.9167 acc_val: 0.7000 | f1_train: 0.8844 f1_val: 0.5609
Epoch: 0014 | loss_train: 0.3231 loss_val: 0.9327 | acc_train: 0.9500 acc_val: 0.7000 | f1_train: 0.9142 f1_val: 0.5516
Epoch: 0015 | loss_train: 0.2508 loss_val: 0.9685 | acc_train: 0.9500 acc_val: 0.6875 | f1_train: 0.9209 f1_val: 0.4886
Epoch: 0016 | loss_train: 0.1914 loss_val: 1.0292 | acc_train: 0.9583 acc_val: 0.7000 | f1_train: 0.9259 f1_val: 0.4882
Epoch: 0017 | loss_train: 0.1423 loss_val: 1.1019 | acc_train: 0.9667 acc_val: 0.7000 | f1_train: 0.9297 f1_val: 0.4825
Epoch: 0018 | loss_train: 0.1004 loss_val: 1.1785 | acc_train: 0.9917 acc_val: 0.6750 | f1_train: 0.9694 f1_val: 0.4658
Epoch: 0019 | loss_train: 0.0704 loss_val: 1.2618 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4528
Epoch: 0020 | loss_train: 0.0536 loss_val: 1.3591 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0021 | loss_train: 0.0349 loss_val: 1.4517 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4389
Epoch: 0022 | loss_train: 0.0250 loss_val: 1.5643 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0023 | loss_train: 0.0185 loss_val: 1.6873 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0024 | loss_train: 0.0115 loss_val: 1.7941 | acc_train: 1.0000 acc_val: 0.6500 | f1_train: 1.0000 f1_val: 0.4348
Epoch: 0025 | loss_train: 0.0134 loss_val: 1.8016 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0026 | loss_train: 0.0071 loss_val: 1.8700 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4412
Epoch: 0027 | loss_train: 0.0042 loss_val: 1.9669 | acc_train: 1.0000 acc_val: 0.6375 | f1_train: 1.0000 f1_val: 0.4397
Epoch: 0028 | loss_train: 0.0032 loss_val: 2.0669 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4652
Epoch: 0029 | loss_train: 0.0030 loss_val: 2.1614 | acc_train: 1.0000 acc_val: 0.6625 | f1_train: 1.0000 f1_val: 0.4627
Epoch: 0030 | loss_train: 0.0027 loss_val: 2.2514 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4694
Epoch: 0031 | loss_train: 0.0019 loss_val: 2.3389 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0032 | loss_train: 0.0016 loss_val: 2.4217 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4762
Epoch: 0033 | loss_train: 0.0012 loss_val: 2.4973 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0034 | loss_train: 0.0011 loss_val: 2.5645 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0035 | loss_train: 0.0008 loss_val: 2.6212 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0036 | loss_train: 0.0006 loss_val: 2.6685 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0037 | loss_train: 0.0005 loss_val: 2.7098 | acc_train: 1.0000 acc_val: 0.6750 | f1_train: 1.0000 f1_val: 0.4676
Epoch: 0038 | loss_train: 0.0003 loss_val: 2.7466 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0039 | loss_train: 0.0004 loss_val: 2.7690 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0040 | loss_train: 0.0003 loss_val: 2.7881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0041 | loss_train: 0.0002 loss_val: 2.8042 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0042 | loss_train: 0.0002 loss_val: 2.8187 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0043 | loss_train: 0.0002 loss_val: 2.8326 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0044 | loss_train: 0.0002 loss_val: 2.8465 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0045 | loss_train: 0.0001 loss_val: 2.8602 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0046 | loss_train: 0.0001 loss_val: 2.8742 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4773
Epoch: 0047 | loss_train: 0.0001 loss_val: 2.8881 | acc_train: 1.0000 acc_val: 0.6875 | f1_train: 1.0000 f1_val: 0.4756
Optimization Finished!
Train cost: 39.5726s
Loading 13th epoch
Test set results: loss= 1.1536 accuracy= 0.5686

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='hidden_dim', log_path='log/nagphormer/wisconsin/hidden_dim', method='nagphormer', path='data/nagphormer', plot_path='plots/nagphormer/wisconsin/hidden_dim')

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=32, log_path='log/graphsage/cora/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9412, Val loss: 1.9422 | Train F1: 0.33, Val F1: 0.23
Epoch: 001 | Train loss: 1.9368, Val loss: 1.9404 | Train F1: 0.39, Val F1: 0.26
Epoch: 002 | Train loss: 1.9320, Val loss: 1.9385 | Train F1: 0.49, Val F1: 0.28
Epoch: 003 | Train loss: 1.9267, Val loss: 1.9364 | Train F1: 0.55, Val F1: 0.30
Epoch: 004 | Train loss: 1.9207, Val loss: 1.9341 | Train F1: 0.64, Val F1: 0.31
Epoch: 005 | Train loss: 1.9139, Val loss: 1.9315 | Train F1: 0.71, Val F1: 0.33
Epoch: 006 | Train loss: 1.9062, Val loss: 1.9285 | Train F1: 0.82, Val F1: 0.40
Epoch: 007 | Train loss: 1.8975, Val loss: 1.9251 | Train F1: 0.84, Val F1: 0.45
Epoch: 008 | Train loss: 1.8875, Val loss: 1.9213 | Train F1: 0.85, Val F1: 0.50
Epoch: 009 | Train loss: 1.8762, Val loss: 1.9170 | Train F1: 0.88, Val F1: 0.52
Epoch: 010 | Train loss: 1.8634, Val loss: 1.9123 | Train F1: 0.90, Val F1: 0.54
Epoch: 011 | Train loss: 1.8489, Val loss: 1.9069 | Train F1: 0.92, Val F1: 0.56
Epoch: 012 | Train loss: 1.8325, Val loss: 1.9008 | Train F1: 0.92, Val F1: 0.58
Epoch: 013 | Train loss: 1.8142, Val loss: 1.8940 | Train F1: 0.93, Val F1: 0.58
Epoch: 014 | Train loss: 1.7937, Val loss: 1.8863 | Train F1: 0.92, Val F1: 0.59
Epoch: 015 | Train loss: 1.7713, Val loss: 1.8777 | Train F1: 0.94, Val F1: 0.61
Epoch: 016 | Train loss: 1.7469, Val loss: 1.8681 | Train F1: 0.94, Val F1: 0.61
Epoch: 017 | Train loss: 1.7210, Val loss: 1.8574 | Train F1: 0.94, Val F1: 0.62
Epoch: 018 | Train loss: 1.6938, Val loss: 1.8456 | Train F1: 0.94, Val F1: 0.62
Epoch: 019 | Train loss: 1.6657, Val loss: 1.8326 | Train F1: 0.94, Val F1: 0.62
Epoch: 020 | Train loss: 1.6370, Val loss: 1.8183 | Train F1: 0.95, Val F1: 0.63
Epoch: 021 | Train loss: 1.6080, Val loss: 1.8028 | Train F1: 0.95, Val F1: 0.62
Epoch: 022 | Train loss: 1.5791, Val loss: 1.7862 | Train F1: 0.95, Val F1: 0.63
Epoch: 023 | Train loss: 1.5504, Val loss: 1.7684 | Train F1: 0.95, Val F1: 0.63
Epoch: 024 | Train loss: 1.5222, Val loss: 1.7499 | Train F1: 0.95, Val F1: 0.64
Epoch: 025 | Train loss: 1.4946, Val loss: 1.7305 | Train F1: 0.96, Val F1: 0.66
Epoch: 026 | Train loss: 1.4679, Val loss: 1.7108 | Train F1: 0.96, Val F1: 0.67
Epoch: 027 | Train loss: 1.4422, Val loss: 1.6909 | Train F1: 0.98, Val F1: 0.68
Epoch: 028 | Train loss: 1.4175, Val loss: 1.6713 | Train F1: 0.99, Val F1: 0.69
Epoch: 029 | Train loss: 1.3938, Val loss: 1.6523 | Train F1: 0.99, Val F1: 0.70
Epoch: 030 | Train loss: 1.3712, Val loss: 1.6341 | Train F1: 0.99, Val F1: 0.71
Epoch: 031 | Train loss: 1.3496, Val loss: 1.6172 | Train F1: 0.99, Val F1: 0.73
Epoch: 032 | Train loss: 1.3291, Val loss: 1.6016 | Train F1: 0.99, Val F1: 0.74
Epoch: 033 | Train loss: 1.3096, Val loss: 1.5874 | Train F1: 0.99, Val F1: 0.75
Epoch: 034 | Train loss: 1.2912, Val loss: 1.5744 | Train F1: 0.99, Val F1: 0.77
Epoch: 035 | Train loss: 1.2743, Val loss: 1.5628 | Train F1: 0.99, Val F1: 0.77
Epoch: 036 | Train loss: 1.2589, Val loss: 1.5524 | Train F1: 0.99, Val F1: 0.77
Epoch: 037 | Train loss: 1.2453, Val loss: 1.5431 | Train F1: 0.99, Val F1: 0.76
Epoch: 038 | Train loss: 1.2334, Val loss: 1.5349 | Train F1: 0.99, Val F1: 0.76
Epoch: 039 | Train loss: 1.2230, Val loss: 1.5273 | Train F1: 0.99, Val F1: 0.76
Epoch: 040 | Train loss: 1.2141, Val loss: 1.5203 | Train F1: 0.99, Val F1: 0.76
Epoch: 041 | Train loss: 1.2065, Val loss: 1.5137 | Train F1: 0.99, Val F1: 0.76
Epoch: 042 | Train loss: 1.2001, Val loss: 1.5071 | Train F1: 0.99, Val F1: 0.76
Epoch: 043 | Train loss: 1.1947, Val loss: 1.5009 | Train F1: 0.99, Val F1: 0.75
Epoch: 044 | Train loss: 1.1901, Val loss: 1.4949 | Train F1: 0.99, Val F1: 0.75
Epoch: 045 | Train loss: 1.1864, Val loss: 1.4891 | Train F1: 0.99, Val F1: 0.75
Epoch: 046 | Train loss: 1.1834, Val loss: 1.4838 | Train F1: 0.99, Val F1: 0.75
Epoch: 047 | Train loss: 1.1810, Val loss: 1.4788 | Train F1: 0.99, Val F1: 0.75
Epoch: 048 | Train loss: 1.1790, Val loss: 1.4742 | Train F1: 0.99, Val F1: 0.75
Epoch: 049 | Train loss: 1.1773, Val loss: 1.4699 | Train F1: 0.99, Val F1: 0.75
Epoch: 050 | Train loss: 1.1758, Val loss: 1.4660 | Train F1: 0.99, Val F1: 0.75
Epoch: 051 | Train loss: 1.1746, Val loss: 1.4623 | Train F1: 0.99, Val F1: 0.75
Epoch: 052 | Train loss: 1.1734, Val loss: 1.4589 | Train F1: 0.99, Val F1: 0.75
Epoch: 053 | Train loss: 1.1724, Val loss: 1.4558 | Train F1: 1.00, Val F1: 0.75
Epoch: 054 | Train loss: 1.1714, Val loss: 1.4531 | Train F1: 1.00, Val F1: 0.75
Epoch: 055 | Train loss: 1.1705, Val loss: 1.4508 | Train F1: 1.00, Val F1: 0.75
Epoch: 056 | Train loss: 1.1698, Val loss: 1.4487 | Train F1: 1.00, Val F1: 0.75
Epoch: 057 | Train loss: 1.1692, Val loss: 1.4467 | Train F1: 1.00, Val F1: 0.75
Epoch: 058 | Train loss: 1.1687, Val loss: 1.4451 | Train F1: 1.00, Val F1: 0.75
Epoch: 059 | Train loss: 1.1684, Val loss: 1.4436 | Train F1: 1.00, Val F1: 0.75
Epoch: 060 | Train loss: 1.1681, Val loss: 1.4423 | Train F1: 1.00, Val F1: 0.75
Epoch: 061 | Train loss: 1.1678, Val loss: 1.4411 | Train F1: 1.00, Val F1: 0.75
Epoch: 062 | Train loss: 1.1676, Val loss: 1.4400 | Train F1: 1.00, Val F1: 0.75
Epoch: 063 | Train loss: 1.1674, Val loss: 1.4391 | Train F1: 1.00, Val F1: 0.75
Epoch: 064 | Train loss: 1.1673, Val loss: 1.4382 | Train F1: 1.00, Val F1: 0.75
Epoch: 065 | Train loss: 1.1671, Val loss: 1.4375 | Train F1: 1.00, Val F1: 0.75
Epoch: 066 | Train loss: 1.1670, Val loss: 1.4368 | Train F1: 1.00, Val F1: 0.75
Epoch: 067 | Train loss: 1.1668, Val loss: 1.4362 | Train F1: 1.00, Val F1: 0.75
Epoch: 068 | Train loss: 1.1667, Val loss: 1.4356 | Train F1: 1.00, Val F1: 0.75
Epoch: 069 | Train loss: 1.1666, Val loss: 1.4351 | Train F1: 1.00, Val F1: 0.75
Epoch: 070 | Train loss: 1.1665, Val loss: 1.4346 | Train F1: 1.00, Val F1: 0.75
Epoch: 071 | Train loss: 1.1665, Val loss: 1.4342 | Train F1: 1.00, Val F1: 0.75
Epoch: 072 | Train loss: 1.1664, Val loss: 1.4338 | Train F1: 1.00, Val F1: 0.75
Epoch: 073 | Train loss: 1.1663, Val loss: 1.4334 | Train F1: 1.00, Val F1: 0.74
Epoch: 074 | Train loss: 1.1663, Val loss: 1.4331 | Train F1: 1.00, Val F1: 0.75
Epoch: 075 | Train loss: 1.1662, Val loss: 1.4328 | Train F1: 1.00, Val F1: 0.75
Epoch: 076 | Train loss: 1.1662, Val loss: 1.4326 | Train F1: 1.00, Val F1: 0.75
Epoch: 077 | Train loss: 1.1661, Val loss: 1.4325 | Train F1: 1.00, Val F1: 0.75
Epoch: 078 | Train loss: 1.1661, Val loss: 1.4323 | Train F1: 1.00, Val F1: 0.75
Epoch: 079 | Train loss: 1.1661, Val loss: 1.4322 | Train F1: 1.00, Val F1: 0.75
Epoch: 080 | Train loss: 1.1661, Val loss: 1.4321 | Train F1: 1.00, Val F1: 0.75
Epoch: 081 | Train loss: 1.1660, Val loss: 1.4320 | Train F1: 1.00, Val F1: 0.75
Epoch: 082 | Train loss: 1.1660, Val loss: 1.4320 | Train F1: 1.00, Val F1: 0.74
Epoch: 083 | Train loss: 1.1660, Val loss: 1.4319 | Train F1: 1.00, Val F1: 0.75
Epoch: 084 | Train loss: 1.1660, Val loss: 1.4319 | Train F1: 1.00, Val F1: 0.74
Epoch: 085 | Train loss: 1.1660, Val loss: 1.4319 | Train F1: 1.00, Val F1: 0.74
Epoch: 086 | Train loss: 1.1659, Val loss: 1.4318 | Train F1: 1.00, Val F1: 0.74
Epoch: 087 | Train loss: 1.1659, Val loss: 1.4318 | Train F1: 1.00, Val F1: 0.74
Epoch: 088 | Train loss: 1.1659, Val loss: 1.4317 | Train F1: 1.00, Val F1: 0.74
Epoch: 089 | Train loss: 1.1659, Val loss: 1.4316 | Train F1: 1.00, Val F1: 0.74
Epoch: 090 | Train loss: 1.1659, Val loss: 1.4315 | Train F1: 1.00, Val F1: 0.74
Epoch: 091 | Train loss: 1.1659, Val loss: 1.4314 | Train F1: 1.00, Val F1: 0.74
Epoch: 092 | Train loss: 1.1659, Val loss: 1.4313 | Train F1: 1.00, Val F1: 0.74
Epoch: 093 | Train loss: 1.1659, Val loss: 1.4312 | Train F1: 1.00, Val F1: 0.74
Epoch: 094 | Train loss: 1.1659, Val loss: 1.4312 | Train F1: 1.00, Val F1: 0.74
Best model:
Train loss: 1.1673, Val loss: 1.4382, Test loss: 1.4164
Train F1: 1.00, Val F1: 0.75, Test F1: 0.77

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/cora/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9405, Val loss: 1.9392 | Train F1: 0.21, Val F1: 0.10
Epoch: 001 | Train loss: 1.9346, Val loss: 1.9371 | Train F1: 0.31, Val F1: 0.17
Epoch: 002 | Train loss: 1.9277, Val loss: 1.9345 | Train F1: 0.38, Val F1: 0.22
Epoch: 003 | Train loss: 1.9196, Val loss: 1.9313 | Train F1: 0.57, Val F1: 0.30
Epoch: 004 | Train loss: 1.9097, Val loss: 1.9275 | Train F1: 0.65, Val F1: 0.36
Epoch: 005 | Train loss: 1.8976, Val loss: 1.9229 | Train F1: 0.78, Val F1: 0.42
Epoch: 006 | Train loss: 1.8829, Val loss: 1.9174 | Train F1: 0.85, Val F1: 0.53
Epoch: 007 | Train loss: 1.8650, Val loss: 1.9108 | Train F1: 0.96, Val F1: 0.60
Epoch: 008 | Train loss: 1.8434, Val loss: 1.9030 | Train F1: 0.98, Val F1: 0.64
Epoch: 009 | Train loss: 1.8174, Val loss: 1.8936 | Train F1: 0.98, Val F1: 0.68
Epoch: 010 | Train loss: 1.7866, Val loss: 1.8826 | Train F1: 0.99, Val F1: 0.69
Epoch: 011 | Train loss: 1.7509, Val loss: 1.8697 | Train F1: 0.99, Val F1: 0.70
Epoch: 012 | Train loss: 1.7106, Val loss: 1.8546 | Train F1: 0.98, Val F1: 0.70
Epoch: 013 | Train loss: 1.6667, Val loss: 1.8370 | Train F1: 0.98, Val F1: 0.70
Epoch: 014 | Train loss: 1.6203, Val loss: 1.8168 | Train F1: 0.98, Val F1: 0.70
Epoch: 015 | Train loss: 1.5728, Val loss: 1.7938 | Train F1: 0.98, Val F1: 0.70
Epoch: 016 | Train loss: 1.5254, Val loss: 1.7680 | Train F1: 0.98, Val F1: 0.72
Epoch: 017 | Train loss: 1.4789, Val loss: 1.7396 | Train F1: 0.98, Val F1: 0.73
Epoch: 018 | Train loss: 1.4340, Val loss: 1.7092 | Train F1: 0.98, Val F1: 0.75
Epoch: 019 | Train loss: 1.3913, Val loss: 1.6773 | Train F1: 0.98, Val F1: 0.76
Epoch: 020 | Train loss: 1.3516, Val loss: 1.6447 | Train F1: 0.99, Val F1: 0.77
Epoch: 021 | Train loss: 1.3159, Val loss: 1.6127 | Train F1: 0.99, Val F1: 0.77
Epoch: 022 | Train loss: 1.2849, Val loss: 1.5821 | Train F1: 0.99, Val F1: 0.77
Epoch: 023 | Train loss: 1.2591, Val loss: 1.5541 | Train F1: 0.99, Val F1: 0.77
Epoch: 024 | Train loss: 1.2383, Val loss: 1.5295 | Train F1: 0.99, Val F1: 0.77
Epoch: 025 | Train loss: 1.2221, Val loss: 1.5089 | Train F1: 0.99, Val F1: 0.76
Epoch: 026 | Train loss: 1.2098, Val loss: 1.4922 | Train F1: 0.99, Val F1: 0.75
Epoch: 027 | Train loss: 1.2004, Val loss: 1.4789 | Train F1: 0.99, Val F1: 0.75
Epoch: 028 | Train loss: 1.1932, Val loss: 1.4685 | Train F1: 0.99, Val F1: 0.74
Epoch: 029 | Train loss: 1.1876, Val loss: 1.4603 | Train F1: 0.99, Val F1: 0.74
Epoch: 030 | Train loss: 1.1832, Val loss: 1.4534 | Train F1: 0.99, Val F1: 0.74
Epoch: 031 | Train loss: 1.1796, Val loss: 1.4474 | Train F1: 0.99, Val F1: 0.74
Epoch: 032 | Train loss: 1.1767, Val loss: 1.4421 | Train F1: 0.99, Val F1: 0.75
Epoch: 033 | Train loss: 1.1742, Val loss: 1.4375 | Train F1: 1.00, Val F1: 0.76
Epoch: 034 | Train loss: 1.1722, Val loss: 1.4334 | Train F1: 1.00, Val F1: 0.76
Epoch: 035 | Train loss: 1.1705, Val loss: 1.4300 | Train F1: 1.00, Val F1: 0.77
Epoch: 036 | Train loss: 1.1693, Val loss: 1.4270 | Train F1: 1.00, Val F1: 0.76
Epoch: 037 | Train loss: 1.1683, Val loss: 1.4246 | Train F1: 1.00, Val F1: 0.76
Epoch: 038 | Train loss: 1.1677, Val loss: 1.4227 | Train F1: 1.00, Val F1: 0.76
Epoch: 039 | Train loss: 1.1672, Val loss: 1.4210 | Train F1: 1.00, Val F1: 0.76
Epoch: 040 | Train loss: 1.1669, Val loss: 1.4197 | Train F1: 1.00, Val F1: 0.76
Epoch: 041 | Train loss: 1.1667, Val loss: 1.4186 | Train F1: 1.00, Val F1: 0.76
Epoch: 042 | Train loss: 1.1665, Val loss: 1.4176 | Train F1: 1.00, Val F1: 0.76
Epoch: 043 | Train loss: 1.1663, Val loss: 1.4168 | Train F1: 1.00, Val F1: 0.76
Epoch: 044 | Train loss: 1.1662, Val loss: 1.4161 | Train F1: 1.00, Val F1: 0.76
Epoch: 045 | Train loss: 1.1660, Val loss: 1.4156 | Train F1: 1.00, Val F1: 0.76
Epoch: 046 | Train loss: 1.1659, Val loss: 1.4152 | Train F1: 1.00, Val F1: 0.76
Epoch: 047 | Train loss: 1.1659, Val loss: 1.4149 | Train F1: 1.00, Val F1: 0.76
Epoch: 048 | Train loss: 1.1658, Val loss: 1.4146 | Train F1: 1.00, Val F1: 0.76
Epoch: 049 | Train loss: 1.1657, Val loss: 1.4145 | Train F1: 1.00, Val F1: 0.76
Epoch: 050 | Train loss: 1.1657, Val loss: 1.4143 | Train F1: 1.00, Val F1: 0.76
Epoch: 051 | Train loss: 1.1657, Val loss: 1.4142 | Train F1: 1.00, Val F1: 0.76
Epoch: 052 | Train loss: 1.1656, Val loss: 1.4141 | Train F1: 1.00, Val F1: 0.76
Epoch: 053 | Train loss: 1.1656, Val loss: 1.4140 | Train F1: 1.00, Val F1: 0.76
Epoch: 054 | Train loss: 1.1656, Val loss: 1.4140 | Train F1: 1.00, Val F1: 0.76
Epoch: 055 | Train loss: 1.1656, Val loss: 1.4139 | Train F1: 1.00, Val F1: 0.76
Epoch: 056 | Train loss: 1.1656, Val loss: 1.4139 | Train F1: 1.00, Val F1: 0.76
Epoch: 057 | Train loss: 1.1655, Val loss: 1.4139 | Train F1: 1.00, Val F1: 0.75
Epoch: 058 | Train loss: 1.1655, Val loss: 1.4138 | Train F1: 1.00, Val F1: 0.75
Epoch: 059 | Train loss: 1.1655, Val loss: 1.4138 | Train F1: 1.00, Val F1: 0.75
Epoch: 060 | Train loss: 1.1655, Val loss: 1.4138 | Train F1: 1.00, Val F1: 0.75
Epoch: 061 | Train loss: 1.1655, Val loss: 1.4137 | Train F1: 1.00, Val F1: 0.75
Epoch: 062 | Train loss: 1.1655, Val loss: 1.4136 | Train F1: 1.00, Val F1: 0.75
Epoch: 063 | Train loss: 1.1655, Val loss: 1.4136 | Train F1: 1.00, Val F1: 0.75
Epoch: 064 | Train loss: 1.1655, Val loss: 1.4136 | Train F1: 1.00, Val F1: 0.75
Epoch: 065 | Train loss: 1.1655, Val loss: 1.4135 | Train F1: 1.00, Val F1: 0.75
Epoch: 066 | Train loss: 1.1655, Val loss: 1.4135 | Train F1: 1.00, Val F1: 0.75
Epoch: 067 | Train loss: 1.1655, Val loss: 1.4134 | Train F1: 1.00, Val F1: 0.75
Epoch: 068 | Train loss: 1.1655, Val loss: 1.4134 | Train F1: 1.00, Val F1: 0.75
Epoch: 069 | Train loss: 1.1655, Val loss: 1.4133 | Train F1: 1.00, Val F1: 0.75
Epoch: 070 | Train loss: 1.1655, Val loss: 1.4132 | Train F1: 1.00, Val F1: 0.75
Best model:
Train loss: 1.1669, Val loss: 1.4197, Test loss: 1.3990
Train F1: 1.00, Val F1: 0.76, Test F1: 0.79

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=128, log_path='log/graphsage/cora/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9358, Val loss: 1.9383 | Train F1: 0.37, Val F1: 0.17
Epoch: 001 | Train loss: 1.9246, Val loss: 1.9335 | Train F1: 0.71, Val F1: 0.39
Epoch: 002 | Train loss: 1.9099, Val loss: 1.9272 | Train F1: 0.92, Val F1: 0.63
Epoch: 003 | Train loss: 1.8899, Val loss: 1.9189 | Train F1: 0.98, Val F1: 0.70
Epoch: 004 | Train loss: 1.8627, Val loss: 1.9080 | Train F1: 0.99, Val F1: 0.70
Epoch: 005 | Train loss: 1.8260, Val loss: 1.8934 | Train F1: 0.99, Val F1: 0.73
Epoch: 006 | Train loss: 1.7780, Val loss: 1.8744 | Train F1: 0.99, Val F1: 0.75
Epoch: 007 | Train loss: 1.7177, Val loss: 1.8496 | Train F1: 0.99, Val F1: 0.76
Epoch: 008 | Train loss: 1.6467, Val loss: 1.8181 | Train F1: 0.99, Val F1: 0.76
Epoch: 009 | Train loss: 1.5689, Val loss: 1.7787 | Train F1: 0.99, Val F1: 0.76
Epoch: 010 | Train loss: 1.4902, Val loss: 1.7319 | Train F1: 0.99, Val F1: 0.77
Epoch: 011 | Train loss: 1.4162, Val loss: 1.6804 | Train F1: 0.99, Val F1: 0.77
Epoch: 012 | Train loss: 1.3517, Val loss: 1.6283 | Train F1: 0.99, Val F1: 0.77
Epoch: 013 | Train loss: 1.2990, Val loss: 1.5801 | Train F1: 0.99, Val F1: 0.78
Epoch: 014 | Train loss: 1.2584, Val loss: 1.5385 | Train F1: 0.99, Val F1: 0.78
Epoch: 015 | Train loss: 1.2288, Val loss: 1.5048 | Train F1: 0.99, Val F1: 0.78
Epoch: 016 | Train loss: 1.2083, Val loss: 1.4791 | Train F1: 0.99, Val F1: 0.78
Epoch: 017 | Train loss: 1.1945, Val loss: 1.4597 | Train F1: 0.99, Val F1: 0.78
Epoch: 018 | Train loss: 1.1852, Val loss: 1.4453 | Train F1: 0.99, Val F1: 0.78
Epoch: 019 | Train loss: 1.1790, Val loss: 1.4342 | Train F1: 0.99, Val F1: 0.77
Epoch: 020 | Train loss: 1.1748, Val loss: 1.4253 | Train F1: 0.99, Val F1: 0.78
Epoch: 021 | Train loss: 1.1718, Val loss: 1.4187 | Train F1: 1.00, Val F1: 0.77
Epoch: 022 | Train loss: 1.1694, Val loss: 1.4139 | Train F1: 1.00, Val F1: 0.77
Epoch: 023 | Train loss: 1.1677, Val loss: 1.4105 | Train F1: 1.00, Val F1: 0.77
Epoch: 024 | Train loss: 1.1667, Val loss: 1.4084 | Train F1: 1.00, Val F1: 0.77
Epoch: 025 | Train loss: 1.1662, Val loss: 1.4071 | Train F1: 1.00, Val F1: 0.77
Epoch: 026 | Train loss: 1.1660, Val loss: 1.4064 | Train F1: 1.00, Val F1: 0.77
Epoch: 027 | Train loss: 1.1659, Val loss: 1.4062 | Train F1: 1.00, Val F1: 0.76
Epoch: 028 | Train loss: 1.1658, Val loss: 1.4060 | Train F1: 1.00, Val F1: 0.76
Epoch: 029 | Train loss: 1.1657, Val loss: 1.4059 | Train F1: 1.00, Val F1: 0.76
Epoch: 030 | Train loss: 1.1656, Val loss: 1.4059 | Train F1: 1.00, Val F1: 0.76
Epoch: 031 | Train loss: 1.1656, Val loss: 1.4059 | Train F1: 1.00, Val F1: 0.76
Epoch: 032 | Train loss: 1.1655, Val loss: 1.4060 | Train F1: 1.00, Val F1: 0.76
Epoch: 033 | Train loss: 1.1655, Val loss: 1.4060 | Train F1: 1.00, Val F1: 0.76
Epoch: 034 | Train loss: 1.1655, Val loss: 1.4061 | Train F1: 1.00, Val F1: 0.76
Epoch: 035 | Train loss: 1.1655, Val loss: 1.4061 | Train F1: 1.00, Val F1: 0.76
Epoch: 036 | Train loss: 1.1655, Val loss: 1.4062 | Train F1: 1.00, Val F1: 0.76
Epoch: 037 | Train loss: 1.1655, Val loss: 1.4062 | Train F1: 1.00, Val F1: 0.76
Epoch: 038 | Train loss: 1.1654, Val loss: 1.4063 | Train F1: 1.00, Val F1: 0.76
Epoch: 039 | Train loss: 1.1654, Val loss: 1.4063 | Train F1: 1.00, Val F1: 0.76
Epoch: 040 | Train loss: 1.1654, Val loss: 1.4064 | Train F1: 1.00, Val F1: 0.75
Epoch: 041 | Train loss: 1.1654, Val loss: 1.4064 | Train F1: 1.00, Val F1: 0.75
Epoch: 042 | Train loss: 1.1654, Val loss: 1.4065 | Train F1: 1.00, Val F1: 0.75
Epoch: 043 | Train loss: 1.1654, Val loss: 1.4065 | Train F1: 1.00, Val F1: 0.75
Epoch: 044 | Train loss: 1.1654, Val loss: 1.4065 | Train F1: 1.00, Val F1: 0.75
Epoch: 045 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 046 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 047 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 048 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 049 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 050 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 051 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 052 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 053 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Epoch: 054 | Train loss: 1.1654, Val loss: 1.4066 | Train F1: 1.00, Val F1: 0.75
Best model:
Train loss: 1.1667, Val loss: 1.4084, Test loss: 1.3950
Train F1: 1.00, Val F1: 0.77, Test F1: 0.78

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=256, log_path='log/graphsage/cora/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9303, Val loss: 1.9387 | Train F1: 0.70, Val F1: 0.44
Epoch: 001 | Train loss: 1.9082, Val loss: 1.9300 | Train F1: 0.94, Val F1: 0.61
Epoch: 002 | Train loss: 1.8722, Val loss: 1.9164 | Train F1: 0.98, Val F1: 0.67
Epoch: 003 | Train loss: 1.8135, Val loss: 1.8951 | Train F1: 0.99, Val F1: 0.67
Epoch: 004 | Train loss: 1.7287, Val loss: 1.8621 | Train F1: 0.97, Val F1: 0.68
Epoch: 005 | Train loss: 1.6246, Val loss: 1.8124 | Train F1: 0.96, Val F1: 0.67
Epoch: 006 | Train loss: 1.5134, Val loss: 1.7441 | Train F1: 0.96, Val F1: 0.69
Epoch: 007 | Train loss: 1.4120, Val loss: 1.6653 | Train F1: 0.96, Val F1: 0.72
Epoch: 008 | Train loss: 1.3303, Val loss: 1.5890 | Train F1: 0.96, Val F1: 0.75
Epoch: 009 | Train loss: 1.2686, Val loss: 1.5250 | Train F1: 0.99, Val F1: 0.78
Epoch: 010 | Train loss: 1.2243, Val loss: 1.4792 | Train F1: 0.99, Val F1: 0.79
Epoch: 011 | Train loss: 1.1968, Val loss: 1.4511 | Train F1: 0.99, Val F1: 0.78
Epoch: 012 | Train loss: 1.1834, Val loss: 1.4346 | Train F1: 0.99, Val F1: 0.77
Epoch: 013 | Train loss: 1.1769, Val loss: 1.4235 | Train F1: 0.99, Val F1: 0.76
Epoch: 014 | Train loss: 1.1730, Val loss: 1.4145 | Train F1: 0.99, Val F1: 0.76
Epoch: 015 | Train loss: 1.1699, Val loss: 1.4066 | Train F1: 0.99, Val F1: 0.76
Epoch: 016 | Train loss: 1.1670, Val loss: 1.4008 | Train F1: 1.00, Val F1: 0.77
Epoch: 017 | Train loss: 1.1658, Val loss: 1.3970 | Train F1: 1.00, Val F1: 0.76
Epoch: 018 | Train loss: 1.1656, Val loss: 1.3946 | Train F1: 1.00, Val F1: 0.76
Epoch: 019 | Train loss: 1.1656, Val loss: 1.3934 | Train F1: 1.00, Val F1: 0.77
Epoch: 020 | Train loss: 1.1656, Val loss: 1.3930 | Train F1: 1.00, Val F1: 0.76
Epoch: 021 | Train loss: 1.1656, Val loss: 1.3931 | Train F1: 1.00, Val F1: 0.76
Epoch: 022 | Train loss: 1.1656, Val loss: 1.3934 | Train F1: 1.00, Val F1: 0.76
Epoch: 023 | Train loss: 1.1656, Val loss: 1.3935 | Train F1: 1.00, Val F1: 0.76
Epoch: 024 | Train loss: 1.1655, Val loss: 1.3936 | Train F1: 1.00, Val F1: 0.76
Epoch: 025 | Train loss: 1.1655, Val loss: 1.3937 | Train F1: 1.00, Val F1: 0.76
Epoch: 026 | Train loss: 1.1655, Val loss: 1.3937 | Train F1: 1.00, Val F1: 0.76
Epoch: 027 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 028 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 029 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 030 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 031 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 032 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 033 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 034 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 035 | Train loss: 1.1654, Val loss: 1.3938 | Train F1: 1.00, Val F1: 0.76
Epoch: 036 | Train loss: 1.1654, Val loss: 1.3937 | Train F1: 1.00, Val F1: 0.76
Epoch: 037 | Train loss: 1.1654, Val loss: 1.3937 | Train F1: 1.00, Val F1: 0.76
Epoch: 038 | Train loss: 1.1654, Val loss: 1.3937 | Train F1: 1.00, Val F1: 0.76
Epoch: 039 | Train loss: 1.1654, Val loss: 1.3937 | Train F1: 1.00, Val F1: 0.76
Epoch: 040 | Train loss: 1.1654, Val loss: 1.3937 | Train F1: 1.00, Val F1: 0.76
Epoch: 041 | Train loss: 1.1654, Val loss: 1.3936 | Train F1: 1.00, Val F1: 0.76
Epoch: 042 | Train loss: 1.1654, Val loss: 1.3936 | Train F1: 1.00, Val F1: 0.76
Epoch: 043 | Train loss: 1.1654, Val loss: 1.3935 | Train F1: 1.00, Val F1: 0.76
Epoch: 044 | Train loss: 1.1654, Val loss: 1.3934 | Train F1: 1.00, Val F1: 0.76
Epoch: 045 | Train loss: 1.1654, Val loss: 1.3934 | Train F1: 1.00, Val F1: 0.76
Epoch: 046 | Train loss: 1.1654, Val loss: 1.3933 | Train F1: 1.00, Val F1: 0.76
Epoch: 047 | Train loss: 1.1654, Val loss: 1.3933 | Train F1: 1.00, Val F1: 0.76
Epoch: 048 | Train loss: 1.1654, Val loss: 1.3932 | Train F1: 1.00, Val F1: 0.76
Best model:
Train loss: 1.1656, Val loss: 1.3946, Test loss: 1.3723
Train F1: 1.00, Val F1: 0.76, Test F1: 0.80

>>> train.py: Namespace(dataset='cora', device=1, dropout=0.1, hidden_dim=512, log_path='log/graphsage/cora/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.9106, Val loss: 1.9311 | Train F1: 0.91, Val F1: 0.58
Epoch: 001 | Train loss: 1.8452, Val loss: 1.9052 | Train F1: 0.98, Val F1: 0.68
Epoch: 002 | Train loss: 1.7147, Val loss: 1.8534 | Train F1: 0.97, Val F1: 0.69
Epoch: 003 | Train loss: 1.5382, Val loss: 1.7636 | Train F1: 0.95, Val F1: 0.69
Epoch: 004 | Train loss: 1.3815, Val loss: 1.6451 | Train F1: 0.97, Val F1: 0.72
Epoch: 005 | Train loss: 1.2741, Val loss: 1.5320 | Train F1: 0.97, Val F1: 0.78
Epoch: 006 | Train loss: 1.2091, Val loss: 1.4531 | Train F1: 0.99, Val F1: 0.79
Epoch: 007 | Train loss: 1.1814, Val loss: 1.4203 | Train F1: 0.99, Val F1: 0.77
Epoch: 008 | Train loss: 1.1747, Val loss: 1.4127 | Train F1: 0.99, Val F1: 0.76
Epoch: 009 | Train loss: 1.1689, Val loss: 1.4036 | Train F1: 1.00, Val F1: 0.76
Epoch: 010 | Train loss: 1.1658, Val loss: 1.3967 | Train F1: 1.00, Val F1: 0.77
Epoch: 011 | Train loss: 1.1656, Val loss: 1.3929 | Train F1: 1.00, Val F1: 0.77
Epoch: 012 | Train loss: 1.1660, Val loss: 1.3904 | Train F1: 1.00, Val F1: 0.78
Epoch: 013 | Train loss: 1.1660, Val loss: 1.3895 | Train F1: 1.00, Val F1: 0.78
Epoch: 014 | Train loss: 1.1656, Val loss: 1.3889 | Train F1: 1.00, Val F1: 0.78
Epoch: 015 | Train loss: 1.1655, Val loss: 1.3886 | Train F1: 1.00, Val F1: 0.77
Epoch: 016 | Train loss: 1.1654, Val loss: 1.3885 | Train F1: 1.00, Val F1: 0.78
Epoch: 017 | Train loss: 1.1654, Val loss: 1.3886 | Train F1: 1.00, Val F1: 0.78
Epoch: 018 | Train loss: 1.1654, Val loss: 1.3889 | Train F1: 1.00, Val F1: 0.78
Epoch: 019 | Train loss: 1.1654, Val loss: 1.3893 | Train F1: 1.00, Val F1: 0.78
Epoch: 020 | Train loss: 1.1654, Val loss: 1.3896 | Train F1: 1.00, Val F1: 0.77
Epoch: 021 | Train loss: 1.1654, Val loss: 1.3900 | Train F1: 1.00, Val F1: 0.77
Epoch: 022 | Train loss: 1.1654, Val loss: 1.3903 | Train F1: 1.00, Val F1: 0.77
Epoch: 023 | Train loss: 1.1654, Val loss: 1.3904 | Train F1: 1.00, Val F1: 0.77
Epoch: 024 | Train loss: 1.1654, Val loss: 1.3904 | Train F1: 1.00, Val F1: 0.77
Epoch: 025 | Train loss: 1.1654, Val loss: 1.3904 | Train F1: 1.00, Val F1: 0.78
Epoch: 026 | Train loss: 1.1654, Val loss: 1.3904 | Train F1: 1.00, Val F1: 0.77
Epoch: 027 | Train loss: 1.1654, Val loss: 1.3904 | Train F1: 1.00, Val F1: 0.77
Epoch: 028 | Train loss: 1.1654, Val loss: 1.3904 | Train F1: 1.00, Val F1: 0.77
Epoch: 029 | Train loss: 1.1654, Val loss: 1.3903 | Train F1: 1.00, Val F1: 0.77
Epoch: 030 | Train loss: 1.1654, Val loss: 1.3903 | Train F1: 1.00, Val F1: 0.77
Epoch: 031 | Train loss: 1.1654, Val loss: 1.3903 | Train F1: 1.00, Val F1: 0.77
Epoch: 032 | Train loss: 1.1654, Val loss: 1.3902 | Train F1: 1.00, Val F1: 0.77
Epoch: 033 | Train loss: 1.1654, Val loss: 1.3902 | Train F1: 1.00, Val F1: 0.77
Epoch: 034 | Train loss: 1.1654, Val loss: 1.3902 | Train F1: 1.00, Val F1: 0.77
Epoch: 035 | Train loss: 1.1654, Val loss: 1.3901 | Train F1: 1.00, Val F1: 0.77
Epoch: 036 | Train loss: 1.1654, Val loss: 1.3901 | Train F1: 1.00, Val F1: 0.77
Epoch: 037 | Train loss: 1.1654, Val loss: 1.3900 | Train F1: 1.00, Val F1: 0.77
Epoch: 038 | Train loss: 1.1654, Val loss: 1.3900 | Train F1: 1.00, Val F1: 0.77
Epoch: 039 | Train loss: 1.1654, Val loss: 1.3899 | Train F1: 1.00, Val F1: 0.77
Epoch: 040 | Train loss: 1.1654, Val loss: 1.3899 | Train F1: 1.00, Val F1: 0.77
Epoch: 041 | Train loss: 1.1654, Val loss: 1.3898 | Train F1: 1.00, Val F1: 0.77
Best model:
Train loss: 1.1656, Val loss: 1.3929, Test loss: 1.3710
Train F1: 1.00, Val F1: 0.77, Test F1: 0.79

>>> run.py: Namespace(dataset='cora', device=1, experiment='hidden_dim', log_path='log/graphsage/cora/hidden_dim', method='graphsage', path='data/graphsage', plot_path='plots/graphsage/cora/hidden_dim')

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=32, log_path='log/graphsage/wisconsin/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5880, Val loss: 1.5954 | Train F1: 0.27, Val F1: 0.17
Epoch: 001 | Train loss: 1.5594, Val loss: 1.5738 | Train F1: 0.28, Val F1: 0.22
Epoch: 002 | Train loss: 1.5278, Val loss: 1.5491 | Train F1: 0.26, Val F1: 0.19
Epoch: 003 | Train loss: 1.4953, Val loss: 1.5228 | Train F1: 0.19, Val F1: 0.18
Epoch: 004 | Train loss: 1.4650, Val loss: 1.4962 | Train F1: 0.18, Val F1: 0.17
Epoch: 005 | Train loss: 1.4391, Val loss: 1.4711 | Train F1: 0.19, Val F1: 0.19
Epoch: 006 | Train loss: 1.4170, Val loss: 1.4489 | Train F1: 0.22, Val F1: 0.21
Epoch: 007 | Train loss: 1.3960, Val loss: 1.4298 | Train F1: 0.24, Val F1: 0.23
Epoch: 008 | Train loss: 1.3735, Val loss: 1.4125 | Train F1: 0.28, Val F1: 0.24
Epoch: 009 | Train loss: 1.3497, Val loss: 1.3960 | Train F1: 0.29, Val F1: 0.28
Epoch: 010 | Train loss: 1.3278, Val loss: 1.3811 | Train F1: 0.31, Val F1: 0.30
Epoch: 011 | Train loss: 1.3102, Val loss: 1.3703 | Train F1: 0.31, Val F1: 0.29
Epoch: 012 | Train loss: 1.2961, Val loss: 1.3635 | Train F1: 0.30, Val F1: 0.28
Epoch: 013 | Train loss: 1.2828, Val loss: 1.3567 | Train F1: 0.30, Val F1: 0.27
Epoch: 014 | Train loss: 1.2684, Val loss: 1.3467 | Train F1: 0.31, Val F1: 0.27
Epoch: 015 | Train loss: 1.2538, Val loss: 1.3366 | Train F1: 0.31, Val F1: 0.28
Epoch: 016 | Train loss: 1.2392, Val loss: 1.3276 | Train F1: 0.36, Val F1: 0.29
Epoch: 017 | Train loss: 1.2241, Val loss: 1.3201 | Train F1: 0.36, Val F1: 0.29
Epoch: 018 | Train loss: 1.2086, Val loss: 1.3135 | Train F1: 0.43, Val F1: 0.29
Epoch: 019 | Train loss: 1.1930, Val loss: 1.3085 | Train F1: 0.50, Val F1: 0.30
Epoch: 020 | Train loss: 1.1795, Val loss: 1.3041 | Train F1: 0.53, Val F1: 0.30
Epoch: 021 | Train loss: 1.1682, Val loss: 1.3007 | Train F1: 0.55, Val F1: 0.29
Epoch: 022 | Train loss: 1.1588, Val loss: 1.2986 | Train F1: 0.57, Val F1: 0.35
Epoch: 023 | Train loss: 1.1505, Val loss: 1.2960 | Train F1: 0.57, Val F1: 0.35
Epoch: 024 | Train loss: 1.1427, Val loss: 1.2925 | Train F1: 0.59, Val F1: 0.39
Epoch: 025 | Train loss: 1.1354, Val loss: 1.2887 | Train F1: 0.59, Val F1: 0.39
Epoch: 026 | Train loss: 1.1284, Val loss: 1.2853 | Train F1: 0.58, Val F1: 0.44
Epoch: 027 | Train loss: 1.1214, Val loss: 1.2822 | Train F1: 0.58, Val F1: 0.43
Epoch: 028 | Train loss: 1.1138, Val loss: 1.2794 | Train F1: 0.58, Val F1: 0.43
Epoch: 029 | Train loss: 1.1053, Val loss: 1.2770 | Train F1: 0.58, Val F1: 0.42
Epoch: 030 | Train loss: 1.0957, Val loss: 1.2754 | Train F1: 0.59, Val F1: 0.44
Epoch: 031 | Train loss: 1.0850, Val loss: 1.2743 | Train F1: 0.62, Val F1: 0.43
Epoch: 032 | Train loss: 1.0739, Val loss: 1.2737 | Train F1: 0.74, Val F1: 0.46
Epoch: 033 | Train loss: 1.0633, Val loss: 1.2734 | Train F1: 0.74, Val F1: 0.46
Epoch: 034 | Train loss: 1.0541, Val loss: 1.2732 | Train F1: 0.75, Val F1: 0.44
Epoch: 035 | Train loss: 1.0464, Val loss: 1.2727 | Train F1: 0.76, Val F1: 0.44
Epoch: 036 | Train loss: 1.0399, Val loss: 1.2718 | Train F1: 0.76, Val F1: 0.43
Epoch: 037 | Train loss: 1.0338, Val loss: 1.2702 | Train F1: 0.77, Val F1: 0.43
Epoch: 038 | Train loss: 1.0276, Val loss: 1.2678 | Train F1: 0.77, Val F1: 0.45
Epoch: 039 | Train loss: 1.0214, Val loss: 1.2648 | Train F1: 0.77, Val F1: 0.46
Epoch: 040 | Train loss: 1.0153, Val loss: 1.2612 | Train F1: 0.77, Val F1: 0.47
Epoch: 041 | Train loss: 1.0097, Val loss: 1.2577 | Train F1: 0.77, Val F1: 0.47
Epoch: 042 | Train loss: 1.0046, Val loss: 1.2543 | Train F1: 0.84, Val F1: 0.47
Epoch: 043 | Train loss: 1.0002, Val loss: 1.2516 | Train F1: 0.84, Val F1: 0.44
Epoch: 044 | Train loss: 0.9963, Val loss: 1.2491 | Train F1: 0.84, Val F1: 0.42
Epoch: 045 | Train loss: 0.9926, Val loss: 1.2461 | Train F1: 0.84, Val F1: 0.42
Epoch: 046 | Train loss: 0.9892, Val loss: 1.2431 | Train F1: 0.84, Val F1: 0.42
Epoch: 047 | Train loss: 0.9859, Val loss: 1.2401 | Train F1: 0.84, Val F1: 0.43
Epoch: 048 | Train loss: 0.9828, Val loss: 1.2369 | Train F1: 0.84, Val F1: 0.43
Epoch: 049 | Train loss: 0.9798, Val loss: 1.2344 | Train F1: 0.86, Val F1: 0.43
Epoch: 050 | Train loss: 0.9771, Val loss: 1.2325 | Train F1: 0.86, Val F1: 0.43
Epoch: 051 | Train loss: 0.9747, Val loss: 1.2310 | Train F1: 0.90, Val F1: 0.43
Epoch: 052 | Train loss: 0.9727, Val loss: 1.2295 | Train F1: 0.90, Val F1: 0.44
Epoch: 053 | Train loss: 0.9708, Val loss: 1.2279 | Train F1: 0.90, Val F1: 0.44
Epoch: 054 | Train loss: 0.9691, Val loss: 1.2262 | Train F1: 0.90, Val F1: 0.44
Epoch: 055 | Train loss: 0.9676, Val loss: 1.2244 | Train F1: 0.90, Val F1: 0.44
Epoch: 056 | Train loss: 0.9661, Val loss: 1.2226 | Train F1: 0.90, Val F1: 0.44
Epoch: 057 | Train loss: 0.9647, Val loss: 1.2208 | Train F1: 0.90, Val F1: 0.44
Epoch: 058 | Train loss: 0.9634, Val loss: 1.2191 | Train F1: 0.91, Val F1: 0.44
Epoch: 059 | Train loss: 0.9623, Val loss: 1.2172 | Train F1: 0.91, Val F1: 0.44
Epoch: 060 | Train loss: 0.9612, Val loss: 1.2154 | Train F1: 0.91, Val F1: 0.44
Epoch: 061 | Train loss: 0.9601, Val loss: 1.2137 | Train F1: 0.91, Val F1: 0.43
Epoch: 062 | Train loss: 0.9592, Val loss: 1.2122 | Train F1: 0.91, Val F1: 0.43
Epoch: 063 | Train loss: 0.9583, Val loss: 1.2108 | Train F1: 0.91, Val F1: 0.43
Epoch: 064 | Train loss: 0.9574, Val loss: 1.2095 | Train F1: 0.91, Val F1: 0.45
Epoch: 065 | Train loss: 0.9566, Val loss: 1.2082 | Train F1: 0.91, Val F1: 0.45
Epoch: 066 | Train loss: 0.9559, Val loss: 1.2069 | Train F1: 0.91, Val F1: 0.45
Epoch: 067 | Train loss: 0.9552, Val loss: 1.2058 | Train F1: 0.91, Val F1: 0.45
Epoch: 068 | Train loss: 0.9545, Val loss: 1.2049 | Train F1: 0.91, Val F1: 0.45
Epoch: 069 | Train loss: 0.9539, Val loss: 1.2041 | Train F1: 0.91, Val F1: 0.45
Epoch: 070 | Train loss: 0.9533, Val loss: 1.2034 | Train F1: 0.91, Val F1: 0.45
Epoch: 071 | Train loss: 0.9526, Val loss: 1.2029 | Train F1: 0.91, Val F1: 0.45
Epoch: 072 | Train loss: 0.9520, Val loss: 1.2027 | Train F1: 0.91, Val F1: 0.45
Epoch: 073 | Train loss: 0.9513, Val loss: 1.2026 | Train F1: 0.91, Val F1: 0.45
Epoch: 074 | Train loss: 0.9507, Val loss: 1.2026 | Train F1: 0.91, Val F1: 0.44
Epoch: 075 | Train loss: 0.9499, Val loss: 1.2030 | Train F1: 0.91, Val F1: 0.46
Epoch: 076 | Train loss: 0.9492, Val loss: 1.2037 | Train F1: 0.91, Val F1: 0.46
Epoch: 077 | Train loss: 0.9484, Val loss: 1.2047 | Train F1: 0.92, Val F1: 0.46
Epoch: 078 | Train loss: 0.9475, Val loss: 1.2061 | Train F1: 0.90, Val F1: 0.45
Epoch: 079 | Train loss: 0.9467, Val loss: 1.2076 | Train F1: 0.90, Val F1: 0.45
Epoch: 080 | Train loss: 0.9457, Val loss: 1.2094 | Train F1: 0.90, Val F1: 0.45
Epoch: 081 | Train loss: 0.9446, Val loss: 1.2118 | Train F1: 0.90, Val F1: 0.45
Epoch: 082 | Train loss: 0.9432, Val loss: 1.2148 | Train F1: 0.90, Val F1: 0.44
Epoch: 083 | Train loss: 0.9417, Val loss: 1.2180 | Train F1: 0.94, Val F1: 0.44
Epoch: 084 | Train loss: 0.9400, Val loss: 1.2212 | Train F1: 0.95, Val F1: 0.44
Epoch: 085 | Train loss: 0.9384, Val loss: 1.2250 | Train F1: 0.95, Val F1: 0.44
Epoch: 086 | Train loss: 0.9372, Val loss: 1.2282 | Train F1: 0.95, Val F1: 0.46
Epoch: 087 | Train loss: 0.9361, Val loss: 1.2300 | Train F1: 0.95, Val F1: 0.46
Epoch: 088 | Train loss: 0.9351, Val loss: 1.2301 | Train F1: 0.95, Val F1: 0.46
Epoch: 089 | Train loss: 0.9340, Val loss: 1.2289 | Train F1: 0.95, Val F1: 0.46
Epoch: 090 | Train loss: 0.9330, Val loss: 1.2265 | Train F1: 0.95, Val F1: 0.46
Epoch: 091 | Train loss: 0.9321, Val loss: 1.2238 | Train F1: 0.95, Val F1: 0.46
Epoch: 092 | Train loss: 0.9312, Val loss: 1.2209 | Train F1: 0.95, Val F1: 0.46
Epoch: 093 | Train loss: 0.9304, Val loss: 1.2181 | Train F1: 0.95, Val F1: 0.46
Epoch: 094 | Train loss: 0.9297, Val loss: 1.2149 | Train F1: 0.95, Val F1: 0.44
Epoch: 095 | Train loss: 0.9290, Val loss: 1.2120 | Train F1: 0.95, Val F1: 0.44
Epoch: 096 | Train loss: 0.9284, Val loss: 1.2094 | Train F1: 0.98, Val F1: 0.44
Epoch: 097 | Train loss: 0.9277, Val loss: 1.2076 | Train F1: 0.98, Val F1: 0.44
Epoch: 098 | Train loss: 0.9270, Val loss: 1.2063 | Train F1: 0.98, Val F1: 0.45
Epoch: 099 | Train loss: 0.9264, Val loss: 1.2052 | Train F1: 0.98, Val F1: 0.45
Epoch: 100 | Train loss: 0.9257, Val loss: 1.2039 | Train F1: 0.98, Val F1: 0.48
Epoch: 101 | Train loss: 0.9251, Val loss: 1.2025 | Train F1: 0.97, Val F1: 0.48
Epoch: 102 | Train loss: 0.9245, Val loss: 1.2016 | Train F1: 0.97, Val F1: 0.48
Epoch: 103 | Train loss: 0.9239, Val loss: 1.2007 | Train F1: 0.97, Val F1: 0.48
Epoch: 104 | Train loss: 0.9233, Val loss: 1.1996 | Train F1: 0.97, Val F1: 0.48
Epoch: 105 | Train loss: 0.9228, Val loss: 1.1987 | Train F1: 0.97, Val F1: 0.48
Epoch: 106 | Train loss: 0.9223, Val loss: 1.1979 | Train F1: 0.97, Val F1: 0.49
Epoch: 107 | Train loss: 0.9218, Val loss: 1.1970 | Train F1: 0.97, Val F1: 0.49
Epoch: 108 | Train loss: 0.9214, Val loss: 1.1963 | Train F1: 0.97, Val F1: 0.49
Epoch: 109 | Train loss: 0.9211, Val loss: 1.1956 | Train F1: 0.97, Val F1: 0.54
Epoch: 110 | Train loss: 0.9207, Val loss: 1.1950 | Train F1: 0.97, Val F1: 0.54
Epoch: 111 | Train loss: 0.9204, Val loss: 1.1949 | Train F1: 0.97, Val F1: 0.54
Epoch: 112 | Train loss: 0.9202, Val loss: 1.1949 | Train F1: 0.97, Val F1: 0.54
Epoch: 113 | Train loss: 0.9199, Val loss: 1.1949 | Train F1: 0.97, Val F1: 0.54
Epoch: 114 | Train loss: 0.9197, Val loss: 1.1952 | Train F1: 0.97, Val F1: 0.54
Epoch: 115 | Train loss: 0.9195, Val loss: 1.1956 | Train F1: 0.97, Val F1: 0.54
Epoch: 116 | Train loss: 0.9193, Val loss: 1.1961 | Train F1: 0.97, Val F1: 0.54
Epoch: 117 | Train loss: 0.9191, Val loss: 1.1965 | Train F1: 0.97, Val F1: 0.54
Epoch: 118 | Train loss: 0.9189, Val loss: 1.1970 | Train F1: 0.97, Val F1: 0.49
Epoch: 119 | Train loss: 0.9187, Val loss: 1.1974 | Train F1: 0.97, Val F1: 0.49
Epoch: 120 | Train loss: 0.9185, Val loss: 1.1979 | Train F1: 0.97, Val F1: 0.49
Epoch: 121 | Train loss: 0.9184, Val loss: 1.1984 | Train F1: 0.97, Val F1: 0.49
Epoch: 122 | Train loss: 0.9182, Val loss: 1.1990 | Train F1: 0.97, Val F1: 0.49
Epoch: 123 | Train loss: 0.9181, Val loss: 1.1995 | Train F1: 0.97, Val F1: 0.49
Epoch: 124 | Train loss: 0.9179, Val loss: 1.1999 | Train F1: 0.97, Val F1: 0.49
Epoch: 125 | Train loss: 0.9178, Val loss: 1.2003 | Train F1: 0.97, Val F1: 0.49
Epoch: 126 | Train loss: 0.9177, Val loss: 1.2018 | Train F1: 0.97, Val F1: 0.49
Epoch: 127 | Train loss: 0.9175, Val loss: 1.2028 | Train F1: 0.97, Val F1: 0.48
Epoch: 128 | Train loss: 0.9174, Val loss: 1.2034 | Train F1: 0.97, Val F1: 0.48
Epoch: 129 | Train loss: 0.9173, Val loss: 1.2038 | Train F1: 0.97, Val F1: 0.48
Epoch: 130 | Train loss: 0.9172, Val loss: 1.2039 | Train F1: 0.97, Val F1: 0.48
Epoch: 131 | Train loss: 0.9170, Val loss: 1.2039 | Train F1: 0.98, Val F1: 0.48
Epoch: 132 | Train loss: 0.9169, Val loss: 1.2039 | Train F1: 0.98, Val F1: 0.48
Epoch: 133 | Train loss: 0.9168, Val loss: 1.2038 | Train F1: 0.98, Val F1: 0.48
Epoch: 134 | Train loss: 0.9166, Val loss: 1.2035 | Train F1: 1.00, Val F1: 0.48
Epoch: 135 | Train loss: 0.9165, Val loss: 1.2033 | Train F1: 1.00, Val F1: 0.48
Epoch: 136 | Train loss: 0.9164, Val loss: 1.2030 | Train F1: 1.00, Val F1: 0.48
Epoch: 137 | Train loss: 0.9162, Val loss: 1.2025 | Train F1: 1.00, Val F1: 0.45
Best model:
Train loss: 0.9218, Val loss: 1.1970, Test loss: 1.2358
Train F1: 0.97, Val F1: 0.49, Test F1: 0.48

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=64, log_path='log/graphsage/wisconsin/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5716, Val loss: 1.5784 | Train F1: 0.21, Val F1: 0.16
Epoch: 001 | Train loss: 1.5293, Val loss: 1.5459 | Train F1: 0.20, Val F1: 0.16
Epoch: 002 | Train loss: 1.4822, Val loss: 1.5074 | Train F1: 0.18, Val F1: 0.16
Epoch: 003 | Train loss: 1.4395, Val loss: 1.4685 | Train F1: 0.18, Val F1: 0.16
Epoch: 004 | Train loss: 1.4059, Val loss: 1.4360 | Train F1: 0.24, Val F1: 0.17
Epoch: 005 | Train loss: 1.3721, Val loss: 1.4105 | Train F1: 0.26, Val F1: 0.21
Epoch: 006 | Train loss: 1.3333, Val loss: 1.3856 | Train F1: 0.28, Val F1: 0.26
Epoch: 007 | Train loss: 1.2959, Val loss: 1.3623 | Train F1: 0.31, Val F1: 0.28
Epoch: 008 | Train loss: 1.2675, Val loss: 1.3513 | Train F1: 0.31, Val F1: 0.29
Epoch: 009 | Train loss: 1.2481, Val loss: 1.3531 | Train F1: 0.36, Val F1: 0.29
Epoch: 010 | Train loss: 1.2294, Val loss: 1.3510 | Train F1: 0.41, Val F1: 0.27
Epoch: 011 | Train loss: 1.2060, Val loss: 1.3417 | Train F1: 0.48, Val F1: 0.28
Epoch: 012 | Train loss: 1.1831, Val loss: 1.3302 | Train F1: 0.51, Val F1: 0.28
Epoch: 013 | Train loss: 1.1644, Val loss: 1.3199 | Train F1: 0.55, Val F1: 0.29
Epoch: 014 | Train loss: 1.1492, Val loss: 1.3118 | Train F1: 0.58, Val F1: 0.28
Epoch: 015 | Train loss: 1.1362, Val loss: 1.3063 | Train F1: 0.57, Val F1: 0.38
Epoch: 016 | Train loss: 1.1246, Val loss: 1.3023 | Train F1: 0.59, Val F1: 0.39
Epoch: 017 | Train loss: 1.1138, Val loss: 1.2996 | Train F1: 0.60, Val F1: 0.44
Epoch: 018 | Train loss: 1.1029, Val loss: 1.2964 | Train F1: 0.60, Val F1: 0.43
Epoch: 019 | Train loss: 1.0912, Val loss: 1.2944 | Train F1: 0.60, Val F1: 0.40
Epoch: 020 | Train loss: 1.0774, Val loss: 1.2935 | Train F1: 0.61, Val F1: 0.38
Epoch: 021 | Train loss: 1.0622, Val loss: 1.2932 | Train F1: 0.65, Val F1: 0.40
Epoch: 022 | Train loss: 1.0488, Val loss: 1.2900 | Train F1: 0.65, Val F1: 0.42
Epoch: 023 | Train loss: 1.0389, Val loss: 1.2847 | Train F1: 0.67, Val F1: 0.42
Epoch: 024 | Train loss: 1.0306, Val loss: 1.2810 | Train F1: 0.68, Val F1: 0.43
Epoch: 025 | Train loss: 1.0220, Val loss: 1.2777 | Train F1: 0.68, Val F1: 0.42
Epoch: 026 | Train loss: 1.0139, Val loss: 1.2737 | Train F1: 0.68, Val F1: 0.43
Epoch: 027 | Train loss: 1.0062, Val loss: 1.2693 | Train F1: 0.69, Val F1: 0.46
Epoch: 028 | Train loss: 0.9991, Val loss: 1.2655 | Train F1: 0.78, Val F1: 0.45
Epoch: 029 | Train loss: 0.9927, Val loss: 1.2618 | Train F1: 0.78, Val F1: 0.45
Epoch: 030 | Train loss: 0.9869, Val loss: 1.2578 | Train F1: 0.80, Val F1: 0.46
Epoch: 031 | Train loss: 0.9815, Val loss: 1.2530 | Train F1: 0.80, Val F1: 0.46
Epoch: 032 | Train loss: 0.9766, Val loss: 1.2460 | Train F1: 0.80, Val F1: 0.46
Epoch: 033 | Train loss: 0.9726, Val loss: 1.2381 | Train F1: 0.86, Val F1: 0.46
Epoch: 034 | Train loss: 0.9696, Val loss: 1.2292 | Train F1: 0.86, Val F1: 0.47
Epoch: 035 | Train loss: 0.9679, Val loss: 1.2211 | Train F1: 0.86, Val F1: 0.54
Epoch: 036 | Train loss: 0.9663, Val loss: 1.2163 | Train F1: 0.86, Val F1: 0.49
Epoch: 037 | Train loss: 0.9645, Val loss: 1.2128 | Train F1: 0.86, Val F1: 0.50
Epoch: 038 | Train loss: 0.9626, Val loss: 1.2104 | Train F1: 0.87, Val F1: 0.48
Epoch: 039 | Train loss: 0.9610, Val loss: 1.2097 | Train F1: 0.87, Val F1: 0.49
Epoch: 040 | Train loss: 0.9598, Val loss: 1.2096 | Train F1: 0.87, Val F1: 0.47
Epoch: 041 | Train loss: 0.9587, Val loss: 1.2102 | Train F1: 0.87, Val F1: 0.46
Epoch: 042 | Train loss: 0.9577, Val loss: 1.2107 | Train F1: 0.87, Val F1: 0.46
Epoch: 043 | Train loss: 0.9567, Val loss: 1.2110 | Train F1: 0.87, Val F1: 0.46
Epoch: 044 | Train loss: 0.9557, Val loss: 1.2113 | Train F1: 0.87, Val F1: 0.46
Epoch: 045 | Train loss: 0.9546, Val loss: 1.2118 | Train F1: 0.88, Val F1: 0.46
Epoch: 046 | Train loss: 0.9534, Val loss: 1.2124 | Train F1: 0.88, Val F1: 0.47
Epoch: 047 | Train loss: 0.9521, Val loss: 1.2133 | Train F1: 0.92, Val F1: 0.47
Epoch: 048 | Train loss: 0.9507, Val loss: 1.2147 | Train F1: 0.92, Val F1: 0.47
Epoch: 049 | Train loss: 0.9491, Val loss: 1.2171 | Train F1: 0.92, Val F1: 0.46
Epoch: 050 | Train loss: 0.9472, Val loss: 1.2209 | Train F1: 0.92, Val F1: 0.42
Epoch: 051 | Train loss: 0.9447, Val loss: 1.2268 | Train F1: 0.92, Val F1: 0.42
Epoch: 052 | Train loss: 0.9420, Val loss: 1.2356 | Train F1: 0.93, Val F1: 0.42
Epoch: 053 | Train loss: 0.9408, Val loss: 1.2451 | Train F1: 0.93, Val F1: 0.41
Epoch: 054 | Train loss: 0.9398, Val loss: 1.2505 | Train F1: 0.93, Val F1: 0.41
Epoch: 055 | Train loss: 0.9380, Val loss: 1.2496 | Train F1: 0.93, Val F1: 0.41
Epoch: 056 | Train loss: 0.9358, Val loss: 1.2452 | Train F1: 0.93, Val F1: 0.42
Epoch: 057 | Train loss: 0.9336, Val loss: 1.2388 | Train F1: 0.95, Val F1: 0.42
Epoch: 058 | Train loss: 0.9319, Val loss: 1.2317 | Train F1: 0.95, Val F1: 0.42
Epoch: 059 | Train loss: 0.9306, Val loss: 1.2244 | Train F1: 0.95, Val F1: 0.43
Epoch: 060 | Train loss: 0.9296, Val loss: 1.2192 | Train F1: 0.95, Val F1: 0.43
Epoch: 061 | Train loss: 0.9285, Val loss: 1.2165 | Train F1: 0.95, Val F1: 0.43
Epoch: 062 | Train loss: 0.9273, Val loss: 1.2154 | Train F1: 0.94, Val F1: 0.49
Epoch: 063 | Train loss: 0.9261, Val loss: 1.2152 | Train F1: 0.98, Val F1: 0.48
Epoch: 064 | Train loss: 0.9250, Val loss: 1.2158 | Train F1: 0.98, Val F1: 0.51
Epoch: 065 | Train loss: 0.9239, Val loss: 1.2169 | Train F1: 0.98, Val F1: 0.51
Epoch: 066 | Train loss: 0.9230, Val loss: 1.2186 | Train F1: 0.98, Val F1: 0.51
Epoch: 067 | Train loss: 0.9222, Val loss: 1.2203 | Train F1: 0.98, Val F1: 0.51
Epoch: 068 | Train loss: 0.9214, Val loss: 1.2214 | Train F1: 0.98, Val F1: 0.51
Epoch: 069 | Train loss: 0.9207, Val loss: 1.2220 | Train F1: 0.98, Val F1: 0.51
Epoch: 070 | Train loss: 0.9200, Val loss: 1.2223 | Train F1: 0.98, Val F1: 0.50
Epoch: 071 | Train loss: 0.9194, Val loss: 1.2223 | Train F1: 0.98, Val F1: 0.50
Epoch: 072 | Train loss: 0.9188, Val loss: 1.2219 | Train F1: 0.98, Val F1: 0.53
Epoch: 073 | Train loss: 0.9183, Val loss: 1.2210 | Train F1: 0.98, Val F1: 0.49
Epoch: 074 | Train loss: 0.9178, Val loss: 1.2200 | Train F1: 0.98, Val F1: 0.49
Epoch: 075 | Train loss: 0.9174, Val loss: 1.2184 | Train F1: 0.98, Val F1: 0.49
Epoch: 076 | Train loss: 0.9169, Val loss: 1.2166 | Train F1: 0.98, Val F1: 0.49
Epoch: 077 | Train loss: 0.9166, Val loss: 1.2148 | Train F1: 0.99, Val F1: 0.50
Epoch: 078 | Train loss: 0.9162, Val loss: 1.2128 | Train F1: 0.99, Val F1: 0.50
Epoch: 079 | Train loss: 0.9159, Val loss: 1.2110 | Train F1: 1.00, Val F1: 0.58
Epoch: 080 | Train loss: 0.9157, Val loss: 1.2093 | Train F1: 1.00, Val F1: 0.58
Epoch: 081 | Train loss: 0.9154, Val loss: 1.2078 | Train F1: 1.00, Val F1: 0.58
Epoch: 082 | Train loss: 0.9152, Val loss: 1.2064 | Train F1: 1.00, Val F1: 0.58
Epoch: 083 | Train loss: 0.9149, Val loss: 1.2052 | Train F1: 1.00, Val F1: 0.58
Epoch: 084 | Train loss: 0.9147, Val loss: 1.2041 | Train F1: 1.00, Val F1: 0.58
Epoch: 085 | Train loss: 0.9145, Val loss: 1.2031 | Train F1: 1.00, Val F1: 0.58
Epoch: 086 | Train loss: 0.9142, Val loss: 1.2023 | Train F1: 1.00, Val F1: 0.58
Epoch: 087 | Train loss: 0.9140, Val loss: 1.2016 | Train F1: 1.00, Val F1: 0.58
Epoch: 088 | Train loss: 0.9138, Val loss: 1.2010 | Train F1: 1.00, Val F1: 0.58
Epoch: 089 | Train loss: 0.9136, Val loss: 1.2005 | Train F1: 1.00, Val F1: 0.59
Epoch: 090 | Train loss: 0.9134, Val loss: 1.2000 | Train F1: 1.00, Val F1: 0.59
Epoch: 091 | Train loss: 0.9131, Val loss: 1.1997 | Train F1: 1.00, Val F1: 0.59
Epoch: 092 | Train loss: 0.9129, Val loss: 1.1994 | Train F1: 1.00, Val F1: 0.59
Epoch: 093 | Train loss: 0.9127, Val loss: 1.1992 | Train F1: 1.00, Val F1: 0.59
Epoch: 094 | Train loss: 0.9125, Val loss: 1.1991 | Train F1: 1.00, Val F1: 0.59
Epoch: 095 | Train loss: 0.9123, Val loss: 1.1990 | Train F1: 1.00, Val F1: 0.59
Epoch: 096 | Train loss: 0.9121, Val loss: 1.1990 | Train F1: 1.00, Val F1: 0.59
Epoch: 097 | Train loss: 0.9119, Val loss: 1.1989 | Train F1: 1.00, Val F1: 0.59
Epoch: 098 | Train loss: 0.9117, Val loss: 1.1989 | Train F1: 1.00, Val F1: 0.59
Epoch: 099 | Train loss: 0.9115, Val loss: 1.1989 | Train F1: 1.00, Val F1: 0.56
Epoch: 100 | Train loss: 0.9114, Val loss: 1.1990 | Train F1: 1.00, Val F1: 0.56
Epoch: 101 | Train loss: 0.9112, Val loss: 1.1991 | Train F1: 1.00, Val F1: 0.56
Epoch: 102 | Train loss: 0.9110, Val loss: 1.1992 | Train F1: 1.00, Val F1: 0.56
Epoch: 103 | Train loss: 0.9109, Val loss: 1.1994 | Train F1: 1.00, Val F1: 0.56
Epoch: 104 | Train loss: 0.9108, Val loss: 1.1996 | Train F1: 1.00, Val F1: 0.56
Epoch: 105 | Train loss: 0.9106, Val loss: 1.1999 | Train F1: 1.00, Val F1: 0.56
Epoch: 106 | Train loss: 0.9105, Val loss: 1.2001 | Train F1: 1.00, Val F1: 0.56
Epoch: 107 | Train loss: 0.9104, Val loss: 1.2004 | Train F1: 1.00, Val F1: 0.56
Epoch: 108 | Train loss: 0.9103, Val loss: 1.2008 | Train F1: 1.00, Val F1: 0.56
Epoch: 109 | Train loss: 0.9102, Val loss: 1.2012 | Train F1: 1.00, Val F1: 0.56
Epoch: 110 | Train loss: 0.9101, Val loss: 1.2015 | Train F1: 1.00, Val F1: 0.56
Epoch: 111 | Train loss: 0.9100, Val loss: 1.2019 | Train F1: 1.00, Val F1: 0.56
Best model:
Train loss: 0.9154, Val loss: 1.2078, Test loss: 1.2180
Train F1: 1.00, Val F1: 0.58, Test F1: 0.50

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=128, log_path='log/graphsage/wisconsin/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5552, Val loss: 1.5720 | Train F1: 0.33, Val F1: 0.25
Epoch: 001 | Train loss: 1.4845, Val loss: 1.5177 | Train F1: 0.33, Val F1: 0.26
Epoch: 002 | Train loss: 1.4125, Val loss: 1.4562 | Train F1: 0.31, Val F1: 0.22
Epoch: 003 | Train loss: 1.3555, Val loss: 1.4059 | Train F1: 0.30, Val F1: 0.25
Epoch: 004 | Train loss: 1.3033, Val loss: 1.3682 | Train F1: 0.34, Val F1: 0.28
Epoch: 005 | Train loss: 1.2692, Val loss: 1.3518 | Train F1: 0.36, Val F1: 0.28
Epoch: 006 | Train loss: 1.2369, Val loss: 1.3324 | Train F1: 0.41, Val F1: 0.28
Epoch: 007 | Train loss: 1.2097, Val loss: 1.3142 | Train F1: 0.45, Val F1: 0.30
Epoch: 008 | Train loss: 1.1891, Val loss: 1.3020 | Train F1: 0.45, Val F1: 0.35
Epoch: 009 | Train loss: 1.1704, Val loss: 1.2945 | Train F1: 0.45, Val F1: 0.39
Epoch: 010 | Train loss: 1.1573, Val loss: 1.2960 | Train F1: 0.46, Val F1: 0.37
Epoch: 011 | Train loss: 1.1465, Val loss: 1.2975 | Train F1: 0.46, Val F1: 0.37
Epoch: 012 | Train loss: 1.1375, Val loss: 1.2929 | Train F1: 0.46, Val F1: 0.36
Epoch: 013 | Train loss: 1.1312, Val loss: 1.2872 | Train F1: 0.47, Val F1: 0.35
Epoch: 014 | Train loss: 1.1195, Val loss: 1.2737 | Train F1: 0.47, Val F1: 0.36
Epoch: 015 | Train loss: 1.1060, Val loss: 1.2638 | Train F1: 0.50, Val F1: 0.36
Epoch: 016 | Train loss: 1.0857, Val loss: 1.2553 | Train F1: 0.53, Val F1: 0.39
Epoch: 017 | Train loss: 1.0698, Val loss: 1.2465 | Train F1: 0.53, Val F1: 0.39
Epoch: 018 | Train loss: 1.0533, Val loss: 1.2366 | Train F1: 0.54, Val F1: 0.42
Epoch: 019 | Train loss: 1.0493, Val loss: 1.2436 | Train F1: 0.57, Val F1: 0.41
Epoch: 020 | Train loss: 1.0497, Val loss: 1.2580 | Train F1: 0.62, Val F1: 0.41
Epoch: 021 | Train loss: 1.0369, Val loss: 1.2545 | Train F1: 0.62, Val F1: 0.41
Epoch: 022 | Train loss: 1.0240, Val loss: 1.2392 | Train F1: 0.63, Val F1: 0.41
Epoch: 023 | Train loss: 1.0158, Val loss: 1.2244 | Train F1: 0.65, Val F1: 0.42
Epoch: 024 | Train loss: 1.0118, Val loss: 1.2152 | Train F1: 0.66, Val F1: 0.42
Epoch: 025 | Train loss: 1.0088, Val loss: 1.2099 | Train F1: 0.66, Val F1: 0.47
Epoch: 026 | Train loss: 1.0067, Val loss: 1.2087 | Train F1: 0.66, Val F1: 0.47
Epoch: 027 | Train loss: 1.0045, Val loss: 1.2094 | Train F1: 0.68, Val F1: 0.44
Epoch: 028 | Train loss: 0.9997, Val loss: 1.2113 | Train F1: 0.68, Val F1: 0.44
Epoch: 029 | Train loss: 0.9961, Val loss: 1.2159 | Train F1: 0.69, Val F1: 0.45
Epoch: 030 | Train loss: 0.9929, Val loss: 1.2214 | Train F1: 0.69, Val F1: 0.44
Epoch: 031 | Train loss: 0.9885, Val loss: 1.2260 | Train F1: 0.69, Val F1: 0.42
Epoch: 032 | Train loss: 0.9832, Val loss: 1.2285 | Train F1: 0.69, Val F1: 0.43
Epoch: 033 | Train loss: 0.9774, Val loss: 1.2296 | Train F1: 0.72, Val F1: 0.42
Epoch: 034 | Train loss: 0.9693, Val loss: 1.2275 | Train F1: 0.82, Val F1: 0.42
Epoch: 035 | Train loss: 0.9635, Val loss: 1.2241 | Train F1: 0.88, Val F1: 0.49
Epoch: 036 | Train loss: 0.9609, Val loss: 1.2234 | Train F1: 0.88, Val F1: 0.46
Epoch: 037 | Train loss: 0.9586, Val loss: 1.2235 | Train F1: 0.88, Val F1: 0.47
Epoch: 038 | Train loss: 0.9536, Val loss: 1.2277 | Train F1: 0.88, Val F1: 0.47
Epoch: 039 | Train loss: 0.9488, Val loss: 1.2374 | Train F1: 0.93, Val F1: 0.45
Epoch: 040 | Train loss: 0.9482, Val loss: 1.2557 | Train F1: 0.93, Val F1: 0.39
Epoch: 041 | Train loss: 0.9499, Val loss: 1.2694 | Train F1: 0.93, Val F1: 0.40
Epoch: 042 | Train loss: 0.9462, Val loss: 1.2591 | Train F1: 0.93, Val F1: 0.40
Epoch: 043 | Train loss: 0.9423, Val loss: 1.2409 | Train F1: 0.93, Val F1: 0.42
Epoch: 044 | Train loss: 0.9406, Val loss: 1.2246 | Train F1: 0.93, Val F1: 0.46
Epoch: 045 | Train loss: 0.9396, Val loss: 1.2136 | Train F1: 0.94, Val F1: 0.47
Epoch: 046 | Train loss: 0.9375, Val loss: 1.2074 | Train F1: 0.95, Val F1: 0.47
Epoch: 047 | Train loss: 0.9346, Val loss: 1.2056 | Train F1: 0.95, Val F1: 0.46
Epoch: 048 | Train loss: 0.9330, Val loss: 1.2069 | Train F1: 0.95, Val F1: 0.53
Epoch: 049 | Train loss: 0.9321, Val loss: 1.2115 | Train F1: 0.95, Val F1: 0.52
Epoch: 050 | Train loss: 0.9309, Val loss: 1.2172 | Train F1: 0.93, Val F1: 0.52
Epoch: 051 | Train loss: 0.9292, Val loss: 1.2237 | Train F1: 0.93, Val F1: 0.53
Epoch: 052 | Train loss: 0.9273, Val loss: 1.2302 | Train F1: 0.94, Val F1: 0.46
Epoch: 053 | Train loss: 0.9253, Val loss: 1.2358 | Train F1: 0.94, Val F1: 0.45
Epoch: 054 | Train loss: 0.9235, Val loss: 1.2403 | Train F1: 0.95, Val F1: 0.44
Epoch: 055 | Train loss: 0.9222, Val loss: 1.2429 | Train F1: 0.99, Val F1: 0.43
Epoch: 056 | Train loss: 0.9210, Val loss: 1.2448 | Train F1: 0.99, Val F1: 0.43
Epoch: 057 | Train loss: 0.9198, Val loss: 1.2460 | Train F1: 0.99, Val F1: 0.43
Epoch: 058 | Train loss: 0.9186, Val loss: 1.2455 | Train F1: 0.99, Val F1: 0.48
Epoch: 059 | Train loss: 0.9176, Val loss: 1.2443 | Train F1: 0.99, Val F1: 0.48
Epoch: 060 | Train loss: 0.9168, Val loss: 1.2412 | Train F1: 0.99, Val F1: 0.51
Epoch: 061 | Train loss: 0.9163, Val loss: 1.2379 | Train F1: 0.99, Val F1: 0.51
Epoch: 062 | Train loss: 0.9159, Val loss: 1.2347 | Train F1: 1.00, Val F1: 0.50
Epoch: 063 | Train loss: 0.9157, Val loss: 1.2314 | Train F1: 1.00, Val F1: 0.51
Epoch: 064 | Train loss: 0.9154, Val loss: 1.2283 | Train F1: 1.00, Val F1: 0.51
Epoch: 065 | Train loss: 0.9151, Val loss: 1.2253 | Train F1: 1.00, Val F1: 0.52
Epoch: 066 | Train loss: 0.9149, Val loss: 1.2224 | Train F1: 1.00, Val F1: 0.52
Epoch: 067 | Train loss: 0.9146, Val loss: 1.2194 | Train F1: 1.00, Val F1: 0.52
Epoch: 068 | Train loss: 0.9143, Val loss: 1.2167 | Train F1: 1.00, Val F1: 0.51
Epoch: 069 | Train loss: 0.9140, Val loss: 1.2140 | Train F1: 1.00, Val F1: 0.51
Epoch: 070 | Train loss: 0.9138, Val loss: 1.2119 | Train F1: 1.00, Val F1: 0.51
Epoch: 071 | Train loss: 0.9135, Val loss: 1.2098 | Train F1: 1.00, Val F1: 0.51
Epoch: 072 | Train loss: 0.9132, Val loss: 1.2078 | Train F1: 1.00, Val F1: 0.52
Epoch: 073 | Train loss: 0.9129, Val loss: 1.2058 | Train F1: 1.00, Val F1: 0.56
Epoch: 074 | Train loss: 0.9126, Val loss: 1.2041 | Train F1: 1.00, Val F1: 0.56
Epoch: 075 | Train loss: 0.9123, Val loss: 1.2026 | Train F1: 1.00, Val F1: 0.60
Epoch: 076 | Train loss: 0.9120, Val loss: 1.2012 | Train F1: 1.00, Val F1: 0.60
Epoch: 077 | Train loss: 0.9117, Val loss: 1.1999 | Train F1: 1.00, Val F1: 0.60
Epoch: 078 | Train loss: 0.9114, Val loss: 1.1988 | Train F1: 1.00, Val F1: 0.62
Epoch: 079 | Train loss: 0.9112, Val loss: 1.1979 | Train F1: 1.00, Val F1: 0.59
Epoch: 080 | Train loss: 0.9109, Val loss: 1.1971 | Train F1: 1.00, Val F1: 0.61
Epoch: 081 | Train loss: 0.9107, Val loss: 1.1964 | Train F1: 1.00, Val F1: 0.61
Epoch: 082 | Train loss: 0.9105, Val loss: 1.1958 | Train F1: 1.00, Val F1: 0.61
Epoch: 083 | Train loss: 0.9103, Val loss: 1.1953 | Train F1: 1.00, Val F1: 0.56
Epoch: 084 | Train loss: 0.9102, Val loss: 1.1949 | Train F1: 1.00, Val F1: 0.56
Epoch: 085 | Train loss: 0.9100, Val loss: 1.1946 | Train F1: 1.00, Val F1: 0.56
Epoch: 086 | Train loss: 0.9099, Val loss: 1.1943 | Train F1: 1.00, Val F1: 0.56
Epoch: 087 | Train loss: 0.9097, Val loss: 1.1940 | Train F1: 1.00, Val F1: 0.56
Epoch: 088 | Train loss: 0.9096, Val loss: 1.1938 | Train F1: 1.00, Val F1: 0.56
Epoch: 089 | Train loss: 0.9095, Val loss: 1.1936 | Train F1: 1.00, Val F1: 0.56
Epoch: 090 | Train loss: 0.9094, Val loss: 1.1934 | Train F1: 1.00, Val F1: 0.56
Epoch: 091 | Train loss: 0.9093, Val loss: 1.1931 | Train F1: 1.00, Val F1: 0.56
Epoch: 092 | Train loss: 0.9092, Val loss: 1.1929 | Train F1: 1.00, Val F1: 0.56
Epoch: 093 | Train loss: 0.9091, Val loss: 1.1927 | Train F1: 1.00, Val F1: 0.56
Epoch: 094 | Train loss: 0.9090, Val loss: 1.1925 | Train F1: 1.00, Val F1: 0.58
Epoch: 095 | Train loss: 0.9089, Val loss: 1.1924 | Train F1: 1.00, Val F1: 0.58
Epoch: 096 | Train loss: 0.9088, Val loss: 1.1923 | Train F1: 1.00, Val F1: 0.58
Epoch: 097 | Train loss: 0.9087, Val loss: 1.1922 | Train F1: 1.00, Val F1: 0.56
Epoch: 098 | Train loss: 0.9086, Val loss: 1.1922 | Train F1: 1.00, Val F1: 0.56
Epoch: 099 | Train loss: 0.9085, Val loss: 1.1923 | Train F1: 1.00, Val F1: 0.56
Epoch: 100 | Train loss: 0.9084, Val loss: 1.1924 | Train F1: 1.00, Val F1: 0.56
Epoch: 101 | Train loss: 0.9083, Val loss: 1.1924 | Train F1: 1.00, Val F1: 0.56
Epoch: 102 | Train loss: 0.9082, Val loss: 1.1924 | Train F1: 1.00, Val F1: 0.56
Epoch: 103 | Train loss: 0.9081, Val loss: 1.1924 | Train F1: 1.00, Val F1: 0.56
Epoch: 104 | Train loss: 0.9080, Val loss: 1.1923 | Train F1: 1.00, Val F1: 0.56
Epoch: 105 | Train loss: 0.9079, Val loss: 1.1922 | Train F1: 1.00, Val F1: 0.56
Epoch: 106 | Train loss: 0.9078, Val loss: 1.1922 | Train F1: 1.00, Val F1: 0.56
Best model:
Train loss: 0.9120, Val loss: 1.2012, Test loss: 1.2510
Train F1: 1.00, Val F1: 0.60, Test F1: 0.47

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=256, log_path='log/graphsage/wisconsin/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.5154, Val loss: 1.5387 | Train F1: 0.38, Val F1: 0.27
Epoch: 001 | Train loss: 1.4031, Val loss: 1.4429 | Train F1: 0.34, Val F1: 0.23
Epoch: 002 | Train loss: 1.3187, Val loss: 1.3725 | Train F1: 0.37, Val F1: 0.29
Epoch: 003 | Train loss: 1.2688, Val loss: 1.3455 | Train F1: 0.40, Val F1: 0.29
Epoch: 004 | Train loss: 1.2220, Val loss: 1.3110 | Train F1: 0.43, Val F1: 0.30
Epoch: 005 | Train loss: 1.1802, Val loss: 1.2967 | Train F1: 0.53, Val F1: 0.39
Epoch: 006 | Train loss: 1.1361, Val loss: 1.2894 | Train F1: 0.55, Val F1: 0.38
Epoch: 007 | Train loss: 1.1158, Val loss: 1.3002 | Train F1: 0.58, Val F1: 0.35
Epoch: 008 | Train loss: 1.0948, Val loss: 1.2884 | Train F1: 0.59, Val F1: 0.41
Epoch: 009 | Train loss: 1.0628, Val loss: 1.2624 | Train F1: 0.63, Val F1: 0.44
Epoch: 010 | Train loss: 1.0365, Val loss: 1.2531 | Train F1: 0.66, Val F1: 0.46
Epoch: 011 | Train loss: 1.0164, Val loss: 1.2569 | Train F1: 0.69, Val F1: 0.47
Epoch: 012 | Train loss: 1.0141, Val loss: 1.2812 | Train F1: 0.68, Val F1: 0.45
Epoch: 013 | Train loss: 0.9956, Val loss: 1.2662 | Train F1: 0.71, Val F1: 0.45
Epoch: 014 | Train loss: 0.9832, Val loss: 1.2401 | Train F1: 0.71, Val F1: 0.46
Epoch: 015 | Train loss: 0.9760, Val loss: 1.2218 | Train F1: 0.81, Val F1: 0.44
Epoch: 016 | Train loss: 0.9727, Val loss: 1.2110 | Train F1: 0.81, Val F1: 0.49
Epoch: 017 | Train loss: 0.9726, Val loss: 1.2067 | Train F1: 0.78, Val F1: 0.48
Epoch: 018 | Train loss: 0.9677, Val loss: 1.2038 | Train F1: 0.80, Val F1: 0.49
Epoch: 019 | Train loss: 0.9625, Val loss: 1.2011 | Train F1: 0.87, Val F1: 0.49
Epoch: 020 | Train loss: 0.9593, Val loss: 1.2024 | Train F1: 0.87, Val F1: 0.49
Epoch: 021 | Train loss: 0.9573, Val loss: 1.2057 | Train F1: 0.87, Val F1: 0.45
Epoch: 022 | Train loss: 0.9549, Val loss: 1.2056 | Train F1: 0.88, Val F1: 0.46
Epoch: 023 | Train loss: 0.9515, Val loss: 1.2031 | Train F1: 0.88, Val F1: 0.44
Epoch: 024 | Train loss: 0.9480, Val loss: 1.2056 | Train F1: 0.88, Val F1: 0.42
Epoch: 025 | Train loss: 0.9439, Val loss: 1.2078 | Train F1: 0.89, Val F1: 0.43
Epoch: 026 | Train loss: 0.9415, Val loss: 1.2140 | Train F1: 0.89, Val F1: 0.43
Epoch: 027 | Train loss: 0.9405, Val loss: 1.2244 | Train F1: 0.89, Val F1: 0.43
Epoch: 028 | Train loss: 0.9394, Val loss: 1.2310 | Train F1: 0.95, Val F1: 0.52
Epoch: 029 | Train loss: 0.9423, Val loss: 1.2348 | Train F1: 0.95, Val F1: 0.55
Epoch: 030 | Train loss: 0.9365, Val loss: 1.2342 | Train F1: 0.95, Val F1: 0.53
Epoch: 031 | Train loss: 0.9355, Val loss: 1.2266 | Train F1: 0.95, Val F1: 0.53
Epoch: 032 | Train loss: 0.9349, Val loss: 1.2205 | Train F1: 0.95, Val F1: 0.54
Epoch: 033 | Train loss: 0.9346, Val loss: 1.2200 | Train F1: 0.95, Val F1: 0.49
Epoch: 034 | Train loss: 0.9343, Val loss: 1.2232 | Train F1: 0.93, Val F1: 0.49
Epoch: 035 | Train loss: 0.9342, Val loss: 1.2276 | Train F1: 0.93, Val F1: 0.48
Epoch: 036 | Train loss: 0.9337, Val loss: 1.2332 | Train F1: 0.93, Val F1: 0.47
Epoch: 037 | Train loss: 0.9327, Val loss: 1.2409 | Train F1: 0.93, Val F1: 0.47
Epoch: 038 | Train loss: 0.9311, Val loss: 1.2514 | Train F1: 0.93, Val F1: 0.47
Epoch: 039 | Train loss: 0.9298, Val loss: 1.2632 | Train F1: 0.92, Val F1: 0.48
Epoch: 040 | Train loss: 0.9278, Val loss: 1.2672 | Train F1: 0.92, Val F1: 0.49
Epoch: 041 | Train loss: 0.9242, Val loss: 1.2663 | Train F1: 0.97, Val F1: 0.49
Epoch: 042 | Train loss: 0.9211, Val loss: 1.2578 | Train F1: 0.97, Val F1: 0.50
Epoch: 043 | Train loss: 0.9199, Val loss: 1.2508 | Train F1: 0.97, Val F1: 0.50
Epoch: 044 | Train loss: 0.9196, Val loss: 1.2429 | Train F1: 0.97, Val F1: 0.46
Epoch: 045 | Train loss: 0.9193, Val loss: 1.2346 | Train F1: 0.97, Val F1: 0.51
Epoch: 046 | Train loss: 0.9185, Val loss: 1.2217 | Train F1: 0.97, Val F1: 0.52
Epoch: 047 | Train loss: 0.9178, Val loss: 1.2141 | Train F1: 0.99, Val F1: 0.50
Epoch: 048 | Train loss: 0.9176, Val loss: 1.2091 | Train F1: 0.99, Val F1: 0.52
Epoch: 049 | Train loss: 0.9174, Val loss: 1.2065 | Train F1: 0.99, Val F1: 0.52
Epoch: 050 | Train loss: 0.9172, Val loss: 1.2062 | Train F1: 0.99, Val F1: 0.52
Epoch: 051 | Train loss: 0.9169, Val loss: 1.2067 | Train F1: 0.99, Val F1: 0.51
Epoch: 052 | Train loss: 0.9167, Val loss: 1.2073 | Train F1: 0.99, Val F1: 0.51
Epoch: 053 | Train loss: 0.9164, Val loss: 1.2074 | Train F1: 0.99, Val F1: 0.51
Epoch: 054 | Train loss: 0.9162, Val loss: 1.2066 | Train F1: 0.99, Val F1: 0.51
Epoch: 055 | Train loss: 0.9159, Val loss: 1.2056 | Train F1: 0.99, Val F1: 0.51
Epoch: 056 | Train loss: 0.9157, Val loss: 1.2046 | Train F1: 0.99, Val F1: 0.53
Epoch: 057 | Train loss: 0.9155, Val loss: 1.2037 | Train F1: 0.99, Val F1: 0.53
Epoch: 058 | Train loss: 0.9153, Val loss: 1.2017 | Train F1: 0.99, Val F1: 0.53
Epoch: 059 | Train loss: 0.9151, Val loss: 1.1998 | Train F1: 0.99, Val F1: 0.55
Epoch: 060 | Train loss: 0.9149, Val loss: 1.1982 | Train F1: 0.99, Val F1: 0.55
Epoch: 061 | Train loss: 0.9148, Val loss: 1.1968 | Train F1: 0.99, Val F1: 0.57
Epoch: 062 | Train loss: 0.9146, Val loss: 1.1958 | Train F1: 0.99, Val F1: 0.57
Epoch: 063 | Train loss: 0.9145, Val loss: 1.1954 | Train F1: 0.99, Val F1: 0.56
Epoch: 064 | Train loss: 0.9143, Val loss: 1.1953 | Train F1: 0.99, Val F1: 0.56
Epoch: 065 | Train loss: 0.9142, Val loss: 1.1956 | Train F1: 0.99, Val F1: 0.55
Epoch: 066 | Train loss: 0.9141, Val loss: 1.1962 | Train F1: 0.99, Val F1: 0.55
Epoch: 067 | Train loss: 0.9139, Val loss: 1.1974 | Train F1: 0.99, Val F1: 0.55
Epoch: 068 | Train loss: 0.9137, Val loss: 1.1991 | Train F1: 0.99, Val F1: 0.55
Epoch: 069 | Train loss: 0.9134, Val loss: 1.2011 | Train F1: 0.99, Val F1: 0.55
Epoch: 070 | Train loss: 0.9134, Val loss: 1.2037 | Train F1: 0.97, Val F1: 0.55
Epoch: 071 | Train loss: 0.9135, Val loss: 1.2062 | Train F1: 0.97, Val F1: 0.55
Epoch: 072 | Train loss: 0.9135, Val loss: 1.2079 | Train F1: 0.97, Val F1: 0.55
Epoch: 073 | Train loss: 0.9135, Val loss: 1.2088 | Train F1: 0.97, Val F1: 0.55
Epoch: 074 | Train loss: 0.9134, Val loss: 1.2085 | Train F1: 0.97, Val F1: 0.55
Epoch: 075 | Train loss: 0.9133, Val loss: 1.2080 | Train F1: 0.97, Val F1: 0.55
Epoch: 076 | Train loss: 0.9132, Val loss: 1.2065 | Train F1: 0.97, Val F1: 0.55
Epoch: 077 | Train loss: 0.9130, Val loss: 1.2049 | Train F1: 0.99, Val F1: 0.55
Epoch: 078 | Train loss: 0.9131, Val loss: 1.2045 | Train F1: 0.99, Val F1: 0.55
Epoch: 079 | Train loss: 0.9131, Val loss: 1.2045 | Train F1: 0.99, Val F1: 0.55
Epoch: 080 | Train loss: 0.9130, Val loss: 1.2053 | Train F1: 0.99, Val F1: 0.55
Epoch: 081 | Train loss: 0.9130, Val loss: 1.2063 | Train F1: 0.99, Val F1: 0.55
Epoch: 082 | Train loss: 0.9129, Val loss: 1.2080 | Train F1: 0.97, Val F1: 0.55
Epoch: 083 | Train loss: 0.9130, Val loss: 1.2094 | Train F1: 0.97, Val F1: 0.55
Epoch: 084 | Train loss: 0.9129, Val loss: 1.2097 | Train F1: 0.97, Val F1: 0.55
Epoch: 085 | Train loss: 0.9129, Val loss: 1.2097 | Train F1: 0.97, Val F1: 0.55
Epoch: 086 | Train loss: 0.9129, Val loss: 1.2093 | Train F1: 0.97, Val F1: 0.55
Epoch: 087 | Train loss: 0.9128, Val loss: 1.2096 | Train F1: 0.97, Val F1: 0.55
Epoch: 088 | Train loss: 0.9128, Val loss: 1.2104 | Train F1: 0.97, Val F1: 0.55
Epoch: 089 | Train loss: 0.9128, Val loss: 1.2106 | Train F1: 0.97, Val F1: 0.55
Epoch: 090 | Train loss: 0.9128, Val loss: 1.2100 | Train F1: 0.99, Val F1: 0.55
Epoch: 091 | Train loss: 0.9128, Val loss: 1.2103 | Train F1: 0.99, Val F1: 0.55
Epoch: 092 | Train loss: 0.9128, Val loss: 1.2103 | Train F1: 0.99, Val F1: 0.55
Epoch: 093 | Train loss: 0.9128, Val loss: 1.2111 | Train F1: 0.99, Val F1: 0.55
Epoch: 094 | Train loss: 0.9128, Val loss: 1.2119 | Train F1: 0.97, Val F1: 0.55
Epoch: 095 | Train loss: 0.9128, Val loss: 1.2119 | Train F1: 0.99, Val F1: 0.55
Epoch: 096 | Train loss: 0.9128, Val loss: 1.2121 | Train F1: 0.99, Val F1: 0.55
Epoch: 097 | Train loss: 0.9128, Val loss: 1.2122 | Train F1: 0.99, Val F1: 0.55
Epoch: 098 | Train loss: 0.9128, Val loss: 1.2129 | Train F1: 0.97, Val F1: 0.55
Epoch: 099 | Train loss: 0.9128, Val loss: 1.2130 | Train F1: 0.97, Val F1: 0.55
Epoch: 100 | Train loss: 0.9128, Val loss: 1.2126 | Train F1: 0.99, Val F1: 0.53
Epoch: 101 | Train loss: 0.9128, Val loss: 1.2129 | Train F1: 0.99, Val F1: 0.53
Epoch: 102 | Train loss: 0.9128, Val loss: 1.2140 | Train F1: 0.97, Val F1: 0.53
Epoch: 103 | Train loss: 0.9129, Val loss: 1.2157 | Train F1: 0.97, Val F1: 0.53
Best model:
Train loss: 0.9135, Val loss: 1.2088, Test loss: 1.2796
Train F1: 0.97, Val F1: 0.55, Test F1: 0.45

>>> train.py: Namespace(dataset='wisconsin', device=1, dropout=0.1, hidden_dim=512, log_path='log/graphsage/wisconsin/hidden_dim', lr=0.001, max_epochs=1000, n_layers=3, path='data/graphsage', split=1.0, weight_decay=1e-05)
Epoch: 000 | Train loss: 1.4233, Val loss: 1.4583 | Train F1: 0.30, Val F1: 0.23
Epoch: 001 | Train loss: 1.2902, Val loss: 1.3448 | Train F1: 0.36, Val F1: 0.29
Epoch: 002 | Train loss: 1.3352, Val loss: 1.4058 | Train F1: 0.29, Val F1: 0.21
Epoch: 003 | Train loss: 1.2132, Val loss: 1.2946 | Train F1: 0.43, Val F1: 0.29
Epoch: 004 | Train loss: 1.1843, Val loss: 1.2841 | Train F1: 0.53, Val F1: 0.40
Epoch: 005 | Train loss: 1.1257, Val loss: 1.2616 | Train F1: 0.56, Val F1: 0.41
Epoch: 006 | Train loss: 1.0641, Val loss: 1.2498 | Train F1: 0.64, Val F1: 0.46
Epoch: 007 | Train loss: 1.0495, Val loss: 1.2830 | Train F1: 0.65, Val F1: 0.46
Epoch: 008 | Train loss: 1.0528, Val loss: 1.3094 | Train F1: 0.65, Val F1: 0.46
Epoch: 009 | Train loss: 0.9995, Val loss: 1.2768 | Train F1: 0.84, Val F1: 0.44
Epoch: 010 | Train loss: 0.9736, Val loss: 1.2272 | Train F1: 0.86, Val F1: 0.48
Epoch: 011 | Train loss: 0.9669, Val loss: 1.2045 | Train F1: 0.86, Val F1: 0.46
Epoch: 012 | Train loss: 0.9612, Val loss: 1.1964 | Train F1: 0.88, Val F1: 0.49
Epoch: 013 | Train loss: 0.9568, Val loss: 1.1926 | Train F1: 0.88, Val F1: 0.47
Epoch: 014 | Train loss: 0.9525, Val loss: 1.1900 | Train F1: 0.92, Val F1: 0.45
Epoch: 015 | Train loss: 0.9496, Val loss: 1.1865 | Train F1: 0.92, Val F1: 0.46
Epoch: 016 | Train loss: 0.9474, Val loss: 1.1844 | Train F1: 0.92, Val F1: 0.48
Epoch: 017 | Train loss: 0.9458, Val loss: 1.1825 | Train F1: 0.92, Val F1: 0.52
Epoch: 018 | Train loss: 0.9448, Val loss: 1.1815 | Train F1: 0.92, Val F1: 0.52
Epoch: 019 | Train loss: 0.9440, Val loss: 1.1820 | Train F1: 0.92, Val F1: 0.52
Epoch: 020 | Train loss: 0.9428, Val loss: 1.1865 | Train F1: 0.90, Val F1: 0.51
Epoch: 021 | Train loss: 0.9427, Val loss: 1.1965 | Train F1: 0.90, Val F1: 0.50
Epoch: 022 | Train loss: 0.9414, Val loss: 1.2020 | Train F1: 0.92, Val F1: 0.50
Epoch: 023 | Train loss: 0.9378, Val loss: 1.2027 | Train F1: 0.95, Val F1: 0.50
Epoch: 024 | Train loss: 0.9356, Val loss: 1.2019 | Train F1: 0.95, Val F1: 0.51
Epoch: 025 | Train loss: 0.9353, Val loss: 1.2030 | Train F1: 0.95, Val F1: 0.55
Epoch: 026 | Train loss: 0.9351, Val loss: 1.2085 | Train F1: 0.95, Val F1: 0.55
Epoch: 027 | Train loss: 0.9351, Val loss: 1.2195 | Train F1: 0.94, Val F1: 0.46
Epoch: 028 | Train loss: 0.9352, Val loss: 1.2308 | Train F1: 0.94, Val F1: 0.46
Epoch: 029 | Train loss: 0.9346, Val loss: 1.2270 | Train F1: 0.94, Val F1: 0.44
Epoch: 030 | Train loss: 0.9342, Val loss: 1.2218 | Train F1: 0.94, Val F1: 0.44
Epoch: 031 | Train loss: 0.9341, Val loss: 1.2214 | Train F1: 0.95, Val F1: 0.45
Epoch: 032 | Train loss: 0.9342, Val loss: 1.2222 | Train F1: 0.95, Val F1: 0.44
Epoch: 033 | Train loss: 0.9341, Val loss: 1.2210 | Train F1: 0.95, Val F1: 0.44
Epoch: 034 | Train loss: 0.9339, Val loss: 1.2179 | Train F1: 0.95, Val F1: 0.44
Epoch: 035 | Train loss: 0.9334, Val loss: 1.2148 | Train F1: 0.95, Val F1: 0.49
Epoch: 036 | Train loss: 0.9326, Val loss: 1.2123 | Train F1: 0.95, Val F1: 0.49
Epoch: 037 | Train loss: 0.9309, Val loss: 1.2101 | Train F1: 0.95, Val F1: 0.49
Epoch: 038 | Train loss: 0.9283, Val loss: 1.2085 | Train F1: 0.96, Val F1: 0.49
Epoch: 039 | Train loss: 0.9265, Val loss: 1.2071 | Train F1: 0.96, Val F1: 0.49
Epoch: 040 | Train loss: 0.9258, Val loss: 1.2063 | Train F1: 0.96, Val F1: 0.51
Epoch: 041 | Train loss: 0.9255, Val loss: 1.2060 | Train F1: 0.96, Val F1: 0.51
Epoch: 042 | Train loss: 0.9252, Val loss: 1.2060 | Train F1: 0.96, Val F1: 0.51
Epoch: 043 | Train loss: 0.9249, Val loss: 1.2064 | Train F1: 0.96, Val F1: 0.51
Epoch: 044 | Train loss: 0.9245, Val loss: 1.2071 | Train F1: 0.96, Val F1: 0.51
Epoch: 045 | Train loss: 0.9241, Val loss: 1.2080 | Train F1: 0.96, Val F1: 0.57
Epoch: 046 | Train loss: 0.9238, Val loss: 1.2093 | Train F1: 0.96, Val F1: 0.57
Epoch: 047 | Train loss: 0.9235, Val loss: 1.2112 | Train F1: 0.96, Val F1: 0.57
Epoch: 048 | Train loss: 0.9232, Val loss: 1.2140 | Train F1: 0.96, Val F1: 0.57
Epoch: 049 | Train loss: 0.9230, Val loss: 1.2180 | Train F1: 0.96, Val F1: 0.57
Epoch: 050 | Train loss: 0.9228, Val loss: 1.2209 | Train F1: 0.96, Val F1: 0.56
Epoch: 051 | Train loss: 0.9226, Val loss: 1.2248 | Train F1: 0.96, Val F1: 0.55
Epoch: 052 | Train loss: 0.9224, Val loss: 1.2285 | Train F1: 0.96, Val F1: 0.51
Epoch: 053 | Train loss: 0.9219, Val loss: 1.2333 | Train F1: 0.95, Val F1: 0.51
Epoch: 054 | Train loss: 0.9222, Val loss: 1.2369 | Train F1: 0.95, Val F1: 0.53
Epoch: 055 | Train loss: 0.9222, Val loss: 1.2385 | Train F1: 0.95, Val F1: 0.53
Epoch: 056 | Train loss: 0.9221, Val loss: 1.2373 | Train F1: 0.95, Val F1: 0.53
Epoch: 057 | Train loss: 0.9219, Val loss: 1.2338 | Train F1: 0.95, Val F1: 0.53
Epoch: 058 | Train loss: 0.9218, Val loss: 1.2290 | Train F1: 0.95, Val F1: 0.53
Epoch: 059 | Train loss: 0.9214, Val loss: 1.2228 | Train F1: 0.95, Val F1: 0.54
Epoch: 060 | Train loss: 0.9217, Val loss: 1.2128 | Train F1: 0.96, Val F1: 0.57
Epoch: 061 | Train loss: 0.9217, Val loss: 1.2076 | Train F1: 0.96, Val F1: 0.57
Epoch: 062 | Train loss: 0.9217, Val loss: 1.2046 | Train F1: 0.96, Val F1: 0.57
Epoch: 063 | Train loss: 0.9217, Val loss: 1.2028 | Train F1: 0.96, Val F1: 0.57
Epoch: 064 | Train loss: 0.9217, Val loss: 1.2015 | Train F1: 0.96, Val F1: 0.57
Epoch: 065 | Train loss: 0.9217, Val loss: 1.2054 | Train F1: 0.96, Val F1: 0.57
Epoch: 066 | Train loss: 0.9217, Val loss: 1.2083 | Train F1: 0.96, Val F1: 0.54
Epoch: 067 | Train loss: 0.9217, Val loss: 1.2137 | Train F1: 0.96, Val F1: 0.50
Epoch: 068 | Train loss: 0.9217, Val loss: 1.2147 | Train F1: 0.96, Val F1: 0.50
Epoch: 069 | Train loss: 0.9217, Val loss: 1.2144 | Train F1: 0.96, Val F1: 0.50
Epoch: 070 | Train loss: 0.9217, Val loss: 1.2139 | Train F1: 0.96, Val F1: 0.50
Epoch: 071 | Train loss: 0.9216, Val loss: 1.2140 | Train F1: 0.96, Val F1: 0.50
Epoch: 072 | Train loss: 0.9216, Val loss: 1.2145 | Train F1: 0.96, Val F1: 0.50
Epoch: 073 | Train loss: 0.9216, Val loss: 1.2154 | Train F1: 0.96, Val F1: 0.49
Epoch: 074 | Train loss: 0.9216, Val loss: 1.2162 | Train F1: 0.96, Val F1: 0.49
Epoch: 075 | Train loss: 0.9216, Val loss: 1.2170 | Train F1: 0.96, Val F1: 0.49
Epoch: 076 | Train loss: 0.9216, Val loss: 1.2176 | Train F1: 0.96, Val F1: 0.49
Epoch: 077 | Train loss: 0.9216, Val loss: 1.2179 | Train F1: 0.96, Val F1: 0.49
Epoch: 078 | Train loss: 0.9216, Val loss: 1.2182 | Train F1: 0.96, Val F1: 0.49
Epoch: 079 | Train loss: 0.9216, Val loss: 1.2183 | Train F1: 0.96, Val F1: 0.49
Epoch: 080 | Train loss: 0.9216, Val loss: 1.2184 | Train F1: 0.96, Val F1: 0.49
Epoch: 081 | Train loss: 0.9216, Val loss: 1.2184 | Train F1: 0.96, Val F1: 0.49
Epoch: 082 | Train loss: 0.9216, Val loss: 1.2183 | Train F1: 0.96, Val F1: 0.49
Epoch: 083 | Train loss: 0.9216, Val loss: 1.2181 | Train F1: 0.96, Val F1: 0.49
Epoch: 084 | Train loss: 0.9216, Val loss: 1.2180 | Train F1: 0.96, Val F1: 0.49
Epoch: 085 | Train loss: 0.9216, Val loss: 1.2177 | Train F1: 0.96, Val F1: 0.49
Epoch: 086 | Train loss: 0.9216, Val loss: 1.2174 | Train F1: 0.96, Val F1: 0.49
Epoch: 087 | Train loss: 0.9216, Val loss: 1.2171 | Train F1: 0.96, Val F1: 0.49
Epoch: 088 | Train loss: 0.9216, Val loss: 1.2167 | Train F1: 0.96, Val F1: 0.49
Epoch: 089 | Train loss: 0.9216, Val loss: 1.2163 | Train F1: 0.96, Val F1: 0.49
Epoch: 090 | Train loss: 0.9216, Val loss: 1.2158 | Train F1: 0.96, Val F1: 0.49
Epoch: 091 | Train loss: 0.9216, Val loss: 1.2153 | Train F1: 0.96, Val F1: 0.49
Epoch: 092 | Train loss: 0.9216, Val loss: 1.2148 | Train F1: 0.96, Val F1: 0.49
Epoch: 093 | Train loss: 0.9216, Val loss: 1.2143 | Train F1: 0.96, Val F1: 0.49
Epoch: 094 | Train loss: 0.9216, Val loss: 1.2138 | Train F1: 0.96, Val F1: 0.49
Epoch: 095 | Train loss: 0.9216, Val loss: 1.2133 | Train F1: 0.96, Val F1: 0.49
Epoch: 096 | Train loss: 0.9216, Val loss: 1.2128 | Train F1: 0.96, Val F1: 0.49
Epoch: 097 | Train loss: 0.9216, Val loss: 1.2122 | Train F1: 0.96, Val F1: 0.49
Epoch: 098 | Train loss: 0.9216, Val loss: 1.2117 | Train F1: 0.96, Val F1: 0.49
Epoch: 099 | Train loss: 0.9216, Val loss: 1.2112 | Train F1: 0.96, Val F1: 0.49
Epoch: 100 | Train loss: 0.9216, Val loss: 1.2106 | Train F1: 0.96, Val F1: 0.49
Epoch: 101 | Train loss: 0.9216, Val loss: 1.2101 | Train F1: 0.96, Val F1: 0.49
Epoch: 102 | Train loss: 0.9216, Val loss: 1.2096 | Train F1: 0.96, Val F1: 0.49
Epoch: 103 | Train loss: 0.9215, Val loss: 1.2090 | Train F1: 0.96, Val F1: 0.49
Epoch: 104 | Train loss: 0.9215, Val loss: 1.2085 | Train F1: 0.96, Val F1: 0.49
Epoch: 105 | Train loss: 0.9215, Val loss: 1.2080 | Train F1: 0.96, Val F1: 0.49
Epoch: 106 | Train loss: 0.9215, Val loss: 1.2074 | Train F1: 0.96, Val F1: 0.49
Epoch: 107 | Train loss: 0.9215, Val loss: 1.2069 | Train F1: 0.96, Val F1: 0.49
Epoch: 108 | Train loss: 0.9215, Val loss: 1.2064 | Train F1: 0.96, Val F1: 0.49
Epoch: 109 | Train loss: 0.9215, Val loss: 1.2059 | Train F1: 0.96, Val F1: 0.49
Epoch: 110 | Train loss: 0.9215, Val loss: 1.2054 | Train F1: 0.96, Val F1: 0.49
Epoch: 111 | Train loss: 0.9215, Val loss: 1.2049 | Train F1: 0.96, Val F1: 0.49
Epoch: 112 | Train loss: 0.9215, Val loss: 1.2045 | Train F1: 0.96, Val F1: 0.49
Epoch: 113 | Train loss: 0.9215, Val loss: 1.2040 | Train F1: 0.96, Val F1: 0.49
Epoch: 114 | Train loss: 0.9215, Val loss: 1.2036 | Train F1: 0.96, Val F1: 0.49
Epoch: 115 | Train loss: 0.9215, Val loss: 1.2032 | Train F1: 0.96, Val F1: 0.49
Epoch: 116 | Train loss: 0.9215, Val loss: 1.2028 | Train F1: 0.96, Val F1: 0.49
Epoch: 117 | Train loss: 0.9215, Val loss: 1.2024 | Train F1: 0.96, Val F1: 0.49
Epoch: 118 | Train loss: 0.9215, Val loss: 1.2020 | Train F1: 0.96, Val F1: 0.49
Epoch: 119 | Train loss: 0.9215, Val loss: 1.2016 | Train F1: 0.96, Val F1: 0.49
Epoch: 120 | Train loss: 0.9215, Val loss: 1.2013 | Train F1: 0.96, Val F1: 0.49
Epoch: 121 | Train loss: 0.9215, Val loss: 1.2009 | Train F1: 0.96, Val F1: 0.49
Epoch: 122 | Train loss: 0.9215, Val loss: 1.2005 | Train F1: 0.96, Val F1: 0.49
Epoch: 123 | Train loss: 0.9215, Val loss: 1.2002 | Train F1: 0.96, Val F1: 0.49
Epoch: 124 | Train loss: 0.9215, Val loss: 1.1999 | Train F1: 0.96, Val F1: 0.49
Epoch: 125 | Train loss: 0.9215, Val loss: 1.1996 | Train F1: 0.96, Val F1: 0.49
Epoch: 126 | Train loss: 0.9215, Val loss: 1.1993 | Train F1: 0.96, Val F1: 0.49
Epoch: 127 | Train loss: 0.9215, Val loss: 1.1989 | Train F1: 0.96, Val F1: 0.49
Epoch: 128 | Train loss: 0.9215, Val loss: 1.1986 | Train F1: 0.96, Val F1: 0.49
Epoch: 129 | Train loss: 0.9215, Val loss: 1.1983 | Train F1: 0.96, Val F1: 0.49
Epoch: 130 | Train loss: 0.9215, Val loss: 1.1981 | Train F1: 0.96, Val F1: 0.49
Epoch: 131 | Train loss: 0.9215, Val loss: 1.1978 | Train F1: 0.96, Val F1: 0.49
Epoch: 132 | Train loss: 0.9215, Val loss: 1.1975 | Train F1: 0.96, Val F1: 0.49
Epoch: 133 | Train loss: 0.9215, Val loss: 1.1972 | Train F1: 0.96, Val F1: 0.54
Epoch: 134 | Train loss: 0.9215, Val loss: 1.1969 | Train F1: 0.96, Val F1: 0.54
Epoch: 135 | Train loss: 0.9215, Val loss: 1.1967 | Train F1: 0.96, Val F1: 0.54
Epoch: 136 | Train loss: 0.9215, Val loss: 1.1964 | Train F1: 0.96, Val F1: 0.54
Epoch: 137 | Train loss: 0.9215, Val loss: 1.1961 | Train F1: 0.96, Val F1: 0.54
Epoch: 138 | Train loss: 0.9215, Val loss: 1.1959 | Train F1: 0.96, Val F1: 0.54
Epoch: 139 | Train loss: 0.9215, Val loss: 1.1956 | Train F1: 0.96, Val F1: 0.54
Epoch: 140 | Train loss: 0.9215, Val loss: 1.1954 | Train F1: 0.96, Val F1: 0.54
Epoch: 141 | Train loss: 0.9215, Val loss: 1.1952 | Train F1: 0.96, Val F1: 0.54
Epoch: 142 | Train loss: 0.9215, Val loss: 1.1949 | Train F1: 0.96, Val F1: 0.54
Epoch: 143 | Train loss: 0.9215, Val loss: 1.1947 | Train F1: 0.96, Val F1: 0.54
Epoch: 144 | Train loss: 0.9215, Val loss: 1.1945 | Train F1: 0.96, Val F1: 0.54
Epoch: 145 | Train loss: 0.9215, Val loss: 1.1942 | Train F1: 0.96, Val F1: 0.54
Epoch: 146 | Train loss: 0.9215, Val loss: 1.1940 | Train F1: 0.96, Val F1: 0.54
Epoch: 147 | Train loss: 0.9215, Val loss: 1.1938 | Train F1: 0.96, Val F1: 0.54
Epoch: 148 | Train loss: 0.9215, Val loss: 1.1935 | Train F1: 0.96, Val F1: 0.54
Epoch: 149 | Train loss: 0.9215, Val loss: 1.1933 | Train F1: 0.96, Val F1: 0.54
Epoch: 150 | Train loss: 0.9215, Val loss: 1.1931 | Train F1: 0.96, Val F1: 0.54
Epoch: 151 | Train loss: 0.9215, Val loss: 1.1929 | Train F1: 0.96, Val F1: 0.54
Epoch: 152 | Train loss: 0.9215, Val loss: 1.1927 | Train F1: 0.96, Val F1: 0.54
Epoch: 153 | Train loss: 0.9215, Val loss: 1.1924 | Train F1: 0.96, Val F1: 0.54
Epoch: 154 | Train loss: 0.9215, Val loss: 1.1922 | Train F1: 0.96, Val F1: 0.54
Epoch: 155 | Train loss: 0.9215, Val loss: 1.1920 | Train F1: 0.96, Val F1: 0.54
Epoch: 156 | Train loss: 0.9215, Val loss: 1.1918 | Train F1: 0.96, Val F1: 0.54
Epoch: 157 | Train loss: 0.9215, Val loss: 1.1916 | Train F1: 0.96, Val F1: 0.54
Epoch: 158 | Train loss: 0.9215, Val loss: 1.1914 | Train F1: 0.96, Val F1: 0.54
Epoch: 159 | Train loss: 0.9215, Val loss: 1.1912 | Train F1: 0.96, Val F1: 0.54
Epoch: 160 | Train loss: 0.9215, Val loss: 1.1910 | Train F1: 0.96, Val F1: 0.54
Epoch: 161 | Train loss: 0.9215, Val loss: 1.1908 | Train F1: 0.96, Val F1: 0.54
Epoch: 162 | Train loss: 0.9215, Val loss: 1.1906 | Train F1: 0.96, Val F1: 0.54
Epoch: 163 | Train loss: 0.9215, Val loss: 1.1904 | Train F1: 0.96, Val F1: 0.54
Best model:
Train loss: 0.9215, Val loss: 1.1972, Test loss: 1.2246
Train F1: 0.96, Val F1: 0.54, Test F1: 0.46

>>> run.py: Namespace(dataset='wisconsin', device=1, experiment='hidden_dim', log_path='log/graphsage/wisconsin/hidden_dim', method='graphsage', path='data/graphsage', plot_path='plots/graphsage/wisconsin/hidden_dim')

##################################################

Method: nagphormer, Experiment: default, Dataset: cora

##################################################

Method: nagphormer, Experiment: default, Dataset: wisconsin

##################################################

Method: graphsage, Experiment: default, Dataset: cora

##################################################

Method: graphsage, Experiment: default, Dataset: wisconsin

##################################################

Method: nagphormer, Experiment: training-data, Dataset: cora

##################################################

Method: nagphormer, Experiment: training-data, Dataset: wisconsin

##################################################

Method: graphsage, Experiment: training-data, Dataset: cora

##################################################

Method: graphsage, Experiment: training-data, Dataset: wisconsin

##################################################

Method: nagphormer, Experiment: hops, Dataset: cora

##################################################

Method: nagphormer, Experiment: hops, Dataset: wisconsin

##################################################

Method: nagphormer, Experiment: pe_dim, Dataset: cora

##################################################

Method: nagphormer, Experiment: pe_dim, Dataset: wisconsin

##################################################

Method: nagphormer, Experiment: n_layers, Dataset: cora

##################################################

Method: nagphormer, Experiment: n_layers, Dataset: wisconsin

##################################################

Method: graphsage, Experiment: n_layers, Dataset: cora

##################################################

Method: graphsage, Experiment: n_layers, Dataset: wisconsin

##################################################

Method: nagphormer, Experiment: hidden_dim, Dataset: cora

##################################################

Method: nagphormer, Experiment: hidden_dim, Dataset: wisconsin

##################################################

Method: graphsage, Experiment: hidden_dim, Dataset: cora

##################################################

Method: graphsage, Experiment: hidden_dim, Dataset: wisconsin
